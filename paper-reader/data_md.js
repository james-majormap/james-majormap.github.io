const PAPER_DATA_MD = [
  {
    "id": "paper_1",
    "title": "SocraticLM : Exploring Socratic Personalized Teaching with Large Language Models",
    "eng_md": "# **SocraticLM : Exploring Socratic Personalized Teaching** **with Large Language Models**\n\n**Jiayu Liu** [1] _[,]_ [2] **Zhenya Huang** [1] _[,]_ [2] _[∗]_ **Tong Xiao** [1] _[,]_ [2] **Jing Sha** [2] **Jinze Wu** [2]\n**Qi Liu** [1] _[,]_ [2] **Shijin Wang** [2] **Enhong Chen** [1] _[,]_ [2] _[∗]_\n1: University of Science and Technology of China\n2: State Key Laboratory of Cognitive Intelligence\n```\n         {jy251198,tongxiao2002}@mail.ustc.edu.cn;\n          {huangzhy,qiliuql,cheneh}@ustc.edu.cn;\n           {jingsha,jzwu4,sjwang3}@ifytek.com\n```\n\n**Abstract**\nLarge language models (LLMs) are considered a crucial technology for advancing intelligent education since they exhibit the potential for an in-depth understanding of teaching scenarios and providing students with personalized guidance.\nNonetheless, current LLM-based application in personalized teaching predominantly follows a “Question-Answering” paradigm, where students are _passively_\nprovided with answers and explanations. In this paper, we propose _SocraticLM_,\nwhich achieves a Socratic “Thought-Provoking” teaching paradigm that fulfills\nthe role of a real classroom teacher in _actively_ engaging students in the thought\nprocess required for genuine problem-solving mastery. To build _SocraticLM_, we\nfirst propose a novel _“Dean-Teacher-Student”_ multi-agent pipeline to construct a\nnew dataset, _SocraTeach_, which contains 35K meticulously crafted Socratic-style\nmulti-round (equivalent to 208K single-round) teaching dialogues grounded in\nfundamental mathematical problems. Our dataset simulates authentic teaching scenarios, interacting with six representative types of simulated students with different\ncognitive states, and strengthening four crucial teaching abilities. _SocraticLM_ is\nthen fine-tuned on _SocraTeach_ with three strategies balancing its teaching and\nreasoning abilities. Moreover, we contribute a comprehensive evaluation system\nencompassing five pedagogical dimensions for assessing the teaching quality of\nLLMs. Extensive experiments verify that _SocraticLM_ achieves significant improvements in the teaching performance, outperforming GPT4 by more than 12%. Our\ndataset and code is available at `[https://github.com/Ljyustc/SocraticLM](https://github.com/Ljyustc/SocraticLM)` .\n\n\n**1** **Introduction**\nLarge language models (LLMs) have achieved impressive results across a variety of tasks including\nnatural language processing, translation, and question-answering [54, 56, 59]. This draws widespread\nattention to the potential of using LLMs to revolutionize intelligent education, especially personalized\nteaching [2, 23, 39], mainly due to their two advantages. On one hand, LLMs have displayed humanlike mastery and proficiency of knowledge in fundamental subjects like math and physics [1, 48].\nTherefore, they can effectively deliver subject-specific instructions to students like a real teacher. On\nthe other hand, LLMs exhibit exceptional understanding and adaptability to users’ (i.e., students’)\ninputs. They are capable of comprehending diverse students’ demands, providing instant feedback,\nand engaging in interactive pedagogy. Based on these two aspects, there has been a lot of research\n_∗_ Corresponding Authors\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n\n\n85693 https://doi.org/10.52202/079017-2721\n**Problem** : Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many\npages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-1-0.png)\n\n**(a) \"Question-Answering\" teaching paradigm**\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-1-1.png)\n\n\n\n\n\n**(b) \"Thought-Provoking\" teaching paradigm**\nFigure 1: Teaching paradigms: “ _Question-Answering_ ” vs “ _Thought-Provoking_ ”.\n\n\ninvestigating to apply a general LLM (e.g., ChatGPT) to personalized teaching [5, 18, 44, 51], or\nbuilding specific teaching LLMs, such as MathGPT [2], EduGPT [3], and EduChat [10].\n\n\nHowever, current LLMs-based personalized teaching methods predominantly adhere to a “ _Question-_\n_Answering_ ” paradigm. As shown in Figure 1(a), they passively offer functionalities such as providing\nanswers to questions and explaining knowledge concepts to students’ queries. In this process, they\noversimplify the teaching into a series of Q&As, directly delivering complete answers based on\nCoT [56], ToT [57], etc., which falls short of truly identifying the issues students may have and\noffering targeted assistance. Consequently, students may struggle to comprehend the problem-solving\nprocess, lack a genuine improvement in their ability, and fail to resolve similar issues in the future.\n\n\nIn this paper, we draw inspiration from the Socratic method of teaching [13, 45] and propose _Socrati-_\n_cLM_, which achieves a novel “ _Thought-Provoking_ ” teaching paradigm as depicted in Figure 1(b).\nThe key of this paradigm is to engage students in a dialogue to active participation in the learning\nprocess, which continually poses open-ended questions (marked red, e.g., “... how to calculate it?”)\nto encourage them to articulate their thoughts, challenge assumptions, and think independently. This\nprocess enables students to learn to solve a problem by themselves, thereby fostering a deeper mastery\nand ability. Compared with LLM-based applications using prompt engineering directly (e.g., GPT4),\nwe aim to systematically study 1) _The pedagogical demands of “Thought-Provoking” teaching_ and\nempower _SocraticLM_ to fulfill these demands. 2) _The teaching abilities of teachers_ and reinforce\nthese abilities in _SocraticLM_ . 3) _The cognitive states of students_ and enable _SocraticLM_ to accurately\nidentify them during the teaching process. Consequently, our _SocraticLM_ can provide higher quality\nguidance that is more tailored and appropriate for each student’s needs, transitioning from a “guardian\nof knowledge” to “choreographer of learning”.\n\n\nTo build _SocraticLM_, we first construct a new dataset, _SocraTeach_, which consists of 35K high-quality,\nfine-grained Socratic-style multi-round teaching dialogues grounded in mathematical problems. In\nconstructing the dataset, we propose a novel _“Dean-Teacher-Student”_ pipeline, implementing three\nLLM agents to simulate the key roles in authentic teaching scenarios: _Dean, Teacher_, and _Student_ . The\n_Dean_ is a director that oversees and refines the _Teacher_ ’s instructions before they are presented to the\n_Student_, ensuring that the whole teaching process adheres to the Socratic style. The _Teacher_ actively\nand gradually guides the _Student_ to solve a problem by generating Socratic instructions, inspired by\nclassic pedagogical theories [13, 45]. The _Student_ responds to the _Teacher_ ’s instructions, where we\nestablish a student cognitive state system that simulates six kinds of students in classroom to cover\nreal and diverse teaching scenarios. Through multiple rounds of _“Teacher-Student”_ interaction under\nthe supervision of _Dean_, a comprehensive Socratic teaching dialogue is formed. One step further, to\nenhance the diversity and robustness of our dataset, we summarize four types of student responses\nfrom real teaching scenarios and perform data augmentation to generate extra 22K single-round\nteaching dialogues, specifically tailored to enhance four corresponding crucial teaching abilities.\n\n\n2https://www.mathgpt.com/\n3https://edugpt.com/\n2\nhttps://doi.org/10.52202/079017-2721 85694\nWe fine-tune ChatGLM3-6b [12] on our _SocraTeach_ dataset to obtain _SocraticLM_ . During this\nprocess, we elaborate three training strategies to improve the pedagogical abilities while ensuring\nthe problem-solving capacity of _SocraticLM_ simultaneously. In addition, we contribute a novel\nevaluation system encompassing five pedagogical dimensions for assessing the teaching quality of\nLLMs, which to the best of our knowledge, is the first exploration in this field. Experimental results\nshow that our dataset can enhance the pedagogical performances of LLMs and the teaching quality of\nour _SocraticLM_ surpasses GPT4 by more than 12%.\n\n\nThe contributions of this paper are:\n\n\n    - We present _SocraticLM_, a language model that achieves Socratic “Thought-Provoking” teaching paradigm. Experimental results show that its Socratic teaching quality exceeds GPT4 by\n12%, while maintaining the good problem-solving ability of the original ChatGLM3-6b.\n\n\n    - We construct a new dataset _SocraTeach_ that contains massive, fine-grained Socratic teaching\ndialogues. To construct _SocraTeach_, we propose a novel _“Dean-Teacher-Student”_ multiagent pipeline, in which we design an innovative supervisory role _Dean_, a cognitive state\nsystem to direct the _Student_ ’s behavior, and an enhancement in four teaching abilities for\nthe _Teacher_ . This pipeline is general and can be transferred to the teaching in other subjects.\n\n\n    - We develop a five-dimensional comprehensive evaluation system to assess the teaching\nquality of LLMs, which to the best of our knowledge, is the first attempt in the field.\n\n\n**2** **Related Work**\n**LLMs-enhanced intelligent education.** Large language models (LLMs) revolutionize three typical\napplications of intelligent education, namely automatic generation of educational resources, instant\nassessment of student learning outcomes, and personalized teaching assistance [26, 29, 42]. For\neducational resources, there is a tendency to use LLMs to generate textbooks, exercises, etc., based on\nteaching goals and needs, providing teachers with richer inspiration [4, 16]. For students’ outcomes,\nLLMs can analyze students’ homework and exams to provide assessments and feedbacks on their\nlearning progress [9]. As for the most concerned personalized teaching in this paper, one line of\nresearch uses general LLMs like ChatGPT to provide students with multi-level assistance [44, 51, 58]\nin multiple disciplines, such as writing [18], programming [5], and medical education [25]. By\nanalyzing students’ learning data and behavioral patterns, these LLMs also have the potential to\ndesign unique learning paths to help students learn more effectively [20]. Another line of research\nis to collect a large amount of teaching instructions to fine-tune large models (e.g., EduChat [10]),\ngiving them targeted teaching capabilities such as problem solving and emotional support.\n\n\n**Personalized teaching dialogue dataset.** Constructing teaching dialogues is the basis for building\nLLM-based personalized teaching systems. In the literature, early attempts relied on crowd-sourcing\n(e.g., CIMA [49]) or rules (e.g., AutoTutor [17]) to create authentic dialogues. Subsequently, researchers adopted human-computer collaborative approaches. For instance, QuizBot [46] leveraged\nsemantic similarity algorithms to analyze real students’ responses and provided adaptive question\nby predefined teaching workflow. However, these methods required substantial manual effort or\nwere constrained by predefined teaching procedures, resulting in limited scalability and difficulty in\ncovering diverse real-world teaching scenarios. Recently, with LLMs demonstrating advantages in\nsynthetic data generation [28], utilizing them to assist in teaching dialogue generation has attracted\nmuch attention. However, existing research [51] suggests that GPTs make for a bad teacher, thus current efforts mainly use LLMs to simulate students with different backgrounds [41], personalities [41]\nand error types [40], followed by human teachers providing explanations. Nevertheless, this process\nstill requires human involvement, resulting in the latest dataset MATHDIAL [40] containing only 3K\nsamples. Besides, these datasets also lack a systematic investigation into Socratic teaching.\n\n\n**3** **The** _**SocraTeach**_ **Dataset**\nPedagogical theories point out that there are two basic demands for Socratic teaching [13, 45]: 1)\nit is fundamentally dialogic, relying on conversations between teachers and students to facilitate\nlearning; 2) it uses probing questions to actively engage students, promoting independent thinking\nand encouraging them to find answers themselves. In building our _SocraTeach_ dataset to meet these\n3\n85695 https://doi.org/10.52202/079017-2721\n\n\n|Problem: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to<br>read half of the remaining pages tomorrow, how many pages should she read?|Col2|\n|---|---|\n|||\n|**Step-by-step Guiding Questions**<br>**Q1**: How many pages did Julie read today?<br>**Q3**: How many pages are left to be read?<br>**Q2**: How many pages did Julie read since yesterday?<br>**Q4**: How many pages should she read tomorrow?|**Step-by-step Guiding Questions**<br>**Q1**: How many pages did Julie read today?<br>**Q3**: How many pages are left to be read?<br>**Q2**: How many pages did Julie read since yesterday?<br>**Q4**: How many pages should she read tomorrow?|\n|**瀂Dean-Teacher-Student瀃Pipeline**<br>**_Teacher_**<br>**_Student_**<br>**_Dean_**<br>_Judge_ &_Revise_<br> !:  Was it 12 pages?<br> !: Nice try, but Julie read twice as many pages today<br>as she did yesterday. If she read 12 pages yesterday,<br>**can you calculate the result of 12 times 2**?<br>The [Teacher]'s response**doesn't meet the teaching**<br>**criteria** because it directly asked the [Student] to<br>calculate the result of 12 times 2 instead of pointing<br>out the error the [Student] made in their response.<br> !: Nice try, but Julie read twice as many pages today<br>as she did yesterday. If she read 12 pages yesterday,<br>**how many pages would she have read today**?<br>_Judge_<br>_Revise_<br>…<br>**Student Cognitive**<br>**State System**<br>**Teaching Ability Enhancement**<br>**Multi-round Teaching Dialogue**<br> ! : Was it 12 pages?<br>**Single-round Teaching Dialogue**<br>How's the weather today?<br>Why count how many<br>pages Julie read today first?<br>Was it 14 pages?<br>Was it 24 pages?<br>**_Irrelevant_**<br>**_Questioning_**<br>**_Incorrect-reply_**<br>**_Correct-reply_**<br>**_Teacher_**<br>Augmentation<br>This question is<br>unrelated to …<br>Good question! This<br>is because …<br>Nice try, but Julie<br>read twice …<br>Correct! Now we<br>move on to …<br> !: How many pages did Julie read today?|**瀂Dean-Teacher-Student瀃Pipeline**<br>**_Teacher_**<br>**_Student_**<br>**_Dean_**<br>_Judge_ &_Revise_<br> !:  Was it 12 pages?<br> !: Nice try, but Julie read twice as many pages today<br>as she did yesterday. If she read 12 pages yesterday,<br>**can you calculate the result of 12 times 2**?<br>The [Teacher]'s response**doesn't meet the teaching**<br>**criteria** because it directly asked the [Student] to<br>calculate the result of 12 times 2 instead of pointing<br>out the error the [Student] made in their response.<br> !: Nice try, but Julie read twice as many pages today<br>as she did yesterday. If she read 12 pages yesterday,<br>**how many pages would she have read today**?<br>_Judge_<br>_Revise_<br>…<br>**Student Cognitive**<br>**State System**<br>**Teaching Ability Enhancement**<br>**Multi-round Teaching Dialogue**<br> ! : Was it 12 pages?<br>**Single-round Teaching Dialogue**<br>How's the weather today?<br>Why count how many<br>pages Julie read today first?<br>Was it 14 pages?<br>Was it 24 pages?<br>**_Irrelevant_**<br>**_Questioning_**<br>**_Incorrect-reply_**<br>**_Correct-reply_**<br>**_Teacher_**<br>Augmentation<br>This question is<br>unrelated to …<br>Good question! This<br>is because …<br>Nice try, but Julie<br>read twice …<br>Correct! Now we<br>move on to …<br> !: How many pages did Julie read today?|\n|**瀂Dean-Teacher-Student瀃Pipeline**<br>**_Teacher_**<br>**_Student_**<br>**_Dean_**<br>_Judge_ &_Revise_<br> !:  Was it 12 pages?<br> !: Nice try, but Julie read twice as many pages today<br>as she did yesterday. If she read 12 pages yesterday,<br>**can you calculate the result of 12 times 2**?<br>The [Teacher]'s response**doesn't meet the teaching**<br>**criteria** because it directly asked the [Student] to<br>calculate the result of 12 times 2 instead of pointing<br>out the error the [Student] made in their response.<br> !: Nice try, but Julie read twice as many pages today<br>as she did yesterday. If she read 12 pages yesterday,<br>**how many pages would she have read today**?<br>_Judge_<br>_Revise_<br>…<br>**Student Cognitive**<br>**State System**<br>**Multi-round Teaching Dialogue**<br> !: How many pages did Julie read today?|**Teaching Ability Enhancement**<br> ! : Was it 12 pages?<br>**Single-round Teaching Dialogue**<br>How's the weather today?<br>Why count how many<br>pages Julie read today first?<br>Was it 14 pages?<br>Was it 24 pages?<br>**_Irrelevant_**<br>**_Questioning_**<br>**_Incorrect-reply_**<br>**_Correct-reply_**<br>**_Teacher_**<br>Augmentation<br>This question is<br>unrelated to …<br>Good question! This<br>is because …<br>Nice try, but Julie<br>read twice …<br>Correct! Now we<br>move on to …|\n\n\n\nFigure 2: Workflow of our _SocraTeach_ dataset construction.\n\n\nrequirements, we face the following challenges. First, for the teacher, there is considerable variability\nin pedagogical methodologies and presentation styles among teachers. It may be difficult for a\nmodel to learn all of them at once, which may result in confusion and errors within the teaching\nlogic. Second, for the student, in real teaching scenarios, students’ cognitive states are intricate\nand heterogeneous [22, 35]. While some students have strong understanding abilities and sufficient\nknowledge, there are also a considerable number of students who cannot understand the problem or\neven lack the essential knowledge. We expect that our dataset should cover all these situations, so\nthat the model can learn to provide different levels of guidance for students with different states.\n\n\nTo solve the above challenges, we construct _SocraTeach_ as follows. First, for each problem that needs\nto be taught, we decompose a list of step-wise guiding questions (Section 3.1). On this basis, we can\ncontrol the simulation of teachers by aligning the instructional approach and explanatory style with\nthese questions. Second, we devise an innovative _“Dean-Teacher-Student”_ pipeline, implementing\nthree LLM agents including _“Dean”_, _“Teacher”_, and _“Student”_ to collect fine-grained multi-round\nteaching dialogues (Section 3.2). Especially, to align with student profiles in authentic scenarios,\nwe build a cognitive state system to simulate six kinds of students in _“Student”_ from the aspects of\ncomprehension, calculation, knowledge mastery, etc. (Section 3.3). Finally, to further enhance the\ndiversity and robustness of _SocraTeach_, we design data augmentation methods to construct additional\nsingle-round teaching dialogues for improving four crucial teaching abilities (Section 3.4).\n\n\n**3.1** **Problem Collection & Step-by-Step Guiding Questions**\nIn this paper, we take the teaching of mathematical problems at the primary school level as an example\nfor exploration, because mathematics is a fundamental and critical subject and such problems involve\nthe examination of students’ basic understanding and reasoning abilities [32, 34]. Our problems are\nsourced from two representative datasets: MAWPS [27] and GSM8K [8], which contain 2 _._ 3K and\n8 _._ 8K problems, respectively.\n\n\nTo ensure that the expression style and teaching approach are consistent in simulating the teacher\nrole, we decompose each problem into a series of step-by-step guiding questions, such as **Q1**       - **Q4** in\nFigure 2 (please refer to Appendix A for details). It should be noted that to ensure the efficiency and\nconciseness of teaching, a numerical calculation and a summary of the solution do not count as a step.\n\n\n**3.2** **The** _**Dean-Teacher-Student**_ **Pipeline**\nTo create our _SocraTeach_ dataset that achieves the Socratic “Thought-Provoking” teaching paradigm,\nwe propose a novel “ _Dean-Teacher-Student_ ” (DTS) pipeline to collect one-to-one, multi-round,\nteacher-student dialogues, which consists of three LLM agents:\n\n\n            - _**Dean**_ _D_ : Research has indicated that GPTs have inadequacies in understanding students and\nlanguage expression required to serve as a teacher [51]. To address this issue, we propose a\n4\nhttps://doi.org/10.52202/079017-2721 85696\n_Dean_ agent to serve as an oversight role, which judges whether the _Teacher_ ’s instructions\nmeet the requirements of Socratic teaching. If it thinks the instructions do not meet the\nrequirements, it has the authority to revise them before they are presented to the _Student_ .\n\n\n    - _**Teacher**_ _T_ : The _Teacher_ agent actively provokes the _Student_ agent to solve problems in a\nSocratic style, serving two primary purposes according to Socrates’ educational theory [13,\n45]. First, it should prompt the _Student_ to think at the appropriate time with Socratic\nquestions, such as guiding the _Student_ to consider the next step after completing a reasoning\nstep. Second, it needs to provide the _Student_ with explanations of the steps and the involved\nknowledge points. To maintain a consistent teaching style for a given problem, the _Teacher_\nis asked to deliver teaching following the step-by-step questions constructed in Section 3.1.\n\n\n    - _**Student**_ _S_ : Representing the learner within the dataset, the _Student_ agent generates replies\nto the _Teacher_ ’s instructions (i.e., questions and explanations). To ensure the authenticity\nand diversity of _Student_, we build a cognitive state system that describes six kinds of real\nstudents in Section 3.3 and set _Student_ to simulate one of them each time it replies.\n\n\nIn DTS pipeline, each teaching dialogue _{_ ( _T_ 1 _, S_ 1) _,_ ( _T_ 2 _, S_ 2) _, ...}_ is formed by a cycle of interaction\nbetween _Teacher_ and _Student_ under the supervision of _Dean_, and each agent is simulated with GPT4.\nTaking Figure 2 as an example, in the first round ( _t_ = 1), the _Teacher_ directly gives the question of\nthe first step (i.e., _T_ 1 = **Q1** ) constructed in Section 3.1 (this process does not need the use of LLMs).\nThen, the _Student_ selects a state profile (e.g., weak knowledge mastery) from six types of cognitive\nportraits in Section 3.3 and generates a corresponding response _S_ 1 based on it (see Appendix B.1\nfor prompt). After that, at _t_ = 2, the _Teacher_ provides instruction _T_ 2 in a Socratic style, in which\nwe design pedagogical demands such as not providing answers but rather following the flow of the\nstep-by-step questions. It should be emphasized that here we set up a different response style for each\nstudent profile through examples (Appendix B.2). After the _Teacher_ generates _T_ 2, the _Dean_ judges\n(e.g., “... doesn’t meet the teaching criteria”) and revises it (e.g., change “can you ... 12 times 2?” to\n“how many ... read today?”), focusing on 1) Whether it conforms to the Socratic style. 2) Whether it\nclearly points out the mistakes made by the _Student_ . 3) Whether its language style resembles that of\na real teacher (Appendix B.3), i.e., _T_ 2 _←_ _D_ ( _T_ 2). The revised response is then sent to the _Student_,\nand the next round of dialogue begins. Ultimately, if the _Teacher_ thinks that the teaching process has\nbeen completed, it will output an “[END]” token as the ending of its output, indicating to terminate\nthe cycle. It is worth noting that although we focus on teaching mathematical problems in this paper,\nour DTS pipeline is general and can be extended to problems in other subjects (e.g., physics).\n\n\n**3.3** **Student Cognitive State System**\nTo ensure that our dataset covers the real and diverse student status throughout the teaching process,\nit is necessary to simulate different student cognitive states within the _Student_ agent. However, a\nsystematic and unified definition of these states has not been established in existing research [15].\nSome previous studies have concentrated on states that are specific to particular subjects such as\nmath and English [3, 11, 19, 50], while others abstractly define general states based on human\ncognitive science, such as concentration, working memory, and logical reasoning [14, 47, 53, 33].\nUnfortunately, these definitions are either unadaptable to the teaching process or difficult to implement\nwith LLMs. To address this issue, we review the Socratic teaching process from the perspective of\nstudents as follows. Initially, a student needs to grasp the meaning of the problem at hand. Then,\nhe/she comprehends the instructions provided by the teachers and utilizes the computational ability\nand acquired knowledge to execute the instructions. Ultimately, this process fosters an interest\nin learning and helps to cultivate effective study results. Based on this idea, we summarize five\ndimensions of cognitive state:\n\n\n(1) _**Problem Understanding**_ : refers to the degree to which students understand the given problem.\n\n\n(2) _**Instruction Understanding**_ : refers to the degree to which students understand and carry out the\nteacher’s instructions. A student in a good state should easily accomplish these instructions.\n\n\n(3) _**Calculation**_ : refers to the ability to derive mathematical expressions and numbers correctly.\n\n\n(4) _**Knowledge Mastery**_ : refers to the extent to which students have mastered knowledge.\n\n\n(5) _**Thirst**_ _**for**_ _**Learning**_ : refers to the students’ desire or inclination to seek and acquire new\ninformation, ask questions, and explore possibilities.\n\n\n5\n85697 https://doi.org/10.52202/079017-2721\nBasically, we can define five types of students who perform poorly on one of the above dimensions.\nMoreover, we add a sixth type of student who excels at all dimensions.\n\n\n**3.4** **Teaching Ability Enhancement**\nThe multi-round dialogues constructed by DTS pipeline (denoted as _DiaM_ ) ensure a model to grasp\nthe fundamental Socratic teaching paradigm. However, in _DiaM_, the _Student_ responds only once to\neach instruction given by the _Teacher_ and tends to choose simpler student portraits, leading to a lack\nof simulation for long-tail student responses (as discussed in Appendix D). In this section, to further\nenhance the diversity and robustness of our dataset, we construct more _Student-Teacher_ single-round\ndialogues _DiaS_ through data augmentation on _DiaM_, improving four important teaching abilities.\n\n\nSpecifically, in real teaching processes, students’ responses can be classified as follows. First, from the\nmacro perspective, the responses can be divided into _“Irrelevant”_ and _“Relevant”_ . _“Relevant”_ refers\nto responses directly related to the problem or instruction, while _“Irrelevant”_ means that the responses\nare unrelated to the instructional content, such as asking “How’s the weather today?” in Figure 2.\nSecond, within the _“Relevant”_ category, it can be further divided into _“Questioning”_ and _“Replying”_,\nwhich refers to students asking questions to the teacher and answering the teacher’s questions,\nrespectively. Third, _“Replying”_ can be further classified as _“Incorrect-reply”_ and _“Correct-reply”_\nbased on whether the students’ responses to the teacher’s question are correct or not. Along this line,\nstudents’ responses include four categories: _“Irrelevant”_, _“Questioning”_, _“Incorrect-reply”_, and\n_“Correct-reply”_ . On this basis, there are four key teaching abilities that need targeted enhancement.\n\n\nFirst, for _“Irrelevant”_ responses, we expect a teacher to recognize them and redirect the conversation\ntowards teaching, such as responding “This question is unrelated to ... Let’s focus on the problem\nfirst ...”. To achieve this, we collect 200 genuine student inquiries from MOOCs that are unrelated to\nteaching and then construct 2 _,_ 000 single-round _Student-Teacher_ dialogues by randomly inserting\nthem into _DiaM_ and asking _Teacher_ to refuse answering (please refer to Appendix C for details).\n\n\n\nSecond, _“Questioning”_ corresponds to the most crucial teaching ability, that is, a teacher should provide students with accurate explanations. Regarding it, we randomly sample 2 _,_ 000 _Teacher-Student_\nconversation ( _Ti, Si_ ) from _DiaM_ and use _Student_ agent to ask three more questions _Si_ [1] _[,][ S]_ _i_ [2] _[,][ S]_ _i_ [3] [for]\n_i_ [1] _[,][ S]_ _i_ [2]\n_i_ [2] _[,][ S]_ _i_ [3]\nconversation ( _Ti, Si_ ) from _DiaM_ and use _Student_ agent to ask three more questions _Si_ _[,][ S]_ _i_ _[,][ S]_ _i_ [for]\n_Ti_ (see Appendix C.1 for prompt). Then, we ask the _Teacher_ agent to provide _Ti_ [1] +1 _[,][ T]_ _i_ [2] +1 _[,][ T]_ _i_ [3] +1 [,]\n_i_ [1] +1 _[,][ T]_ _i_ [2]\n_i_ [2] +1 _[,][ T]_ _i_ [3]\n_Ti_ (see Appendix C.1 for prompt). Then, we ask the _Teacher_ agent to provide _Ti_ +1 _[,][ T]_ _i_ +1 _[,][ T]_ _i_ +1 [,]\nultimately forming 6 _,_ 000 single-round _Student-Teacher {_ ( _S_ _[j][,][ T]_ _[j]_ [)] _[|][j]_ [= 1] _[,]_ [ 2] _[,]_ [ 3] _[}]_ [ dialogues.]\n_i_ _[j][,][ T]_ _i_ _[j]_\n_i_ +1 [)] _[|][j]_ [= 1] _[,]_ [ 2] _[,]_ [ 3] _[}]_ [ dialogues.]\nThird, a teacher should accurately identify students’ _“Incorrect-reply”_ and point out the idea of\ncorrection. To achieve this, we similarly sample 2 _,_ 000 _Teacher-Student_ conversation from _DiaM_ and\nemploy rules and generation techniques to rewrite the _Student_ ’s responses into five wrong answers.\nThen, we explicitly prompt the _Teacher_ to identify the errors and provide a response, obtaining\nanother 10K instances of _Student-Teacher_ dialogues (please refer to Appendix C for details).\n\n\nFinally, to enable teachers to identify different expressions of the same _“Correct-reply”_ for enhancing\nrobustness, we take the same 2 _,_ 000 single-round “ _Teacher-Student_ ” dialogues used for _“Incorrect-_\n_reply”_ and create two correct responses with _Student_ (see Appendix C.2 for prompt). Subsequently,\nwe collect _Teacher_ ’s replies and obtain another 4 _,_ 000 single-round “ _Student-Teacher_ ” dialogues.\n\n\n**3.5** **Dataset Overview**\nIn summary, our _SocraTeach_ consists of 35K multi-round dialogues _DiaM_ constructed by “ _Dean-_\n_Teacher-Student_ ” pipeline in Sections 3.2 and 22K single-round dialogues _DiaS_ through data aug\nmentation in Section 3.4. The average number of rounds in _DiaM_ is 5 _._ 28, resulting in a total of 208K\nsingle-round dialogue examples. More statistics of _SocraTeach_ are summarized in Appendix D.\n\n\nIn comparison to existing teaching dialogue datasets [17, 40, 41, 46, 49], our _SocraTeach_ first\naddresses the deficiency of LLMs inadequately simulating teachers [51] by introducing the role\nof “ _Dean_ ” for supervision and correction. Secondly, to the best of our knowledge, _SocraTeach_ is\nthe first publicly available dataset designed for Socratic teaching, which specifically enhances four\nkey teaching abilities of _Teacher_ . Thirdly, while existing datasets simulate different students by\nsetting their demographic backgrounds (e.g., grade) or specific error types, _SocraTeach_ models six\ncognitive states of _Student_ during the teaching process based on pedagogical experience, which\ncovers a wider range of authentic teaching scenarios and enables LLMs to possess better teaching\ncapabilities. Lastly, _SocraTeach_ is a fully automatically generated large-scale dataset containing 35K\n6\nhttps://doi.org/10.52202/079017-2721 85698\nmulti-round dialogues and 22K single-round dialogues, significantly surpassing the existing datasets\nthat rely on real human students/teachers (e.g., the latest MATHDIAL [40] contains 3K dialogues).\n\n\n**4** **Fine-tune** _**SocraticLM**_\nBased on _SocraTeach_, we can fine-tune a SOTA LLM (e.g., ChatGLM3-6b [12]) by splitting\neach dialogue _{_ ( _T_ 1 _, S_ 1) _,_ ( _T_ 2 _, S_ 2) _, ...}_ into multiple rounds, using the preceding context of each\nround _{_ ( _T_ 1 _, S_ 1) _, ...,_ ( _Ti, Si_ ) _}_ as input and the _Teacher_ ’s response _Ti_ +1 as output. However, it may\nlead to catastrophic forgetting and reduce the problem-solving ability that the model already has\nbecause these dialogues may differ from the data used for pre-training [21, 30, 38]. Specifically,\nwe observe a decrease of 31.2%/9.7% in the accuracy of _SocraticLM_ on the GSM8K/MAWPS\ndataset in Section 6.2. Therefore, to enhance _SocraticLM_ ’s teaching ability without compromising its\nfundamental problem-solving capability, we explore the following three training strategies:\n\n\n**Separate Training.** To maintain problem-solving ability, one direct way is to mix the dialogue and\nproblem-solving data for training. However, we discover that it does not yield satisfactory results as\nshown in Section 6.2. Therefore, we adopt a separate training approach wherein we first fine-tune\n_SocraticLM_ using dialogue data and then fine-tune it on a small amount of problem-solving data\nrandomly sampled from GSM8K and MAWPS. Our experiments revealed that optimal performance\nis achieved when the ratio _α_ of problem-solving data to the dialogue data is approximately 101 [.]\n**Instruction Tuning.** Inspired by [37], we employ different instructions for the dialogue data and\nproblem-solving data, with the templates presented in Appendix E. It is worth noting that, unlike the\nprompt for _Teacher_ in Section 3.2, here our instruction for dialogues does not require the model to\nfollow the step-by-step guiding questions in Section 3.1. This is because providing such information\nin training may lead the model to take shortcuts, that is, to simplify the teaching process into\ninformation extraction from the prompt, without truly mastering the pedagogical ability.\n\n\n**Mixed** **Prompt** **Setting.** Training with mixed prompt settings for the same task is an important\nmethod for improving LLMs’ reasoning abilities [7, 55]. To this end, in addition to the original\nzero-shot problem-solving data of GSM8K and MAWPS, we also construct their one-shot version for\ntraining, which consists of approximately 101 [of the amount of zero-shot data.]\n**5** **Our Socratic Teaching Evaluation System**\nSince there is no standard answer for the teaching process, previous metrics that calculate the similarity\nbetween model-generated responses and annotated responses (e.g., BLEU [43], Rouge [31]) may\nnot fully assess the teaching quality of LLMs. To address this issue, in this paper, we contribute an\nevaluation system encompassing five pedagogical dimensions for Socratic style and teaching abilities,\nwhich to the best of our knowledge, is the first comprehensive exploration in this field.\n\n\n(1) _**Overall Quality (Overall)**_ : This metric is a holistic and subjective evaluation of teaching quality,\nrequiring that the instruction satisfies Socratic style and enhances students’ experience.\n\n\nFor _Overall Quality_, we randomly select 1 _,_ 000 single-round “ _Student-Teacher_ ” conversations from\n_DiaM_ and recruit 10 well-educated annotators to blindly rank the _Teacher_ response pairs provided\nby each model and GPT4 in the same context (please refer to Appendix F for details). The _Overall_\n_Quality_ is estimated by a normalized win rate difference [1] 2 [(1 +] _W inW in_ + _Lost−Lost_ + _T ie_ [)] _[∈]_ [(0] _[,]_ [ 1)][ (GPT4’s]\nown result is 0 _._ 5). To ensure agreement of quality judge among humans, we also randomly construct\n100 _Teacher_ response pairs of _SocraticLM_ and GPT4 and ask all annotators to judge which one is\nbetter. The Kappa score is 0 _._ 70, indicating good agreement among human annotators.\n\n\nFor the four Socratic teaching abilities we elaborated in Section 3.4, we propose metrics (2)-(5). For\neach of them, we randomly select 100 corresponding single-round dialogues from _DiaS_ for testing.\n\n\n(2) _**Incorrect Answer Recognition Accuracy (IARA)**_ : This dimension focuses on whether the teacher\ncan accurately identify students’ _“Incorrect-reply”_ . For example, in Figure 2, if a student provides\nan incorrect answer (e.g., “14”), a competent teacher should be able to recognize and point it out.\nThis process is objective and can be considered as a binary classification task.\n\n\n7\n85699 https://doi.org/10.52202/079017-2721\n\n\n|Overall IARA CARA SER SRR BLEU-4 Rouge-1 Rouge-2 Rouge-l|Col2|Col3|\n|---|---|---|\n|ChatGPT<br>GPT4<br>Vicuna-7b<br>Llama2-7b<br>Llama2-13b<br>Llama3-8b<br>ChatGLM3-6b<br>EduChat-32b|0.29<br>0.42<br>0.93<br>0.62<br>0.19<br>0.50<br>0.76<br>0.91<br>0.65<br>0.55<br>0.15<br>0.16<br>0.77<br>0.16<br>0.39<br>0.27<br>0.15<br>0.86<br>0.32<br>0.13<br>0.25<br>0.23<br>0.87<br>0.30<br>0.08<br>0.33<br>0.75<br>0.77<br>0.39<br>0.52<br>0.11<br>0.18<br>0.87<br>0.46<br>0.07<br>0.37<br>0.48<br>0.77<br>0.40<br>0.03|22.8<br>34.3<br>14.4<br>21.3<br>36.2<br>42.9<br>19.4<br>32.4<br>22.8<br>34.9<br>14.4<br>22.8<br>28.3<br>35.7<br>14.0<br>24.1<br>27.9<br>36.4<br>14.3<br>23.6<br>27.4<br>33.0<br>10.9<br>22.0<br>17.1<br>26.2<br>9.1<br>15.7<br>27.8<br>38.4<br>17.9<br>29.2|\n|**_SocraticLM_** (**ours**)<br>w/o_ DiaS_<br>w/o_ Irrelevant_<br>w/o_ Questioning_<br>w/o_ Incorrect_<br>w/o_ Correct_|**0.62**<br>**0.83**<br>**0.98**<br>**0.74**<br>**0.78**<br>0.54<br>0.27<br>0.89<br>0.67<br>0.34<br>0.57<br>0.79<br>0.87<br>0.69<br>0.43<br>0.58<br>0.74<br>0.92<br>0.53<br>0.83<br>0.51<br>0.33<br>0.93<br>0.68<br>0.65<br>0.60<br>0.70<br>0.58<br>0.70<br>0.76|**48.6**<br>**56.2**<br>**33.7**<br>**47.5**<br>42.6<br>52.2<br>32.3<br>44.4<br>45.7<br>52.3<br>29.8<br>44.2<br>47.8<br>55.0<br>31.9<br>45.9<br>41.8<br>48.2<br>30.4<br>38.9<br>47.4<br>55.1<br>32.8<br>46.6|\n\n\nTable 1: Teaching performances. For all metrics, the higher value denotes the better performance.\nThe best methods are highlighted in bold. The runner-up baselines are represented by underline.\n\n\n(3) _**Correct Answer Recognition Accuracy (CARA)**_ : In contrast to error recognition, this dimension\nfocuses on whether the model can accurately identify students’ _“Correct-reply”_ . Neglecting this\nmetric may mislead the LLMs to consider any answer provided by the student as incorrect.\n\n\n(4) _**Successful Explanation Rate (SER)**_ : This dimension focuses on whether the model can provide\nstudents with satisfactory explanations for their _“Questioning”_ . This metric is subjective, but can be\nconverted to binary classification based on students’ real experience.\n\n\n(5) _**Successful Rejection Rate (SRR)**_ : This metric is designed for the case where a teacher should\nrefuse to answer students’ _“Irrelevant”_ questions and redirect them back to the instructional content.\nBased on whether the model refuses to answer the question, it is also calculated as binary classification.\n\n\nCompared with existing works in evaluating LLMs for education, our evaluation system offers three\nmain advantages. First, it provides a more comprehensive and adequate assessment. While previous\nworks either rely on similarity metrics (e.g., BLEU [43]) or limited-scale manual evaluations (e.g.,\nissuing questionnaires [18]), our system assesses overall teaching quality along with four key teaching\nabilities, which provides a more systematic organization of evaluation. Second, it enables better\ncomparability across LLMs. Limited by the fact that a student can only interact with one LLM at\na time, traditional human evaluations [5, 24] are difficult to compare the effectiveness of multiple\nmodels. In contrast, our system uses the same teaching dialogues shared across different models as\ntest samples, allowing for a fair comparison of different LLMs simultaneously. Third, our system is\nmore extensive and reliable benefiting from our larger _SocraTeach_ dataset, while recent studies rely\non smaller datasets, such as the latest one [40] with only around 600 testing dialogues.\n\n\n**6** **Experiments**\nIn this section, we verify the effectiveness of our _SocraticLM_ by taking ChatGPT, GPT4, Vicuna7b [6], Llama2-7b, Llama2-13b, Llama3-8b [52], ChatGLM3-6b [12], and EduChat-32b [10] as\nbaselines. The implementation details are described in Appendix G. Especially, for fair comparison,\nfor the problems taught in the testing dialogues, we omit all of their dialogues in training. In addition\nto our proposed evaluation system in Section 5, we also invite human annotators to give a real teacher\nresponse for each testing dialogue, taking it as a standard to calculate the BLEU and Rouge.\n\n\n**6.1** **Main Results**\nTable 1 summarizes the results for all models. First, our _SocraticLM_, which contains 6 billion\nparameters, demonstrates a significant improvement in all Socratic teaching abilities. Notably, it\noutperforms GPT4 by 12% on _Overall_, 6% on _IARA_, 7% on _CARA_, 9% on _SER_, and 23% on _SRR_, as\nwell as over 12% in mirroring the responses of human teachers as measured by BLEU and Rouge. In\nAppendix H, we present and analyze examples of their outputs. Second, our _SocraticLM_ demonstrates\na significant improvement in _SER_ . This indicates that the judgement and correction of our proposed\n_Dean_ agent can critically boost the explanatory capabilities of large models when they function\n8\nhttps://doi.org/10.52202/079017-2721 85700\n���\n���\n���\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-8-3.png)\n\n\n\n���\n���\n����\n����\n����\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-8-2.png)\n\n\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-8-0.png)\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-8-1.png)\n\n\n\n\n\n\n\n�$���!� ���$�������$�\"#� �\"\n���\n���\nnumbers of step-by-step guiding questions, which can reflect the difficulty of the problems. It is\nevident that our _SocraticLM_ consistently outperforms GPT4 across all difficulty levels.\n\n\n**6.2** **Ablation Study**\n**Importance of Teaching Ability Enhancement.** We explore the importance of single-round teaching\ndialogues _DiaS_ built for the four key teaching abilities in Section 3.4. From Table 1, we first observe a\nsignificant decline (e.g., _Overall Quality_ by 8%) when these dialogues are removed (i.e., “w/o _DiaS_ ”).\nThis illustrates the necessity of our teaching ability enhancement and confirms that our proposed four\nteaching abilities are effective in meeting the real demands of Socratic teaching. Second, the _IARA_\nand _SRR_ metrics decrease the most, indicating the greatest disparity between the current LLM-based\nteaching and human teaching may lie in the response to students’ incorrect answers and irrelevant\nquestions. Third, each time a type of single-round dialogue data is eliminated, all teaching abilities\nwill show a decline, which indicates that there is a coupling effect among different teaching abilities.\nEspecially, the _CARA_ metric in the absence of dialogues for students’ _“Correct-reply”_ (i.e., “w/o\n_Correct_ ”) is even lower than when all single-round data is removed (“w/o _DiaS_ ”). We hold the reason\nmay be that in this case, _SocraticLM_ was still fine-tuned on dialogues corresponding to students’\n_“Incorrect-reply”_ . This causes the model to develop a stronger tendency to perceive a student’s reply\nas incorrect. This phenomenon further indicates that it is necessary to balance different types of\nsingle-round dialogues to avoid overfitting on specific instructional patterns.\n\n\n\n**Importance of Ability-balancing Strategies.**\nHere we discard the problem-solving data\nSection 4 in training to investigate their influence. From Table 2, fine-tuning without\n6b. This could be attributed to the notable dif\n|Overall ACC G ACC M|Col2|\n|---|---|\n|ChatGLM3-6b|0.11<br>0.624<br>0.798|\n|_SocraticLM_<br>w/o_ Problem_<br>w/o_ Separate_<br>w/o_ Instruction_<br>w/o_ Mixed-Prompt_|0.62<br>0.606<br>0.814<br>0.58<br>0.312<br>0.701<br>0.54<br>0.159<br>0.646<br>0.02<br>0.320<br>0.625<br>0.56<br>0.605<br>0.804|\n\n\nTable 2: Performance without problem-solving\nferences between teaching dialogues and data\ndata and three ability-balancing training strategies\nused for LLM pre-training, causing a dramatic\ndisturbance in the parameters. Besides, all three in Section 4. _ACCG_, _ACCM_ represent the accu\nracy on GSM8K and MAWPS, respectively.\n\ntraining strategies are effective. Among them,\n_Separate Training_ / _Instruction Tuning_ has the greatest influence on problem-solving/Socratic teaching\nrespectively. _Mixed Prompt Setting_ might have already been employed in the LLM pre-training, hence\nexhibiting less noticeable improvements. Moreover, it is worth noting that _SocraticLM_ achieves\nhigher accuracy on MAWPS than ChatGLM3-6b. We speculate the reason is that, through finetuning on our _SocraTeach_ dataset, _SocraticLM_ indeed learns to address multiple student questions\nabout various aspects of a single problem (e.g., asking about each reasoning step and the knowledge\ninvolved). This process allows _SocraticLM_ to develop a deeper understanding of the problem-solving\nprocess, which in turn can improve its problem-solving accuracy.\n\n\n\n\n\nTable 2: Performance without problem-solving\ndata and three ability-balancing training strategies\nin Section 4. _ACCG_, _ACCM_ represent the accuracy on GSM8K and MAWPS, respectively.\n\n\n\n**6.3** **Influence of Data Scale**\nData scale is crucial for both the efficiency and effectiveness of training large language models.\nIn order to investigate this issue, in this section, we vary the amount of multi-round dialogues in\n_SocraTeach_ dataset and the ratio _α_ between multi-round dialogues and problem-solving data.\n\n\n9\n85701 https://doi.org/10.52202/079017-2721\n_icLM_ . The results in Figure 4 indicate that (i) Our data is not\n����\nonly effective but also impactful at different scales, which can\nsignificantly enhance the teaching capability of LLMs. (ii) ����\nAs the volume of data increases, we observe a corresponding\n\n|\u0012#\u0018 \u0015<br>\u0010 \u0013<br>\u0013|Col2|Col3|Col4|\n|---|---|---|---|\n|<br>\u0014\u000f\u0013<br>\u0014\u0013\u0013||||\n|||||\n|||||\n|||<br>||\n\nincrease in the teaching ability. This correlation highlights the ����������\"�!����\"���������\"�\nimportance of data quantity in model performance. Specifi\nFigure 4: Effects of dialogue scale.\n\ncally, it is noteworthy that a minimum of 75% ( _≈_ 26K) dialogues is required for surpassing the _Overall Quality_ of GPT4. (iii) As the volume of data surpasses\nthe 35K threshold, it tends to approach a saturation point where further increases in data volume yield\nsmaller incremental benefits to the model’s capability. Specifically, at the 125% data scale, the _IARA_\nmetric shows a decline, indicating that the root cause of this saturation is a decrease in the model’s\nability to identify incorrect answers (the decline in _Overall Quality_ is a subsequent result). This may\nbe because, with the increase in multi-round dialogue data, the proportion of single-round dialogue\ndata for _“Incorrect reply”_ decreases. When multi-round data scale exceeds 125%, this proportion\nmay fall below a certain threshold, which results in diminishing effectiveness.\n\n\n\n����\n����\n����\n����\n����\n����������\"�!����\"���������\"�\nFigure 4: Effects of dialogue scale.\n\n\n\n**Scale of Problem-solving Data.** Figure 5 shows the performance changes of _SocraticLM_ as we adjust the ratio _α_ between\nproblem-solving data and dialogue data. The trend indicates\nthat having too few or too much problem-solving data does not\nlead to satisfactory problem-solving ability. Instead, a balance\nneeds to be struck with the teaching dialogue data. In fact,\nan excessive introduction of problem-solving data may even\nresult in 1.9% decrease in accuracy on GSM8K. This could be\nattributed to that the parameters corresponding to _SocraticLM_ ’\nproblem-solving ability might undergo disturbances after initial fine-tuning using the teaching dialogues. When retraining\nwith problem-solving data, it requires to re-strike a delicate\nbalance between underfitting and overfitting of this ability.\n\n\n**7** **Conclusion**\n����\n����\n����\n����\n����\n����\n����\n\n\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n||||\u0010 \u0015\u001d\u0013\u0019<br>~~\n\u000e\u000e~~<br>|\n||||<br>~~G~~<br>\n\u000e\u000e<br>M|\n|||||\n|||||\n\n\n���������������������� ��������\nFigure 5: Effects of problem-solving\ndata scale.\n\n\n\nIn this paper, we introduced _SocraticLM_, a LLM designed to facilitate Socratic “Thought-Provoking”\npersonalized teaching. To build _SocraticLM_, we proposed a “ _Dean-Teacher-Student_ ” pipeline to\nconstruct _SocraTeach_ dataset, which simulated six student cognitive states and strengthened four\ncrucial teaching abilities. Besides, we developed a comprehensive teaching ability evaluation system\nfor LLMs. Experiments demonstrated that _SocraticLM_ significantly outperforms current LLMs such\nas GPT4 and validated the necessity of each component within the _SocraTeach_ dataset. We discuss\nmore cases, the broader impacts, limitations, and future work in Appendix H, I, and J.\n\n\n**Acknowledgments and Disclosure of Funding**\nThis research was partially supported by grants from the National Key Research and Development Program of China (No.2021YFF0901005), National Natural Science Foundation of China (No.62477044,\n62337001), and the Key Technologies R&D Program of Anhui Province (No.202423k09020039).\n\n\n**References**\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. _arXiv preprint arXiv:2303.08774_, 2023.\n\n\n[2] Tufan Adıgüzel, Mehmet Haldun Kaya, and Fatih Kür¸sat Cansu. Revolutionizing education with\nai: Exploring the transformative potential of chatgpt. _Contemporary Educational Technology_,\n2023.\n\n\n10\nhttps://doi.org/10.52202/079017-2721 85702\n[3] Geoffrey D Borman, Gina M Hewes, Laura T Overman, and Shelly Brown. Comprehensive\nschool reform and achievement: A meta-analysis. _Review of educational research_, 73(2):125–\n230, 2003.\n\n\n[4] Andrew Caines, Luca Benedetto, Shiva Taslimipoor, Christopher Davis, Yuan Gao, Oeistein\nAndersen, Zheng Yuan, Mark Elliott, Russell Moore, Christopher Bryant, et al. On the application of large language models for language teaching and assessment technology. _arXiv preprint_\n_arXiv:2307.08393_, 2023.\n\n\n[5] Eason Chen, Ray Huang, Han-Shin Chen, Yuen-Hsien Tseng, and Liang-Yi Li. Gptutor: a\nchatgpt-powered programming tool for code explanation. In _International_ _Conference_ _on_\n_Artificial Intelligence in Education_, pages 321–327. Springer, 2023.\n\n\n[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality. _See https://vicuna. lmsys. org (accessed 14 April_\n_2023)_, 2(3):6, 2023.\n\n\n[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. _arXiv preprint arXiv:2210.11416_, 2022.\n\n\n[8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.\n\n\n[9] Wei Dai, Jionghao Lin, Hua Jin, Tongguang Li, Yi-Shan Tsai, Dragan Gaševi´c, and Guanliang\nChen. Can large language models provide feedback to students? a case study on chatgpt.\nIn _2023 IEEE International Conference on Advanced Learning Technologies (ICALT)_, pages\n323–325. IEEE, 2023.\n\n\n[10] Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tie,\nYougen Zhou, Yilei Wang, et al. Educhat: A large-scale language model-based chatbot system\nfor intelligent education. _arXiv preprint arXiv:2308.02773_, 2023.\n\n\n[11] Ian J Deary, Steve Strand, Pauline Smith, and Cres Fernandes. Intelligence and educational\nachievement. _Intelligence_, 35(1):13–21, 2007.\n\n\n[12] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\n\nGlm: General language model pretraining with autoregressive blank infilling. In _Proceedings of_\n_the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:_ _Long_\n_Papers)_, pages 320–335, 2022.\n\n\n[13] Linda Elder and Richard Paul. The role of socratic questioning in thinking, teaching, and\nlearning. _The Clearing House_, 71(5):297–301, 1998.\n\n\n[14] Amy S Finn, Matthew A Kraft, Martin R West, Julia A Leonard, Crystal E Bish, Rebecca E\nMartin, Margaret A Sheridan, Christopher FO Gabrieli, and John DE Gabrieli. Cognitive skills,\nstudent achievement tests, and schools. _Psychological science_, 25(3):736–744, 2014.\n\n\n[15] Maren Formazin, Ulrich Schroeders, Olaf Köller, Oliver Wilhelm, and Hans Westmeyer.\n\nStudierendenauswahl im fach psychologie. _Psychologische Rundschau_, 2011.\n\n\n[16] Wensheng Gan, Zhenlian Qi, Jiayang Wu, and Jerry Chun-Wei Lin. Large language models\nin education: Vision and opportunities. In _2023 IEEE International Conference on Big Data_\n_(BigData)_, pages 4776–4785. IEEE, 2023.\n\n\n[17] Arthur C Graesser, Shulan Lu, George Tanner Jackson, Heather Hite Mitchell, Mathew Ventura,\nAndrew Olney, and Max M Louwerse. Autotutor: A tutor with dialogue in natural language.\n_Behavior Research Methods, Instruments, & Computers_, 36:180–192, 2004.\n\n\n[18] Jieun Han, Haneul Yoo, Yoonsu Kim, Junho Myung, Minsun Kim, Hyunseung Lim, Juho Kim,\nTak Yeon Lee, Hwajung Hong, So-Yeon Ahn, et al. Recipe: How to integrate chatgpt into efl\nwriting education. In _Proceedings of the Tenth ACM Conference on Learning@ Scale_, pages\n416–420, 2023.\n\n\n11\n85703 https://doi.org/10.52202/079017-2721\n[19] Eric A Hanushek and Steven G Rivkin. Generalizations about using value-added measures of\nteacher quality. _American economic review_, 100(2):267–271, 2010.\n\n\n[20] Christothea Herodotou, Bart Rienties, Avinash Boroowa, Zdenek Zdrahal, and Martin Hlosta.\n\nA large-scale implementation of predictive learning analytics in higher education: The teachers’\nrole and perspective. _Educational Technology Research and Development_, 67:1273–1306, 2019.\n\n\n[21] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\nChen, et al. Lora: Low-rank adaptation of large language models. In _International Conference_\n_on Learning Representations_, 2021.\n\n\n[22] Liya Hu, Zhiang Dong, Jingyuan Chen, Guifeng Wang, Zhihua Wang, Zhou Zhao, and Fei\nWu. Ptadisc: a cross-course dataset supporting personalized learning in cold-start scenarios.\n_Advances in Neural Information Processing Systems_, 36:44976–44996, 2023.\n\n\n[23] Zhenya Huang, Qi Liu, Yuying Chen, Le Wu, Keli Xiao, Enhong Chen, Haiping Ma, and\nGuoping Hu. Learning or forgetting? a dynamic approach for tracking the knowledge proficiency\nof students. _ACM Transactions on Information Systems (TOIS)_, 38(2):1–33, 2020.\n\n\n[24] Jennifer Jin and Mira Kim. Gpt-empowered personalized elearning system for programming\nlanguages. _Applied Sciences_, 13(23):12773, 2023.\n\n\n[25] Tanisha Jowsey, Jessica Stokes-Parish, Rachelle Singleton, and Michael Todorovic. Medical\neducation empowered by generative artificial intelligence large language models. _Trends in_\n_Molecular Medicine_, 2023.\n\n\n[26] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva,\nFrank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt\nfor good? on opportunities and challenges of large language models for education. _Learning_\n_and individual differences_, 103:102274, 2023.\n\n\n[27] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.\n\nMawps: A math word problem repository. In _Proceedings_ _of_ _the_ _2016_ _conference_ _of_ _the_\n_north_ _american_ _chapter_ _of_ _the_ _association_ _for_ _computational_ _linguistics:_ _human_ _language_\n_technologies_, pages 1152–1157, 2016.\n\n\n[28] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel:\n\nCommunicative agents for\" mind\" exploration of large language model society. _Advances in_\n_Neural Information Processing Systems_, 36, 2024.\n\n\n[29] Qingyao Li, Lingyue Fu, Weiming Zhang, Xianyu Chen, Jingwei Yu, Wei Xia, Weinan Zhang,\nRuiming Tang, and Yong Yu. Adapting large language models for education: Foundational\ncapabilities, potentials, and challenges. _arXiv preprint arXiv:2401.08664_, 2023.\n\n\n[30] Zhizhong Li and Derek Hoiem. Learning without forgetting. _IEEE transactions on pattern_\n_analysis and machine intelligence_, 40(12):2935–2947, 2017.\n\n\n[31] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization_\n_branches out_, pages 74–81, 2004.\n\n\n[32] Xin Lin, Zhenya Huang, Hongke Zhao, Enhong Chen, Qi Liu, Defu Lian, Xin Li, and Hao Wang.\n\nLearning relation-enhanced hierarchical solver for math word problems. _IEEE Transactions on_\n_Neural Networks and Learning Systems_, 2023.\n\n\n[33] Jia-Yu Liu, Fei Wang, Hai-Ping Ma, Zhen-Ya Huang, Qi Liu, En-Hong Chen, and Yu Su. A\nprobabilistic framework for temporal cognitive diagnosis in online learning systems. _Journal of_\n_Computer Science and Technology_, 38(6):1203–1222, 2023.\n\n\n[34] Jiayu Liu, Zhenya Huang, Zhiyuan Ma, Qi Liu, Enhong Chen, Tianhuang Su, and Haifeng\nLiu. Guiding mathematical reasoning via mastering commonsense formula knowledge. In\n_Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_,\npages 1477–1488, 2023.\n\n\n12\nhttps://doi.org/10.52202/079017-2721 85704\n[35] Qi Liu, Zhenya Huang, Yu Yin, Enhong Chen, Hui Xiong, Yu Su, and Guoping Hu. Ekt:\n\nExercise-aware knowledge tracing for student performance prediction. _IEEE Transactions on_\n_Knowledge and Data Engineering_, 33(1):100–115, 2019.\n\n\n[36] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P\ntuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In _Proceedings_\n_of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2:_ _Short_\n_Papers)_ . Association for Computational Linguistics, 2022.\n\n\n[37] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,\nQuoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods\nfor effective instruction tuning. In _International_ _Conference_ _on_ _Machine_ _Learning_, pages\n22631–22648. PMLR, 2023.\n\n\n[38] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of\ncatastrophic forgetting in large language models during continual fine-tuning. _arXiv preprint_\n_arXiv:2308.08747_, 2023.\n\n\n[39] Haiping Ma, Changqian Wang, Hengshu Zhu, Shangshang Yang, Xiaoming Zhang, and Xingyi\nZhang. Enhancing cognitive diagnosis using un-interacted exercises: A collaboration-aware\nmixed sampling approach. In _Proceedings of the AAAI Conference on Artificial Intelligence_,\nvolume 38, pages 8877–8885, 2024.\n\n\n[40] Jakub Macina, Nico Daheim, Sankalan Chowdhury, Tanmay Sinha, Manu Kapur, Iryna\nGurevych, and Mrinmaya Sachan. Mathdial: A dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems. In _Findings of the Association for_\n_Computational Linguistics:_ _EMNLP 2023_, pages 5602–5621, 2023.\n\n\n[41] Julia M Markel, Steven G Opferman, James A Landay, and Chris Piech. Gpteach: Interactive\nta training with gpt-based students. In _Proceedings of the tenth acm conference on learning@_\n_scale_, pages 226–236, 2023.\n\n\n[42] Steven Moore, Richard Tong, Anjali Singh, Zitao Liu, Xiangen Hu, Yu Lu, Joleen Liang,\nChen Cao, Hassan Khosravi, Paul Denny, et al. Empowering education with llms-the next-gen\ninterface and content generation. In _International_ _Conference_ _on_ _Artificial_ _Intelligence_ _in_\n_Education_, pages 32–37. Springer, 2023.\n\n\n[43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association_\n_for Computational Linguistics_, pages 311–318, 2002.\n\n\n[44] Minju Park, Sojung Kim, Seunghyun Lee, Soonwoo Kwon, and Kyuseok Kim. Empowering\npersonalized learning through a conversation-based tutoring system with student modeling.\n_arXiv preprint arXiv:2403.14071_, 2024.\n\n\n[45] Richard Paul and Linda Elder. Critical thinking: The art of socratic questioning. _Journal of_\n_developmental education_, 31(1):36, 2007.\n\n\n[46] Sherry Ruan, Liwei Jiang, Justin Xu, Bryce Joe-Kun Tham, Zhengneng Qiu, Yeshuang Zhu,\nElizabeth L Murnane, Emma Brunskill, and James A Landay. Quizbot: A dialogue-based\nadaptive learning system for factual knowledge. In _Proceedings of the 2019 CHI conference on_\n_human factors in computing systems_, pages 1–13, 2019.\n\n\n[47] Yueqi Shi and Shaowei Qu. Analysis of the effect of cognitive ability on academic achievement:\n\nModerating role of self-monitoring. _Frontiers in Psychology_, 13:996504, 2022.\n\n\n[48] Aarohi Srivastava, Denis Kleyjo, and Ziyi Wu. Beyond the imitation game: Quantifying and\nextrapolatingthe capabilities of language models. _Transactions on Machine Learning Research_,\n(5), 2023.\n\n\n[49] Katherine Stasaski, Kimberly Kao, and Marti A Hearst. Cima: A large open access dialogue\ndataset for tutoring. In _Proceedings of the Fifteenth Workshop on Innovative Use of NLP for_\n_Building Educational Applications_, pages 52–64, 2020.\n\n\n13\n85705 https://doi.org/10.52202/079017-2721\n[50] Jianwen Sun, Rui Zou, Ruxia Liang, Lu Gao, Sannyuya Liu, Qing Li, Kai Zhang, and Lulu\nJiang. Ensemble knowledge tracing: Modeling interactions in learning process. _Expert Systems_\n_with Applications_, 207:117680, 2022.\n\n\n[51] Anaïs Tack and Chris Piech. The ai teacher test: Measuring the pedagogical ability of blender\nand gpt-3 in educational dialogues. _International Educational Data Mining Society_, 2022.\n\n\n[52] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n\n\n[53] Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yu Yin, Shijin Wang, and Yu Su. Neuralcd:\n\na general framework for cognitive diagnosis. _IEEE_ _Transactions_ _on_ _Knowledge_ _and_ _Data_\n_Engineering_, 35(8):8312–8327, 2022.\n\n\n[54] Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng\nTu. Document-level machine translation with large language models. In _The 2023 Conference_\n_on Empirical Methods in Natural Language Processing_, 2023.\n\n\n[55] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\n_International Conference on Learning Representations_, 2021.\n\n\n[56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\n_Advances in neural information processing systems_, 35:24824–24837, 2022.\n\n\n[57] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. _Ad-_\n_vances in Neural Information Processing Systems_, 36, 2024.\n\n\n[58] Liang Zhang, Jionghao Lin, Ziyi Kuang, Sheng Xu, Mohammed Yeasin, and Xiangen Hu.\n\nSpl: A socratic playground for learning powered by large language mode. _arXiv_ _preprint_\n_arXiv:2406.13919_, 2024.\n\n\n[59] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv_\n_preprint arXiv:2303.18223_, 2023.\n\n\n14\nhttps://doi.org/10.52202/079017-2721 85706\n**A** **More details about problem decomposition**\nA decomposition of each problem was provided in the GSM8K raw data, which we found largely\nmet our needs. Thus, we use GPT4 to decompose the problems in MAWPS with the prompt in A.1.\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-14-0.png)\n\n\n\n**B** **Prompts of “** _**Dean-Teacher-Student**_ **” pipeline**\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-14-1.png)\n\n\n\n\n\n15\n85707 https://doi.org/10.52202/079017-2721\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-15-0.png)\n\n\n\n\n\n\n|Col1|B.3: Prompt of Dean Agent|Col3|\n|---|---|---|\n||You are a teaching director. The following is the teaching process of a Socratic teacher. Please<br>evaluate the response of [Teacher] in the last round and judge whether it meets the following<br>teaching criteria:<br>-<br>If the [Student] is asking a commonsense or struggles to understand a concept, the<br>[Teacher] is allowed to give direct explanation.<br>-<br>In other cases, no direct answer or solution to the problem can be given in a declarative or<br>rhetorical manner.<br>-<br>If [Student] makes a mistake, it is necessary to imply by way of question what error he/she<br>made.<br>-<br>[Teacher] needs to speak like a teacher and does not use phrases like \\\"let me ask you\\\".<br>-<br>The response must be smooth and fluent.<br>If you think the response meets the criteria, please reply [True], otherwise reply [False],<br>explain your judgement and modify it according to the criteria.|You are a teaching director. The following is the teaching process of a Socratic teacher. Please<br>evaluate the response of [Teacher] in the last round and judge whether it meets the following<br>teaching criteria:<br>-<br>If the [Student] is asking a commonsense or struggles to understand a concept, the<br>[Teacher] is allowed to give direct explanation.<br>-<br>In other cases, no direct answer or solution to the problem can be given in a declarative or<br>rhetorical manner.<br>-<br>If [Student] makes a mistake, it is necessary to imply by way of question what error he/she<br>made.<br>-<br>[Teacher] needs to speak like a teacher and does not use phrases like \\\"let me ask you\\\".<br>-<br>The response must be smooth and fluent.<br>If you think the response meets the criteria, please reply [True], otherwise reply [False],<br>explain your judgement and modify it according to the criteria.|\n|https://doi.org/10.|16<br>85708<br>52202/079017-2721||\n\n\n**Here are some examples:**\n#Problem: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips\nin May. How many clips did Natalia sell altogether in April and May?\n#Dialogue:[Teacher]: How many clips did Natalia sell in May?\\n[Student]: I'm confused.\nDoes \\\"half as many\\\" mean the same number as April?\\n[Teacher]: Not exactly, \\\"half as\nmany\\\" means half the amount. If Natalia sold 48 clips in April, what would be half of 48?\nThat would tell us how many she sold in May. Can you solve that?\n#Response:[ **True** ].\n\n\n#Problem: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips\nin May. How many clips did Natalia sell altogether in April and May?\n#Dialogue:[Teacher]: How many clips did Natalia sell in May?\\n[Student]: Is it 48 plus 48\ndivided by 2? So, 72?\\n[Teacher]: You're on the right track, dividing by 2 to figure out the\nnumber of clips sold in May. But let me ask you this: does the 48 add to 48 divided by 2, or do\nyou divide 48 by 2 on its own first?\n#Response:[ **False** ].The [Teacher]’s response is incorrect and does not point out that the\n[Student] made a calculation error. [ **Modified Teacher** ]: You're on the right track! But you\nmay make a mistake in calculations. Can you calculate again what is the result of 48 plus 48\ndivided by 2?\n\n\n#Problem: Betty is 60 years old, and she is the oldest person in the family. Her daughter is 40\npercent younger than she is, and her granddaughter is one-third her mother's age. How old is\nthe granddaughter?\n#Dialogue:[Teacher]: How old is Betty's daughter?\\n[Student]: 60 percent of 60 is... 36, so\n36?\\n[Teacher]: You're on the right track thinking about percentages, but remember when we\nsay someone is \\\"40 percent younger\\\", we need to subtract that percentage from 100%. Can\nyou revisit your calculation with this in mind?\n#Response:[ **False** ]. The [Student] correctly gets the answer of “How old is Betty's daughter”\nbut the [Teacher] still asks the same question. [ **Modified Teacher** ]: Yes! You have got the\ncorrect result of Betty’s daughter’s age. Now you can calculate how old is Betty's\ngranddaughter. Can you have a try?\n\n\n**Here is the target problem:**\n#Problem: {Here is the target problem.}\n#Dialogue: {Here is the target dialogue.}\n**C** **More Details about Teaching Ability Enhancement**\nTo build single-round dialogues for “ _Irrelevant_ ” student response, we first randomly select 2 _,_ 000\ndialogues from _DiaM_ constructed by DTS pipeline. From each dialogue, we randomly select one\nround of _Student_ ’s response and replace it with a randomly selected question from the 200 questions\ncollected from MOOCs. We then use _Teacher_ agent to refuse answering the question under the\nsupervision of _Dean_, finally forming 2 _,_ 000 single-round _Student-Teacher_ dialogues.\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-16-0.png)\n\nTo build five wrong answers for “ _Incorrect-rely_ ”, on one hand, we identify all the numbers (e.g.,\n“2”) and operators (e.g., “+”) in a _Student_ ’s reply and randomly introduce a perturbation within the\n17\n85709 https://doi.org/10.52202/079017-2721\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-17-0.png)\n\n\n\n\n\n|Number of Problems 11,147<br>Number of Multi-round Dialogues in Dia 35,151<br>M<br>Number of Single-round Dialogues in Dia 22,000<br>S<br>Total Number of Single-round Dialogues 207,581|Col2|\n|---|---|\n|Maximum / Minimum Number of Rounds|12 / 3|\n|Average Number of Step-by-step Guiding Questions<br>Average Number of Rounds in_ DiaM_<br>Average Length of_ Student_’s response<br>Average Length of_ Teacher_’s response|3.29<br>5.28<br>16.4<br>30.3|\n\n\nTable 3: Statistics of our _SocraTeach_ dataset.\n\n\nrange of 10 to one number or randomly replace one operator with another. We apply these rules to\nobtain two new _Student_ ’s responses. On the other hand, we use GPT4 to rewrite the original _Student_ ’s\nresponse and generate three incorrect answers using prompt in C.2. With these five responses, we\nprompt the _Teacher_ to response and obtain 10K _Student-Teacher_ dialogues.\n\n\n**D** **Statistics of** _**SocraTeach**_ **Dataset**\nAs stated in Section 3.5, our _SocraTeach_ consists of 35K multi-round dialogues _DiaM_ and 22K\nsingle-round dialogues _DiaS_ . The average number of rounds in _DiaM_ is 5 _._ 28. On average, the _Stu-_\n_dent_ / _Teacher_ ’s responses contain 16 _._ 4/30 _._ 3 words respectively. The overall statistics is summarized\nin Table 3. Moreover, from Figure 6(a), most teaching dialogues consist of 5-6 rounds. In Figure 6(b),\nwe visualize the probability mass function of _Student_ ’s cognitive states in _SocraTeach_, where (1)-(5)\ncorrespond to a student portrait that performs poor in one of the dimensions in our proposed student\ncognitive system (Section 3.3), while (6) corresponds to a student with strong states in all dimensions.\nIt can be seen that the _Student_ agent tends to simulate a student who is excellent in all aspects or\nalternatively, has problems in calculation ability (i.e., “(3)”) or knowledge mastery (i.e., “(4)”).\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-17-1.png)\n\n��� ��� ��� ��� ��� ���\n\n    - �����������������������������\n(b)\n����\n����\n����\n����\n����\n����\n����\n����\n\n\n  \n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-17-2.png)\n\n\n- - - - - - - �� �� ��\n����������������\n����\n����\n����\n����\n����\n����\n����\n(a)\nFigure 6: Distributions of Number of Rounds (a) and Student Cognitive State (b).\n\n\n18\nhttps://doi.org/10.52202/079017-2721 85710\n**E** **Instruction Tuning Template**\nFigure 7 shows the instruction template for teaching dialogue data and problem-solving data.\n\n\nProblem **Template for teaching dialogue data**\nNatalia sold clips to 48 of her friends in\nApril, and then she sold half as many\nclips in May. How many clips did Natalia\nsell altogether in April and May?\n\n\n\nAnswer\n72\nAnalysis\nNatalia sold 48/2 = 24 clips in\nMay. Natalia sold 48+24 = 72\nclips altogether in April and May.\n\n\n\nYou are a Socratic teacher, please guide me to\nsolve the problem with heuristic questions based on\nthe following information. [Problem] <Problem>\n[Answer] <Answer> [Analysis] <Analysis>\n**Template for problem-solving data**\nPlease analyze and solve the following problem\nstep by step: <Problem>\nFigure 7: The templates for instruction tuning.\n\n\n**F** **Human Evaluation for Teaching Quality**\nTo evaluate the _Overall Quality_ of different LLMs, we recruit 10 well-educated annotators to rank the\ninstructions provided by each model compared with GPT4. Figure 8 shows the template.\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-18-0.png)\n\n\n\n\n\n\n\n\n\nFigure 8: The template of human evaluation for _Overall Quality_ .\n\n\n19\n85711 https://doi.org/10.52202/079017-2721\n**G** **Implementation Details**\nOur _SocraticLM_ is obtained by P-Tuning [36] ChatGLM3-6b (not ChatGLM3-6b-Base) for 2\nepochs with a learning rate of 0 _._ 02 and batch size of 64. For dataset partition, as described in\nSection 5, the testing set is composed of 400 single-round dialogues for evaluating objective teaching\nability (i.e., _IARA, CARA, SER, SRR_ ) and 1 _,_ 000 multiple-round dialogues for evaluating subjective\n_Overall Quality_ . Of the remaining data in _SocraTeach_, 10%/90% is used for validation/training. All\nexperiments are conducted on a server with six NVIDIA RTX 3090 GPUs. Our dataset and code is\navailable at `[https://github.com/Ljyustc/SocraticLM](https://github.com/Ljyustc/SocraticLM)` .\n\n\n**H** **Additional Model Samples**\nIn the following, we present four samples of our _SocraticLM_ and GPT4 for students’ “ _Incorrect-_\n_reply_ ”, “ _Correct-reply_ ”, “ _Questioning_ ” and “ _Irrelevant_ ” response respectively. In the first example,\nboth _SocraticLM_ and GPT4 identify the student’s mistake, but GPT4 gives the answer “7” directly,\nwhich contradicts the demands of Socratic teaching. In the second example, GPT4 also gives an\nalmost complete solution process, which does not meet the requirements of step-by-step Socratic\nguidance of thinking. In the third example, GPT4’s expression is more stilted and lacks the sense\nof a real human teacher. In the fourth example, _SocraticLM_ redirects the student to solve the math\nproblem first, while GPT4 is shifted to answer the irrelevant question.\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-19-0.png)\n\n\n\n\n|Col1|Sample 2: Correct-reply|Col3|\n|---|---|---|\n||#Problem: Jeff makes 10 donuts each day for 12 days. Jeff eats one of the donuts each day.<br>Chris then comes over and eats 8 donuts. If 10 donuts fit in each box, how many boxes can<br>Jeff fill with his donuts?<br>#Dialogue history:<br>[Teacher]: How many donuts does Jeff make?|#Problem: Jeff makes 10 donuts each day for 12 days. Jeff eats one of the donuts each day.<br>Chris then comes over and eats 8 donuts. If 10 donuts fit in each box, how many boxes can<br>Jeff fill with his donuts?<br>#Dialogue history:<br>[Teacher]: How many donuts does Jeff make?|\n|https://doi.org/10.|20<br>85712<br>52202/079017-2721||\n\n\n[Student]: Does \\\"makes\\\" include the ones he eats or not?\n\n[Teacher]: When we say \\\"Jeff makes\\\", we're referring to the total number of donuts Jeff\ncreates, whether he eats them or not. So, how many donuts in total did Jeff make?\n\n[Student]: Jeff makes 10 donuts each day and he made donuts for 12 days, so he made 120\ndonuts in total.\n\n[Teacher]: That's correct! Now, how many donuts does Jeff eat?\n\n[Student]: Jeff eats 12 donuts because he has one each day and he made donuts for 12 days.\n\n\n**SocraticLM:**\nExcellent! Now, if Jeff eats 12 donuts and he made 120 donuts in total, how many donuts are\nleft after he eats?\n\n\n**GPT4:**\nExactly! Now, there is another character introduced in the problem named Chris. How many\ndonuts does Chris eat? Given that Chris eats 8 donuts, how we calculate the total number of\nremaining donuts? Once we have the total number of remaining donuts, what do we do to find\nout the number of boxes required? Now, can you calculate the number of boxes that can Jeff\nneeds to store all of his remaining donuts?\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-20-0.png)\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models.pdf-20-1.png)\n\n\n\n21\n85713 https://doi.org/10.52202/079017-2721\n**I** **Broader Impacts**\nIn this paper, we propose a large language model _SocraticLM_ to achieve Socratic teaching, which is\na crucial pedagogy in intelligent education and has a significant impact on our daily lives. On the\none hand, as explained in Section 3.3 and 3.4, our _SocraticLM_ can provide instructions to students\nwith different cognitive states and various types of responses. Therefore, it can support multiple\npersonalized applications in classrooms and online platforms, delivering high-quality instructions\nmore efficiently and conveniently to students from various backgrounds, thus promoting educational\nequity and rapid development. On the other hand, in our work, we use GPT4 to simulate conversations\nbetween teachers and students to generate dialogue data, entirely without the involvement of real\nhuman teachers or students. This approach provides a way to collect data without interacting with\nor testing on real humans, reducing the burden of human interaction while also avoiding privacy,\nsecurity, and other ethical concerns. However, since our model needs to be trained with multi-round\nteaching dialogue data, in order to expand to more subjects (e.g., physics), it may require more\neducational resources (e.g., problems in textbooks) and training costs.\n\n\n**J** **Limitations and Future Work**\nFirst, from Table 1, the _SER_ metric of _SocraticLM_ and GPT4 is 0 _._ 74 and 0 _._ 65, respectively. This\nshows that the current models have room for improvement in their ability to respond to real and\ncomplex student questions. Second, we focus on the teaching of mathematical problems in this paper.\nFor other subjects, we need additional data construction and training processes. Third, the testset of\nproblem-solving ability in this paper is the problems that our Socratic teaching dialogues are based on\n(i.e. GSM8K and MAWPS). In order to more accurately assess the changes in reasoning ability, we\nwill test additional datasets and explore ability-balancing training strategies for more tasks. Finally,\nin this paper, we use ChatGLM3-6b, an open source large language model as our base to construct\n_SocraticLM_, because it is easy to fine-tune and has not been pre-trained with teaching capabilities,\nwhich can better verify the effect of our _SocraTeach_ dataset. In the future, we will use our dataset to\nfine-tune more LLMs and explore their potential for teaching and intelligent education.\n\n\n22\nhttps://doi.org/10.52202/079017-2721 85714\n**NeurIPS Paper Checklist**\n\n\n1. **Claims**\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\n\nAnswer: [Yes]\nJustification: We have clearly explained the scope of this paper and listed the contributions.\n\nGuidelines:\n\n\n       - The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\n       - The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\n       - The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n\n       - It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n\n2. **Limitations**\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\n\nAnswer: [Yes]\nJustification: We present a \"Limitations and Future Work\" section in Appendix J to discuss\nthe limitations of our work.\n\nGuidelines:\n\n\n       - The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\n       - The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\n       - The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n\n       - The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\n       - The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n\n       - The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n\n       - If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n\n       - While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n\n3. **Theory Assumptions and Proofs**\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\n\nAnswer: [NA]\n23\n85715 https://doi.org/10.52202/079017-2721\nJustification: This paper does not include theoretical results.\n\n\nGuidelines:\n\n\n               - The answer NA means that the paper does not include theoretical results.\n\n               - All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.\n\n               - All assumptions should be clearly stated or referenced in the statement of any theorems.\n\n               - The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n\n               - Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n\n               - Theorems and Lemmas that the proof relies upon should be properly referenced.\n\n\n4. **Experimental Result Reproducibility**\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\n\n\nAnswer: [Yes]\nJustification: We describe in details the construction process of our dataset in the main\npaper, all prompts used in this paper in Appendix A, B, C, and E, and the setups needed to\nreproduce the experimental results in Appendix G.\n\n\nGuidelines:\n\n\n               - The answer NA means that the paper does not include experiments.\n\n               - If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n\n               - If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n\n               - Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n\n               - While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n\n\n5. **Open access to data and code**\n24\nhttps://doi.org/10.52202/079017-2721 85716\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\n\n\nAnswer: [Yes]\nJustification: Our dataset and code is available at `[https://github.com/Ljyustc/](https://github.com/Ljyustc/SocraticLM)`\n`[SocraticLM](https://github.com/Ljyustc/SocraticLM)` .\n\n\nGuidelines:\n\n\n    - The answer NA means that paper does not include experiments requiring code.\n\n    - Please see the NeurIPS code and data submission guidelines ( `[https://nips.cc/](https://nips.cc/public/guides/CodeSubmissionPolicy)`\n`[public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)` ) for more details.\n\n    - While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n\n    - The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines ( `[https:](https://nips.cc/public/guides/CodeSubmissionPolicy)`\n`[//nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)` ) for more details.\n\n    - The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\n    - The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n\n    - At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n\n    - Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n\n\n6. **Experimental Setting/Details**\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\n\n\nAnswer: [Yes]\nJustification: We clearly describe the dataset splits, hyperparameters, training methods, and\nGPU devices in Appendix G.\n\n\nGuidelines:\n\n\n    - The answer NA means that the paper does not include experiments.\n\n    - The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n\n    - The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n\n\n7. **Experiment Statistical Significance**\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\n\n\nAnswer: [Yes]\nJustification: This paper evaluates the quality of Socratic teaching by human annotators. In\norder to ensure the consistency of annotators, we calculate the Kappa score and the result is\n0 _._ 70, which ensures the credibility of our results.\n\n\nGuidelines:\n\n\n    - The answer NA means that the paper does not include experiments.\n\n    - The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n\n\n25\n85717 https://doi.org/10.52202/079017-2721\n\n\n               - The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n\n               - The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n\n               - The assumptions made should be given (e.g., Normally distributed errors).\n\n               - It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n\n               - It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n\n               - For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n\n               - If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n\n\n8. **Experiments Compute Resources**\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\n\n\nAnswer: [Yes]\nJustification: We present the computation resources in Appendix G.\n\n\nGuidelines:\n\n\n               - The answer NA means that the paper does not include experiments.\n\n               - The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n\n               - The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n\n               - The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n\n\n9. **Code Of Ethics**\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics `[https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)` ?\n\n\nAnswer: [Yes]\nJustification: This paper conforms with the NeurIPS Code of Ethics.\n\n\nGuidelines:\n\n\n               - The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\n               - If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n\n               - The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\n\n\n10. **Broader Impacts**\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\n\n\nAnswer: [Yes]\nJustification: We present a \"Border Impacts\" section in Appendix I to discuss the potential\npositive/negative societal impacts of our work.\n\n\nGuidelines:\n\n\n               - The answer NA means that there is no societal impact of the work performed.\n\n\n26\nhttps://doi.org/10.52202/079017-2721 85718\n\n\n    - If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n\n    - Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n\n    - The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n\n    - The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n\n    - If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n\n\n11. **Safeguards**\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\n\n\nAnswer: [NA]\nJustification: This paper poses no such risks.\n\n\nGuidelines:\n\n\n    - The answer NA means that the paper poses no such risks.\n\n    - Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n\n    - Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n\n    - We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n\n\n12. **Licenses for existing assets**\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\n\n\nAnswer: [Yes]\nJustification: The assets including data and baseline models used in this paper are properly\ncited.\n\n\nGuidelines:\n\n\n    - The answer NA means that the paper does not use existing assets.\n\n    - The authors should cite the original paper that produced the code package or dataset.\n\n    - The authors should state which version of the asset is used and, if possible, include a\nURL.\n\n    - The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\n    - For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n\n\n27\n85719 https://doi.org/10.52202/079017-2721\n\n\n               - If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, `paperswithcode.com/datasets`\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n\n               - For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n\n               - If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n\n\n13. **New Assets**\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\n\n\nAnswer: [Yes]\nJustification: We clearly introduce the new dataset in our paper and provide its documentation\nin the link `[https://github.com/Ljyustc/SocraticLM](https://github.com/Ljyustc/SocraticLM)` .\n\n\nGuidelines:\n\n\n               - The answer NA means that the paper does not release new assets.\n\n               - Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n\n               - The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n\n               - At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n\n\n14. **Crowdsourcing and Research with Human Subjects**\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\n\n\nAnswer: [Yes]\nJustification: We include the full template given to the human annotators in Appendix F.\n\n\nGuidelines:\n\n\n               - The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\n               - Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n\n               - According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n\n\n15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human**\n**Subjects**\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\n\n\nAnswer: [Yes]\nJustification: In this paper, we invite human annotators only to assess LLMs’ outputs (i.e.,\ngive ratings). The annotators themselves are not the subjects of the evaluation and are not\nbeing tested. Besides, as shown our annotation template in Appendix F, this evaluation\nprocess does not collect personal information or privacy of the annotators, and the annotators\nare fully aware of the purpose of the evaluation and have consented to its use.\n\n\nGuidelines:\n\n\n28\nhttps://doi.org/10.52202/079017-2721 85720\n\n\n- The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\n- Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n\n- We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n\n- For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n\n\n29\n85721 https://doi.org/10.52202/079017-2721\n\n\n",
    "kor_md": "# **SocraticLM : 대규모 언어 모델을 활용한 소크라테스식 맞춤형 교육 탐구**\n\n**류자위** [1] _[,]_ [2] **Zhenya Huang** [1] _[,]_ [2] [∗] **Tong Xiao** [1] _[,]_ [2] **Jing Sha** [2] **Jinze Wu** [2]\n**Qi Liu** [1] _[,]_ [2] **왕시진** [2] **첸** **엔홍** [1] _[,]_ [2] [∗]\n1: 중국과학기술대학 2: 인지 지능 국가 핵심 연구소\n```\n                {jy251198,tongxiao2002}@mail.ustc.edu.cn;\n                 {huangzhy,qiliuql,cheneh}@ustc.edu.cn;\n                  {jingsha,jzwu4,sjwang3}@ifytek.com\n```\n\n**초록**\n대규모 언어 모델(LLM)은 교수 시나리오에 대한 심층적 이해와 학생 맞춤형 지도를 제공할 잠재력을 지녀\n지능형 교육 발전을 위한 핵심 기술로 간주된다. 그럼에도 불구하고 현재 맞춤형 교육에 적용된 LLM 기반\n솔루션은 주로 '질의응답' 패러다임을 따르며, 학생들에게 _수동적으로_ 답변과 설명을 제공하는 방식이다. 본\n논문에서는 진정한 문제 해결 숙달을 위한 사고 과정에 학생들을 _능동적으로_ 참여시키는 실제 교실 교사의\n역할을 수행하는 소크라테스식 \"사고 촉진\" 교수 패러다임을 구현하는 _SocraticLM_ 을 제안한다. _SocraticLM_\n구축을 위해 먼저, 근본적인 수학 문제에 기반한 35,000개의 정교하게 설계된 소크라테스식 다중 라운드(단\n일 라운드 208,000회 분량) 교육 대화로 구성된 신규 데이터셋 _SocraTeach를_ 구축하기 위한 혁신적인 _\"학장_\n_-교사-학생\"_ 다중 에이전트 파이프라인을 제안한다. 본 데이터셋은 실제 교육 시나리오를 모사하여, 서로 다\n른 인지 상태를 가진 6가지 대표적 가상 학생 유형과 상호작용하며 네 가지 핵심 교수 능력을 강화합니다. 이\n후 _SocraticLM은_ 교수 능력과 추론 능력의 균형을 맞추는 세 가지 전략으로 _SocraTeach_ 데이터셋을 통해 미\n세 조정됩니다. 또한 LLM의 교육 품질을 평가하기 위한 다섯 가지 교육학적 차원을 포괄하는 종합 평가 시\n스템을 제안합니다. 광범위한 실험을 통해 _SocraticLM이_ 교육 성능에서 상당한 개선을 이루었으며, GPT-4보\n다 12% 이상 우수한 성능을 보인다는 점을 검증했습니다. 데이터셋과 코드는\n`[https://github.com/Ljyustc/SocraticLM](https://github.com/Ljyustc/SocraticLM)` 에서 확인할 수 있습니다.\n\n\n**1** **서론**\n대규모 언어 모델(LLMs)은 자연어 처리, 번역, 질문응답 등 다양한 작업에서 인상적인 성과를 거두었습니다[54, 56, 59]. 이는 주로\n두 가지 장점 덕분에, 특히 맞춤형 교육[2, 23, 39]을 중심으로 지능형 교육에 혁명을 일으킬 수 있는 LLM 활용 가능성에 대한 광범위\n한 관심을 불러일으켰습니다. 한편, LLM은 수학 및 물리학과 같은 기초 과목에서 인간과 유사한 수준의 지식 숙달도와 숙련도를 보\n여주고 있다[1, 48]. 따라서 실제 교사처럼 학생들에게 과목별 지도를 효과적으로 제공할 수 있습니다. 다른 한편으로, LLM은 사용\n자(즉, 학생)의 입력에 대한 탁월한 이해력과 적응력을 보여줍니다. 다양한 학생들의 요구를 이해하고 즉각적인 피드백을 제공하며\n상호작용적인 교수법에 참여할 수 있습니다. 이러한 두 가지 측면을 바탕으로 많은 연구가 진행되어 왔습니다.\n\n\n∗ 교신저자\n제38회 신경정보처리시스템 학회(NeurIPS 2024).\n\n\n85693 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n**문제** : 줄리는 120페이지 분량의 책을 읽고 있습니다. 어제 그녀는 12페이지를 읽었고, 오늘은 어제보다 두 배 많은 페이지를 읽었습니다. 내일 남은 페이지\n의 절반을 읽으려면 오늘 몇 페이지를 읽어야 할까요?\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-1-0.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|이 문제를 어떻게 풀어야 할지 모르겠어요.<br>줄리는 오늘 12 x 2 = 24페이지를 읽었습니다<br>. 따라서 어제부터 총 12 + 24 = 36페이지를<br>읽을 수 있었습니다.<br>남은 페이지는 120 - 36 = 84페이지입니다.<br>내일 남은 페이지의 절반을 읽으려면 84/2 =<br>42페이지를 읽어야 합니다.|Col2|Col3|\n|---|---|---|\n||||\n\n\n\n**(a) \"질문-답변\" 교수** **패러다임** **(b) \"사고를** **자극하는\" 교수** **패러다임**\n그림 1: 교수 패러다임: \" _질문-답변_ \" 대 \" _사고촉진\"_ .\n\n\n일반적인 LLM(예: ChatGPT)을 맞춤형 교육에 적용하기 위한 연구[5, 18, 44, 51] 또는 MathGPT [2], EduGPT [3], EduChat[10]과 같은\n특정 교육용 LLM 구축 연구가 진행 중이다.\n\n\n그러나 현재의 대규모 언어 모델 기반 맞춤형 교수법은 주로 '질문-답변' 패러다임을 따르고 있다. 그림 1(a)에서 보듯이, 이들은 학생\n들의 질의에 대한 답변 제공이나 지식 개념 설명과 같은 기능을 수동적으로 제공한다. 이 과정에서 교육은 일련의 Q&A로 지나치게\n단순화되며, CoT[56], ToT[57] 등에 기반한 완전한 답변을 직접 전달하는 데 그쳐 학생들이 가질 수 있는 문제를 진정으로 파악하고\n맞춤형 지원을 제공하는 데 미흡합니다. 결과적으로 학생들은 문제 해결 과정을 이해하는 데 어려움을 겪고, 능력의 실질적 향상 없이\n유사한 문제를 향후 해결하지 못할 수 있습니다.\n\n\n본 논문에서는 소크라테스식 교육법[13, 45]에서 영감을 얻어, 그림 1(b)에 묘사된 새로운 \"사고를 자극하는\" 교육 패러다임을 구현\n하는 _Socrati-cLM_ 을 제안한다. 이 패러다임의 핵심은 학습 과정에 적극적으로 참여하도록 학생들을 대화로 이끌어내는 데 있으며,\n이를 위해 지속적으로 개방형 질문(빨간색으로 표시, 예: \"... 이를 어떻게 계산할까?\")를 지속적으로 제시하여 학생들이 자신의 생각\n을 명확히 표현하고, 가정을 도전하며, 독립적으로 사고하도록 장려하는 데 있다. 이 과정을 통해 학생들은 스스로 문제를 해결하는\n법을 배우게 되어 더 깊은 숙달도와 능력을 함양할 수 있다. 프롬프트 엔지니어링을 직접 활용하는 LLM 기반 애플리케이션(예:\n\nGPT4)과 비교하여, 우리는 체계적으로 연구하고자 한다: 1) _\"사고를자극하는\" 교수법의교육적_ 요구사항과 _소크라테스LM이_ 이러\n한 요구사항을 충족할 수 있도록 하는 방안. 2) _교사의교수능력을_ 분석하고 _SocraticLM에_ 이를 강화합니다. 3) _학생의인지상태를_\n파악하고 _SocraticLM이_ 교수 과정에서 이를 정확히 식별할 수 있도록 합니다. 결과적으로, 우리의 _SocraticLM은_ 각 학생의 필요에\n더 맞춤화되고 적합한 고품질 지도를 제공하여 \"지식의 수호자\"에서 \"학습의 안무가\"로 전환할 수 있습니다.\n\n\n_소크라테스식대화모델(SocraticLM_ )을 구축하기 위해, 우리는 먼저 수학 문제에 기반한 고품질의 세밀한 소크라테스식 다단계 교육\n대화 35,000건으로 구성된 새로운 데이터셋 _'소크라티치_ ( _SocraTeach_ )'를 구축합니다. 데이터셋 구축 과정에서 우리는 실제 교육 시나리\n오의 핵심 역할을 시뮬레이션하기 위해 세 가지 _대규모언어모델_ (LLM) 에이전트를 구현하는 새로운 _'학장-교사-학생'_ 파이프라인을\n제안합니다. _학장은_ 교사의 지시가 _학생에게_ 전달되기 전에 이를 감독하고 다듬어 전체 교육 과정이 소크라테스식 방식을 따르도록\n보장하는 감독자 역할을 합니다. _교사는_ 고전 교육 이론[13, 45]에서 영감을 얻은 소크라테스식 지시를 생성하여 _학생이_ 문제를 해\n결하도록 적극적이고 점진적으로 안내합니다. _학생은_ 교사의 지시에 응답하며, 우리는 실제적이고 다양한 교육 시나리오를 포괄하\n기 위해 교실 내 여섯 유형의 학생을 시뮬레이션하는 학생 인지 상태 시스템을 구축합니다. _학장의_ 감독 하에 여러 차례의 _\"교사-학_\n_생\"_ 상호작용을 통해 포괄적인 소크라테스식 교육 대화가 형성됩니다. 한 걸음 더 나아가 데이터셋의 다양성과 견고성을 높이기 위\n해 실제 교육 시나리오에서 네 가지 유형의 학생 응답을 요약하고, 데이터 증강을 수행하여 추가로 22K건의 단일 라운드 교육 대화\n를 생성합니다. 이는 특히 네 가지 핵심 교육 능력 향상에 특화되어 있습니다.\n\n\n[2https://www.mathgpt.com/ 3https://edugpt.com/](https://www.mathgpt.com/)\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85694\n우리는 _SocraTeach_ 데이터셋을 활용해 ChatGLM3-6b [12]를 미세 조정하여 _SocraticLM을_ 얻었습니다. 이 과정에서 _SocraticLM의_\n문제 해결 능력을 유지하면서 교육적 역량을 향상시키기 위해 세 가지 훈련 전략을 고안했습니다. 또한 LLM의 교육 품질을 평가하\n기 위한 다섯 가지 교육적 차원을 포괄하는 새로운 평가 체계를 제안하며, 이는 해당 분야에서 최초의 시도라고 알고 있습니다. 실\n험 결과, 본 데이터셋이 LLM의 교육적 성능을 향상시킬 수 있음을 보여주었으며, _SocraticLM의_ 교육 품질은 GPT4보다 12% 이상\n우수한 것으로 나타났습니다.\n\n\n본 논문의 기여는 다음과 같다:\n\n\n    - 소크라테스식 \"사고를 자극하는\" 교육 패러다임을 구현하는 언어 모델인 _SocraticLM_ 을 제시합니다. 실험 결과,\nSocraticLM의 소크라테스식 교육 품질은 GPT4보다 12% 우수하며, 동시에 원본 ChatGLM3-6b의 우수한 문제 해결 능\n력을 유지합니다.\n\n\n    - 우리는 방대한 양의 세밀한 소크라테스식 교육 대화를 포함하는 새로운 데이터셋 _SocraTeach_ 를 구축합니다. _SocraTeach_\n구축을 위해 우리는 혁신적인 감독자 역할인 _'학장_ ( _Dean_ )', 학생의 행동을 지시하는 인지 상태 시스템, 그리고 _교사의_ 네\n가지 교육 능력 향상을 설계한 새로운 _'학장-교사-학생'_ 다중 에이전트 파이프라인을 제안합니다. 이 파이프라인은 일반\n적이며 다른 과목의 교육으로 이전될 수 있습니다.\n\n\n    - 우리는 LLM의 교수 품질을 평가하기 위한 5차원 종합 평가 시스템을 개발했습니다. 이는 우리가 아는 한 해당 분야에\n서 최초의 시도입니다.\n\n\n**2** **관련** **연구**\n**LLM 기반** **지능형** **교육.** 대규모 언어 모델(LLM)은 지능형 교육의 세 가지 대표적인 응용 분야, 즉 교육 자료의 자동 생성, 학생 학\n습 성과에 대한 즉각적 평가, 맞춤형 교수 지원에 혁신을 가져왔다[26, 29, 42]. 교육 자료의 경우, 교수 목표와 요구에 기반해 교과서,\n연습문제 등을 생성하는 데 LLM을 활용하는 경향이 있으며, 이는 교사에게 더 풍부한 영감을 제공한다[4, 16]. 학생 성과 측면에서\n는, LLM이 학생의 숙제와 시험을 분석하여 학습 진도에 대한 평가와 피드백을 제공할 수 있다[9]. 본 논문에서 가장 주목하는 맞춤\n형 교수의 경우, 한 연구 계열은 ChatGPT와 같은 일반 LLM을 활용해 글쓰기[18], 프로그래밍[5], 의학 교육[25] 등 다양한 분야에서\n학생들에게 다단계 지원을 제공한다[44, 51, 58]. 학생들의 학습 데이터와 행동 패턴을 분석함으로써, 이러한 LLM은 학생들이 보다\n효과적으로 학습할 수 있도록 돕기 위한 독특한 학습 경로를 설계할 잠재력도 가지고 있다 [20]. 또 다른 연구 분야는 대량의 교수 지\n침을 수집하여 대규모 모델(예: EduChat [10])을 미세 조정하여 문제 해결 및 정서적 지원과 같은 목표 지향적인 교수 능력을 부여하\n는 것이다.\n\n\n**개인화된** **교수** **대화** **데이터셋.** 교수 대화 구축은 대규모 언어 모델 기반 개인화 교수 시스템 구축의 기초이다. 기존 연구에서는 초기\n시도가 크라우드소싱(예: CIMA [49])이나 규칙(예: AutoTutor [17])에 의존하여 실제와 유사한 대화를 생성했다. 이후 연구자들은\n인간-컴퓨터 협업 접근법을 채택했다. 예를 들어, QuizBot [46]은 의미적 유사성 알고리즘을 활용하여 실제 학생들의 응답을 분석하\n고 사전 정의된 교수 워크플로우에 따라 적응형 질문을 제공했습니다. 그러나 이러한 방법들은 상당한 수작업이 필요하거나 사전 정\n의된 교수 절차에 제약을 받아 확장성이 제한되고 다양한 실제 교수 시나리오를 포괄하기 어려웠습니다. 최근 대규모 언어 모델\n(LLM)이 합성 데이터 생성에서 우위를 보임에 따라 [28], 이를 활용한 교육 대화 생성 지원이 주목받고 있다. 그러나 기존 연구 [51]\n에 따르면 GPT 계열 모델은 효과적인 교사 역할을 수행하지 못하므로, 현재 연구는 주로 LLM을 활용해 다양한 배경 [41], 성격\n[41], 오류 유형 [40]을 가진 학생을 시뮬레이션한 후 인간 교사가 설명을 제공하는 방식을 취하고 있다. 그럼에도 이 과정은 여전히\n인간의 개입이 필요하여 최신 데이터셋 MATHDIAL[40]은 3천 개의 샘플만 포함하고 있다. 또한 이러한 데이터셋들은 소크라테스\n식 교수법에 대한 체계적인 연구도 부족하다.\n\n\n**3** _**소크라테스교육**_ **데이터셋**\n교육 이론은 소크라테스식 교수법에 두 가지 기본 요구 사항이 있다고 지적한다[13, 45]: 1) 근본적으로 대화적이며, 학습 촉진을 위\n해 교사와 학생 간의 대화에 의존한다; 2) 탐구적 질문을 사용하여 학생들을 적극적으로 참여시키고, 독립적 사고를 촉진하며 스스\n로 답을 찾도록 장려한다. 이러한 요구 사항을 충족시키기 위해 _SocraTeach_ 데이터셋을 구축함에 있어\n3\n85695 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n**문제** : 줄리는 120페이지 책 한 권을 읽고 있습니다. 어제 그녀는 12페이지를 읽었고, 오늘은 어제보다 두 배 많은 페이지를 읽었습니다. 내일 남은 페이지의 절반을 읽으려면, 오늘 몇 페이지를 읽어야 할까요?\n\n\n\n**단계별** **안내** **질문** **Q1Q3** : : 줄리가읽어야 할오늘페이지가몇 페이지를몇 페이지읽었나요남았나요? ?\n\n\n\n**Q2** : 줄리는 어제부터 지금까지 몇 페이지를 읽었나요?\n\n**Q4** : 내일 몇 페이지를 읽어야 할까요?\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-3-1.png)\n\n\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-3-0.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그림 2: _SocraTeach_ 데이터셋 구축의 작업 흐름.\n\n\n요구사항을 충족시키기 위해 우리는 다음과 같은 과제에 직면합니다. 첫째, 교사 측면에서 교육 방법론과 발표 스타일에 상당한 차이\n가 존재합니다. 모델이 이를 모두 동시에 학습하기 어려울 수 있으며, 이는 교수 논리 내 혼란과 오류로 이어질 수 있습니다. 둘째,\n학생 측면에서 실제 교육 상황에서는 학생들의 인지 상태가 복잡하고 이질적입니다[22, 35]. 이해 능력이 뛰어나고 충분한 지식을\n가진 학생들도 있지만, 문제를 이해하지 못하거나 필수 지식조차 부족한 학생들도 상당수 존재합니다. 본 데이터셋은 이러한 모든\n상황을 포괄해야 하며, 이를 통해 모델이 서로 다른 상태의 학생들에게 맞춤형 지도를 제공할 수 있도록 학습해야 합니다.\n\n\n위와 같은 과제를 해결하기 위해 우리는 다음과 같이 _SocraTeach를_ 구축한다. 첫째, 가르쳐야 할 각 문제에 대해 단계별 안내 질문 목\n록을 분해한다(3.1절). 이를 바탕으로 교수 접근법과 설명 스타일을 해당 질문들과 정렬하여 교사 시뮬레이션을 제어할 수 있다. 둘\n째, 혁신적인 _\"학장-교사-학생\"_ 파이프라인을 고안하여 세 가지 LLM 에이전트( _\"학장\"_, _\"교사\"_, _\"학생\"_ )를 구현함으로써 세분화된\n다단계 교육 대화를 수집합니다(3.2절). 특히 실제 시나리오의 학생 프로필과 정렬하기 위해, 우리는 이해력, 계산력, 지식 숙달도\n등의 측면에서 여섯 종류의 학생을 _\"학생\"_ 에이전트에 시뮬레이션하기 위한 인지 상태 시스템을 구축합니다(3.3절). 마지막으로,\n_SocraTeach의_ 다양성과 견고성을 더욱 향상시키기 위해, 우리는 네 가지 핵심 교수 능력을 개선하기 위한 추가적인 단일 라운드 교수\n대화를 구축하기 위한 데이터 증강 방법을 설계합니다(3.4절).\n\n\n**3.1** **문제** **수집** **및** **단계별** **안내** **질문**\n본 논문에서는 수학이 기초적이면서도 핵심적인 과목이며, 이러한 문제들은 학생들의 기본적인 이해력과 추론 능력을 평가하는 데\n활용되기 때문에[32, 34], 초등학교 수준의 수학 문제 교수를 탐구의 사례로 삼는다. 본 연구의 문제들은 두 가지 대표적인 데이터셋\n인 MAWPS[27]와 GSM8K[8]에서 추출되었으며, 각각 2 _._ 3K개와\n8 _._ 8K개의 문제를 각각 포함하고 있다.\n\n\n교사 역할을 모의할 때 표현 방식과 교수법이 일관되도록 하기 위해, 각 문제를 일련의 단계별 안내 질문(예: 그림 2의 **Q1-Q4** )으로\n분해합니다(자세한 내용은 부록 A 참조). 단, 교수의 효율성과 간결성을 보장하기 위해 수치 계산과 해법 요약은 단계로 간주하지 않\n습니다.\n\n\n**3.2** **학장** _**-교사-학생**_ **파이프라인**\n소크라테스식 \"사고를 자극하는\" 교수 패러다임을 구현하는 _SocraTeach_ 데이터셋을 구축하기 위해, 우리는 일대일 다중 라운드 교사\n-학생 대화를 수집하는 새로운 \" _학장-교사-학생_ \"(DTS) 파이프라인을 제안합니다. 이 파이프라인은 세 개의 대규모 언어 모델(LLM)\n에이전트로 구성됩니다:\n\n\n            - _**학장**_ D: 연구에 따르면 GPT는 교사 역할을 수행하는 데 필요한 학생 이해 및 언어 표현 능력에 미흡함이 존재합니다\n[51]. 이 문제를 해결하기 위해 우리는\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85696\n감독 역할을 수행하는 _딘_ 에이전트를 제안합니다. 이 에이전트는 교사의 지시가 소크라테스식 교수법의 요구 사항을 충\n족하는지 판단합니다. 요구 사항을 충족하지 못한다고 판단할 경우, _학생에게_ 제시되기 전에 지시를 수정할 권한을 가\n집니다.\n\n\n    - _**교사**_ T: 교사 에이전트는 소크라테스식 방식으로 _학생_ 에이전트가 문제를 해결하도록 적극적으로 유도하며, 소크라테\n스의 교육 이론[13, 45]에 따라 두 가지 주요 목적을 수행한다. 첫째, 추론을 완료한 후 다음 단계를 고려하도록 _안내하는_\n등 소크라테스식 질문을 통해 적절한 시점에 _학생의_ 사고를 촉진해야 한다.\n\n단계. 둘째, 학생에게 단계별 설명과 관련된 지식 포인트를 제공해야 합니다. 특정 문제에 대해 일관된 교수 스타일을 유\n지하기 위해, _교사는_ 3.1절에서 구축된 단계별 질문에 따라 교수를 수행하도록 요청받습니다.\n\n    - _**학생**_ 에이전트 S: 데이터셋 내 학습자를 대표하는 학생 에이전트는 교사의 지시(질문 및 설명)에 대한 답변을 생성합니\n다. _학생의_ 진정성과 다양성을 보장하기 위해, 우리는 제3.3절에서 여섯 종류의 실제 학생을 묘사하는 인지 상태 시스템\n을 구축하고, 학생이 답변할 때마다 이들 중 하나를 시뮬레이션하도록 설정합니다.\n\n\nDTS 파이프라인에서 각 교수 대화 {(T1 _,_ S1) _,_ (T2 _,_ S2) _, ..._ }는 _학장의_ 감독 하에 교수와 _학생_ 간의 상호작용 순환을 통해 형성되며, 각\n에이전트는 GPT4로 시뮬레이션됩니다. 그림 2를 예로 들면, 첫 번째 라운드( _t_ = 1)에서 _교사는_ 3.1절에서 구성한 첫 번째 단계의 질\n문(즉, T1= **Q1** )을 직접 제시합니다(이 과정에는 LLM을 사용할 필요가 없음). 그런 다음, _학생은_ 3.3절의 여섯 가지 유형의 인지 프\n로필에서 상태 프로필(예: 약한 지식 숙달)을 선택하고 이를 기반으로 해당 응답 S1을 생성합니다(부록 B.1 참조).\n(프롬프트용). 이후 _t_ = 2에서 _교사는_ 소크라테스식 방식으로 지시 T(2)를제공하며,\n답변을 제공하지 않고 대화 흐름을 따라가는 등의 교육적 요구 사항을 설계합니다.\n\n단계별 질문. 여기서 우리는 예시를 통해 각 학생 프로필에 대해 서로 다른 응답 스타일을 설정한다는 점을 강조해야 합니다(부록\nB.2). _교사가_ T(2)를생성한 후, _학장이_ 이를 평가(예: \"... 교수 기준을 충족하지 못함\")하고 수정(예: \"12를 2로 곱하면 몇이 되나요?\"를\n\"오늘 몇 개를 읽었나요?\"로 변경)합니다. 이때 다음 사항에 중점을 둡니다. 1) 소크라테스식 질문 방식에 부합하는지. 2) _학생의_ 오\n류를 명확히 지적하는지. 3) 실제 교사의 언어 스타일과 유사한지(부록 B.3), 즉 T2 ← _D_ (T2). 수정된 응답은 _학생에게_ 전송되며 다음\n대화 라운드가 시작됩니다. 궁극적으로 _교사가_ 교수 과정이 완료되었다고 판단하면, 출력의 끝을 나타내는 \"[END]\" 토큰을 출력하\n여 주기 종료를 알립니다. 본 논문에서는 수학 문제 교수에 초점을 맞추고 있지만, 우리의 DTS 파이프라인은 일반적이며 다른 과목(\n예: 물리학)의 문제로 확장될 수 있다는 점은 주목할 만합니다.\n\n\n**3.3** **학생** **인지** **상태** **시스템**\n교육 과정 전반에 걸쳐 실제적이고 다양한 학생 상태를 데이터셋에 반영하기 위해서는 _학생_ 에이전트 내에서 서로 다른 인지 상태를\n시뮬레이션할 필요가 있다. 그러나 기존 연구에서는 이러한 상태들에 대한 체계적이고 통일된 정의가 확립되지 않았다[15]. 일부 선\n행 연구는 수학이나 영어와 같은 특정 과목에 특화된 상태에 집중해 왔습니다[3, 11, 19, 50]. 반면 다른 연구들은 집중력, 작업 기억,\n논리적 추론과 같이 인간 인지과학에 기반한 일반적인 상태를 추상적으로 정의하기도 했습니다[14, 47, 53, 33]. 안타깝게도 이러한\n정의들은 교육 과정에 적용하기 어렵거나 대규모 언어 모델(LLM)로 구현하기 까다롭습니다. 이 문제를 해결하기 위해 우리는 다음\n과 같이 학생의 관점에서 소크라테스식 교수 과정을 재검토한다. 먼저 학생은 주어진 문제의 의미를 파악해야 한다. 그런 다음 교사\n가 제공한 지시를 이해하고, 계산 능력과 습득한 지식을 활용하여 지시를 실행한다. 궁극적으로 이 과정은 학습에 대한 흥미를 유발\n하고 효과적인 학습 성과를 도모한다. 이러한 관점에 기반하여 우리는 인지 상태의 다섯 가지 차원을 다음과 같이 요약한다:\n\n\n(1) _**문제이해도**_ : 학생이 주어진 문제를 이해하는 정도를 의미합니다.\n\n\n(2) _**지시이해**_ : 학생이 교사의 지시를 이해하고 수행하는 정도를 의미합니다. 상태가 양호한 학생은 이러한 지시를 쉽게 수행할 수\n있어야 합니다.\n\n\n(3) _**계산**_ : 수학적 표현과 숫자를 정확히 도출하는 능력을 의미한다.\n\n\n(4) _**지식숙달도**_ : 학생들이 지식을 어느 정도 숙달했는지를 의미합니다.\n\n\n(5) _**학습열망**_ : 학생들이 새로운 정보를 찾고 습득하며, 질문하고 가능성을 탐구하려는 욕구나 성향을 의미한다.\n\n\n3\n85697 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n기본적으로 우리는 위의 차원 중 하나에서 저조한 성적을 보이는 다섯 가지 유형의 학생을 정의할 수 있습니다. 또한 모든 차원에서\n뛰어난 성적을 보이는 여섯 번째 유형의 학생을 추가합니다.\n\n\n**3.4** **교수** **능력** **향상**\nDTS 파이프라인으로 구축된 다중 라운드 대화( _DiaM_ 으로 표기)는 모델이 근본적인 소크라테스식 교육 패러다임을 이해하도록 보\n장한다. 그러나 _DiaM_ 에서는 _학생이교사의_ 각 지시에 단 한 번만 응답하며 단순한 학생 프로필을 선택하는 경향이 있어, 장미형 학\n생 응답에 대한 시뮬레이션이 부족하다(부록 D에서 논의됨). 본 절에서는 데이터셋의 다양성과 견고성을 더욱 강화하기 위해 _DiaM_\n에 대한 데이터 증강을 통해 더 많은 _학생-교사_ 단일 라운드 대화 _DiaS_ 를 구축함으로써 네 가지 중요한 교육 능력을 향상시킵니다.\n\n\n구체적으로 실제 교수 과정에서 학생들의 응답은 다음과 같이 분류될 수 있다. 첫째, 거시적 관점에서 응답은 '비관련성'과 _'관련성'으_\n_로_ 나뉜다. _'관련성'은_ 문제나 지시에 직접적으로 관련된 응답을 의미하는 반면, _'비관련성'은_ 그림 2에서처럼 \"오늘 날씨 어때요?\"와 같\n이 교수 내용과 무관한 응답을 뜻한다. 둘째, _\"관련성있음\"_ 범주 내에서는 _\"질문형\"_ 과 _\"응답형\"_ 으로 세분화할 수 있으며, 각각 학생이\n교사에게 질문하는 경우와 교사의 질문에 답하는 경우를 의미한다. 셋째, _\"응답형\"은_ 교사의 질문에 대한 학생의 답변이 정확한지 여\n부에 따라 _\"_ 잘못된 _답변\"_ 과 _\"_ 올바른 _답변\"_ 으로 추가 분류된다. 이와 같은 맥락에서 학생들의 응답은 _\"비관련\"_, _\"질문형\"_, _\"잘못된답변_\n_\"_, _\"올바른답변\"_ 의 네 가지 범주로 분류됩니다. 이를 바탕으로 집중적으로 향상시켜야 할 네 가지 핵심 교수 능력이 있습니다.\n\n\n첫째, _\"관련없는\"_ 응답의 경우 교사가 이를 인지하고 대화를 교육 방향으로 전환할 것으로 기대합니다. 예를 들어 \"이 질문은 ...과 무\n관합니다. 먼저 문제에 집중합시다...\"와 같이 응답하는 것입니다. 이를 위해, 우리는 MOOC에서 교육과 무관한 200개의 실제 학생\n질문을 수집한 다음, 이를 _DiaM_ 에 무작위로 삽입하고 _교사가_ 답변을 거부하도록 하여 2 _,_ 000개의 단일 라운드 _학생-교사_ 대화를 구\n성합니다(자세한 내용은 부록 C 참조).\n\n\n둘째, _\"질문하기\"는_ 가장 핵심적인 교수 능력, 즉 교사가 학생에게 정확한 설명을 제공해야 하는 능력에 해당합니다. 이에 대해\n2 _,_ 000건의 _교사-학생대화를_ 무작위로 추출합니다.\n대화(T _i,_ S _i_ )를 _DiaM_ 에서 가져오고, _학생_ 에이전트를 사용하여 세 가지 추가 질문 S [1] _,_ S [2] _,_ S [3] 을 _i_ _i_ _i_\nT _i_ (프롬프트는 부록 C.1 참조). 그런 다음, _교사_ 에이전트에게 T [(1)][을] 제공하도록 요청합니다. i+1 _,_ T i+12 _,_ T\nT _i_ (프롬프트는 부록 C.1 참조). 그런 다음, _교사_ 에이전트에게 T [(1)][을] 제공하도록 요청합니다. i+1 _,_ T i+12 _,_ T i+13\n궁극적으로 6 _,_ 000개의 단일 라운드 _학생-교사_ 대화 {(S _[j]_ _,_ T _[j]_ _|j_ = 1 _,_ 2 _,_ 3}을 형성한다.\n\n\n\ni+1\n_,_ T i+1 _,_ T i+1,\n2\ni+1\n_i_ i+1\n셋째, 교사는 학생의 _\"잘못된답변\"_ 을 정확하게 식별하고 수정 아이디어를 지적해야 합니다. 이를 달성하기 위해, 우리는 _DiaM_ 에서\n2 _,_ 000개의 _교사-학생_ 대화를 유사하게 샘플링하고 규칙 및 생성 기법을 사용하여 학생의 응답을 다섯 가지 오답으로 재작성합니다.\n\n그런 다음 _교사가_ 오류를 식별하고 응답을 제공하도록 명시적으로 프롬프트하여 또 다른 10K개의 _학생-교사_ 대화 인스턴스를 얻습\n니다(자세한 내용은 부록 C 참조).\n\n\n마지막으로, 강건성을 높이기 위해 동일한 _\"정답\"_ 의 다양한 표현을 교사가 식별할 수 있도록 하기 위해, _\"오답\"_ 에 사용된 것과 동일한\n2 _,_ 000개의 단일 라운드 \"교사-학생\" 대화를 활용하여 _학생과_ 함께 두 가지 정답 응답을 생성합니다(프롬프트는 부록 C.2 참조). 그\n후, 교사의 답변을 수집하여 또 다른 4 _,_ 000개의 단일 라운드 \" _학생-교사_ \" 대화를 얻습니다.\n\n\n**3.5** **데이터셋** **개요**\n요약하면, 우리의 _SocraTeach는_ 3.2절의 \"Dean-Teacher-Student\" 파이프라인으로 구축된 35K 다중 라운드 대화 _DiaM_ 과 3.4절의 데\n이터 확장을 통한 22K 단일 라운드 대화 _DiaS_ 로 구성됩니다. _DiaM_ 의 평균 대화 횟수는 5 _._ 28회로, 총 208K개의 단일 라운드 대화 예\n제가 생성되었습니다. _SocraTeach의_ 추가 통계는 부록 D에 요약되어 있습니다.\n\n\n기존의 교수 대화 데이터셋[17, 40, 41, 46, 49]과 비교하여, 본 연구의 _SocraTeach는_ 감독 및 교정을 위한 \" _학장_ (Dean)\" 역할을 도\n입함으로써 LLM이 교사를 부적절하게 시뮬레이션하는 문제[51]를 최초로 해결합니다. 둘째, _SocraTeach는_ 소크라테스식 교수법\n을 위해 설계된 최초의 공개 데이터셋으로, _교사의_ 네 가지 핵심 교수 능력을 구체적으로 향상시킵니다. 셋째, 기존 데이터셋이 학\n년 등 인구통계학적 배경이나 특정 오류 유형으로 다양한 학생을 시뮬레이션하는 반면, _SocraTeach는_ 교육학적 경험을 바탕으로\n교수 과정 중 _학생의_ 여섯 가지 인지 상태를 모델링하여 더 광범위한 실제 교수 시나리오를 포괄하고 LLM이 더 나은 교수 능력을\n갖출 수 있도록 합니다. 마지막으로, _SocraTeach는_ 35K개의 대화로 구성된 완전 자동 생성 대규모 데이터셋입니다.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85698\n다중 라운드 대화와 22,000개의 단일 라운드 대화를 포함하며, 실제 인간 학생/교사에 의존하는 기존 데이터셋(예: 최신\nMATHDIAL[40]은 3,000개의 대화를 포함)을 크게 능가합니다.\n\n\n**4** _**SocraticLM**_ **미세** **조정**\n_SocraTeach_ 를 기반으로, 각 대화 {(T1 _,_ S1) _,_ (T2 _,_ S2 ) _, ..._ }를 여러 라운드로 분할하고, 각 라운드의 선행 컨텍스트 {(T1 _,_ S1 ) _, ...,_\n(T _i,_ S _i_ )}를 입력으로, 교사의 응답 T _i_ +1 을 출력으로 사용하여 모델을 미세 조정할 수 있습니다. 그러나 이는 치명적인 망각\n(catastrophic forgetting)을 유발하고 모델이 이미 보유한 문제 해결 능력을 저하시킬 수 있습니다.\n\n이러한 대화들은 사전 훈련에 사용된 데이터와 다를 수 있기 때문입니다[21, 30, 38]. 구체적으로, 6.2절에서 GSM8K/MAWPS 데이\n터셋에 대한 _SocraticLM의_ 정확도가 각각 31.2%/9.7% 감소하는 것을 관찰했습니다. 따라서 SocraticLM의 근본적인 문제 해결 능\n력을 저해하지 않으면서 교육 능력을 향상시키기 위해 다음과 같은 세 가지 훈련 전략을 탐구합니다:\n\n\n**분리** **훈련.** 문제 해결 능력을 유지하기 위한 직접적인 방법 중 하나는 대화 데이터와 문제 해결 데이터를 혼합하여 훈련하는 것이다\n. 그러나 6.2절에서 보듯이 이는 만족스러운 결과를 얻지 못함을 발견했다. 따라서 우리는 먼저 대화 데이터로 _SocraticLM을_ 미세\n조정한 후, GSM8K와 MAWPS에서 무작위로 추출한 소량의 문제 해결 데이터로 다시 미세 조정하는 분리 훈련 방식을 채택한다.\n\n실험 결과, 대화 데이터 대비 문제 해결 데이터의 비율 _α가_ 대략 [1] 일 때 최적 성능이 달성됨을 확인했습니다.\n\n\n**지시문** **조정.** [37]에서 영감을 받아 대화 데이터와 문제 해결 데이터에 서로 다른 지시문을 적용하며, 템플릿은 부록 E에10제시되어\n있습니다. 주목할 점은 3.2절의 _교사용_ 프롬프트와 달리, 여기서 대화용 지시문은 모델이 3.1절의 단계별 안내 질문을 따를 것을 요\n구하지 않는다는 것입니다. 이는 훈련 과정에서 이러한 정보를 제공하면 모델이 지름길을 택할 수 있기 때문이다. 즉, 교육 과정을\n프롬프트에서 정보를 추출하는 과정으로 단순화하여 진정한 교육 능력을 습득하지 못할 수 있다.\n\n\n**혼합** **프롬프트** **설정.** 동일한 작업에 대해 혼합 프롬프트 설정으로 훈련하는 것은 LLM의 추론 능력을 향상시키는 중요한 방법입니\n다 [7, 55]. 이를 위해 GSM8K 및 MAWPS의 기존 제로샷 문제 해결 데이터 외에도, 훈련을 위해 제로샷 데이터 양의 약 [1] 에 해당하\n는 원샷 버전을 구축합니다.\n\n10\n**5** **우리의** **소크라테스식** **교육** **평가** **시스템**\n교육 과정에 대한 표준적인 답안이 존재하지 않기 때문에, 모델이 생성한 응답과 주석이 달린 응답 간의 유사성을 계산하는 기존 지표\n(예: BLEU[43], Rouge[31])는 대규모 언어 모델(LLM)의 교육 품질을 완전히 평가하지 못할 수 있습니다. 이 문제를 해결하기 위해\n본 논문에서는 소크라테스식 스타일과 교수 능력을 위한 다섯 가지 교육적 차원을 포괄하는 평가 시스템을 제안하며, 이는 우리가 아\n는 한 이 분야에서 최초의 포괄적인 연구입니다.\n\n\n(1) _**전반적품질(Overall)**_ : 이 지표는 교육의 질에 대한 전체적이고 주관적인 평가로, 교육이 소크라테스식 스타일을 충족하고 학생\n의 경험을 향상시켜야 합니다.\n\n\n_전체품질_ 평가를 위해 _DiaM_ 에서 무작위로 1 _,_ 000개의 단일 라운드 \"학생-교사\" 대화를 추출하고, 10명의 고학력 어노테이터를 모집\n하여 각 모델과 동일한 맥락에서 제공된 GPT4의 _교사_ 응답 쌍을 맹검 방식으로 순위를 매기게 합니다(자세한 내용은 부록 F 참조).\n\n_전체_\n품질은 정규화된 승률 차이 [1] (1 + _[승][in]_ [−] _[패]_ ) ∈ (0 _,_ 1) (GPT4의\n~~2~~ _승리+패배+무승부)_\n자체 결과는 0 _._ 5이다). 인간 간 품질 평가자의 일치도를 보장하기 위해, 우리는 소크라테스LM과 GPT4의 _교사_ 응답 쌍 100개를 무\n작위로 구성하고 모든 주석가에게 어느 쪽이 더 나은지 평가하도록 요청했다. 카파 점수는 0 _._ 70으로, 인간 주석가들 간의 우수한 일\n치도를 나타낸다.\n\n\n3.4절에서 설명한 네 가지 소크라테스식 교수 능력에 대해 지표 (2)-(5)를 제안합니다. 각 지표에 대해 _DiaS_ 에서 무작위로 100개의\n해당 단일 라운드 대화를 테스트용으로 선정합니다.\n\n\n(2) _**잘못된답변인식정확도(IARA)**_ : 이 차원은 교사가 학생의 _\"잘못된답변\"을_ 정확히 식별할 수 있는지 여부에 초점을 맞춥니다.\n\n예를 들어, 그림 2에서 학생이 잘못된 답변(예: \"14\")을 제공하면 유능한 교사는 이를 인식하고 지적할 수 있어야 합니다. 이 과정은\n객관적이며 이진 분류 작업으로 간주될 수 있습니다.\n\n\n3\n85699 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n\n\n|Col1|전체 IARA CARA SER SRR|BLEU-4 루즈-1 루즈-2 루즈-l|\n|---|---|---|\n|챗GPT<br>GPT4<br>비쿠나-7b<br>Llama2-7b<br>Llama2-13b<br>Llama3-8b<br>ChatGLM3-6b<br>에듀챗-32b<br>|0.29<br>0.42<br>0.93<br>0.62<br>0.19<br>0.50<br>0.76<br>0.91<br>0.65<br>0.55<br>0.15<br>0.16<br>0.77<br>0.16<br>0.39<br>0.27<br>0.15<br>0.86<br>0.32<br>0.13<br>0.25<br>0.23<br>0.87<br>0.30<br>0.08<br>0.33<br>0.75<br>0.77<br>0.39<br>0.52<br>0.11<br>0.18<br>0.87<br>0.46<br>0.07<br>0.37<br>0.48<br>0.77<br>0.40<br>0.03|22.8<br>34.3<br>14.4<br>21.3<br>36.2<br>42.9<br>19.4<br>32.4<br>22.8<br>34.9<br>14.4<br>22.8<br>28.3<br>35.7<br>14.0<br>24.1<br>27.9<br>36.4<br>14.3<br>23.6<br>27.4<br>33.0<br>10.9<br>22.0<br>17.1<br>26.2<br>9.1<br>15.7<br>27.8<br>38.4<br>17.9<br>29.2|\n|**_소크라테스LM_**(**우리 것**)<br>_당뇨병 _제외<br>관련 _없음_<br>_질문 _없이<br>_잘못된 경우 _제외<br>w/o_Correct_|**0.62**<br>**0.83**<br>**0.98**<br>**0.74**<br>**0.78**<br>0.54<br>0.27<br>0.89<br>0.67<br>0.34<br>0.57<br>0.79<br>0.87<br>0.69<br>0.43<br>0.58<br>0.74<br>0.92<br>0.53<br>0.83<br>0.51<br>0.33<br>0.93<br>0.68<br>0.65<br>0.60<br>0.70<br>0.58<br>0.70<br>0.76|**48.6**<br>**56.2**<br>**33.7**<br>**47.5**<br>42.6<br>52.2<br>32.3<br>44.4<br>45.7<br>52.3<br>29.8<br>44.2<br>47.8<br>55.0<br>31.9<br>45.9<br>41.8<br>48.2<br>30.4<br>38.9<br>47.4<br>55.1<br>32.8<br>46.6|\n\n\n표 1: 교수 성과. 모든 지표에서 값이 높을수록 성능이 우수함을 나타냅니다. 최상의 방법은 굵은 글씨로 강조 표시되었습니다. 준우\n승 기준선은 밑줄로 표시되었습니다.\n\n\n(3) _**정답인식정확도(CARA)**_ : 오류 인식과 달리, 이 차원은 모델이 학생의 _\"정답\"을_ 정확히 식별할 수 있는지 여부에 초점을 맞춥\n니다. 이 지표를 무시하면 LLM이 학생이 제공한 모든 답변을 오답으로 간주하도록 오도할 수 있습니다.\n\n\n(4) _**성공적설명률(SER)**_ : 이 차원은 모델이 학생의 _\"질의_ \"에 대해 만족스러운 설명을 제공할 수 있는지 여부에 초점을 맞춥니다.\n\n이 지표는 주관적이지만, 학생의 실제 경험을 바탕으로 이진 분류로 변환할 수 있습니다.\n\n\n(5) _**성공적거절률(SRR)**_ : 이 지표는 교사가 학생의 _\"관련없는\"_ 질문에 답변을 거부하고 교육 콘텐츠로 다시 안내해야 하는 경우를\n위해 설계되었습니다. 모델이 질문에 답변을 거부하는지에 따라 이진 분류로도 계산됩니다.\n\n\n교육 분야 대규모 언어 모델(LLM) 평가에 관한 기존 연구와 비교하여, 본 평가 시스템은 세 가지 주요 장점을 제공한다. 첫째, 보다\n포괄적이고 적절한 평가를 제공합니다. 기존 연구들은 유사도 지표(예: BLEU [43])나 제한된 규모의 수동 평가(예: 설문지 배포\n[18])에 의존하는 반면, 본 시스템은 전반적인 교수 품질과 네 가지 핵심 교수 능력을 함께 평가하여 보다 체계적인 평가 구조를 제공\n합니다. 둘째, LLM 간 비교 가능성을 높입니다. 학생이 한 번에 하나의 LLM과만 상호작용할 수 있다는 한계로 인해 기존 인간 평\n가[5, 24]는 다중 모델의 효과성을 비교하기 어렵습니다. 반면 본 시스템은 서로 다른 모델에 걸쳐 공유되는 동일한 교육 대화를 테\n스트 샘플로 사용함으로써, 서로 다른 LLM을 동시에 공정하게 비교할 수 있게 합니다. 셋째, 본 시스템은 더 방대하고 신뢰할 수 있\n습니다. 최근 연구들은 약 600개의 테스트 대화만 포함된 최신 연구[40]와 같이 소규모 데이터셋에 의존하는 반면, 본 연구는 더 큰\n_SocraTeach_ 데이터셋의 혜택을 받습니다.\n\n\n**6** **실험**\n본 절에서는 ChatGPT, GPT4, Vicuna-7b [6], Llama2-7b, Llama2-13b, Llama3-8b [52], ChatGLM3-6b [12], EduChat-32b [10]을\n기준 모델로 삼아 _제안된 SocraticLM의_ 효과성을 검증한다. 구현 세부 사항은 부록 G에 설명되어 있습니다. 특히 공정한 비교를 위\n해 테스트 대화에서 제시된 문제들에 대해서는 훈련 과정에서 해당 대화 전체를 제외했습니다. 제5장에서 제안한 평가 시스템 외에\n도, 인간 어노테이터를 초청하여 각 테스트 대화에 대한 실제 교사 응답을 제공받았으며, 이를 기준으로 BLEU 및 Rouge 점수를 계\n산했습니다.\n\n\n**6.1** **주요** **결과**\n표 1은 모든 모델의 결과를 요약합니다. 먼저, 60억 개의 매개변수를 포함하는 당사의 _SocraticLM은_ 모든 소크라테스식 교육 능력\n에서 상당한 개선을 보입니다. 특히 GPT4 대비 _종합_ 점수에서 12%, _IARA에서_ 6%, _CARA에서_ 7%, _SER에서_ 9%, _SRR에서_ 23% 우\n위를 보였으며, BLEU 및 Rouge로 측정된 인간 교사의 응답 재현률에서도 12% 이상 높은 성능을 나타냈습니다. 부록 H에서는 두\n모델의 출력 예시를 제시하고 분석합니다. 둘째, _SocraticLM은 SER_ 측면에서 상당한 개선을 보입니다. 이는 제안된 Dean 에이전트\n의 판단 및 수정 기능이 대규모 모델의 설명 능력을 획기적으로 향상시킬 수 있음을 시사합니다.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85700\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-8-1.png)\n\n1 -2  3-4  5-6  >=7\n가이드 질문 수\n1 .0\n0.8\n0.6\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-8-2.png)\n\n1 -2  3-4  5-6  >=7\n가이드 질문 수\n1.00\n0.75\n0.50\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-8-0.png)\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-8-3.png)\n\n\n\n1 .0\n0.5\n1 .0\n0.5\n1 -2  3-4  5-6  >=7\n가이드 질문 수\n1 -2  3-4  5-6  >=7\n가이드 질문 수\n그림 3: 단계별 안내 질문 수가 다른 문제들에 대한 성과.\n\n\n교사로서. 셋째, 그림 3에서는 문제의 난이도를 반영할 수 있는 단계별 안내 질문 수가 다른 문제들에 대해 네 가지 교수 능력을 평\n가합니다. 우리 _소크라테스LM이_ 모든 난이도 수준에서 GPT4를 꾸준히 능가하는 것이 분명합니다.\n\n\n**6.2** **제거** **연구**\n**교수** **능력** **향상의** **중요성** . 제3.4절에서 제시된 네 가지 핵심 교수 능력을 위해 구축된 단일 라운드 교수 대화( _DiaS)_ 의 중요성을 탐구한\n다. 표 1을 통해, 이러한 대화를 제거했을 때(즉, _\"_ w/o _DiaS_ \") 상당한 하락(예: _전체품질_ 8% 감소)이 먼저 관찰된다. 이는 우리의 교수\n능력 강화의 필요성을 보여주며, 제안된 네 가지 교수 능력이 소크라테스식 교수의 실제 요구를 충족시키는 데 효과적임을 확인시켜\n줍니다. 둘째, IARA와 _SRR_ 지표가 가장 크게 감소하여, 현재 LLM 기반 교수와 인간 교수 간의 가장 큰 격차가 학생의 잘못된 답변과\n관련 없는 질문에 대한 응답에 있을 수 있음을 시사합니다. 셋째, 단일 라운드 대화 데이터 유형이 제거될 때마다 모든 교수 능력이 하\n락하는 것으로 나타났는데, 이는 서로 다른 교수 능력 간에 결합 효과가 존재함을 시사합니다. 특히, 학생의 _\"정답응답\"_ (즉, \"정답 없\n음\")에 대한 대화가 없는 경우의 _CARA_ 지표는 모든 단일 라운드 데이터가 제거된 경우(\" _대화_ 없음 _S_ \")보다 더 낮습니다. 우리는 그 이\n유가 이 경우 _SocraticLM이_ 여전히 학생의 _\"틀린답변_ \"에 해당하는 대화에 대해 미세 조정되었기 때문이라고 생각합니다. 이로 인해\n모델은 학생의 답변을 틀린 것으로 인식하는 경향이 더 강해집니다. 이러한 현상은 특정 교육 패턴에 대한 과적합을 피하기 위해 다양\n한 유형의 단일 라운드 대화의 균형을 맞출 필요가 있음을 더욱 시사합니다.\n\n\n**능력** **균형** **전략의** **중요성.** 여기서는 문제 해결 데이터와 제4절\n의 세 가지 능력 균형 전략을 훈련 과정에서 배제하여 그 영향\n을 조사한다. 표 2에서, 문제 해결 데이터 없이 미세 조정(\"w/o\n_Problem_ \")할 경우 ChatGLM3 대비 GSM8K/MAWPS 정확도\n가 각각 31.2%/9.7% 낮아진다.\n\n6b. 이는 LLM 사전 훈련에 사용된 데이터와 교육 대화 간의 현\n저한 차이로 인해 매개변수에 극적인 교란이 발생했기 때문일\n수 있습니다. 또한 세 가지 훈련 전략 모두 효과적입니다. 그중\n에서도\n\n\n\n\n\n\n\n\n|전체<br>ACCG ACCM|Col2|\n|---|---|\n|ChatGLM3-6b<br>|0.11<br>0.624<br>0.798|\n|_소크라테스LM_<br>_문제 _없음<br>_분리 _없음<br>|0.62<br>0.606<br>0.814<br>0.58<br>0.312<br>0.701<br>0.54<br>0.159<br>0.646<br><br><br>|\n|표 2:문제 해결 데이터 없<br><br>_지시사항 _없음<br>_혼합 프롬프트 _제외|이 수행한 성능 및 4절의 세 가지 능력 균<br><br>0.02<br>0.320<br>0.625<br>0.56<br>0.605<br>0.804|\n\n\n\n확도를 나타냅니다.\n\n\n\n_분리된훈련/지도튜닝은_ 각각 문제 해결/소크라테스식 교수법에 가장 큰 영향을 미칩니다. _혼합프롬프트설정은_ LLM 사전 훈련 단\n계에서 이미 적용되었을 가능성이 있어 개선 효과가 덜 두드러질 수 있습니다. 또한 _SocraticLM이_ ChatGLM3-6b보다 MAWPS에서\n더 높은 정확도를 달성한다는 점은 주목할 만합니다. 우리는 그 이유가 _SocraTeach_ 데이터셋을 통한 미세 조정 과정에서\n_SocraticLM이_ 단일 문제의 다양한 측면(예: 각 추론 단계와 관련된 지식에 대한 질문)에 대한 여러 학생 질문을 실제로 학습하기 때\n문이라고 추측합니다. 이 과정을 통해 _SocraticLM은_ 문제 해결 과정에 대한 더 깊은 이해를 발전시킬 수 있으며, 이는 다시 문제 해결\n정확도를 향상시킬 수 있습니다.\n\n\n**6.3** **데이터** **규모의** **영향**\n데이터 규모는 대규모 언어 모델 훈련의 효율성과 효과성 모두에 결정적입니다. 이 문제를 조사하기 위해 본 절에서는 _SocraTeach_\n데이터셋 내 다중 대화의 양과 다중 대화와 문제 해결 데이터 간의 비율 _α를_ 변화시킵니다.\n\n\n3\n85701 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n**다중** **라운드** **대화** **규모.** 다양한 데이터 규모의 영향을 연구하기 위해 _SocraTeach_\n_에서_ 25%, 50%, 75%의 다중 라운드 대화를 무작위로 추출하고, DTS 파이프라인\n을 여러 번 실행하여 125% 규모의 대화로 확장한 후 _Socrat-icLM을_ 훈련시켰습\n니다. 그림 4의 결과는 (i) 본 데이터가 다양한 규모에서 효과적일 뿐만 아니라 영\n향력도 있어 LLM의 교육 능력을 크게 향상시킬 수 있음을 나타냅니다. (ii) 데이\n터 양이 증가함에 따라 교육 능력도 비례하여 증가하는 것을 관찰할 수 있습니다.\n\n\n\n0.90\n0.80\n0.70\n0.60\n0.50\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-9-0.png)\n\n\n\n다중 대화 단계 규모\n모델 성능에서 데이터 양의 중요성. 특히, 최소 75%(≈ 26K)의 대화 규모가\n그림 4: 대화 규모 효과.\n\nGPT4의 _종합품질을_ 뛰어넘기 위해서는 로그 데이터가 필요합니다. (iii) 데이터 양이 35K 임계값을 초과하면 포화점에 접근하는 경\n향이 있으며, 이 시점 이후 데이터 양을 더 늘려도 모델 성능 향상에 미치는 증분 효과는 작아집니다. 구체적으로, 125% 데이터 규모\n에서 _IARA_ 지표는 하락세를 보이며, 이러한 포화 상태의 근본 원인이 모델의 잘못된 답변 식별 능력 감소( _전체품질_ 하락은 그에 따\n른 결과)임을 시사합니다. 이는 다중 라운드 대화 데이터가 증가함에 따라 _\"잘못된답변\"_ 에 대한 단일 라운드 대화 데이터의 비율이\n감소하기 때문일 수 있습니다. 다중 대화 데이터 규모가 125%를 초과할 때 이 비율이 특정 임계값 아래로 떨어지면 효과성이 감소\n할 수 있습니다.\n\n**문제** **해결** **데이터의** **규모.** 그림 5는 **문제** **해결** **데이터와** **대화** **데이터** **간의** **비율 α를** **조정함에** 따른 SocraticLM의 성능 변화를 보여줍니다 **. 이** **추세는** **문제**\n**해결** **데이터가** **너무** **적거나** **너무** **많을** **경우** **만족스러운** **문제** **해결** **능력을** **이끌어내지** **못함을** **시사합니다.**\n\n\n\n문제 해결 데이터와 대화 데이터 간의 비율 _α를_ 조정함에 따라 _SocraticLM의_ 성능\n변화가 _나타_ 납니다. 이 추세는 문제 해결 데이터가 너무 적거나 너무 많을 경우 만\n족스러운 문제 해결 능력을 이끌어내지 못함을 시사합니다. 대신 교육 대화 데이\n터와의 균형을 맞출 필요가 있습니다. 실제로 문제 해결 데이터를 과도하게 도입\n하면 GSM8K 정확도가 1.9% 하락할 수 있다. 이는 SocraticLM의 문제 해결 능력\n에 해당하는 파라미터가 교육 대화로 초기 미세 조정한 후 교란을 받을 수 있기 때\n문으로 해석된다. 재훈련 시\n문제 해결 데이터가 있는 경우, 이 능력의 과소적합과 과적합 사이의 미묘한 균형\n을 재조정해야 합니다.\n\n\n**7** **결론**\n0.85\n0.80\n0.75\n0.70\n0.65\n0.60\n0.55\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-9-1.png)\n\n\n\n1 /40 1 /20 1 /1 0 1 /5\n문제 해결 데이터의 비율\n그림 5: 문제 해결 데이터의 효과\n데이터 규모에 따른 문제 해결 효과.\n\n\n\n본 논문에서는 소크라테스식 \"사고를 자극하는\" 맞춤형 교육을 촉진하기 위해 설계된 대규모 언어 모델(LLM)인 _SocraticLM_ 을 소개\n한다. _SocraticLM_ 구축을 위해 우리는 \"학장-교사-학생\" 파이프라인을 제안하여 _SocraTeach_ 데이터셋을 구축했는데, 이는 여섯 가\n지 학생 인지 상태를 시뮬레이션하고 네 가지 핵심 교수 능력을 강화했다. 또한 LLM을 위한 포괄적인 교수 능력 평가 시스템을 개발\n하였다. 실험 결과 _SocraticLM은_ GPT-4 등 기존 LLM을 현저히 능가하는 성능을 보였으며, _SocraTeach_ 데이터셋 내 각 구성 요소\n의 필요성을 검증하였다. 부록 H, I, J에서는 추가 사례, 광범위한 영향, 한계점 및 향후 연구 방향을 논의한다.\n\n\n**감사와** **자금** **지원** **공개**\n본 연구는 중국 국가 핵심 연구 개발 프로그램(No.2021YFF0901005), 중국 국가 자연 과학 기금(No.62477044, 62337001), 안후이성\n핵심 기술 연구 개발 프로그램(No.202423k09020039)의 지원으로 부분적으로 수행되었습니다..\n\n\n**참고문헌**\n[1] 조쉬 아치암, 스티븐 애들러, 산디니 아가르왈, 라마 아마드, 일게 아카야, 플로렌시아 레오니 알레만, 디오고 알메이다, 얀코\n알텐슈미트, 샘 알트만, 시얌 알 아나드카트 외. GPT-4 기술 보고서. _arXiv 사전인쇄본 arXiv:2303.08774_, 2023.\n\n\n[2] 투판 아디구젤, 메흐메트 할둔 카야, 파티흐 큐르샨 칸수. 인공지능으로 교육 혁명 일으키기: ChatGPT의 변혁적 잠재력 탐구.\n\n_현대교육기술_, 2023.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85702\n[3] Geoffrey D Borman, Gina M Hewes, Laura T Overman, and Shelly Brown. 포괄적 학교 개혁과 학업 성취도: 메타분석. _교육_\n_연구리뷰_, 73(2):125–230, 2003.\n\n\n[4] Andrew Caines, Luca Benedetto, Shiva Taslimipoor, Christopher Davis, Yuan Gao, Oeistein Andersen, Zheng Yuan, Mark\nElliott, Russell Moore, Christopher Bryant 외. 언어 교육 및 평가 기술에 대한 대규모 언어 모델의 적용에 관하여. _arXiv 사전_\n_인쇄본 arXiv:2307.08393_, 2023.\n\n\n[5] 이슨 첸, 레이 황, 한신 첸, 위엔시엔 쳉, 리앙이 리. Gptutor: 코드 설명을 위한 ChatGPT 기반 프로그래밍 도구. _국제인공지_\n_능교육학회_, 321–327쪽. Springer, 2023.\n\n\n[6] W ei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez 외. Vicuna: ChatGPT 품질의 90%*를 구현한 오픈소스 채팅봇으로 GPT-4를 압도하다.\n\n_[https://vicuna.lmsys.org (2023년 4월 14일접속)](https://vicuna/)_, 2(3):6, 2023.\n\n\n[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa\nDehghani, Siddhartha Brahma 외. 명령어 미세 조정 언어 모델의 확장. _arXiv 사전인쇄본 arXiv:2210.11416_, 2022.\n\n\n[8] 칼 코베, 비닛 코사라주, 모하마드 바바리안, 마크 천, 준희우, 루카시 카이저, 마티아스 플라퍼트, 제리 트워렉, 제이콥 힐튼,\n나카노 레이이치로 외. 수학 단어 문제 해결을 위한 검증기 훈련. _arXiv 사전인쇄본 arXiv:2110.14168_, 2021.\n\n\n[9] Wei Dai, Jionghao Lin, Hua Jin, Tongguang Li, Yi-Shan Tsai, Dragan Gaševic´, Guanliang Chen. 대규모 언어 모델이 학생\n들에게 피드백을 제공할 수 있을까? ChatGPT 사례 연구. _2023 IEEE 국제첨단학습기술컨퍼런스(ICALT)_, 323–325쪽.\nIEEE, 2023.\n\n\n[10] 단유하오, 레이즈카이, 구이양, 리용, 인장하오, 린자주, 예린하오, 티에즈이안, 저우유겐, 왕이레이 외. 에듀챗: 지능형 교육\n을 위한 대규모 언어 모델 기반 챗봇 시스템. _arXiv 사전인쇄본 arXiv:2308.02773_, 2023.\n\n\n[11] 이안 제이 디어리, 스티브 스트랜드, 폴린 스미스, 크레스 페르난데스. 지능과 교육 성취도. _Intelligence_, 35(1):13–21, 2007.\n\n\n[12] 정샤오 두, 위지에 치엔, 샤오 리우, 밍 딩, 지에종 치우, 지린 양, 지에 탕. GLM: 자동회귀 공백 채우기를 통한 일반 언어 모델\n사전 훈련. _제60회전산언어학회연차대회논문집 (제1권: 장문논문)_, 320–335쪽, 2022.\n\n\n[13] Linda Elder, Richard Paul. 사고, 교수, 학습에서 소크라테스식 질문의 역할. _The Clearing House_, 71(5):297–301, 1998.\n\n\n[14] 에이미 S. 핀, 매튜 A. 크래프트, 마틴 R. 웨스트, 줄리아 A. 레너드, 크리스털 E. 비시, 레베카 E. 마틴, 마거릿 A. 셰리던, 크\n리스토퍼 F.O. 가브리엘리, 존 D.E. 가브리엘리. 인지 능력, 학생 성취도 평가, 그리고 학교. _심리과학_, 25(3):736–744, 2014.\n\n\n[15] 마렌 포르마진, 울리히 슈뢰더스, 올라프 쾰러, 올리버 빌헬름, 한스 베스트마이어. 심리학 전공 학생 선발. _심리학리뷰_,\n2011.\n\n\n[16] W ensheng Gan, Zhenlian Qi, Jiayang Wu, Jerry Chun-Wei Lin. 교육 분야의 대규모 언어 모델: 비전과 기회. _2023 IEEE 국_\n_제빅데이터컨퍼런스(BigData)_, 4776–4785쪽. IEEE, 2023.\n\n\n[17] Arthur C Graesser, Shulan Lu, George Tanner Jackson, Heather Hite Mitchell, Mathew Ventura, Andrew Olney, Max M\nLouwerse. Autotutor: 자연어 대화 기능을 갖춘 튜터. _행동연구방법, 도구및컴퓨터_, 36:180–192, 2004.\n\n\n[18] 한지은, 유하늘, 김윤수, 명준호, 김민선, 임현승, 김주호, 이탁연, 홍화정, 안소연 외. 레시피: EFL 쓰기 교육에 ChatGPT를 통\n합하는 방법. _제10회 ACM 대규모학습컨퍼런스논문집_, 416–420쪽, 2023.\n\n\n3\n85703 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n[19] 에릭 A. 하누셰크, 스티븐 G. 리브킨. 교사 역량 가치 부가 측정 활용에 관한 일반화. _American economic review_,\n100(2):267–271, 2010.\n\n\n[20] 크리스토테아 헤로도투, 바트 리엔티스, 아비나쉬 보로와, 즈데넥 즈드라할, 마틴 홀스타. 고등교육에서의 예측 학습 분석 대\n규모 구현: 교사의 역할과 관점. _교육기술연구및개발_, 67:1273–1306, 2019.\n\n\n[21] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen 외. Lora: 대규모 언어\n모델의 저순위 적응. _국제학습표현학회_, 2021.\n\n\n[22] Liya Hu, Zhiang Dong, Jingyuan Chen, Guifeng Wang, Zhihua Wang, Zhou Zhao, Fei Wu. Ptadisc: 콜드스타트 시나리오에\n서 맞춤형 학습을 지원하는 교차과목 데이터셋. _신경정보처리시스템발전_, 36:44976–44996, 2023.\n\n\n[23] 황전야, 류치, 천위잉, 우러, 샤오켈리, 천엔홍, 마하이핑, 후구오핑. 학습인가 망각인가? 학생의 지식 숙련도 추적을 위한 동적\n접근법. _ACM Transactions on Information Systems (TOIS)_, 38(2):1–33, 2020.\n\n\n[24] 진 제니퍼, 김 미라. 프로그래밍 언어 학습을 위한 GPT 기반 맞춤형 이러닝 시스템. _응용과학_, 13(23):12773, 2023.\n\n\n[25] 타니샤 조시, 제시카 스토크스-패리시, 레이첼 싱글턴, 마이클 토도로비치. 생성형 인공지능 대규모 언어 모델로 강화된 의학\n교육. _Trends in Molecular Medicine_, 2023.\n\n\n[26] 엔켈레이다 카스네치, 카트린 제슬러, 슈테판 퀴헤만, 마리아 바네르트, 다리나 데멘티에바, 프랑크 피셔, 우르스 가서, 게오\n르크 그로, 슈테판 귄네만, 아이케 휄러마이어 외. ChatGPT를 선한 목적으로? 교육 분야 대규모 언어 모델의 기회와 도전. _학_\n_습과개인차_, 103:102274, 2023.\n\n\n[27] 릭 콘첼-케지오르스키, 수브로 로이, 아이다 아미니, 네이트 쿠시먼, 한나네 하지시르지. Mawps: 수학 단어 문제 저장소.\n\n_2016년북미계산언어학회(ACL) 인간언어기술컨퍼런스논문집_, 1152–1157쪽, 2016.\n\n\n[28] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem. Camel: 대규모 언어 모델 사회의 \"마음\" 탐\n색을 위한 의사소통 에이전트. _신경정보처리시스템발전_, 36, 2024.\n\n\n[29] 리칭야오, 푸링웨, 장웨이밍, 천셴위, 위징웨이, 샤웨이, 장웨이난, 탕루이밍, 위용. 교육용 대규모 언어 모델 적용: 기초 역량,\n잠재력 및 과제. _arXiv 사전인쇄본 arXiv:2401.08664_, 2023.\n\n\n[30] 리즈종, 데릭 호이엠. 잊지 않고 학습하기. _IEEE 패턴분석및기계지능거래지_, 40(12):2935–2947, 2017.\n\n\n[31] 진위 린. Rouge: 요약문 자동 평가 패키지. _텍스트요약의분화_, 74–81쪽, 2004.\n\n\n[32] 신 린, 진야 황, 홍커 자오, 엔홍 천, 치 류, 데푸 롄, 신 리, 하오 왕. 수학 단어 문제 해결을 위한 관계 강화 계층적 솔버 학습.\n_IEEE Transactions on Neural Networks and Learning Systems_, 2023.\n\n\n[33] 류자위, 왕페이, 마하이핑, 황젠야, 류치, 천엔홍, 수유. 온라인 학습 시스템에서의 시간적 인지 진단을 위한 확률적 프레임워\n크. _컴퓨터과학및기술저널_, 38(6):1203–1222, 2023.\n\n\n[34] 류지아유, 황젠야, 마즈위안, 류치, 천홍, 수톈황, 류하이펑. 상식적 공식 지식 습득을 통한 수학적 추론 유도. _제29회 ACM_\n_SIGKDD 지식발견및데이터마이닝컨퍼런스논문집_, 1477–1488쪽, 2023.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85704\n[35] 류치, 황젠야, 인위, 천엔홍, 슝후이, 수위, 후궈핑. Ekt: 학생 성과 예측을 위한 연습 인식 지식 추적. _IEEE Transactions on_\n_Knowledge and Data Engineering_, 33(1):100–115, 2019.\n\n\n[36] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: 프롬프트 튜닝은 규모와\n작업 전반에 걸쳐 파인 튜닝과 비교 가능한 성능을 가질 수 있다. _제60회전산언어학회연차대회논문집 (제2권: 단편논문)_ .\n\n전산언어학회, 2022.\n\n\n[37] S h ayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph,\nJason Wei 외. FLAN 컬렉션: 효과적인 지시 튜닝을 위한 데이터 및 방법 설계. _국제기계학습컨퍼런스_, 22631–22648쪽.\nPMLR, 2023.\n\n\n[38] 윤 뤄, 진 양, 판동 멍, 야푸 리, 지에 저우, 위얼 장. 지속적 미세 조정 중 대규모 언어 모델의 재앙적 망각에 관한 실증 연구.\n\n_arXiv 사전인쇄본 arXiv:2308.08747_, 2023.\n\n\n[39] 마하이핑, 왕창첸, 주헝슈, 양샹샹, 장샤오밍, 장싱이. 상호작용하지 않은 연습을 통한 인지 진단 향상: 협업 인식 혼합 샘플링\n접근법. _AAAI 인공지능컨퍼런스논문집_, 제38권, 8877–8885쪽, 2024.\n\n\n[40] Jakub Macina, Nico Daheim, Sankalan Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan.\n\nMathdial: 수학적 추론 문제에 기반한 풍부한 교육학적 특성을 지닌 대화형 튜터링 데이터셋. In _Findings of the Association_\n_for Computational Linguistics: EMNLP 2023_, pages 5602–5621, 2023.\n\n\n[41] _Julia_ M Markel, Steven G Opferman, James A Landay, Chris Piech. Gpteach: GPT 기반 학생들과의 대화형 TA 훈련. _제10_\n_회 ACM 대규모학습컨퍼런스논문집_, 226–236쪽, 2023.\n\n\n[42] 스티븐 무어, 리처드 통, 안잘리 싱, 류 지타오, 후 시앙젠, 루 위, 리앙 졸린, 차오 천, 하산 코스라비, 폴 데니 외. 대규모 언어\n모델로 교육 역량 강화: 차세대 인터페이스 및 콘텐츠 생성. _《인공지능교육국제학회_ 》, 32–37쪽. 스프링거, 2023.\n\n\n[43] 키쇼어 파피네니, 살림 루코스, 토드 워드, 위징 주. Bleu: 기계 번역 자동 평가 방법. _제40회전산언어학회연차대회논문집_,\n311–318쪽, 2002.\n\n\n[44] 박민주, 김소정, 이승현, 권순우, 김규석. 학생 모델링을 활용한 대화 기반 튜터링 시스템을 통한 맞춤형 학습 강화. _arXiv 사_\n_전인쇄본 arXiv:2403.14071_, 2024.\n\n\n[45] Richard Paul, Linda Elder. 비판적 사고: 소크라테스식 질문의 기술. _발달교육저널_, 31(1):36, 2007.\n\n\n[46] 루안 셰리, 장 리웨이, 쉬 저스틴, 탐 브라이스 조쿤, 추 정넝, 주 예샹, 머넌 엘리자베스 L., 브런스킬 엠마, 랜데이 제임스 A.\n\n퀴즈봇: 사실적 지식을 위한 대화 기반 적응형 학습 시스템. _2019 CHI 인간-컴퓨터상호작용학회논문집_, 1–13쪽, 2019.\n\n\n[47] 시위취와 취샤오웨이. 인지 능력이 학업 성취도에 미치는 영향 분석: 자기 모니터링의 조절 역할. _프론티어스인사이콜로지_,\n13:996504, 2022.\n\n\n[48] 아로히 스리바스타바, 데니스 클레이조, 우즈이. 모방 게임을 넘어: 언어 모델의 능력 정량화 및 외삽. _기계학습연구논문집_,\n(5), 2023.\n\n\n[49] Katherine Stasaski, Kimberly Kao, Marti A Hearst. Cima: 교육용 튜터링을 위한 대규모 오픈 액세스 대화 데이터셋. _제15회_\n_교육애플리케이션구축을위한 NLP 혁신적활용워크숍논문집_, 52–64쪽, 2020.\n\n\n3\n85705 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n[50] 쑨젠원, 저우루이, 리앙루샤, 가오루, 리우산뉴야, 리칭, 장카이, 장루루. 앙상블 지식 추적: 학습 과정에서의 상호작용 모델링\n. _Expert Systems with Applications_, 207:117680, 2022.\n\n\n[51] Anaïs Tack, Chris Piech. AI 교사 테스트: 교육 대화에서 Blender와 GPT-3의 교육적 능력 측정. _국제교육데이터마이닝학_\n_회_, 2022.\n\n\n[52] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale 외. Llama 2: 오픈 파운데이션 및 미세 조정된 채팅 모델. _arXiv 사전인쇄본_\n_arXiv:2307.09288_, 2023.\n\n\n[53] 왕페이, 류치, 천엔홍, 황젠야, 인위, 왕시진, 수위. NeuralCD: 인지 진단을 위한 일반 프레임워크. _IEEE Transactions on_\n_Knowledge and Data Engineering_, 35(8):8312–8327, 2022.\n\n\n[54] 왕 롱위, 류 천양, 지 톈보, 장 지루이, 위 디안, 시 슈밍, 투 자오펑. 대규모 언어 모델을 활용한 문서 수준 기계 번역. _2023년자_\n_연어처리경험적방법론컨퍼런스_, 2023.\n\n\n[55] 제이슨 웨이, 마르텐 보스마, 빈센트 자오, 켈빈 구, 애덤스 웨이 유, 브라이언 레스터, 난 두, 앤드류 M. 다이, 콴 V. 레. 미세\n조정된 언어 모델은 제로샷 학습자이다. _국제표현학습컨퍼런스_, 2021.\n\n\n[56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou 외. 사고의 사슬 프\n롬프팅은 대규모 언어 모델에서 추론을 유도한다. _신경정보처리시스템발전_, 35:24824–24837, 2022.\n\n\n[57] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 사고의 나무: 대규모\n언어 모델을 활용한 의도적 문제 해결. _신경정보처리시스템발전_, 36, 2024.\n\n\n[58] 장량, 린종하오, 광즈이, 쉬셩, 모하메드 예신, 후시안겐. SPL: 대규모 언어 모델 기반 학습을 위한 소크라테스식 학습 플랫폼.\n\n_arXiv 사전인쇄본 arXiv:2406.13919_, 2024.\n\n\n[59] 웨인 신 자오, 쿤 저우, 준이 리, 티안이 탕, 샤오레이 왕, 유펑 후, 잉치안 민, 베이천 장, 준지에 장, 지첸 동 외. 대규모 언어 모\n델에 대한 조사. _arXiv 사전인쇄본 arXiv:2303.18223_, 2023.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85706\n**A** **문제** **분해에** **관한** **추가** **세부** **사항**\nGSM8K 원시 데이터에는 각 문제에 대한 분해가 제공되었으며, 이는 우리의 요구 사항을 대체로 충족하는 것으로 확인되었습니다.\n\n따라서 우리는 A.1의 프롬프트를 사용하여 GPT4로 MAWPS의 문제를 분해합니다.\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-14-0.png)\n\n\n\n**B** **\"학장-교사-학생\" 파이프라인** **프롬프트**\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-14-1.png)\n\n\n\n\n\n\n\n3\n85707 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-15-0.png)\n\n\n\n표 단계가 있습니다.} #대화: {여기 목표 대화가 있습\n니다.}\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-15-1.png)\n\n\n\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85708\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-16-0.png)\n\n\n\n**C** **교수** **능력** **향상에** **관한** **추가** **정보**\n\" _무관한_ \" 학생 응답을 위한 단일 라운드 대화를 구축하기 위해, 먼저 DTS 파이프라인으로 구성된 _DiaM_ 에서 2 _,_ 000개의 대화를 무작\n위로 선택합니다. 각 대화에서 학생 응답의 한 라운드를 무작위로 선택하여 MOOC에서 수집한 200개의 질문 중 무작위로 선택한\n질문으로 대체합니다. 그런 다음 _교장_ 감독 하에 _교사_ 에이전트가 질문에 답변을 거부하도록 하여, 최종적으로 2 _,_ 000개의 단일 라\n운드 _학생-교사_ 대화를 형성합니다.\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-16-1.png)\n\n\n\n\n\n\" _잘못된답에_ 의존\"을 위한 다섯 가지 오답을 구성하기 위해, 한편으로는 학생의 답변에서 모든 숫자(예: \"2\")와 연산자(예: \"+\")를 식\n별하고, 그 안에 무작위로 변동을 도입합니다.\n\n\n3\n85709 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-17-0.png)\n\n\n\n|문제 수 11,147<br>다중 라운드 대화 수 35,151<br>M<br>단일 라운드 대화 수(Dia 22,000<br>S)<br>단일 라운드 대화 총 개수 207,581|Col2|\n|---|---|\n|최대 /최소 라운드 수<br>|12 / 3|\n|단계별 안내 질문 평균 개수<br>_평균 _라운드 수_M_<br>학생 응답 평균 길이<br>교사 응답 평균 길이|3.29<br>5.28<br>16.4<br>30.3|\n\n\n표 3: _SocraTeach_ 데이터셋 통계.\n\n\n10에서 한 숫자로 범위를 조정하거나 임의로 연산자를 다른 연산자로 대체합니다. 이러한 규칙을 적용하여 두 개의 새로운 학생 응\n답을 생성합니다. 한편, GPT4를 사용하여 원본 학생 응답을 재작성하고 C.2의 프롬프트를 활용해 세 개의 오답을 생성합니다. 이 다\n섯 개의 응답을 바탕으로 _교사_ 응답을 유도하여 10,000건의 _학생-교사_ 대화를 확보합니다.\n\n\n**D** _**SocraTeach**_ **데이터셋** **통계**\n제3.5절에서 언급한 바와 같이, 본 연구의 _SocraTeach_ 데이터셋은 35,000건의 다중 라운드 대화 _(DiaM)_ 와 22,000건의 단일 라운드 대\n화( _DiaS_ )로 구성됩니다. _DiaM_ 의 평균 대화 라운드 수는 5 _._ 28회입니다. 학생/교사의 응답은 평균적으로 각각 16 _._ 4단어/30.3단어를 포\n함합니다. 전체 통계는 표 3에 요약되어 있습니다. 또한 그림 6(a)에서 볼 수 있듯이, 대부분의 교수 대화는 5~6라운드로 구성됩니다.\n\n그림 6(b)에서는 _SocraTeach_ 내 학생 인지 상태의 확률 질량 함수를 시각화하였다. 여기서 (1)-(5)는 제안된 학생 인지 시스템(3.3절)\n의 차원 중 하나에서 취약한 학생 프로파일을 나타내며, (6)은 모든 차원에서 우수한 상태를 가진 학생을 나타낸다. _학생_ 에이전트는\n모든 측면에서 우수한 학생을 시뮬레이션하거나, 계산 능력(즉, \"(3)\") 또는 지식 숙달도(즉, \"(4)\")에 문제가 있는 학생을 시뮬레이션\n하는 경향이 있음을 알 수 있습니다.\n\n\n\n8000\n0.30\n6000\n0.20\n5000\n4000\n3000\n2000\n1 000\n0\n0.1 5\n0.1 0\n0.05\n0.00\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||5|6<br>|6<br>|6<br>|6<br>|6<br>|6<br>|6<br>|6<br>|6<br>|6<br>|6<br>|6<br>|6<br>|6<br>|\n||||||||||||||||||||\n|||||||7<br>|7<br>|7<br>|7<br>|7<br>|7<br>|7<br>|7<br>|7<br>|7<br>|7<br>|7<br>|7<br>|\n|||4|4|4|4|4|4||||||||||||\n|||4|4|4|4|4|4|8|8|8|8|8|8|8|8|8|8|8|\n||||||||||||||||||||\n||||||||||||||||||||\n|||||||||||9|9|9|9|9|9|9|9|9|\n|3|3|3|3|3|3|3|3|3|3|3|3||||||||\n|3|3|3|3|3|3|3|3|3|3|3|3|1 0|1 0|1 0|1 0|1 0|1 0|1 0|\n||||||||||||||||||||\n|||||||||||||||11|11|11|11|11|\n||||||||||||||||||||\n|||||||||||||||||1 2|1 2|1 2|\n||||||||||||||||||||\n\n\n\n(a)\n그림 6: 라운드 수(a) 및 학생 인지 상태(b)의 분포.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85710\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-17-1.png)\n\n(1) (2) (3) (4) (5) (6)\n학생 유형 인지 상태\n(b)\n**E** **지시** **조정** **템플릿**\n그림 7은 대화 데이터와 문제 해결 데이터를 가르치기 위한 지시 템플릿을 보여줍니다.\n\n\n문제 **대화** **데이터** **교육용** **템플릿**\n나탈리아는 4월에 친구 48명에게 클립을 팔았고, 5월\n에는 그 절반만큼 팔았습니다. 나탈리아가 4월과 5월에\n모두 몇 개의 클립을 팔았나요?\n\n\n답변 분석\n나탈리아는 5월에 48/2 = 24개의 클립을 판\n매했습니다. 나탈리아는 4월과 5월 합쳐서\n당신은 소크라테스식 교사입니다. 다음 정보를 바탕으로 휴리스틱 질\n문을 통해 문제를 해결하도록 안내해 주세요. [문제] <문제> [답변] <\n답변> [분석] <분석>\n**문제** **해결** **데이터** **템플릿**\n72\n다음 문제를 단계별로 분석하고 해결하십시오: <문제>\n48+24 = 72개의 클립을 판매했습니다.\n\n그림 7: 지시문 조정용 템플릿.\n\n\n\n**F** **교육** **품질에** **대한** **인간** **평가**\n다양한 대규모 언어 모델(LLM)의 _전반적인품질을_ 평가하기 위해, 우리는 10명의 고학력 어노테이터를 모집하여 각 모델이 제공한 지시문을 GPT\n4와 비교하여 순위를 매기도록 했습니다. 그림 8은 해당 템플릿을 보여줍니다.\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-18-0.png)\n\n\n\n\n\n3\n85711 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n그림 8: _전반적품질_ 에 대한 인간 평가 템플릿.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85712\n**G** **구현** **세부** **사항**\n우리의 _SocraticLM_ 은 ChatGLM3-6b(ChatGLM3-6b-Base 아님)를 학습률 0 _._ 02, 배치 크기 64로 2 에포크 동안 P-Tuning[36]하여\n얻었습니다. 데이터셋 분할은 5절에서 설명한 바와 같이, 객관적 교수 능력( _IARA, CARA, SER, SRR_ ) 평가용 단일 라운드 대화 400\n개와 주관적 _전반적품질_ 평가용 다중 라운드 대화 1 _,_ 000개로 구성된 테스트 세트로 이루어집니다. _SocraTeach의_ 나머지 데이터 중\n10%/90%는 검증/훈련에 사용됩니다. 모든 실험은 NVIDIA RTX 3090 GPU 6개를 탑재한 서버에서 수행되었습니다. 데이터셋과\n코드는 `[https://github.com/Ljyustc/SocraticLM](https://github.com/Ljyustc/SocraticLM)` 에서 확인할 수 있습니다.\n\n\n**H** **추가** **모델** **샘플**\n다음은 학생의 \" _잘못된답변_ \", \" _올바른답변_ \", _\"질문형_ 답변\", \" _관련없는_ 답변\"에 대한 SocraticLM과 GPT4의 네 가지 샘플입니다.\n\n첫 번째 예시에서 SocraticLM과 GPT4 모두 학생의 오류를 식별하지만, GPT4는 \"7\"이라는 정답을 직접 제시하여 소크라테스식 교\n육의 요구사항과 상충됩니다. 두 번째 예시에서 GPT4는 거의 완성된 해결 과정을 제시하지만, 단계별 소크라테스식 사고 지도 요\n건을 충족하지 못합니다. 세 번째 예시에서 GPT4의 표현은 더 딱딱하며 실제 인간 교사의 느낌을 결여합니다. 네 번째 예시에서\n_SocraticLM은_ 학생이 먼저 수학 문제를 풀도록 유도하는 반면, GPT4는 관련 없는 질문에 답변하는 방향으로 전환됩니다.\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-20-0.png)\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-20-1.png)\n\n\n\n3\n85713 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-21-0.png)\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-21-1.png)\n\n\n\n![](assets/md_images/소크라틱-AI-SocraticLM--Exploring-Socratic-Personalized-Teaching-with-Large-Language-Models-ko.pdf-21-2.png)\n\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85714\n**I** **더** **넓은** **영향력**\n본 논문에서는 지능형 교육에서 핵심적인 교수법인 소크라테스식 교수법을 구현하기 위해 대규모 언어 모델인 _SocraticLM을_ 제안\n한다. 이는 일상생활에 상당한 영향을 미친다. 한편, 3.3절과 3.4절에서 설명한 바와 같이, 본 _SocraticLM은_ 다양한 인지 상태와 다양\n한 유형의 응답을 가진 학생들에게 지침을 제공할 수 있다. 따라서 교실 및 온라인 플랫폼에서 다중 맞춤형 애플리케이션을 지원하\n며, 다양한 배경을 가진 학생들에게 고품질 지도를 보다 효율적이고 편리하게 전달함으로써 교육적 형평성과 신속한 발전을 촉진할\n수 있다. 다른 한편으로, 본 연구에서는 실제 인간 교사나 학생의 개입 없이 GPT4를 활용해 교사와 학생 간의 대화를 시뮬레이션하여\n대화 데이터를 생성한다. 이러한 접근 방식은 실제 인간과의 상호작용이나 테스트 없이 데이터를 수집할 수 있는 방법을 제공하여\n인간 상호작용의 부담을 줄이는 동시에 개인정보 보호, 보안 및 기타 윤리적 문제를 피할 수 있습니다. 그러나 본 모델은 다중 라운\n드 교육 대화 데이터로 훈련되어야 하므로, 더 많은 과목(예: 물리학)으로 확장하기 위해서는 더 많은 교육 자원(예: 교과서 문제)과\n훈련 비용이 필요할 수 있습니다.\n\n\n**J** **제한** **사항** **및** **향후** **연구** **과제**\n첫째, 표 1에서 SocraticLM과 GPT4의 _SER_ 지표는 각각 0 _._ 74와 0 _._ 65입니다. 이는 현재 모델들이 실제적이고 복잡한 학생 질문에 대\n응하는 능력에 개선의 여지가 있음을 보여줍니다. 둘째, 본 논문에서는 수학 문제 교육에 초점을 맞췄습니다. 다른 과목의 경우 추가\n적인 데이터 구축 및 훈련 과정이 필요합니다. 셋째, 본 논문의 문제 해결 능력 테스트셋은 소크라테스식 교육 대화의 기반이 되는 문\n제들(즉, GSM8K 및 MAWPS)입니다. 추론 능력의 변화를 보다 정확하게 평가하기 위해 추가 데이터셋을 테스트하고 더 많은 과제\n에 대한 능력 균형 훈련 전략을 탐구할 예정입니다. 마지막으로, 본 논문에서는 _소크라테스식대화_ 모델( _SocraticLM_ ) 구축의 기반으\n로 오픈소스 대규모 언어 모델인 ChatGLM3-6b를 사용합니다. 이는 미세 조정이 용이하고 사전 교육 기능으로 사전 훈련되지 않아,\n본 연구의 _SocraTeach_ 데이터셋 효과를 검증하는 데 더 적합하기 때문입니다. 향후에는 본 데이터셋을 활용해 더 많은 대규모 언어\n모델을 미세 조정하고, 교육 및 지능형 교육 분야에서의 잠재력을 탐구할 계획입니다.\n\n\n3\n85715 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n**NeurIPS 논문** **체크리스트**\n\n\n1. **주장**\n질문: 초록과 서론에서 제시된 주요 주장이 논문의 기여도와 범위를 정확히 반영하고 있습니까?\n\n\n답변: [예]\n근거: 본 논문의 범위를 명확히 설명하고 기여 사항을 열거하였습니다. 지침:\n\n\n               - '해당 없음(NA)' 답변은 초록과 서론에 논문에서 주장하는 내용이 포함되어 있지 않음을 의미합니다.\n\n               - 초록 및/또는 서론에는 논문에서 제시된 주장(기여도, 중요한 가정 및 한계 포함)이 명확히 명시되어야 합니다. 이\n질문에 '아니오' 또는 '해당 없음'으로 답변할 경우 심사위원들에게 부정적으로 인식될 수 있습니다.\n\n               - 주장은 이론적 및 실험적 결과와 일치해야 하며, 결과가 다른 환경으로 얼마나 일반화될 수 있는지 반영해야 합니\n다.\n\n               - 논문에서 달성되지 않은 목표임을 명확히 밝힌다면, 동기부여를 위한 포부적인 목표를 포함하는 것은 괜찮습니\n다.\n\n\n2. **제한** **사항**\n질문: 본 논문은 저자들이 수행한 연구의 한계를 논의하고 있습니까? 답변: [예]\n근거: 부록 J에 \"제한사항 및 향후 연구\" 섹션을 마련하여 연구의 한계점을 논의하였습니다.\n\n\n지침:\n\n\n               - 답변 NA는 해당 논문에 한계점이 없음을 의미하며, 답변 No는 해당 논문에 한계점이 있으나 논문 내에서 논의되\n지 않았음을 의미합니다.\n\n               - 저자들은 논문 내에 별도의 \"제한사항\" 섹션을 작성할 것을 권장합니다.\n\n               - 본 논문은 강력한 가정과 이러한 가정이 위반될 경우 결과의 견고성(예: 독립성 가정, 노이즈 없는 설정, 모델의 정\n확한 사양, 국소적으로만 성립하는 점근적 근사)을 지적해야 합니다. 저자는 이러한 가정이 실제 상황에서 어떻게\n위반될 수 있으며 그 영향이 무엇인지에 대해 고찰해야 합니다.\n\n               - 저자는 주장 범위에 대해 고려해야 합니다. 예를 들어 접근법이 소수 데이터셋이나 제한된 실행 횟수로만 검증된\n경우 등입니다. 일반적으로 경험적 결과는 암묵적 가정에 의존하는 경우가 많으며, 이는 명시되어야 합니다.\n\n               - 저자들은 접근법의 성능에 영향을 미치는 요인들을 고려해야 합니다. 예를 들어, 얼굴 인식 알고리즘은 이미지 해상\n도가 낮거나 저조도 환경에서 촬영된 이미지에서는 성능이 저하될 수 있습니다. 또는 음성-텍스트 변환 시스템은\n기술 용어를 처리하지 못해 온라인 강의의 자막 제공에 안정적으로 활용되지 못할 수 있습니다.\n\n               - 저자들은 제안된 알고리즘의 계산 효율성과 데이터셋 규모에 따른 확장성을 논의해야 합니다.\n\n               - 해당되는 경우, 저자들은 개인정보 보호 및 공정성 문제 해결을 위한 접근법의 잠재적 한계를 논의해야 한다.\n\n               - 저자들은 한계에 대한 완전한 정직성이 심사위원들에 의해 거절 사유로 사용될까 두려워할 수 있지만, 더 나쁜 결\n과는 심사위원들이 논문에서 인정되지 않은 한계점을 발견하는 것일 수 있습니다. 저자들은 최선의 판단을 내리\n고, 투명성을 위한 개인의 행동이 공동체의 무결성을 보존하는 규범을 발전시키는 데 중요한 역할을 한다는 점을\n인식해야 합니다. 심사위원들은 한계에 관한 정직성을 불이익으로 삼지 않도록 특별히 지시받을 것입니다.\n\n\n3. **이론적** **가정과** **증명**\n질문: 각 이론적 결과에 대해 논문은 완전한 가정 집합과 완전하고(올바른) 증명을 제공합니까?\n\n\n답변: [해당 없음]\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85716\n정당화: 본 논문은 이론적 결과를 포함하지 않습니다. 지침:\n\n\n    - NA 답변은 논문에 이론적 결과가 포함되지 않음을 의미합니다.\n\n    - 논문의 모든 정리, 공식 및 증명은 번호가 매겨져 상호 참조되어야 합니다.\n\n    - 모든 가정은 정리의 진술에서 명확히 명시되거나 참조되어야 합니다.\n\n    - 증명은 본문에 포함하거나 보충 자료에 수록할 수 있으나, 보충 자료에 수록할 경우 저자들은 직관을 제공하기 위\n한 간략한 증명 개요를 제시할 것을 권장합니다.\n\n    - 반대로, 논문 본문에 제시된 비형식적 증명은 부록이나 보충 자료에 제시된 형식적 증명으로 보완되어야 한다.\n\n    - 증명에 의존하는 정리와 보조정리는 적절히 인용되어야 한다.\n\n\n4. **실험** **결과** **재현성**\n질문: 본 논문의 주요 실험 결과를 재현하는 데 필요한 모든 정보가, 논문의 주요 주장 및/또는 결론에 영향을 미칠 정도로\n충분히 공개되어 있습니까(코드 및 데이터 제공 여부와 무관하게)?\n\n\n답변: [예]\n정당화: 본 논문에서는 데이터셋 구축 과정을 상세히 기술하며, 본문에 사용된 모든 프롬프트는 부록 A, B, C, E에, 실험\n결과 재현에 필요한 설정은 부록 G에 수록하였다.\n\n\n지침:\n\n\n    - '해당 없음(NA)' 답변은 논문에 실험이 포함되지 않음을 의미합니다.\n\n    - 논문에 실험이 포함된 경우, 이 질문에 '아니오'라고 답하면 심사위원들에게 좋게 받아들여지지 않을 수 있습니다\n: 코드와 데이터 제공 여부와 관계없이 논문의 재현 가능성을 확보하는 것은 중요합니다.\n\n    - 기여 내용이 데이터셋 및/또는 모델인 경우, 저자는 결과를 재현 가능하거나 검증 가능하게 만들기 위해 취한 단계\n를 설명해야 합니다.\n\n    - 기여 내용에 따라 재현성은 다양한 방식으로 달성될 수 있습니다. 예를 들어, 기여가 새로운 아키텍처라면 아키텍\n처를 완전히 설명하는 것으로 충분할 수 있으며, 기여가 특정 모델과 경험적 평가라면 다른 연구자가 동일한 데이\n터셋으로 모델을 재현할 수 있도록 하거나 모델에 대한 접근 권한을 제공해야 할 수 있습니다. 일반적으로 코드\n와 데이터를 공개하는 것이 이를 달성하는 좋은 방법 중 하나이지만, 결과 재현 방법에 대한 상세한 지침 제공, 호\n스팅된 모델 접근(예: 대규모 언어 모델의 경우), 모델 체크포인트 공개, 수행된 연구에 적합한 기타 수단을 통해\n서도 재현성을 제공할 수 있습니다.\n\n    - NeurIPS는 코드 공개를 의무화하지 않지만, 모든 제출물에 재현성을 위한 합리적인 방법을 제공할 것을 요구합니\n다. 이는 기여의 성격에 따라 달라질 수 있습니다. 예를 들어\n(a) 기여가 주로 새로운 알고리즘인 경우, 논문은 해당 알고리즘을 재현하는 방법을 명확히 제시해야 합니다.\n(b) 기여가 주로 새로운 모델 아키텍처인 경우, 논문은 아키텍처를 명확하고 완전하게 설명해야 합니다.\n\n(c) 기여물이 새로운 모델(예: 대규모 언어 모델)인 경우, 결과를 재현하기 위해 해당 모델에 접근할 수 있는 방법\n이 있거나 모델을 재현할 수 있는 방법(예: 오픈소스 데이터셋 또는 데이터셋 구축 방법에 대한 지침)이 있어\n야 합니다.\n(d) 재현성이 어려운 경우가 있을 수 있음을 인지하며, 해당 경우 저자들은 재현성을 보장하기 위한 구체적인 방\n법을 기술할 수 있습니다. 폐쇄형 소스 모델의 경우 모델 접근이 제한될 수 있으나(예: 등록된 사용자로 한정\n), 다른 연구자들이 결과를 재현하거나 검증할 수 있는 방법이 제공되어야 합니다.\n\n\n5. **데이터** **및** **코드의** **공개** **접근성**\n3\n85717 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n질문: 본 논문은 보충 자료에 설명된 대로 주요 실험 결과를 충실히 재현할 수 있는 충분한 지침과 함께 데이터 및 코드에\n대한 공개 접근을 제공합니까?\n\n\n답변: [예]\n근거: 본 연구의 데이터셋과 코드는 `[https://github.com/Ljyustc/SocraticLM](https://github.com/Ljyustc/SocraticLM)` 에서 확인할 수 있습니다.\n\n\n지침:\n\n\n               - NA라는 답변은 해당 논문에 코드가 필요한 실험이 포함되어 있지 않음을 의미합니다.\n\n               - 자세한 내용은 NeurIPS 코드 및 데이터 제출 가이드라인\n( `[https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)` )을 참조하십시오.\n\n               - 코드 및 데이터 공개를 권장하지만, 불가능한 경우도 있음을 이해하므로 \"아니오\"도 허용되는 답변입니다. 기여\n의 핵심이 아닌 한(예: 새로운 오픈소스 벤치마크의 경우), 단순히 코드가 포함되지 않았다는 이유로 논문이 거부\n되지는 않습니다.\n\n               - 지침에는 결과를 재현하기 위해 실행해야 하는 정확한 명령어와 환경이 포함되어야 합니다. 자세한 내용은\nNeurIPS 코드 및 데이터 제출 가이드라인( `[https:](https://nips.cc/public/guides/CodeSubmissionPolicy)`\n`[//nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)` )를 참조하십시오.\n\n               - 저자는 원시 데이터, 전처리된 데이터, 중간 데이터, 생성된 데이터 등에 접근하는 방법을 포함한 데이터 접근 및\n준비에 대한 지침을 제공해야 합니다.\n\n               - 저자들은 제안된 새로운 방법과 기준선(baselines)에 대한 모든 실험 결과를 재현할 수 있는 스크립트를 제공해야\n합니다. 일부 실험만 재현 가능한 경우, 스크립트에서 누락된 실험과 그 사유를 명시해야 합니다.\n\n               - 제출 시 익명성을 유지하기 위해 저자는 익명화된 버전(해당되는 경우)을 공개해야 합니다.\n\n               - 보충 자료(논문에 첨부)에 가능한 한 많은 정보를 제공하는 것이 권장되나, 데이터 및 코드에 대한 URL을 포함하\n는 것도 허용됩니다.\n\n\n6. **실험** **설정/세부** **사항**\n질문: 논문은 결과를 이해하는 데 필요한 모든 훈련 및 테스트 세부 사항(예: 데이터 분할, 하이퍼파라미터, 선택 방법, 최\n적화기 유형 등)을 명시하고 있습니까?\n\n\n답변: [예]\n근거: 부록 G에서 데이터셋 분할, 하이퍼파라미터, 훈련 방법 및 GPU 장치를 명확히 기술하였습니다.\n\n\n지침:\n\n\n               - '해당 없음(NA)' 답변은 논문에 실험이 포함되지 않음을 의미합니다.\n\n               - 실험 설정은 결과를 이해하고 해석하는 데 필요한 수준의 세부 사항으로 논문 본문에 제시되어야 합니다.\n\n               - 전체 세부 사항은 코드와 함께, 부록에, 또는 보충 자료로 제공될 수 있습니다.\n\n\n7. **실험의** **통계적** **유의성**\n질문: 논문은 실험의 통계적 유의성에 대해 적절하고 정확하게 정의된 오차 막대 또는 기타 적절한 정보를 보고하고 있습\n니까?\n\n\n답변: [예]\n근거: 본 논문은 인간 주석가들의 소크라테스식 교수법의 질을 평가합니다. 주석가들의 일관성을 보장하기 위해 카파\n점수를 계산한 결과 0 _._ 70으로, 이는 우리 결과의 신뢰성을 보장합니다.\n\n\n지침:\n\n\n               - NA라는 답변은 해당 논문에 실험이 포함되어 있지 않음을 의미합니다.\n\n               - 저자들은 논문의 주요 주장을 뒷받침하는 실험에 대해 최소한 오차 막대, 신뢰 구간 또는 통계적 유의성 검증이 결\n과에 동반된 경우 \"예\"라고 답변해야 합니다.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85718\n\n\n    - 오차 막대가 포착하는 변동성 요인은 명확히 명시되어야 합니다(예: 훈련/테스트 분할, 초기화, 특정 매개변수의\n무작위 추출, 또는 주어진 실험 조건에서의 전체 실행).\n\n    - 오차 막대 계산 방법을 설명해야 합니다(폐쇄형 공식, 라이브러리 함수 호출, 부트스트랩 등).\n\n    - 가정된 조건을 명시해야 합니다(예: 정규 분포된 오차).\n\n    - 오차 막대가 평균의 표준편차인지 표준오차인지 명확히 밝혀야 합니다.\n\n    - 1-시그마 오차 막대를 보고하는 것은 가능하나, 이를 명시해야 합니다. 오차의 정규성 가정이 검증되지 않은 경우\n, 저자들은 96% 신뢰구간을 사용한다고 언급하기보다는 2-시그마 오차 막대를 보고하는 것이 바람직합니다.\n\n    - 비대칭 분포의 경우, 표나 그림에 대칭 오차 막대를 표시하여 결과 범위를 벗어나는 결과(예: 음수 오류율)가 나오\n지 않도록 주의해야 합니다.\n\n    - 오차 막대가 표나 플롯에 보고된 경우, 저자는 본문에서 계산 방법을 설명하고 해당 그림이나 표를 인용해야 합니\n다.\n\n\n8. **실험** **컴퓨팅** **자원**\n질문: 각 실험에 대해, 논문이 실험 재현에 필요한 컴퓨터 자원(컴퓨팅 워커 유형, 메모리, 실행 시간)에 대한 충분한 정\n보를 제공하고 있습니까?\n\n\n답변: [예]\n근거: 부록 G에 계산 자원을 제시합니다. 지침:\n\n\n    - '해당 없음(NA)'은 논문에 실험이 포함되지 않음을 의미합니다.\n\n    - 논문은 CPU 또는 GPU, 내부 클러스터, 클라우드 제공업체 등 컴퓨팅 워커의 유형과 관련 메모리 및 스토리지를\n명시해야 합니다.\n\n    - 논문은 개별 실험 실행마다 필요한 컴퓨팅 양과 총 컴퓨팅 추정량을 제공해야 합니다.\n\n    - 본 논문은 전체 연구 프로젝트가 논문에서 보고된 실험(예: 논문에 포함되지 않은 예비 실험 또는 실패한 실험)보\n다 더 많은 컴퓨팅 자원을 필요로 했는지 여부를 공개해야 한다.\n\n\n9. **윤리** **강령**\n질문: 본 논문에서 수행된 연구는 모든 측면에서 NeurIPS 윤리 강령\n( `[https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)` )을 [준수합니까?](https://neurips.cc/public/EthicsGuidelines)\n답변: [예]\n근거: 본 논문은 NeurIPS 윤리 강령을 준수합니다. 지침:\n\n\n    - '해당 없음(NA)' 답변은 저자가 NeurIPS 윤리 강령을 검토하지 않았음을 의미합니다.\n\n    - 저자가 '아니오'라고 답변할 경우, 윤리 강령에서 벗어날 수밖에 없는 특별한 사정을 설명해야 합니다.\n\n    - 저자는 익명성을 반드시 유지해야 합니다(예: 관할 지역의 법률이나 규정으로 인해 특별한 고려 사항이 있는 경우\n).\n\n10. **광범위한** **영향**\n질문: 본 논문은 수행된 연구의 잠재적 긍정적 사회적 영향과 부정적 사회적 영향 모두를 논의하고 있습니까?\n\n\n답변: [예]\n근거: 부록 I에 \"경계 영향\" 섹션을 제시하여 본 연구의 잠재적 긍정적/부정적 사회적 영향을 논의합니다.\n\n\n지침:\n\n\n    - '해당 없음(NA)' 답변은 수행된 작업이 사회적 영향이 없음을 의미합니다.\n\n\n3\n85719 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n\n\n               - 저자가 NA 또는 No로 답변한 경우, 해당 연구가 사회적 영향을 미치지 않는 이유 또는 논문이 사회적 영향을 다\n루지 않는 이유를 설명해야 합니다.\n\n               - 부정적 사회적 영향의 예로는 잠재적 악의적 또는 의도치 않은 사용(예: 허위 정보 유포, 가짜 프로필 생성, 감시),\n공정성 고려 사항(예: 특정 집단에 불공정하게 영향을 미칠 수 있는 기술의 배포), 개인정보 보호 고려 사항, 보안 고\n려 사항 등이 있습니다.\n\n               - 본 컨퍼런스는 많은 논문이 특정 응용 분야와 무관한 기초 연구일 것으로 예상하며, 실제 적용은 더더욱 고려 대\n상이 아닙니다. 다만 부정적 응용으로의 직접적인 경로가 존재할 경우 저자들은 이를 명시해야 합니다. 예를 들\n어, 생성 모델의 품질 향상이 허위 정보 유포를 위한 딥페이크 제작에 악용될 수 있다는 점을 지적하는 것은 타당\n합니다. 반면, 신경망 최적화를 위한 일반적인 알고리즘이 딥페이크 생성 모델의 훈련 속도를 높일 수 있다는 점\n을 굳이 지적할 필요는 없습니다.\n\n               - 저자들은 기술이 의도된 대로 사용되고 정상적으로 작동할 때 발생할 수 있는 잠재적 피해, 기술이 의도된 대로\n사용되지만 잘못된 결과를 제공할 때 발생할 수 있는 피해, 그리고 기술의 (의도적이든 아니든) 오용으로 인한 피\n해를 고려해야 합니다.\n\n               - 부정적인 사회적 영향이 존재할 경우, 저자들은 가능한 완화 전략(예: 모델의 제한적 공개, 공격 기능 외 방어 기능\n제공, 오용 모니터링 메커니즘, 시스템의 시간 경과에 따른 피드백 학습 모니터링 메커니즘, 머신러닝의 효율성\n및 접근성 개선 등)에 대해서도 논의할 수 있다.\n\n\n11. **안전** **장치**\n질문: 논문은 오용 위험이 높은 데이터나 모델(예: 사전 훈련된 언어 모델, 이미지 생성기, 스크랩된 데이터셋)의 책임 있\n는 공개를 위해 마련된 안전 장치를 설명하고 있습니까?\n\n\n답변: [해당 없음]\n근거: 본 논문은 그러한 위험을 제기하지 않습니다. 지침:\n\n\n               - '해당 없음(NA)' 답변은 해당 논문이 그러한 위험을 제기하지 않음을 의미합니다.\n\n               - 오용 또는 이중 사용 위험이 높은 공개 모델은 사용자가 모델 접근 시 사용 지침이나 제한 사항을 준수하도록 요구\n하거나 안전 필터를 구현하는 등 모델의 통제된 사용을 허용하는 필수 안전 장치를 함께 제공해야 합니다.\n\n               - 인터넷에서 수집된 데이터셋은 안전 위험을 초래할 수 있습니다. 저자들은 안전하지 않은 이미지 공개를 어떻게\n회피했는지 설명해야 합니다.\n\n               - 효과적인 안전장치 마련이 어렵다는 점을 인지하며, 많은 논문에서 이를 요구하지 않지만 저자들이 이를 고려하\n고 성실히 노력할 것을 권장합니다.\n\n\n12. **기존** **자산에** **대한** **라이선스**\n질문: 논문에서 사용된 자산(예: 코드, 데이터, 모델)의 창작자 또는 원 소유자가 적절히 표기되었으며, 라이선스와 이용\n약관이 명시적으로 언급되고 적절히 준수되었습니까?\n\n\n답변: [예]\n근거: 본 논문에 사용된 데이터 및 기준 모델을 포함한 자산들은 적절히 인용되었습니다.\n\n\n지침:\n\n\n               - 답변 NA는 논문이 기존 자산을 사용하지 않음을 의미합니다.\n\n               - 저자는 코드 패키지 또는 데이터셋을 생성한 원본 논문을 인용해야 합니다.\n\n               - 저자는 사용한 자산의 버전을 명시하고 가능하면 URL을 포함해야 합니다.\n\n               - 각 자산에 대해 라이선스명(예: CC-BY 4.0)을 포함해야 합니다.\n\n               - 특정 출처(예: 웹사이트)에서 스크랩한 데이터의 경우 해당 출처의 저작권 및 이용 약관을 제공해야 합니다.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85720\n\n\n    - 자산을 공개하는 경우, 패키지 내 라이선스, 저작권 정보 및 이용 약관을 제공해야 합니다. 대중적인 데이터셋의\n경우, `[paperswithcode.com/datasets](http://paperswithcode.com/datasets)` 에서 일부 데이터셋의 라이선스를 선별하여 제공합니다. 해당 사이트의\n라이선스 가이드를 통해 데이터셋의 라이선스를 확인할 수 있습니다.\n\n    - 기존 데이터셋을 재포장하는 경우, 원본 라이선스와 파생 자산의 라이선스(변경된 경우)를 모두 제공해야 합니다\n.\n\n    - 해당 정보를 온라인에서 확인할 수 없는 경우, 저자는 해당 자산의 제작자에게 문의할 것을 권장합니다.\n\n\n13. **새로운** **자산**\n질문: 논문에서 소개된 새로운 자산은 잘 문서화되어 있으며, 해당 문서가 자산과 함께 제공됩니까?\n\n\n답변: [예]\n근거: 본 논문에서 새로운 데이터셋을 명확히 소개하고, 해당 문서화를 링크 https://github.com/Ljyustc/SocraticLM에서 제\n공합니다.\n\n\n지침:\n\n\n    - '해당 없음(NA)' 답변은 논문이 새로운 자산을 공개하지 않음을 의미합니다.\n\n    - 연구자들은 제출물의 일환으로 구조화된 템플릿을 통해 데이터셋/코드/모델의 세부 사항을 전달해야 합니다. 여\n기에는 훈련, 라이선스, 제한 사항 등에 대한 세부 정보가 포함됩니다.\n\n    - 논문에서는 자산이 사용된 대상자로부터 동의를 얻었는지 여부와 그 방법을 논의해야 합니다.\n\n    - 제출 시점에 해당되는 경우 자산을 익명화해야 합니다. 익명화된 URL을 생성하거나 익명화된 zip 파일을 포함할\n수 있습니다.\n\n\n14. **크라우드소싱** **및** **인간** **대상** **연구**\n질문: 크라우드소싱 실험 및 인간 대상 연구의 경우, 참가자에게 제공된 지침의 전문과 해당되는 경우 스크린샷, 그리고\n보상(있는 경우)에 대한 세부 사항이 논문에 포함되어 있습니까?\n\n\n답변: [예]\n정당화: 부록 F에 인간 주석자에게 제공된 전체 템플릿을 포함합니다. 지침:\n\n\n    - NA 답변은 해당 논문이 크라우드소싱이나 인간 대상 연구를 포함하지 않음을 의미합니다.\n\n    - 이 정보를 보충 자료에 포함하는 것은 괜찮지만, 논문의 주요 기여가 인간 피험자를 포함하는 경우 가능한 한 많은\n세부 사항을 본문에 포함해야 합니다.\n\n    - NeurIPS 윤리 강령에 따르면, 데이터 수집, 관리 또는 기타 작업에 참여한 작업자에게는 데이터 수집 기관 소재국\n의 최저 임금 이상을 지급해야 합니다.\n\n\n15. **인간** **대상** **연구에** **대한** **기관** **심사** **위원회(IRB) 승인** **또는** **이에** **상응하는** **절차**\n질문: 본 논문은 연구 참여자가 감수할 수 있는 잠재적 위험을 설명하고 있으며, 그러한 위험이 피험자에게 공개되었는\n지, 그리고 기관윤리위원회(IRB) 승인(또는 귀하의 국가 또는 기관의 요구 사항에 따른 동등한 승인/검토)을 획득했는\n지 여부를 명시하고 있습니까?\n\n\n답변: [예]\n근거: 본 논문에서는 인간 주석가들에게 LLM의 출력물 평가(즉, 등급 부여)만을 요청합니다. 주석가들 자체가 평가 대\n상이 아니며 테스트를 받는 것도 아닙니다. 또한 부록 F의 주석 템플릿에서 확인할 수 있듯이, 이 평가 과정은 주석가들\n의 개인정보나 프라이버시를 수집하지 않으며, 주석가들은 평가 목적을 완전히 인지하고 그 사용에 동의하였습니다.\n\n\n지침:\n\n\n3\n85721 [https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721)\n\n\n               - '해당 없음(NA)' 답변은 해당 논문이 크라우드소싱이나 인간 대상 연구를 포함하지 않음을 의미합니다.\n\n               - 연구가 수행되는 국가에 따라 인간 대상 연구에는 IRB 승인(또는 이에 상응하는 절차)이 필요할 수 있습니다. IRB\n승인을 획득한 경우, 논문에서 이를 명확히 명시해야 합니다.\n\n               - 해당 절차는 기관 및 지역에 따라 크게 다를 수 있음을 인지하며, 저자들은 NeurIPS 윤리 강령과 소속 기관의 지\n침을 준수할 것을 기대합니다.\n\n               - 초기 제출 시에는 검토를 수행하는 기관 등 익명성을 해칠 수 있는 정보(해당되는 경우)를 포함하지 마십시오.\n\n\n2\n[https://doi.org/10.52202/079017-2721](https://doi.org/10.52202/079017-2721) 85722\n\n\n",
    "eng_pdf": "pdf/eng/소크라틱 AI SocraticLM- Exploring Socratic Personalized Teaching with Large Language Models.pdf",
    "kor_pdf": "pdf/kor/소크라틱 AI SocraticLM- Exploring Socratic Personalized Teaching with Large Language Models ko.pdf"
  },
  {
    "id": "paper_2",
    "title": "SOCRATIC-PRMBENCH: Benchmarking Process Reward Models with Systematic Reasoning Patterns",
    "eng_md": "## **SOCRATIC-PRMBENCH: Benchmarking Process Reward Models with** **Systematic Reasoning Patterns**\n\n**Xiang Li** **[1,2,3]** **, Haiyang Yu** **[3]** **, Xinghua Zhang** **[3]** **, Ziyang Huang** **[1,2]** **,**\n**Shizhu He** **[1,2]** [*], **Kang Liu** **[1,2]**, **Jun Zhao** **[1,2]**, **Fei Huang** **[3]**, **Yongbin Li** **[3]**\n1Institute of Automation, Chinese Academy of Sciences\n2School of Artificial Intelligence, University of Chinese Academy of Sciences\n3Tongyi Lab, Alibaba Group\n\n\n\n![](assets/md_images/소크라틱-AI2-SOCRATIC-PRMBENCH--BenchmarkingProcess-Reward-Models-with-Systematic-Reasoning-Patterns.pdf-0-4.png)\n\n\n\n**Abstract**\nProcess Reward Models (PRMs) are crucial in\ncomplex reasoning and problem-solving tasks\n(e.g., LLM agents with long-horizon decisionmaking) by verifying the correctness of each\nintermediate reasoning step. In real-world scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to solve a problem,\npotentially suffering from errors under various reasoning patterns. Therefore, PRMs are\nrequired to identify errors under various reasoning patterns during the reasoning process.\nHowever, existing benchmarks mainly focus on\nevaluating PRMs with stepwise correctness, ignoring a systematic evaluation of PRMs under\nvarious reasoning patterns. To mitigate this gap,\nwe introduce SOCRATIC-PRMBENCH, a new\nbenchmark to evaluate PRMs systematically\nunder six reasoning patterns, including _Trans-_\n_formation_, _Decomposition_, _Regather_, _Deduc-_\n_tion_, _Verification_, and _Integration_ . SOCRATICPRMBENCH comprises 2995 reasoning paths\nwith flaws within the aforementioned six reasoning patterns. Through our experiments on\nboth PRMs and LLMs prompted as critic models, we identify notable deficiencies in existing\nPRMs. These observations underscore the significant weakness of current PRMs in conducting evaluations on reasoning steps under various reasoning patterns. We hope SOCRATICPRMBENCH can serve as a comprehensive\ntestbed for systematic evaluation of PRMs under diverse reasoning patterns and pave the way\nfor future development of PRMs [1] .\n\n\n**1** **Introduction**\nLarge Language Models (LLMs) (OpenAI, 2024b;\nDeepSeek-AI, 2025; Team, 2024b) augmented by\nmethodologies like Reinforcement Learning with\nVerifialble Rewards (RLVR) (Trung et al., 2024;\nShao et al., 2024) and Test-Time Scaling (Snell\n*Corresponding author\n1Our code and data are available at [https://github.](https://github.com/Xiang-Li-oss/Socratic-PRMBench)\n[com/Xiang-Li-oss/Socratic-PRMBench](https://github.com/Xiang-Li-oss/Socratic-PRMBench)\nFigure 1: (Left): Given a question, the reasoning step\n2 and 5 contain errors. (Medium): Each step applys a\nspecific reasoning pattern. (Right): The process reward\nmodel successfully detects the error of _Deduction_ pattern but fails with the _Decomposition_ reasoning pattern.\n\n\net al., 2025; Bansal et al., 2025), have demonstrated\nsignificant capabilities in complex reasoning and\ndecision-making tasks. Process Reward Models\n(PRMs) (Lightman et al., 2024; Wang et al., 2023;\nZhang et al., 2025) play a crucial role in these advancements, especially for LLM agents which involve long-horizon decision-making steps (Choudhury, 2025; Ma et al., 2025; Xiong et al., 2025). By\nproviding step-level rewards during the reasoning\nprocess, PRMs offer more accurate and denser reward signals, which in turn guide the optimization\nof LLMs and the exploration of reasoning trajectories (Tie et al., 2025; Ji et al., 2025).\n\n\nHowever, the diverse reasoning patterns applied\nby LLMs during reasoning process (Dong et al.,\n2023; Li et al., 2024) pose a challenge for PRMs in\nconsistently providing accurate rewards. Figure 1\nillustrates such a scenario: according to the thoery\nof ancient Greek philosopher Socrate (Dong et al.,\n2023; Qi et al., 2023), the reasoning pattern for\nStep 1 is ‘ _Transformation_ ’, for Step 2 ‘ _Decompo-_\n_sition_ ’, and for Steps 3-5 ‘ _Deduction_ ’. Although\nthe existing PRM identifies the error in Step 5 ( _De-_\n_duction_ pattern), it does not detect the fundamental\ncause of this error from the _Decomposition_ pattern.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**PRM** **Error Type** **Fine-grained** **Reasoning** **Test Case** **Average**\n**Annotator**\n**Benchmarks?** **Detection?** **Classes** **Patterns** _[†]_ **Size** **Steps**\nRMBench (Liu et al., 2025) ✗ ✗ 1 1 Synthetic + Human 1,327    CriticBench (Lin et al., 2024) ✗ ✗ 1 1    -    -    MathCheck-GSM (Zhou et al., 2025) ✗ ✗ 1 1 Synthetic 516   ProcessBench (Zheng et al., 2024) ✓ ✗ 1 1 Human 3,400 7.1\nPRMBench (Song et al., 2025) ✓ ✓ 9 1 Synthetic + Human 6,216 13.4\nSOCRATIC-PRMBENCH ✓ ✓ 20 6 Synthetic + Human 2995 8.7\nTable 1: Comparison between our proposed SOCRATIC-PRMBENCH and other benchmarks or datasets for reward\nmodel evaluation. _[†]_ : the number of reasoning patterns covered within the benchmark.\n\n\n\nSpecifically, in Step 2, the omission of substituting\na point in the solution of the differential equation\nto calculate the constant _C_, causes _C_ to remain\nundetermined throughout the subsequent reasoning\nprocess, resulting in a flawed final answer. This\nobservation indicates the unreliability of current\nPRMs towards diverse reasoning patterns.\n\n\nFor a comprehensive assessment of PRMs’ error detection capabilities across various reasoning\npatterns, we introduce SOCRATIC-PRMBENCH,\na systematic and fine-grained benchmark. In contrast to prior benchmarks with limited systematic\nevaluation (Zheng et al., 2024; Song et al., 2025),\ninspired by the ancient Greek philosopher Socrates,\nwe design to evaluate PRMs’ proficiency in detecting errors across 6 reasoning patterns: _Transforma-_\n_tion_, _Decomposition_, _Regather_, _Deduction_, _Verifi-_\n_cation_, and _Integration_ . Specifically, SOCRATICPRMBENCH comprises 2995 reasoning paths,\nwith flaws categorized into six primary categories\nby reasoning pattern and 20 sub-categories of finegrained error types. The data annotation process\nfor SOCRATIC-PRMBENCH is fully automated using LLMs, thereby obviating the need for extensive\nhuman labor. We ensure the difficulty of the data\nthrough rule-based filtering and guarantee its quality through manual expert review.\n\n\nWe conducted extensive experiments on a\nwide range of models, including open-source\nPRMs, and a series of general-purpose and\nreasoning-specialized LLMs. The findings reveal considerable scope for improvement in current PRMs. Notably, Qwen2.5-Math-PRM, the\nhighest-performing PRM, attained a mere 68.0\noverall score. Through detailed analytical experiments, we identified substantial disparities in the\nerror detection capabilities of current PRMs across\ndifferent reasoning patterns, alongside evident latency in indentifying error steps and significant bias\nof reward generation. By leveraging SOCRATICPRMBENCH for evaluation, we offer a pathway to\ncomprehensively assess PRMs from the perspec\ntive of reasoning patterns. This can be potentially\nhelpful for mitigating the risk of reward hacking in\nfuture PRM development. In general, our contributions are summarized as follows:\n\n\n  - We propose SOCRATIC-PRMBENCH, the first\nsystematic PRM benchmark from reasoning\npattern perspective, comprising 2995 samples\nfor a comprehensive and fine-grained evaluation on process reward models.\n\n\n  - Based on ancient Greek logic theory (Qi\net al., 2023), SOCRATIC-PRMBENCH covers 6 carefully designed reasoning patterns,\nincluding _Transformation_, _Decomposition_, _Re-_\n_gather_, _Deduction_, _Verification_, and _Integra-_\n_tion_, with 20 sub-categories of fine-grained\nerror types. This systematic and granular evaluation framework enables a comprehensive\nassessment of PRMs and facilitates the identification of their potential shortcomings.\n\n\n  - We perform extensive experiments on a\nwide range of SOTA PRMs and LLMs with\nSOCRATIC-PRMBENCH. Our results reveal\nessential limitations in current PRMs and offer\ninsights for future progress in this area.\n\n\n**2** **Related Work**\n**Process Reward Models** Process reward models (PRMs) have demonstrated their superiority\nover outcome reward models (ORMs) (Zhang et al.,\n2024; Ankner et al., 2024) by providing more accurate and dense reward signals for intermediate reasoning steps. As a result, the development of PRMs\nis gaining increasing attention. Lightman et al.\n(2024) contributes a manually annoted dataset for\nPRM training, Wang et al. (2024) propose an automatic step-level labeling method with Monte Carlo\nestimation. Moreover, Dong et al. (2024); Zhao\net al. (2025) forms process reward modeling as generation task and improve generative capabilities of\nPRMs using CoT reasoning. In contrast to the flourish of PRMs’ training, PRMs’ evaluation remaines\ncomparatively underdeveloped. To remedy this imbalance, we present SCORATIC-PRMBENCH, a\nnovel benchmark for PRMs’ evaluation.\n\n\n**Reward** **Model** **Benchmarks** Reward benchmarks are crucial for evaluating reward models,\nas they provide a direct and quantifiable measure.\nDespite the emergence of numerous benchmarks\n(Liu et al., 2025; Lin et al., 2024; Lambert et al.,\n2024), they are are primarily designed to evaluate\nORMs, without any step-level annotations. Zheng\net al. (2024); Song et al. (2025) annotate step-level\nlabels using LLMs and human experts to create\nbenchmarks for PRMs. However, their evaluation\nare not systematic and ignore the need to evaluate PRMs’ error detection capabilities towards\ndiverse reasoning patterns (Dong et al., 2023; Li\net al., 2024). To address this gap, we propose\nSOCRATIC-PRMBENCH, a systematic and granular benchmark to provide a comprehensive assessment of PRMs from the perspective of reasoning\npatterns. A comparison between our SOCRATICPRMBENCH and existing reward model benchmarks is summarized in Table 1.\n\n\n**3** **Socratic-PRMBench**\n**3.1** **Reasoning Patterns**\nThe design of the reasoning patterns in SOCRATICPRMBENCHMARK is inspired by the logical theories of the ancient Greek philosopher Socrates.\nAs Socrates once stated, \"I cannot teach anybody\nanything. I can only make them think.\" Following\nthis philosophical wisdom, we categorize reasoning into six atomic reasoning patterns, within these\nsix reasoning patterns, we systematically design a\ntotal of 20 types of reasoning errors. The atomic\nreasoning patterns and the fine-grained categories\nof error types under Socrates’ logical framework\nare illustrated in Figure 2.\n**Transformation** transforms the problem into a\nhomogeneous or similar problem, or abstract the\nproblem. It usually explains the problem from\na problem-solving perspective, aiming at gaining a more comprehensive and clear understanding of the problem. Specifically, the _Transforma-_\n_tion_ evaluation category can be divided into two\nsub-categories: _**Transformation Inconsistency**_ and\n_**Transformation Counter-Factuality**_ . For a _Trans-_\n_formation_ step _P_ _→_ _P_ _[′]_, Transformation Inconsistency refers that _P_ _[′]_ lacks consistency in logic,\nsemantics, or understanding with _P_ . Transformation Counter-Factuality refers to including factual\nerror that against ground truth _G_ in _P_ _[′]_ .\n**Decomposition** breaks the problem into manageable subproblems, or makes a plan for reasoning steps, resolving the main problem by tackling each sub-problem. Specifically, the _Decom-_\n_position_ evaluation category can be divided into\nthree sub-categories: _**Decomposition**_ _**Unsound-**_\n_**ness**_, _**Decomposition**_ _**Redundancy**_, and _**Decom-**_\n_**position**_ _**Incompleteness**_ . For a _Decomposition_\nstep _P_ _→{P_ 1 _, P_ 2 _, ..., Pn}_, each of the three subcategories represents a distinct type of error in subproblem _Pi_, which can be incorrect caused by logical inequality, missing important sub-problems and\nconditions, or including redundant sub-problems\nand constrains.\n**Regather** collects key information from the input relevant to solving the problem and identifies\ncrucial principles, and other concepts related to\nsolving the problem. Specifically, the _Regather_\nevaluation category can be divided into three subcategories: _**Regather Imprecision**_, _**Regather Re-**_\n_**dundancy**_, and _**Regather Incompleteness**_ . For a\n_Regather_ step _P_ _→{Q_ 1 _, Q_ 2 _, ..., Qn}_, Regather\nimprecision refers to collecting a _Qi_ with misinformation, misusing definations that are not suitable\nfor solving the problem _P_ . Regather Redundancy\ngathers redundant or unrelevant information not\nrelated with _P_ . Regather Incompleteness refers to\nthe absence of core definations, critical principles\nand concepts.\n**Deduction** derives a conclusion for a given premise\ndirectly. Specifically, the _Deduction_ evaluation\ncategory can be divided into six sub-categories:\n_**Premise Unsoundness**_, _**Premise Incompleteness**_,\n_**Premise Redundancy**_, _**Conclusion Invalidity**_, _**Con-**_\n_**clusion**_ _**Inconsistency**_ and _**Conclusion**_ _**Counter-**_\n_**Factuality**_ . For a _Deduction_ step _P_ _→_ _C_, the first\nthree sub-categories arise from the premise and\ninclude: (1) starting deduction resoning from an\nunreasonable or incorrect premise, (2) introducing redundant assumptions into the premise, and\n(3) omitting key conditions and constraints. The\nremaining three sub-categories originate from the\nconclusion and include: (1) deriving an invalid\nconclusion from correct premises, (2) deriving a\nconclusion that contradicts a previous conclusion,\nand (3) deriving a conclusion that is inconsistent\nwith known ground truth.\n**Verification** examines reasoning steps in terms of\nfactual accuracy, logical consistency, etc, detecting potential errors and refining them iteratively.\nSpecifically, the _Verification_ evaluation category\n\n\n![](assets/md_images/소크라틱-AI2-SOCRATIC-PRMBENCH--BenchmarkingProcess-Reward-Models-with-Systematic-Reasoning-Patterns.pdf-3-0.png)\n\nFigure 2: An overview of our SOCRATIC-PRMBENCH. The left part illustrates our dataset constuction procedure.\nThe right part illustrates the 6 reasoning patterns and 20 sub-categories of fine-grained error types. We use _P_ and\n_C_ to represent (sub)problems and conclusions, respectively. We use _Q_, _R_, _G_ to represent gathered information,\nredundant contents, and ground truth.\n\n\n\ncan be divided into two sub-categories: _**Detection**_\n_**Error**_ and _**Correction**_ _**Error**_ . The former refers\nto failing to identify an incorrect conclusion _C_,\nThe latter, however, involves recognizing the initial\nerror in _C_ but introducing a new error during the attempted correction, leading to a different, incorrect\nconclusion _C_ _[′]_ .\n**Integration** summarizes concluded conclusions\nto derive a new conclusion, integrating all current reasoning processes to form the final conclusion. Specifically, the _Integration_ evaluation\ncategory can be divided into four sub-categories:\n_**Integration**_ _**Inconsistency**_, _**Integration**_ _**Incom-**_\n_**pleteness**_, _**Integration**_ _**Redundancy**_, and _**Inte-**_\n_**gration**_ _**Unsoundness**_ . For an integration step\n_{C_ 1 _, C_ 2 _, ..., Cn}_ _→_ _C_, the first three error types\noriginate from a intermediate conclusion _Ci_, including the presence of conclusions that contradicts\nprior findings,the absence of crucial conclusions,\nand the introduction of unnecessary or redundant\nconclusions. The final error type, namely Integration Unsoundness, refers to concluding a final\nconlusion _C_ that is incorrect or unreasonable, even\nwhen integrated conclusions all satisfy soundness\nan completeness.\n\n\n**3.2** **Benchmark Construction**\nThe dataset construction pipeline comprises two\ncore stages: **Socratic Reasoning Generation** and\n**Test Case Construction** .\n\n\n**3.2.1** **Socratic Reasoning Generation**\nThis stage aims to create a data pool of Socratic\nreasoning process, represented as a sequence of\natomic Socratic reasoning actions. As illustrated\nin left part of Figure 2, each reasoning step is enclosed with a start tag <[Pattern]> and an end tag\n</[Pattern]>. The content within the [Pattern] placeholder indicates the specific reasoning pattern that\ncharacterizes this particular step.\n\n\n**Socratic Reasoning Model Training** Given the\nscarcity of available Socratic reasoning data, we\ninitially trained a specialized Socratic reasoning\nmodel to facilitate data generation. To achieve this,\nwe sample 19k instances from the MATH-Hard\n(Hendrycks et al., 2021) and Open-o1 (OpenO1,\n2024) datasets and prompt GPT-4o to transform\ntheir existing Chain-of-Thought (CoT) annotations\ninto Socratic reasoning processes. We then finetune Qwen2.5-72b-instruct (Team, 2024a) on these\nSocratic reasoning processes, yielding our Socratic\nreasoning model, denoted as _MSocratic_ .\n\n\n**Socratic Reasoning Generation** Subsequently,\nwe leverage _MSocratic_ to generate new Socratic reasoning processes from metadata. To this end, we\nfirst collect samples from GSM8k (Cobbe et al.,\n2021), Omni-Math (Gao et al., 2024), MathBench\n(Liu et al., 2024), and OlympiadBench (He et al.,\n2024a). In order to ensure that our problems are\nadequately challenging, we carefully curated the\nOmni-Math and MathBench datasets. Specifically,\nwe excluded any Omni-Math samples with a difficulty rating lower than 4.0. For MathBench, we\nfocused solely on MathBench-A, as this subset emphasizes theoretical application rather than conceptual understanding. Furthermore, we only retained\ninstances from MathBench-A that are designated\n**Overall** _**Transformation**_ _**Decomposition**_ _**Regather**_ _**Deduction**_ _**Integration**_ _**Verifcation**_\nAvg. Steps 8.7 8.5 8.7 8.6 8.5 8.5 10.8\nAvg. Error Steps 3.0 4.2 3.3 2.9 3.0 2.0 3.8\nAvg. First Error Step 4.7 1.5 3.0 3.1 5.4 7.2 6.9\nAvg. Question Length 209.6 224.4 220.7 207.5 221.7 191.3 169.4\n# of Instances 2995 313 463 463 926 615 215\n\n\nTable 2: Statistics of SOCRATIC-PRMBENCH.\n\n\n\nas high school or university level. This procedure\nfinally results in a data pool _D_ . For each questionanswer pair ( _qi, ai_ ) in _D_, _MSocratic_ generates a Socratic reasoning process _ri_, resulting in a ( _qi, ri,_ _a_ ˆ _i_ )\ntriplet.\n\n\n**Socratic** **Reasoning** **Curation** Finally, each\n( _qi, ri,_ _a_ ˆ _i_ ) tuple undergos a rigorous dual verification process: answer correctness was first assessed,\nfollowed by LLM-based verification of each individual step. Only tuples that pass both verifications are retained, resulting in our curated metadata set _D_ _[′]_ . For answer verification, we follow\nQwen2.5-Math (Yang et al., 2024), requiring that\nthe predicted answer _a_ ˆ _i_ satisfies both numerical\nand symbolic equivalence with the ground truth answer _a_ . For step verification, we leverage GPT-4o\n(OpenAI, 2024a) to assess the correctness of each\nindividual step in the reasoning process, with the\ndetailed prompt in Appendix B.\n\n\n**3.2.2** **Test Case Constuction**\nIn this stage, we generate test sets for each error\ntype _C_ (as classified in Section 3.1) by employing a\ncontrolled error injection procedure. For each error\ntype _C_ (e.g., Repeat Inconsistency), we create a test\nset _TC_ . This is achieved by first randomly select\n_N_ samples from the metadata set _D_ _[′]_ . And then for\neach sample ( _qi, ri, ai_ ), including a problem _qi_, a\nSocraitc reasoning path _ri_ guaranteed completely\ncorrect through our dual verification process, we\nprompt GPT-4o to modify the originally correct\nreasoning process _ri_, intentionally introducing an\nerror consistent with error type _C_ :\n\n\n_r_ ˜ _i_ = LLM( _I,_ [ _qi, ri, ai_ ] _, C_ )\n(1)\n_TC_ = _{ti_ = ( _qi,_ ˜ _ri,_ ˜ _ai_ ) _}_ _[N]_ _i_ =1\nwhere ˜ _ri_ is the modified socratic reasoning process\nwith the type of error _C_ and _I_ is the instruction\nprompt for GPT-4o to modify original process _ri_\nto ˜ _ri_, with detailed prompt in Appendix B.\n\n\n**3.3** **Quality Control**\nTo ensure the high quality and reliability of\nSOCRATIC-PRMBENCH, we utilize both rule\nbased and LLM-based method to filter out any unsuitable samples, thereby ultimately creating our\nSOCRATIC-PRMBENCH.\n\n\n**Rule-based Fitering** Despite providing detailed\ntask descriptions and output format requirements\nin the instruction _I_, GPT-4o may still occasional\nfail to follow the instruction _I_ strictly. Therefore,\nwe implement a rule-based filtering method. First,\nwe use string matching to identify and remove any\nsample that fails to produce output in JSON format, which is required in _I_ . Second, we use regular\nexpression to discard any sample that fail to successfully output the final answer.\n\n\n**LLM-based Filtering** To ensure the quality of\nour generated test cases, we employ Gemini2.5-Pro\nto evaluate each sample ( _qi,_ ˜ _ri,_ ˜ _ai_ ), within a test\nset _TC_ for a given error type _C_ . Specifically, we\ninstruct Gemini2.5-Pro to assess the sample based\non two criteria: (1) the reasoning path _r_ ˜ _i_ appears\nsuperficially plausible yet contains an underlying\nreasoning error, and (2) the identified error should\ndefinitively belong to the targeted error type _C_,\nwith detailed prompt shown in Appendix B. After\nfiltering by Gemini2.5-Pro, the acceptance rate of\nsamples reaches 92.7%, and 2995 samples are retained to form the final Socratic-PRMBench. The\nstatistics of Socratic-PRMBench are shown in Table 2.\n\n\n**LLM’s** **Consistency** **with** **Human** **Annotators**\nTo demonstrate Gemini2.5-Pro’s ability to perform\nthis quality filtering task, we measure its agreement\nwith human annotators. We recruit three volunteer\nannotators, each holding at least a bachelor’s degree, and ask them to verify a randomly sampled\n10% subset of our data using the exact same criteria\nwith Gemini2.5-Pro. We then calculate the agreement rate between Gemini2.5-Pro and the human\nannotators. As a result, Gemini2.5-Pro shows a\nhigh degree of consistency with the human annotators, achieving an average agreement rate of 93.3%.\nThis high level of consistency provides strong evidence that Gemini2.5-Pro can effectively replace\nhuman annotators in performing quality filtering\n**Transformation** **Decomposition** **Regather** **Verifcation**\n**Model**\n**TT.** **TF.** **DC.** **DR** **DS.** **GP.** **GC.** **GR.** **CE.** **DE.**\n_**Process Reward Models (PRMs)**_\n[Skywork-PRM-7B](https://huggingface.co/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B) 38.7 38.4 42.7 42.5 38.0 42.8 44.8 41.3 47.9 46.7\n[ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) 50.9 50.9 59.3 50.1 53.7 52.4 59.6 49.7 66.7 59.2\n[RLHFlow-PRM-Mistral-8B](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Mistral-Data) 50.6 52.7 46.6 47.3 42.7 38.0 44.6 48.7 53.1 49.5\n[RLHFlow-PRM-Deepseek-8B](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) 47.5 50.8 50.6 50.9 44.0 41.6 48.6 55.4 45.9 47.6\n[MathShepherd-Mistral-7B](https://huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm) 54.5 50.9 59.4 **57.4** 56.7 **60.9** 59.4 54.6 **72.7** **72.1**\n[Qwen2.5-Math-PRM-7B](https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B) **55.8** **64.3** **61.7** 51.6 **58.4** 57.5 **61.8** **58.2** 67.4 64.1\n_**LLMs, Prompted as Critic Models**_\n[GPT-4o](https://openai.com/index/hello-gpt-4o/) **62.4** 60.5 69.9 60.0 66.1 64.9 **74.1** 57.9 74.4 75.8\n[Deepseek-R1](https://github.com/deepseek-ai/DeepSeek-R1) 51.9 **72.6** 63.4 64.4 67.1 70.9 64.6 54.8 75.0 77.1\n[QwQ-32B](https://huggingface.co/Qwen/QwQ-32B-Preview) 60.2 68.6 70.0 **67.9** 59.8 73.7 65.8 55.4 75.8 75.7\n[Gemini-2.5-Pro](https://deepmind.google/technologies/gemini/pro/) 62.3 64.4 67.3 61.4 **68.5** 70.2 69.2 **58.6** **78.3** **78.0**\n[o3-mini](https://openai.com/index/openai-o3-mini/) **62.4** 67.4 **70.4** 57.3 68.0 **77.3** 71.3 53.0 77.2 72.6\n**Deduction** **Integration**\n**Model** **Overall**\n**CF.** **CT.** **CV.** **PC.** **PR.** **PS.** **IC.** **IT.** **IR.** **IS.**\n_**Process Reward Models (PRMs)**_\n[Skywork-PRM-7B](https://huggingface.co/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B)) 43.6 42.5 41.2 40.0 41.8 42.8 39.8 38.7 42.6 39.4 44.2\n[ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) 61.9 63.6 63.6 66.3 61.9 65.2 63.5 69.7 78.2 68.7 76.1\n[RLHFlow-PRM-Mistral-8B](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Mistral-Data) 48.8 50.4 46.2 45.2 46.1 44.5 43.3 51.2 58.1 46.6 56.3\n[RLHFlow-PRM-Deepseek-8B](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) 51.5 51.5 52.4 52.0 47.6 51.4 45.2 55.3 63.7 53.3 66.7\n[MathShepherd-Mistral-7B](https://huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm) 64.4 68.0 65.9 66.5 62.4 65.9 65.4 63.1 74.2 60.1 72.3\n[Qwen2.5-Math-PRM-7B](https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B) **68.0** **74.7** **73.1** **72.2** **66.6** **72.4** **67.2** **75.0** **85.2** **69.6** **86.9**\n_**LLMs, Prompted as Critic Models**_\n[GPT-4o](https://openai.com/index/hello-gpt-4o/) 70.8 63.6 62.7 74.5 73.2 60.1 76.1 73.4 80.8 52.7 88.7\n[Deepseek-R1](https://github.com/deepseek-ai/DeepSeek-R1) 73.0 80.8 72.6 77.2 68.6 72.0 76.9 75.9 78.9 59.9 88.6\n[QwQ-32B](https://huggingface.co/Qwen/QwQ-32B-Preview) 73.8 70.3 75.0 **85.2** **74.0** 69.5 77.5 **81.8** 83.5 58.7 96.7\n[Gemini-2.5-Pro](https://deepmind.google/technologies/gemini/pro/) 73.5 72.8 77.7 83.5 69.0 65.9 73.5 73.2 **88.9** 56.9 **96.9**\n[o3-mini](https://openai.com/index/openai-o3-mini/) **75.7** **83.3** **81.0** 81.4 73.9 **75.3** **78.6** 78.7 87.3 **72.0** 87.0\nTable 3: Evaluation results on SOCRATIC-PRMBENCH. (Up): The PRM-Score of _Transformation_, _Decomposition_,\n_Regather_, and _Verification_ . (Down): The PRM-Score of _Deduction_, _Integration_ and _Overall_ performance. The best\nperformance for each category and task is in **bold** . The full names of abbreviations are shown in Appendix A\nacross the entire dataset, reducing the burden of\nextensive manual work.\n\n\n**4** **Experiments**\n**4.1** **Models**\nIn our setting, we consider two types of model: Process Reward Models (PRMs) and Large Language\nModels (LLMs) prompted as critic models.\n\n\n**Process** **Reward** **Models** **(PRMs)** are trained\nwith annotations of intermediate reasoning steps\nto evaluate and supervise intermediate reasoning\nprocess of language models. Our evaluation includes state-of-the-art open-source PRMs, such as:\n(1) MathShepherd (Wang et al., 2023), which obtains the process label for each step by estimating the empirical probability of that step leading\nto the correct final answer. (2) Two LLaMA-3.1based Generative PRMs (Dong et al., 2024) that\ndetermine correctness based on the output probabilities of \"Yes/No\" tokens. (3) ReasonEval (Mondorf\nand Plank, 2024), which asseses redundancy in\naddition to validity of reasoning steps. (4) Two\nPRMs trained on the popular mathematical model\nQwen2.5-Math, namely Skywork-PRM (He et al.,\n2024b) and Qwen2.5-Math-PRM (Zhang et al.,\n2025).\n\n\n**Large Language Models (LLMs) Prompted as**\n**Critic Models** Critic models aim to provide feedback and critique directly on model-generated\ntexts, harnessing the generative power of Large\nLanguage Models. Our evaluation includes both\ngeneral-purpose models, including GPT-4o (OpenAI, 2024a), Gemini2.5-Pro (Deepmind, 2025),\nand models specilized on reasoning, including\nDeepseek-R1 (DeepSeek-AI, 2025), QwQ-32B\n(Team, 2024b), and o3-mini (OpenAI, 2025).\n\n\n**4.2** **Evaluation Metrics**\nGiven that the evaluation of PRM centers on the\ndetection of flawed reasoning steps, a straightforward application of Accuracy or F1-score may be\naffected by inherent biases of models. To address\nthis concern, we follow (Song et al., 2025; Zheng\net al., 2024) and employ the PRM-score as our\n\n\n![](assets/md_images/소크라틱-AI2-SOCRATIC-PRMBENCH--BenchmarkingProcess-Reward-Models-with-Systematic-Reasoning-Patterns.pdf-6-0.png)\n\n\n\nevlauation metric, defined formally as:\n\n\nPRM-Score = _w_ 1 _×_ F1neg + _w_ 2 _×_ F1 (2)\nwhere F1 and F1neg refer to F1 scores and negative F1 scores. _w_ 1 and _w_ 2 are weights that balance the contributions of the F1-score and negative\nF1-score. Following previous studies (Song et al.,\n2025; Zheng et al., 2024), we set _w_ 1 = _w_ 2 = 0 _._ 5.\n\n\n**4.3** **Main Results**\nOur evluation results are exhibited in Table 3. Our\nfindings are as follow:\n\n\n**Comparision between PRMs and LLMs** The\nperformance of PRMs is demonstrably inferior to\nthat of LLMs. The top-performing PRM, Qwen2.5Math-PRM-7B, achieves a score of only 68.0,\nwhich is lower than even the least effective LLM,\nGPT-4o. Furthermore, some PRMs perform below\nthe level of random guess, highlighting their limitations in handling reasoning errors across diverse\nreasoning patterns. This suggests a considerable\ngap between PRMs and LLMs, indicating a need\nfor substantial improvement. The challenges of\nPRM data annotation and the difficulty in ensuring\nthe quality of synthetic data likely contribute to this\ndisparity. For instance, Math-shepherd leverages\nsynthetic data where step correctness is measured\nbased on the estimated probability of arriving at the\ncorrect final answer, whereas Qwen2.5-Math-PRM7B uses the manually labeled PRM800k dataset.\n\n\n**Comparision** **among** **LLMs** In contrast to\nPRMs, LLMs exhibit the potential to provide more\nrobust and reliable rewards in critique, owing to\ntheir sophisticated language and reasoning skills.\nConsistent with this, we observe that reasoningspecialized LLMs outperforms general-purpose\nLLMs. Notably, QWQ-32B performs best among\nthe open-source models and even outperforms GPT4o. While QWQ -32B demonstrates impressive\nperformance, it still underperforms o3-mini, indicating that although the gap in problem-solving\nperformance is getting closer between open-source\nand proprietary models, a significant gap persists\nin their capabilities as critic models.\n\n\n**Redundant Errors Are More Challenging** We\nobserved notable performance variations across\nfine-grained error types, even within the same reasoning pattern. Redundant errors, such as decomposition redundancy, regather redundancy, and integration redundancy within the Decomposition, Re\nFigure 3: Average PRM-Score of representative PRMs\nand LLMs across 6 reasoning patterns. Both PRMs and\nLLMs shows imbalanced performance.\n\n\ngather, and Integration patterns, consistently posed\na greater challenge for both PRMs and LLMs compared to other error types within the same reasoning\npattern. This may be attributted that redundant error steps often appear more \"normal\" or plausible\nthan other types of erroneous steps, hindering the\nmodels’ ability to identify them based on surfacelevel textual cues. This suggests that current PRMs\nmay be limited by their reliance on surface-level\npattern recognition for error detection, highlighting\nthe need for more profound reasoning and analytical capabilities.\n\n\n**4.4** **Detailed Analysis**\nThis section delves into a more nuanced analysis\nof our proposed SOCRATIC-PRMBENCH, aiming\nto identify current models’ limitations in providing\nprocess-level rewards and provide insights to guide\nthe future development of PRMs.\n\n\n**Disparities** **in** **Performance** **across** **Reasoning**\n**Patterns** As shown in Figure 3, we present the\naverage PRM-Scores of representative PRMs and\nLLMs across the six reasoning patterns. A notable\nfinding is the imbalanced performance exhibited by\nboth PRMs and LLMs across different reasoning\npatterns. The performance of almost all models\nwas consistently weaker on _Transformation_, _De-_\n_composition_, and _Regather_ patterns compared to\n_Deduction_, _Integration_, and _Verification_ . This issue is more pronounced for PRMs, for example,\nQwen2.5-Math-PRM-7B achieved a PRM-Score\nclose to 80.0 on the _Integration_ pattern but strug\nTransformation\n20.0\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\n\n\n\n|Q<br>M|Col2|Col3|wen2.5-Math<br>athShepherd|Col5|Col6|-PRM|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|O<br>~~So~~|O<br>~~So~~|O<br>~~So~~|3-mini<br>~~cratic-PRMB~~|3-mini<br>~~cratic-PRMB~~|3-mini<br>~~cratic-PRMB~~|~~ench~~|~~ench~~||||||||||\n|O<br>~~So~~|O<br>~~So~~|O<br>~~So~~|3-mini<br>~~cratic-PRMB~~|3-mini<br>~~cratic-PRMB~~|3-mini<br>~~cratic-PRMB~~|~~ench~~|||||||||||\n||||||||||||||||||\n||||||||||||||||||\n||||||||||||||||||\n||||||||||||||||||\n||||||||||||||||||\n||||||||||||||||||\n||||||||||||||||||\n\n\n2 4 6 8 10 12\nError Position\nFigure 4: Error position distribution (truncated to 12) of\nSOCRATIC-PRMBENCH and the predicted error position distribution of several PRMs and LLMs.\n\n\ngles to reach 60.0 on the _Decomposition_ pattern.\nThis finding highlights a potential bias in the current PRM training data construction process. Existing PRM datasets, regardless of whether they’re\nmanually annotated or synthetically generated, appear to lack adequate representation of different\nreasoning patterns. Due to the greater frequency of\ncertain patterns like _Deduction_, these datasets tend\nto be dominated by those patterns, resulting in significantly worse performance on rarer patterns such\nas _Decomposition_ . This observation underscores\nthe importance of considering the distribution of\ndifferent reasoning patterns in future PRM training\ndata construction, as early detection of reasoning\nerrors is critical to mitigate error propagation.\n\n\n**Models** **Show** **Latency** **in** **Indentifying** **Error**\n**Steps** To investigate the ability of models to detect reasoning errors in time, we compared the distribution of the ground truth error step positions in\nSOCRATIC-PRMBENCH with the distributions of\npredicted error positions for representative PRMs\nand LLMs. As evidenced by Figure 4, Qwen2.5Math-PRM and o3-mini show a marked shift towards later steps compared to the ground truth distribution, indicating a delay in detecting early errors. This implies a limited ability to detect errors early on, allowing them to propagate. On the\nother hand, MathShepherd exhibits an opposite\ntrend, with its predicted distribution shifts toward\nthe beginning of the reasoning chain, suggesting\nthat MathShepherd is prone to falsely identifying\ncorrect steps as errors, especially in the early stages\nof reasoning. This inspires us that both early detection and avoidance of excessive false positives\nare crucial. Although propagation of errors will\nwaste computational resources and reduces sam\n**Accuracy** **PRM**\n**Model**\nCorr. Err. All. **Score**\n_**Process Reward Models (PRMs)**_\nReasonEval-7B 87.3 35.7 69.6 61.9\nSkywork-PRM-7B 22.7 93.0 44.5 43.6\nMathShepherd 73.3 56.0 67.4 64.4\nQwen2.5-Math-PRM-7B 90.8 42.9 74.5 68.0\n_**LLMs, Prompted as Critic Models**_\nGPT-4o 83.0 57.5 74.6 70.8\nQwQ-32B 83.9 63.1 76.8 73.8\no3-mini 82.6 69.0 78.0 75.7\nGemini-2.5-Pro 83.6 62.8 76.5 73.5\nTable 4: Comparison of model performance on positive\nand negative test cases. _[†]_ represents performance of Random Guess.\n\n\npling efficiencys, overly aggressive error detection\ncan prematurely terminate correct reasoning paths,\nhindering the exploration of potentially optimal\nsolutions.\n\n\n**Reward Bias of PRMs** Table 3 reveals that some\nPRMs perform even worse than random guessing,\nsuggesting a substantial bias in their predictions.\nTo further quantify this bias, we calculated accuracy for correct and error reasoning steps for each\nmodel. As shown in Table 4, the results reveal a\nclear reward bias within PRMs, with some models heavily favoring positive rewards and others\ntending to provide negative rewards. For instance,\nQwen2.5-Math-PRM-7B displays a 90.8% accuracy on correct steps but only a 42.9% accuracy on\nerror steps. In stark contrast, Skywork-PRM-7B\nshows a 93.0% accuracy on error steps but only\na 22.7% accuracy on correct steps. While LLMs\nexhibits less pronounced bias than PRMs, however,\na considerable gap remained in accuracy between\ncorrect and error steps. Moreover, all the evaluated\nLLMs tended to favor positive rewards, which may\nlimit their reliability in identifying subtle errors\nwhen serve as critic models.\n\n\n**5** **Conclusion**\nIn this work, we propose SOCRATIC-PRMBENCH,\na systematic and fine-grained benchmark for\nPRMs. SOCRATIC-PRMBENCH comprises 2995\ninstances, categorized into six primary reasoning\npatterns and 20 sub-categories of fine-grained error\ntypes. Through a systematic and comprehensive\nevaluations of existing PRMs and LLMs prompted\nas critic models, we observe potential shortcomings\nin existing models and provide valuable insights\nfor future efforts on upgrading PRMs.\n\n\n**Limitations**\nAlthough our work can provide a systematic and\ncomprehensive evaluationg for PRMs, the current\nversion of our benchmark primarily focuses on reasoning tasks with objectively verifiable answers,\nsuch as mathematical problem. Applying our existing data construction methods to tasks in domains\nlike literature, medicine, or law, where definitive\nground truth is often absent, needs further exploration. We intend to expand our benchmark to encompass a broader range of tasks in future versions\nof our benchmark.\n\n\n**References**\nZachary Ankner, Mansheej Paul, Brandon Cui,\nJonathan D. Chang, and Prithviraj Ammanabrolu.\n2024. [Critique-out-loud](https://arxiv.org/abs/2408.11791) reward models. _Preprint_,\narXiv:2408.11791.\n\n\nHritik Bansal, Arian Hosseini, Rishabh Agarwal,\nVinh Q. Tran, and Mehran Kazemi. 2025. [Smaller,](https://openreview.net/forum?id=3OyaXFQuDl)\nweaker, yet better: [Training](https://openreview.net/forum?id=3OyaXFQuDl) LLM reasoners via\n[compute-optimal sampling.](https://openreview.net/forum?id=3OyaXFQuDl) In _The Thirteenth Inter-_\n_national Conference on Learning Representations_ .\n\n\nSanjiban Choudhury. 2025. Process reward models\nfor llm agents: Practical framework and directions.\n_arXiv preprint arXiv:2502.10325_ .\n\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, and 1 others. 2021. Training verifiers\nto solve math word problems. _arXiv_ _preprint_\n_arXiv:2110.14168_ .\n\n\nDeepmind. 2025. Gemini2.5-pro. [https://deepmind.](https://deepmind.google/technologies/gemini/pro/)\n[google/technologies/gemini/pro/.](https://deepmind.google/technologies/gemini/pro/)\nDeepSeek-AI. 2025. Deepseek-r1: [Incentivizing](https://arxiv.org/abs/2501.12948) rea[soning capability in llms via reinforcement learning.](https://arxiv.org/abs/2501.12948)\n_Preprint_, arXiv:2501.12948.\n\n\nHanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang,\nHan Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo,\nCaiming Xiong, and Tong Zhang. 2024. [RLHF work-](https://openreview.net/forum?id=a13aYUU9eU)\nflow: [From reward modeling to online RLHF.](https://openreview.net/forum?id=a13aYUU9eU) _Trans-_\n_actions on Machine Learning Research_ .\n\n\nQingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru\nHao, Zhifang Sui, and Furu Wei. 2023. Large language model for science: a study on p vs. np. _arXiv_\n_preprint arXiv:2309.05689_ .\n\n\nBofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo\nMiao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang\nChen, Runxin Xu, Zhengyang Tang, Benyou Wang,\nDaoguang Zan, Shanghaoran Quan, Ge Zhang, Lei\nSha, Yichang Zhang, Xuancheng Ren, Tianyu Liu,\nand Baobao Chang. 2024. [Omni-math:](https://arxiv.org/abs/2410.07985) A univer[sal olympiad level mathematic benchmark for large](https://arxiv.org/abs/2410.07985)\n[language models.](https://arxiv.org/abs/2410.07985) _Preprint_, arXiv:2410.07985.\n\n\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu,\nZhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie\nHuang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan\nLiu, and Maosong Sun. 2024a. [OlympiadBench:](https://doi.org/10.18653/v1/2024.acl-long.211)\n[A challenging benchmark for promoting AGI with](https://doi.org/10.18653/v1/2024.acl-long.211)\n[olympiad-level bilingual multimodal scientific prob-](https://doi.org/10.18653/v1/2024.acl-long.211)\n[lems.](https://doi.org/10.18653/v1/2024.acl-long.211) In _Proceedings of the 62nd Annual Meeting of_\n_the Association for Computational Linguistics (Vol-_\n_ume 1:_ _Long Papers)_, pages 3828–3850, Bangkok,\nThailand. Association for Computational Linguistics.\n\n\nJujie He, Tianwen Wei, Rui Yan, Jiacai Liu, Chaojie\nWang, Yimeng Gan, Shiwen Tu, Chris Yuhao Liu,\nLiang Zeng, Xiaokun Wang, Boyang Wang, Yongcong Li, Fuxiang Zhang, Jiacheng Xu, Bo An, Yang\nLiu, and Yahui Zhou. 2024b. [Skywork-o1 open se-](https://huggingface.co/Skywork)\n[ries.](https://huggingface.co/Skywork) [https://huggingface.co/Skywork.](https://huggingface.co/Skywork)\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring [mathematical](https://openreview.net/forum?id=7Bywt2mQsCe)\n[problem solving with the MATH dataset.](https://openreview.net/forum?id=7Bywt2mQsCe) In _Thirty-_\n_fifth Conference on Neural Information Processing_\n_Systems Datasets and Benchmarks Track (Round 2)_ .\n\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: [Low-rank adaptation of](https://arxiv.org/abs/2106.09685)\n[large language models.](https://arxiv.org/abs/2106.09685) _Preprint_, arXiv:2106.09685.\n\n\nYixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian\nMo, and Min Zhang. 2025. Test-time computing:\nfrom system-1 thinking to system-2 thinking. _arXiv_\n_preprint arXiv:2501.02497_ .\n\n\nNathan Lambert, Valentina Pyatkin, Jacob Morrison,\nLJ Miranda, Bill Yuchen Lin, Khyathi Chandu,\nNouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,\nand 1 others. 2024. Rewardbench: Evaluating reward models for language modeling. _arXiv preprint_\n_arXiv:2403.13787_ .\n\n\nChangcheng Li, Xiangyu Wang, Qiuju Chen, Xiren\nZhou, and Huanhuan Chen. 2024. Mtmt: Consolidating multiple thinking modes to form a thought tree for\nstrengthening llm. _arXiv preprint arXiv:2412.03987_ .\n\n\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2024. [Let’s verify step by step.](https://openreview.net/forum?id=v8L0pN6EOi) In _The Twelfth Inter-_\n_national Conference on Learning Representations_ .\n\n\nZicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo,\nHaowei Liu, and Yujiu Yang. 2024. [CriticBench:](https://doi.org/10.18653/v1/2024.findings-acl.91)\n[Benchmarking LLMs for critique-correct reasoning.](https://doi.org/10.18653/v1/2024.findings-acl.91)\nIn _Findings_ _of_ _the_ _Association_ _for_ _Computational_\n_Linguistics:_ _ACL 2024_, pages 1552–1587, Bangkok,\nThailand. Association for Computational Linguistics.\n\n\nHongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong\nDuan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang,\nSongyang Zhang, Dahua Lin, and Kai Chen. 2024.\nMathBench: [Evaluating the theory and application](https://doi.org/10.18653/v1/2024.findings-acl.411)\n[proficiency of LLMs with a hierarchical mathematics](https://doi.org/10.18653/v1/2024.findings-acl.411)\n[benchmark.](https://doi.org/10.18653/v1/2024.findings-acl.411) In _Findings of the Association for Com-_\n_putational Linguistics:_ _ACL 2024_, pages 6884–6915,\nBangkok, Thailand. Association for Computational\nLinguistics.\n\n\nYantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou,\nand Juanzi Li. 2025. RM-bench: [Benchmarking](https://openreview.net/forum?id=QEHrmQPBdd)\n[reward models of language models with subtlety and](https://openreview.net/forum?id=QEHrmQPBdd)\n[style.](https://openreview.net/forum?id=QEHrmQPBdd) In _The Thirteenth International Conference on_\n_Learning Representations_ .\n\n\nYingwei Ma, Yongbin Li, Yihong Dong, Xue Jiang,\nRongyu Cao, Jue Chen, Fei Huang, and Binhua Li.\n2025. Thinking longer, not larger: Enhancing software engineering agents via scaling test-time compute. _arXiv preprint arXiv:2503.23803_ .\n\n\nPhilipp Mondorf and Barbara Plank. 2024. [Beyond](https://openreview.net/forum?id=Lmjgl2n11u)\naccuracy: [Evaluating the reasoning behavior of large](https://openreview.net/forum?id=Lmjgl2n11u)\n[language models - a survey.](https://openreview.net/forum?id=Lmjgl2n11u) In _First Conference on_\n_Language Modeling_ .\n\n\nOpenAI. 2024a. Gpt-4o system card. [https://](https://cdn.openai.com/gpt-4o-system-card.pdf)\n[cdn.openai.com/gpt-4o-system-card.pdf.](https://cdn.openai.com/gpt-4o-system-card.pdf) Accessed: 2024-09-26.\n\n\nOpenAI. 2024b. Learning to reason with\n[llms.](https://openai.com/index/learning-to-reason-with-llms/) [https://openai.com/index/](https://openai.com/index/learning-to-reason-with-llms/)\n[learning-to-reason-with-llms/.](https://openai.com/index/learning-to-reason-with-llms/)\nOpenAI. 2025. Openai o3-mini system card. [https:](https://openai.com/index/o3-mini-system-card/)\n[//openai.com/index/o3-mini-system-card/.](https://openai.com/index/o3-mini-system-card/)\nOpenO1. 2024. Open-o1. [https://opensource-o1.](https://opensource-o1.github.io/)\n[github.io/.](https://opensource-o1.github.io/)\nJingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu,\nDi Jin, Qifan Wang, and Lifu Huang. 2023. [The](https://arxiv.org/abs/2305.14999)\n[art of socratic questioning:](https://arxiv.org/abs/2305.14999) Recursive thinking with\n[large language models.](https://arxiv.org/abs/2305.14999) _Preprint_, arXiv:2305.14999.\n\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. _arXiv_ _preprint_\n_arXiv:2402.03300_ .\n\n\nCharlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2025. Scaling LLM [test-time](https://openreview.net/forum?id=4FWAwZtd2n) compute\n[optimally can be more effective than scaling param-](https://openreview.net/forum?id=4FWAwZtd2n)\n[eters for reasoning.](https://openreview.net/forum?id=4FWAwZtd2n) In _The Thirteenth International_\n_Conference on Learning Representations_ .\n\n\nMingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou,\nand Yu Cheng. 2025. Prmbench: A fine-grained\nand challenging benchmark for process-level reward\nmodels. _arXiv preprint arXiv:2501.03124_ .\n\n\nQwen Team. 2024a. Qwen2.5: [A party of foundation](https://qwenlm.github.io/blog/qwen2.5/)\n[models.](https://qwenlm.github.io/blog/qwen2.5/)\nQwen Team. 2024b. Qwq: [Reflect deeply on the bound-](https://qwenlm.github.io/blog/qwq-32b-preview/)\n[aries of the unknown.](https://qwenlm.github.io/blog/qwq-32b-preview/)\nGuiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong\nZhou, Yurou Dai, Wen Yin, Zhejian Yang, Jiangyue\nYan, Yao Su, and 1 others. 2025. A survey on posttraining of large language models. _arXiv_ _preprint_\n_arXiv:2503.06072_ .\n\n\nLuong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun,\nXiaoran Jin, and Hang Li. 2024. [ReFT: Reasoning](https://doi.org/10.18653/v1/2024.acl-long.410)\nwith [reinforced](https://doi.org/10.18653/v1/2024.acl-long.410) fine-tuning. In _Proceedings_ _of_ _the_\n_62nd_ _Annual_ _Meeting_ _of_ _the_ _Association_ _for_ _Com-_\n_putational_ _Linguistics_ _(Volume_ _1:_ _Long_ _Papers)_,\npages 7601–7614, Bangkok, Thailand. Association\nfor Computational Linguistics.\n\n\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai\nDai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\n2024. Math-shepherd: [Verify and reinforce LLMs](https://doi.org/10.18653/v1/2024.acl-long.510)\n[step-by-step without human annotations.](https://doi.org/10.18653/v1/2024.acl-long.510) In _Proceed-_\n_ings of the 62nd Annual Meeting of the Association_\n_for Computational Linguistics (Volume 1:_ _Long Pa-_\n_pers)_, pages 9426–9439, Bangkok, Thailand. Association for Computational Linguistics.\n\n\nPeiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai\nDai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\n2023. Math-shepherd: Verify and reinforce llms stepby-step without human annotations. _arXiv preprint_\n_arXiv:2312.08935_ .\n\n\nGuangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang,\nHaolin Liu, Yifan Yang, Fangyuan Chen, Zhixing\nSong, Dengyu Wang, Minjia Zhang, and 1 others.\n2025. Rag-gym: Optimizing reasoning and search\nagents with process supervision. _arXiv_ _preprint_\n_arXiv:2502.13957_ .\n\n\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao,\nBowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu,\nMingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang\nRen, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via\nself-improvement. _arXiv preprint arXiv:2409.12122_ .\n\n\nLunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran\nKazemi, Aviral Kumar, and Rishabh Agarwal. 2024.\nGenerative verifiers: [Reward modeling as next-token](https://openreview.net/forum?id=CxHRoTLmPX)\n[prediction.](https://openreview.net/forum?id=CxHRoTLmPX) In _The 4th Workshop on Mathematical_\n_Reasoning and AI at NeurIPS’24_ .\n\n\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen\nZhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025. The lessons of\ndeveloping process reward models in mathematical\nreasoning. _arXiv preprint arXiv:2501.07301_ .\n\n\nJian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou,\nJunqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing\nQi, Xiu Li, and Bowen Zhou. 2025. [Genprm:](https://arxiv.org/abs/2504.00891) Scal[ing test-time compute of process reward models via](https://arxiv.org/abs/2504.00891)\n[generative reasoning.](https://arxiv.org/abs/2504.00891) _Preprint_, arXiv:2504.00891.\n\n\nChujie Zheng, Zhenru Zhang, Beichen Zhang, Runji\nLin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench:\nIdentifying process errors in mathematical reasoning.\n_arXiv preprint arXiv:2412.06559_ .\n\n\nZihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu,\nJindong Wang, Derek F. Wong, Xiaowei Huang, Qiufeng Wang, and Kaizhu Huang. 2025. [Is your model](https://openreview.net/forum?id=nDvgHIBRxQ)\n[really a good math reasoner?](https://openreview.net/forum?id=nDvgHIBRxQ) evaluating mathemati[cal reasoning with checklist.](https://openreview.net/forum?id=nDvgHIBRxQ) In _The Thirteenth Inter-_\n_national Conference on Learning Representations_ .\n\n\n**A** **Experimental Details**\n**Abbreviation of Sub-Categories** The full names\nof abbreviations used in our experiments are shown\nin Table 5.\n\n\n**Abbr.** **Full Name** **Reasoning Pattern**\nTT. Transformation Inconsistency Tansformation\nTF. Transformation Counter-Factuality Transformation\nDC. Decomposition Incompleteness Decomposition\nDR. Decomposition Redundancy Decomposition\nDS. Decomposition Unsoundness Decomposition\nGP. Regather Imprecision Regather\nGC. Regather Incompleteness Regather\nGR. Regather Redundancy Regather\nCE. Correction Error Verification\nDE. Detection Error Verification\nCF. Conclusion Counter-Factuality Deduction\nCT. Conclusion Inconsistency Deduction\nCV. Conclusion Invalidity Deduction\nPC. Premise Incompleteness Deduction\nPR. Premise Redundancy Deduction\nPS. Premise Unsoundness Deduction\nIC. Integration Incompleteness Integration\nIT. Integration Inconsistency Integration\nIR. Integration Redundancy Integration\nIS. Integration Unsoundness Integration\nTable 5: The full names of abbreviations.\n\n\n**Implementation Details** For the training of Socratic reasoning model, we use LoRA tuning (Hu\net al., 2021) to fine-tune a Qwen2.5-72B-Instruct\nwith LLaMA-Factory library [2] . For the evaluation of open-source PRMs, we utilize PRM Eval\nToolKit [3] for implementation. For the evalutation of\nLLMs prompted as critic models, we prompt LLMs\nwith the prompt template in Table 6, with default\ntemperature set to 1.0. During the test case construction procedure, we select _N_ = 150 samples\nfrom metadata set _D_ _[′]_, including 10 samples from\nGSM8k and 50 samples each from Omni-Math,\nMathBench, and OlympiadBench.\n\n\n**B** **Prompts**\nAs described in Section 3, LLMs play a crutial role\nin our method. In the socratic reasoning curation\nstage, the prompt for step-level verification is illustrated in Table 7. In the test case construction\nstage, we follow (Song et al., 2025) and design task\nprompt and output format prompt seperately, as\nshown in Table 8 and Table 9 respectively. For the\nLLM-based filering procedure, we use the prompt\ntemplate in Table 10.\n\n\n2https://github.com/hiyouga/LLaMA-Factory\n3https://github.com/ssmisya/PRMBench\n**Prompt Template for Evaluation of LLMs prompted as critic models**\n**[System Prompt]**\nYou are a mathematical reasoning evaluator. Your task is to analyze mathematical problem-solving\nsteps and provide structured assessments in JSON format.\n\n\nFor each solution step, you need to evaluate its Validity Score (-1 to +1):\n\n- +1: Completely correct mathematical reasoning\n\n- 0: Partially correct with some mistakes\n\n- -1: Completely incorrect\n\n- Use any value in between to indicate varying degrees of correctness\nRequirements:\n\n- Evaluate each step independently\n\n- Provide scores as floating-point numbers\n\n- Return results in strict JSON format: {\"validity \": [scores]}\n\n- Ensure the array have the same length as the number of steps\n\n- Maintain mathematical rigor in your evaluation\n\n- Consider mathematical accuracy, logical coherence, and solution efficiency\nExample output format:\n\n\n{\"validity \": [0.8, -0.5, 1.0]}\nYou will be presented with a mathematical problem and its step-by-step solution. Please analyze each\nstep and provide your evaluation in the specified JSON format.\n\n\n**[User]**\nQuestion: {question}\nSolutions: {solution}\nTable 6: Prompt template for evaluation of LLMs prompted as critic models\n**Prompt Template for Step Verification**\nYou are an expert on reasoning process verification, you will be given a question, a solution(split into\nparagraphs, enclosed with tags and indexed from 1, and a reference answer.\n\n\n**[Question]**\n{question}\n**[Solution]**\n{solution}\n**[Reference Answer]**\n{answer}\nYour task is to review and critique the solution paragraph by paragraph. Once you identify an error in a\nparagraph, return the index of the paragraph where the earliest error occurs. Otherwise, return the index\nof -1 (which typically denotes \"not found\"). Please put your final answer (i.e., the index) in \\boxed{}.\n\n\nTable 7: Prompt template for step verification.\n\n\n**Task Prompt for Test Case Construction**\nYou are a helpful AI assistant that is very good at reasoning and data construction. Now I want to\ntest the ability of process-level reward models to judge whether a step within reasoning process is\ncorrect. To do this, please help me build flawed cases by introducing specific types of errors into a\ngiven reasoning process.\nYou will be provided with:\n1. A mathematical problem.\n2. A correct step-by-step reasoning process used to solve it. Each step is in a form of Action, posssibly\nincluding [Transformation], [Decomposition], [Regather], [Deduction], [Verification], [Integration],\n[Answer], [LVerification] and [GVerification].\n\n\nThe description of Actions are as follows:\n\n\n## [Transformation] (Identifier: <Repeat>xxx</Repeat>)\n\n- Explain the problem from a problem-solving perspective\n\n- Gain a more comprehensive and clear understanding of the problem through rephrasing\n\n\n## [Decomposition] (Identifier: <Decomposition>xxx</Decomposition>)\n\n- Break down the problem into several core sub-problems; resolve the main problem by tackling each\nsub-problem\n\n- If no breakdown is necessary, provide the solution approach\n\n\n## [Regather] (Identifier: <Regather>xxx</Regather>)\n\n- Collect key information from the input relevant to solving the problem\n\n- Output definitions, principles, and other concepts related to solving the problem, and provide\nexplanations\n\n\n## [Deduction] (Identifier: <Deduction>xxx</Deduction>)\n\n- Observe existing information and extract key parts\n\n- Identify explicit and implicit requirements, considering constraints and limitations\n\n- Propose concrete ideas for solving the problem\n\n- Execute reasoning according to the ideas\n\n\n## [LVerification]&[GVerification] (Identifier: <L(G)Verification>xxx</L(G)Verification>)\n\n- Verify the logical consistency of the reasoning process\n\n- Check the reasoning process against existing evidence\n\n- Look for potential flaws in the reasoning process and refine them\n\n- Review the completeness of understanding\n\n- Question your assumptions and consider alternative viewpoints\n\n- [LVerification] may occur after any reasoning step, verifying local steps\n\n- [GVerification] only occurs between [Integration] step and [Answer] step, verifying global process\n\n\n## [Integration] (Identifier: <Integration>xxx</Integration>)\n\n- Integrate all current reasoning processes to form the current conclusion\n\n\n## [Answer] (Identifier: <Answer>xxx</Answer>)\n\n- Output the final answer to the original problem\nYour task is to modify the question, adjust original steps, or introduce additional steps into the original\nprocess chain to create a reasoning process that appears plausible but is incorrect, which leads to a\nwrong answer. The objective is to simulate flawed solutions by incorporating the specified error detailed\nafter ’### Error Type to Introduce’.\n\n\n### Error Type to Introduce\n{Error type}\nTable 8: Task prompt for test case construction.\n\n\n**Output Format Prompt for Test Case Construction**\n\n\n### Formatting Instructions:\n\n\nAfter making the modifications, provide the following structured output:\n{\n\"original_question\": \"The original mathematical problem.\",\n\"modified_question \": \"The modified problem or original problem, \"\n\"original_process\": [\"original_step 1 \", \"original_step 2\", ...],\n\"modified_process\": [\"modified_step 1\", \"modified_step 2 \", ...],\n\"modified_steps\": [1, 5, 7, . . . ],\n\"error_steps\": [5, 6, . . . ],\n\"reason\": \"Explanation for the changes.\"\n}\nDetailed Requirements:\n1. original_question: A string representing the original mathematical problem as provided.\n2. modified_question: A string representing the modified problem after your changes. If the problem\nremains the same, you can copy the original question.\n3. original_process: A non-empty list of strings representing the original reasoning steps provided as\ninput.\n4. modified_process: A non-empty list of strings representing the reasoning process after your\nmodifications.\n5. modified_steps: A non-empty list of integers indicating the indexes of all modified steps. Indexing\nstarts at 1.\n6. error_steps: A non-empty list of integers representing the steps that contain hallucinations or errors.\nThese should also be part of modified_steps.\n7. reason: A clear explanation of the modifications made, why they were introduced, and how they\nalign with the specified error types.\n\n\n### Notes:\n1. Ensure all lists are non-empty.\n2. Use LaTeX format for all mathematical symbols (e.g., _x_ [2] for _x_ squared). Do not use Unicode\nsymbols such as \\u2248 or \\u00f7.\n3. Ensure the JSON object is well-formed, with proper escaping for special characters like backslash n\n(e.g., use backslash backslash n for newlines).\n4. All indexes start from 1, that is, the first step’s index is 1, not 0.\n5. You can choose to modify the question or not, if the question remains the same, you can copy the\noriginal question. But if the question is modified, ensure that the steps is judged based on the\nmodified question.\n6. Please give original process as provided by the prompt, do not modify it.\n\n\nTable 9: Output format prompt for test case construction.\n\n\n**Prompt Template for LLM-based Filtering**\nYou are an expert on reasoning process verification, you will be given a question, a solution (split into\nparagraphs, enclosed with tags.\n\n\nYour task is to decide whether the step-by-step solution generated by LLMs satisfies:\n1. The process generated by LLMs seems like a possible solution path that could happen.\n2. The process generated by LLMs is exactly wrong and the type of error is suitable for the description\nof [classification]\n**[Classification]**\n{classification}\n**[Question]**\n{question}\n**[Solution]**\n{Solution}\nPlease answer a “Yes” if both of the two aspects are satisfied, otherwise answer ’No’.\nPlease put your final answer (Yes or No) in \\boxed {}.\n\n\nTable 10: Prompt template for LLM-based Filtering\n\n\n",
    "kor_md": "**소크라테스-PRM벤치: 체계적** **추론** **패턴을** **통한** **프로세스** **보상** **모델** **벤치마킹**\n**Xiang Li** **[1,2,3]** **, Haiyang Yu** **[3]** **, Xinghua Zhang** **[3]** **, Ziyang Huang** **[1,2]** **, Shizhu He** **[1,2]** [ *],\n**Kang Liu** **[1,2]**, **Jun Zhao** **[1,2]**, **Fei Huang** **[3]**, **Yongbin Li** **[3 ]** [1] 중국과학원 자동화연구소\n( [2)] 중국과학원 대학 인공지능 학부\n3 알리바바 그룹 통이 연구소\n**초록**\n프로세스 보상 모델(PRM)은 각 중간 추론 단계의 정확성\n을 검증함으로써 복잡한 추론 및 문제 해결 작업(예: 장기\n적 의사 결정을 수행하는 대규모 언어 모델 에이전트)에서\n핵심적인 역할을 합니다. 실제 시나리오에서 대규모 언어\n모델은 문제 해결을 위해 다양한 추론 패턴(예: 분해)을 적\n용할 수 있으며, 이로 인해 다양한 추론 패턴 하에서 오류\n가 발생할 가능성이 있습니다. 따라서 PRM은 추론 과정\n중 다양한 추론 패턴 하에서의 오류를 식별할 수 있어야\n합니다. 그러나 기존 벤치마크는 주로 단계별 정확성을 평\n가하는 데 초점을 맞추어 다양한 추론 패턴 하에서의 체계\n적인 PRM 평가를 간과해 왔습니다. 이러한 격차를 해소하\n기 위해, 우리는 _변환(Transformation_ ), _분해_\n( _Decomposition_ ), _재수집(Regather_ ), _추론(Deduction)_, _검_\n_증_ ( _Verification_ ), _통합(Integration_ ) 등 여섯 가지 추론 패\n턴 하에서 PRM을 체계적으로 평가하는 새로운 벤치마크\n인 SOCRATIC-PRMBENCH를 소개합니다. SOCRATIC\nPRMBENCH는 앞서 언급한 여섯 가지 추론 패턴 내에서 결\n함이 있는 2995개의 추론 경로로 구성됩니다. 비판 모델\n로 프로ンプ팅된 PRM과 LLM에 대한 실험을 통해 기존\nPRM의 현저한 결함을 확인하였다. 이러한 관찰은 다양한\n추론 패턴 하에서 추론 단계를 평가하는 데 있어 현재\nPRM의 중대한 취약점을 부각시킨다. SOCRATIC\nPRMBENCH가 다양한 추론 패턴 하에서 PRM을 체계적으\n로 평가하는 포괄적인 테스트베드 역할을 하고 향후 PRM\n발전의 토대를 마련하기를 바란다 [1] .\n\n\n**1** **서론**\n검증 가능한 보상 기반 강화 학습(RLVR) (Trung et al., 2024;\n\nShao et al., 2024) 및 실행 시 스케일링(Snell\n\n\n   - 교신저자\n1 본 연구의 코드와 데이터는 [https://github. com/Xiang-Li-](https://github.com/Xiang-Li-oss/Socratic-PRMBench)\n[oss/Socratic-PRMBench](https://github.com/Xiang-Li-oss/Socratic-PRMBench) 에서 확인할 수 있습니다.\n\n\n\n![](assets/md_images/소크라틱-AI2-SOCRATIC-PRMBENCH--BenchmarkingProcess-Reward-Models-with-Systematic-Reasoning-Patterns-ko.pdf-0-1.png)\n\n의 값은 ?\n\n\n\n\n\n\n\n_**1**_ 단계 _**.**_ 이 문제는 가\n의 해를 구하라는 것입니다 . 초기 조건은 일 때,\n_**2**_ _**단계**_ _**.**_ 따라서 우리는 다음을 수행해야 합니다 : 미분 방정식을 표준형으로 다시 작성합니다 .\n\n\n적분인자를 사용하여 미분방정식을 풀어야 합니다 . 방정\n식에 대입하고\n_**단계**_ _**3.**_ 미분 방정식을 표준형으로 다시 쓰기 :\n\n\n\n\n\n\n\n\n\n\n\n_**단계**_ _**5.**_ 평가, 대입 해에 대입 :\n\n\n\n그림 1: (왼쪽): 주어진 질문에서 추론 단계 2와 5에 오류가 존재함. (\n가운데): 각 단계는 특정 추론 패턴을 적용함. (오른쪽): 프로세스 보\n상 모델은 연역 패턴의 오류를 성공적으로 탐지하지만 분해 추론 패\n턴에서는 실패함.\n\n\net al., 2025; Bansal et al., 2025)는 복잡한 추론 및 의사 결정 작업\n에서 상당한 능력을 입증했습니다. 이러한 발전에서 프로세스\n보상 모델(PRMs)(Lightman et al., 2024; Wang et al., 2023;\n\nZhang et al., 2025)은 특히 장기적 의사결정 단계를 포함하는\nLLM 에이전트에 있어 핵심적인 역할을 수행한다(Choudhury,\n2025; Ma et al., 2025; Xiong et al., 2025). 추론 과정 중 단계별\n보상을 제공함으로써 PRM은 보다 정확하고 밀도 높은 보상 신\n호를 제시하며, 이는 결국 LLM의 최적화와 추론 경로의 탐색을\n유도한다(Tie et al., 2025; Ji et al., 2025).\n\n\n그러나 추론 과정에서 대규모 언어 모델(LLMs)이 적용하는\n다양한 추론 패턴(Dong et al., 2023; Li et al., 2024)은 추론 결과\n에 대한 정확한 보상을 일관되게 제공하는 데 있어 추론 결과 평\n가 모델(PRMs)에 도전 과제를 제기한다. 그림 1은 이러한 시나\n리오를 보여준다: 고대 그리스 철학자 소크라테스의 이론(Dong\net al., 2023; Qi et al., 2023)에 따르면, 단계 1의 추론 패턴은 '변\n환', 단계 2는 '분해', 단계 3-5는 _'연역'이다_ . 기존 PRM은 단계 5(\n_연역_ 패턴)의 오류를 식별하지만, 분해 패턴에서 비롯된 이 오류\n의 근본 원인은 탐지하지 못한다.\n\n\n**테스트** **케이**\n**주석** **작성자**\n**스** **크기**\n**세분화된** **클래스** **추론** **패턴** _[†]_\n**평균** **단계**\n**수**\n**PRM**\n**벤치마크?**\n\n\n\n**오류** **유형** **감지**\n**?**\n\n\n\nRMBench (Liu 외, 2025) ✗ ✗ 1 1 합성 + 인간 1,327    CriticBench (Lin et al., 2024) ✗ ✗ 1 1     -     -     MathCheck-GSM (Zhou 외, 2025) ✗ ✗ 1 1 합성 516    ProcessBench (Zheng et al., 2024) ✓ ✗ 1 1 인간 3,400 7.1\nPRMBench (Song 외, 2025) ✓ ✓ 9 1 합성 + 인간 6,216 13.4\n소크라테스-PRMBENCH ✓ ✓ 20 6 합성 + 인간 2995 8.7\n표 1: 제안된 SOCRATIC-PRMBENCH와 보상 모델 평가를 위한 다른 벤치마크 또는 데이터셋 비교. _[†]_ : 벤치마크 내에서 포함된 추론 패턴의 수\n.\n\n\n\n특히, 단계 2에서 미분 방정식 해에 특정 점을 대입하여 상수 _C_\n_를_ 계산하는 과정을 생략함으로써, 이후 추론 과정 전반에 걸쳐\n_C가_ 미결정 상태로 남아 최종 답변에 오류가 발생한다. 이 관찰\n결과는 현재의 PRM이 다양한 추론 패턴에 대해 신뢰할 수 없음\n을 시사한다.\n\n\n다양한 추론 패턴에 걸친 PRM의 오류 탐지 능력을 포괄적으\n로 평가하기 위해 체계적이고 세분화된 벤치마크인 SOCRATIC\nPRMBENCH를 소개한다. 체계적인 평가가 제한적이었던 기존 벤치\n마크(Zheng et al., 2024; Song et al., 2025)와 달리, 고대 그리스\n철학자 소크라테스에서 영감을 받아 _변환_, _분해_, _재수집_, _추론_, _검_\n_증_, _통합이라는_ 6가지 추론 패턴에 걸쳐 PRM의 오류 탐지 능력을\n평가하도록 설계했습니다. 구체적으로, SOCRATIC-PRMBENCH는\n2995개의 추론 경로로 구성되며, 오류는 추론 패턴에 따라 6개의\n주요 범주와 20개의 세분화된 오류 유형 하위 범주로 분류됩니\n다. SOCRATIC-PRMBENCH의 데이터 주석 과정은 대규모 언어 모델\n(LLM)을 사용하여 완전히 자동화되어, 광범위한 인적 노동의 필\n요성을 없앴습니다. 규칙 기반 필터링을 통해 데이터의 난이도를\n보장하고, 전문가의 수동 검토를 통해 데이터의 품질을 보장합니\n다.\n\n\n우리는 오픈소스 PRM을 비롯해 범용 및 추론 특화 대규모 언\n어 모델(LLM)을 포함한 광범위한 모델에 대한 심층 실험을 수행\n했습니다. 연구 결과는 현재 PRM의 상당한 개선 여지를 드러냈\n습니다. 특히 최고 성능을 보인 Qwen2.5-Math-PRM조차 전체\n점수 68.0에 그쳤습니다. 상세한 분석 실험을 통해 우리는 다양\n한 추론 패턴에 걸쳐 현재 PRM의 오류 탐지 능력에 상당한 차이\n가 있음을 확인했으며, 오류 단계 식별의 명백한 지연과 보상 생\n성의 상당한 편향도 발견했습니다. 평가를 위해 SOCRATIC\nPRMBENCH를 활용함으로써, 우리는 추론 패턴의 관점에서 PRM\n을 종합적으로 평가할 수 있는 방법을 제시합니다.\n\n\n\n이것은 향후 PRM 개발에서 보상 해킹 위험을 완화하는 데 잠재\n적으로 도움이 될 수 있습니다. 전반적으로 우리의 기여는 다음\n과 같이 요약됩니다:\n\n\n  - 추론 패턴 관점에서 최초의 체계적인 PRM 벤치마크인\nSOCRATIC-PRMBENCH를 제안합니다. 이는 프로세스 보상 모\n델에 대한 포괄적이고 세분화된 평가를 위해 2995개의 샘\n플로 구성됩니다.\n\n\n  - 고대 그리스 논리 이론(Qi et al., 2023)을 기반으로 한\nSOCRATIC-PRMBENCH는 변환, 분해, 재집합, 추론, 검증,\n통합 등 6가지 신중하게 설계된 추론 패턴과 20개의 세분\n화된 오류 유형 하위 범주를 포함합니다. 이 체계적인 접\n근법은 2023년 10월 15일부터 11월 15일까지 진행\ners는 _변환_, _분해_, _재집합_, _추론_, _검증_, _통합_ 등 6가지 신중\n하게 설계된 추론 패턴과 20개의 세분화된 오류 유형 하위\n범주를 포함합니다. 이 체계적이고 세분화된 평가 프레임\n워크는 PRM의 포괄적 평가를 가능하게 하며 잠재적 단점\n식별을 용이하게 합니다.\n\n\n  - 우리는 SOCRATIC-PRMBENCH를 활용하여 다양한 최첨단\nPRM 및 대규모 언어 모델(LLM)에 대한 광범위한 실험을\n수행했습니다. 그 결과는 현재 PRM의 근본적인 한계를 드\n러내며, 이 분야의 향후 발전을 위한 통찰력을 제공합니다.\n\n\n**2** **관련** **연구**\n**프로세스** **보상** **모델** 프로세스 보상 모델(PRM)은 중간 추론 단계\n에 대해 보다 정확하고 밀도 높은 보상 신호를 제공함으로써 결\n과 보상 모델(ORM)에 비해 우월성을 입증하였다(Zhang et al.,\n2024; Ankner et al., 2024). 그 결과 PRM 개발에 대한 관심이 점\n차 증가하고 있다. Lightman 등(2024)은 PRM 훈련을 위한 수동\n주석 데이터셋을 기여했으며, Wang 등(2024)은 몬테카를로 추\n정법을 활용한 자동 단계 수준 라벨링 방법을 제안했다. 또한\nDong 등(2024); Zhao 등(2025)은 프로세스 보상 모델링을 생성\n작업으로 구성하고 CoT 추론을 활용하여 PRM의 생성 능력을 향\n상시켰다. PRM 훈련의 활발한 발전과 대조적으로, PRM 평가 방\n법은 여전히\n비교적 덜 발달된 상태이다. 이러한 불균형을 해소하기 위해, 우\n리는 PRM 평가를 위한 새로운 벤치마크인 SCORATIC-PRMBENCH\n를 제시한다.\n\n\n**보상** **모델** **벤치마크** 보상 벤치마크는 보상 모델 평가에 필수적\n이며, 직접적이고 정량화 가능한 측정 기준을 제공합니다. 수많\n은 벤치마크(Liu et al., 2025; Lin et al., 2024; Lambert et al.,\n2024)가 등장했음에도 불구하고, 이들은 주로 ORM 평가를 위해\n설계되었으며 단계별 주석이 전혀 없습니다. Zheng et al.\n\n(2024); Song et al. (2025)은 대규모 언어 모델(LLMs)과 인간 전\n문가를 활용해 단계별 레이블을 주석 처리하여 PRM 벤치마크를\n생성했습니다. 그러나 그들의 평가는 체계적이지 않으며, 다양한\n추론 패턴에 대한 PRM의 오류 탐지 능력 평가 필요성을 간과했\n습니다(Dong et al., 2023; Li et al., 2024). 이러한 격차를 해소\n하기 위해, 우리는 추론 패턴 관점에서 PRM을 포괄적으로 평가\n할 수 있는 체계적이고 세분화된 벤치마크인 SOCRATIC\nPRMBENCH를 제안한다. 본 SOCRATIC-PRMBENCH와 기존 보\n상 모델 벤치마크 간의 비교는 표 1에 요약되어 있다.\n\n\n**3** **소크라테스식-PRM벤치**\n**3.1** **추론** **패턴**\nSOCRATIC-PRMBENCHMARK의 추론 패턴 설계는 고대 그리스 철\n학자 소크라테스의 논리 이론에서 영감을 받았습니다. 소크라테\n스가 말했듯이, \"나는 누구에게도 아무것도 가르칠 수 없다. 나는\n그들에게 생각하게 할 뿐이다.\"라는 철학적 지혜에 따라, 우리는\n추론을 여섯 가지 원자적 추론 패턴으로 분류하고, 이 여섯 가지\n추론 패턴 내에서 총 20가지 유형의 추론 오류를 체계적으로 설\n계했습니다. 소크라테스의 논리적 틀 아래 원자적 추론 패턴과\n세분화된 오류 유형 범주는 그림 2에 설명되어 있습니다.\n\n**변환은** 문제를 동질적이거나 유사한 문제로 변환하거나 문제를\n추상화합니다. 일반적으로 문제 해결 관점에서 문제를 설명하여\n문제에 대한 보다 포괄적이고 명확한 이해를 얻는 것을 목표로\n합니다. 구체적으로 _변환_ 평가 범주는 _**변환**_ 불일치와 _**변환반사**_\n_**실성이라는**_ 두 하위 범주로 나눌 수 있습니다. _변환_ 단계 _P_ _→_ _[P′]_\n_에대해_, 변환 불일치는 _P′_ _가P와_ 논리적, 의미론적 또는 이해 측면\n에서 일관성을 결여함을 의미한다. 변환 반사실성은 사실적\n_P_ _[(′)]_ _[에서]_ 진실값 _G에_ 대한 오차. **분해는** 문제를 관리 가능한 하위 문\n제로 분할하거나 추론 단계를 위한 계획을 수립하여 각 하위 문\n제를 해결함으로써 주요 문제를 해결한다. 구체적으로, _분해_ 평\n가 범주는 세 가지 하위 범주로 나눌 수 있다: _**분해부적합성**_, _**분**_\n_**해중복성**_, _**분해불완전성**_ . _분해_ 단계 _P_ _→ {_ _P_ 1 _, P_ 2 _, ..., Pn_ _}_ _에_\n대해, 세 하위 범주는 각각 _하위_ 문제 _P(i)에서_ 발생하는 서로 다른 유\n형의 오류를 나타낸다. 이는 논리적 부등식으로 인한 부정확성,\n중요한 하위 문제 및 조건의 누락, 또는 중복된 하위 문제와 제\n약 조건의 포함으로 인해 발생할 수 있다.\n\n**재수집은** 문제 해결과 관련된 입력에서 핵심 정보를 수집하고,\n문제 해결에 중요한 원칙 및 기타 개념을 식별합니다. 구체적으\n로 _재수집_ 평가 범주는 세 가지 하위 범주로 나눌 수 있습니다:\n\n_**재수집부정확성**_, _**재수집중복성**_, _**재수집불완전성**_ . _재수집_ 단계\n_P_ _→ {_ _Q_ 1 _, Q_ 2 _, ..., Qn_ _}_ 에서 재수집 부정확성은 문제 _P_ 해결에 부적\n합한 정의의 오용이나 잘못된 정보를 포함한 _Qi_ 를 수집하는 것을\n의미합니다. 재수집 중복성은 _P와_ 관련 없는 중복되거나 관련 없\n는 정보를 수집하는 것을 의미합니다. 재수집 불완전성은 핵심\n정의, 중요한 원칙 및 개념의 부재를 가리킵니다.\n\n**추론은** 주어진 전제에서 직접 결론을 도출한다. 구체적으로 _추론_\n평가 범주는 여섯 가지 하위 범주로 나뉜다: _**전제비건전성**_, _**전제**_\n_**불완전성**_, _**전제중복성**_, _**결론무효성**_, _**결론모순성**_, _**결론반사실성**_ .\n\n_추론_ 단계 _P → C에서_, 처음 세 하위 범주는 전제에서 발생하며\n다음을 포함한다: (1) 불합리하거나 잘못된 전제로부터 추론 시\n작, (2) 전제에 중복된 가정 도입, 그리고\n(3) 핵심 조건 및 제약 사항 생략. 나머지 세 하위 범주는 결론에\n서 비롯되며 다음과 같습니다: (1) 올바른 전제에서 잘못된 결론\n을 도출하는 경우, (2) 이전 결론과 모순되는 결론을 도출하는 경\n우, (3) 알려진 사실과 일치하지 않는 결론을 도출하는 경우.\n\n**검증은** 사실적 정확성, 논리적 일관성 등의 측면에서 추론 단계\n를 검토하여 잠재적 오류를 탐지하고 반복적으로 정제합니다. 구\n체적으로 _검증_ 평가 범주는\n\n\n![](assets/md_images/소크라틱-AI2-SOCRATIC-PRMBENCH--BenchmarkingProcess-Reward-Models-with-Systematic-Reasoning-Patterns-ko.pdf-3-0.png)\n\n그림 2: SOCRATIC-PRMBENCH 개요. 왼쪽은 데이터셋 구축 절차를, 오른쪽은 6가지 추론 패턴과 20가지 세분화된 오류 유형 하위 범주를 보여\n줍니다. (하위)문제와 결론을 각각 P와 C로, 수집된 정보, 중복 내용, 실제 진실을 각각 _Q_, _R_, _G로_ 표기합니다.\n\n\n\n_**은**_ 두 가지 하위 범주로 나뉩니다: _**탐지**_ 오류와 _**수정오류**_ . 전자\n는 잘못된 결론 _C를_ 식별하지 못하는 것을 의미합니다. 반면 후\n자는 _C의_ 초기 오류를 인식했지만 수정 시도 과정에서 새로운 오\n류를 도입하여 다른 잘못된 결론 _C_ _[(′)]_ _[를]_ 초래하는 _[경우를]_ 포함합니다.\n\n**통합은** 도출된 결론을 요약하여 새로운 결론을 도출하며, 모든\n현재 추론 과정을 통합하여 최종 결론을 형성한다. 구체적으로\n_통합_ 평가 범주는 네 가지 하위 범주로 나눌 수 있다: _**통합불일**_\n_**치**_, _**통합불완전성**_, _**통합중복성**_, _**통합비건전성**_ . 통합 단계\n_{_ _C_ 1 _, C_ 2 _, ..., Cn_ _} →_ _C_, 처음 세 가지 오류 유형은 중간 결론\n_Ci_ 에서 비롯되며, 여기에는 이전 발견과 모순되는 결론의 존재,\n중요한 결론의 누락, 불필요하거나 중복된 결론의 도입이 포함\n됩니다. 마지막 오류 유형인 통합 부합성 오류는 통합된 결론이\n모두 타당성과 완전성을 만족하더라도 최종 결론 C가 부정확하\n거나 비합리적인 경우를 의미합니다.\n\n\n**3.2** **벤치마크** **구축**\n데이터셋 구축 파이프라인은 **소크라테스적** **추론** **생성(SRG)** 과\n**테스트** **케이스** **구축(TCG)** 이라는 두 가지 핵심 단계로 구성됩\n니다.\n\n\n**3.2.1** **소크라테스적** **추론** **생성**\n이 단계는 일련의 순서로 표현되는 소크라테스식 추론 과정의\n데이터 풀을 생성하는 것을 목표로 합니다.\n\n\n\n원자적 소크라테스식 추론 행위. 그림 2의 왼쪽 부분에서 설명된\n바와 같이, 각 추론 단계는 시작 태그 <[Pattern]>과 종료 태그\n</[Pattern]>로 둘러싸여 있습니다. [Pattern] 자리표시자 내부의 내\n용은 이 특정 단계를 특징짓는 구체적인 추론 패턴을 나타냅니다\n.\n\n\n**소크라테스식** **추론** **모델** **훈련** 사용 가능한 소크라테스식 추\n론 데이터가 부족함을 감안하여, 데이터 생성을 용이하게 하기\n위해 초기 단계에서 특수화된 소크라테스식 추론 모델을 훈련\n시켰다. 이를 위해 MATH-Hard (Hendrycks et al., 2021) 및\nOpen-o1 (OpenO1, 2024) 데이터 세트에서 19,000개의 인스\n턴스를 샘플링하고 GPT-4o에 기존 사고의 사슬(CoT) 주석을\n소크라테스식 추론 과정으로 변환하도록 프롬프트합니다. 그\n런 다음 Qwen2.5-72b-instruct (Team, 2024a)를 이러한 소크라테\n스식 추론 과정에 대해 미세 조정하여 _M(Socratic)_ 으로 표시되는 소\n크라테스식 추론 모델을 얻었습니다.\n\n\n**소크라테스식** **추론** **생성** 이후, 메타데이터로부터 새로운 소크라\n테스식 추론 과정을 생성하기 위해 _MSocratic_ 을 활용한다. 이를 위\n해 먼저 GSM8k (Cobbe et al., 2021), Omni-Math (Gao et al.,\n2024), MathBench (Liu et al., 2024), OlympiadBench (He et al.,\n2024a)에서 샘플을 수집합니다. 문제의 적절한 난이도를 보장하\n기 위해 Omni-Math 및 MathBench 데이터셋을 신중하게 선별했\n습니다. 구체적으로, 난이도 평가가 4.0 미만인 Omni-Math 샘플\n은 모두 제외했습니다. MathBench의 경우, 개념적 이해보다는\n이론적 적용을 강조하는 MathBench-A 하위 집합에만 집중했습\n니다. 또한 MathBench-A에서 다음으로 지정된 인스턴스만 유지\n했습니다.\n\n\n**전체** _**변환**_ _**분해**_ _**재구성**_ _**추론**_ _**통합**_ _**검증**_\n평균 단계 수 8.7 8.5 8.7 8.6 8.5 8.5 10.8\n평균 오차 단계 3.0 4.2 3.3 2.9 3.0 2.0 3.8\n평균 첫 번째 오류 단계 4.7 1.5 3.0 3.1 5.4 7.2 6.9\n평균 질문 길이 209.6 224.4 220.7 207.5 221.7 191.3 169.4\n인스턴스 수 2995 313 463 463 926 615 215\n표 2: SOCRATIC-PRMBENCH 통계.\n\n\n\n고등학교 또는 대학 수준으로. 이 절차는 최종적으로 데이터 풀\n_D_ 를 생성한다. _D_ 내의 각 질문-답변 쌍 ( _qi, ai_ )에 대해, _MSocratic_\n은 소크라테스식 추론 과정 _r(i) 을생성_ 하여 ( _qi, ri,_ aˆ _i_ ) 삼중항을\n산출한다.\n\n\n**소크라테스식** **추론** **큐레이션** 마지막으로, 각 ( _qi, ri,_ aˆ _i_ ) 튜플은\n엄격한 이중 검증 과정을 거칩니다: 먼저 답변의 정확성을 평가한\n후, 각 개별 단계를 LLM 기반 검증으로 확인합니다. 두 검증 모\n두 통과한 튜플만 유지되어 큐레이션된 메타데이터 세트 _D_ _[′]_ 가\n_생성됩니다_ . 답변 검증에는 Qwen2.5-Math(Yang et al., 2024) 방식\n을 따르며, 예측된 답변 aˆ _i_ 가 _정답 a_ 와 수치적 및 기호적 동등성\n을 모두 충족해야 합니다. 단계 검증에는 GPT-4o(OpenAI,\n2024a)를 활용하여 추론 과정의 각 개별 단계 정확성을 평가하\n며, 상세 프롬프트는 부록 B에 제시합니다.\n\n\n**3.2.2** **테스트** **케이스** **구성**\n이 단계에서는 제어된 오류 주입 절차를 활용하여 각 오류 유형\n_C_ (3.1절에서 분류된 바와 같이)에 대한 테스트 세트를 생성합니\n다. 각 오류 유형 _C_ (예: 반복 불일치)에 대해 테스트 세트 _T(C)를_ 생\n성합니다. 이는 먼저 메타데이터 세트 _D_ _[′]_ 에서 _N개의_ 샘플을 무\n작위로 선택함으로써 _[달성됩니다]_ . 그런 다음 문제 _qi_, 이중 검증 과정\n을 통해 완전히 정확함이 보장된 추론 경로 _ri_ 를 포함하는 각 샘\n플 ( _q(i)_, _ri, ai_ )에 대해, GPT-4o에게 원래 정확한 추론 과정 _r(i)를_\n수정하도록 프롬프트하여 _오류유형 C와_ 일관된 오류를 의도적\n으로 도입합니다:\n\n\nr˜ _i_ = LLM( _I,_ [ _qi, ri, ai_ ] _, C_ )\n기초 및 LLM 기반 방법을 통해 부적합한 샘플을 걸러내어, 최종\n적으로 SOCRATIC-PRMBENCH를 구축하였습니다.\n\n\n**규칙** **기반** **필터링** _지침 I에서_ 상세한 작업 설명과 출력 형식 요\n구사항을 제공했음에도 불구하고, GPT-4o는 가끔 지침 _I을_ 엄\n격히 따르지 못할 수 있습니다. 따라서 규칙 기반 필터링 방법\n을 구현합니다. 첫째, 문자열 일치 기능을 사용하여 _지침 I에서_\n요구하는 JSON 형식으로 출력을 생성하지 못한 샘플을 식별하\n고 제거합니다. 둘째, 정규 표현식을 사용하여 최종 답변을 성\n공적으로 출력하지 못한 샘플을 제거합니다.\n\n\n**LLM 기반** **필터링** 생성된 테스트 케이스의 품질을 보장하기 위\n해, 주어진 오류 유형 _C에_ 대한 테스트 세트 _TC_ 내의 각 샘플 ( _qi,_\nr˜ _i,_ a˜ _i_ ) 을 평가하기 위해 Gemini2.5-Pro를 활용합니다. 구체\n적으로, Gemini2.5-Pro가 다음 두 기준에 따라 샘플을 평가하도\n록 지시합니다: (1) 추론 경로 r˜ _i_ 가 표면적으로는 타당해 보이지\n만 근본적인 추론 오류가 포함되어 있는지, (2) 식별된 오류가 대\n상 오류 _유형 C에_ 확실히 속하는지 여부입니다. 자세한 프롬프트\n는 부록 B에 제시되어 있습니다. Gemini2.5-Pro에 의한 필터링\n후 샘플의 승인률은 92.7%에 달하며, 2995개의 샘플이 최종\nSocratic-PRMBench를 구성하기 위해 유지됩니다. Socratic\nPRMBench의 통계는 표 2에 제시되어 있습니다.\n\n\n**LLM과** **인간** **어노테이터** **간의** **일관성** Gemini2.5-Pro가 이러한\n품질 필터링 작업을 수행할 수 있는 능력을 입증하기 위해, 우리\n는 인간 어노테이터와의 일치도를 측정합니다. 우리는 각각 최소\n학사 학위를 보유한 세 명의 자원 어노테이터를 모집합니다.\n\n\n\n_T_ = _{_ _t_ = ( _q,_ r˜ _,_ a˜ ) _}_ _[N]_ (1) gree를 보유하고 있으며, 무작위로 추출된\n_C_ _i_ _i_ _i_ _i_ i=1 동일한 기준을 적용한 데이터의 10% 부분집합\nr˜ _(i)는_ 오류 유형 _C를_ 적용한 수정된 소크라테스식 추론 과정이며\n, _I는_ GPT-4o가 원래 과정 _ri를_ r˜ _i_ 로 수정하도록 지시하는 프롬\n프트이다. 상세한 프롬프트는 부록 B에 수록되어 있다.\n\n\n**3.3** **품질** **관리**\nSOCRATIC-PRMBENCH의 높은 품질과 신뢰성을 보장하기 위해,\n우리는 규칙 기반\nGemini2.5-Pro와 함께 사용합니다. 그런 다음 Gemini2.5-Pro와\n인간 주석자 간의 일치율을 계산합니다. 그 결과, Gemini2.5-Pro\n는 인간 주석자와 높은 수준의 일관성을 보여 평균 93.3%의 일치\n율을 달성했습니다. 이러한 높은 수준의 일관성은 Gemini2.5\nPro가 품질 필터링 수행에서 인간 주석자를 효과적으로 대체할\n수 있다는 강력한 증거를 제공합니다.\n\n\n**변환** **분해** **재집결** **검증**\n**모델**\n**TT.** **TF.** **DC.** **DR** **DS.** **GP.** **GC.** **GR.** **CE.** **DE.**\n\n_**프로세스보상모델(PRM)**_\n[Skywork-PRM-7B](https://huggingface.co/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B) 38.7 38.4 42.7 42.5 38.0 42.8 44.8 41.3 47.9 46.7\n[ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) 50.9 50.9 59.3 50.1 53.7 52.4 59.6 49.7 66.7 59.2\n[RLHFlow-PRM-Mistral-8B](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Mistral-Data) 50.6 52.7 46.6 47.3 42.7 38.0 44.6 48.7 53.1 49.5\n[RLHFlow-PRM-Deepseek-8B](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) 47.5 50.8 50.6 50.9 44.0 41.6 48.6 55.4 45.9 47.6\n[MathShepherd-Mistral-7B](https://huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm) 54.5 50.9 59.4 **57.4** 56.7 **60.9** 59.4 54.6 **72.7** **72.1**\n[Qwen2.5-수학-PRM-7B](https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B) **55.8** **64.3** **61.7** 51.6 **58.4** 57.5 **61.8** **58.2** 67.4 64.1\n_**비평가모델로프롬프트된대규모언어모델(LLMs)**_\n[GPT-4o](https://openai.com/index/hello-gpt-4o/) **62.4** 60.5 69.9 60.0 66.1 64.9 **74.1** 57.9 74.4 75.8\n[딥시크-R1](https://github.com/deepseek-ai/DeepSeek-R1) 51.9 **72.6** 63.4 64.4 67.1 70.9 64.6 54.8 75.0 77.1\n[QwQ-32B](https://huggingface.co/Qwen/QwQ-32B-Preview) 60.2 68.6 70.0 **67.9** 59.8 73.7 65.8 55.4 75.8 75.7\n[Gemini-2.5-Pro](https://deepmind.google/technologies/gemini/pro/) 62.3 64.4 67.3 61.4 **68.5** 70.2 69.2 **58.6** **78.3** **78.0**\n[o3-mini](https://openai.com/index/openai-o3-mini/) **62.4** 67.4 **70.4** 57.3 68.0 **77.3** 71.3 53.0 77.2 72.6\n**통합**\n**IC.** **IT.** **IR.** **IS.**\n\n\n\n**모델** **전체** **CF.** **CT.**\n\n\n\n**공제**\n**CV.** **PC.** **PR.** **PS.**\n\n\n_**프로세스보상모델(PRM)**_\n[Skywork-PRM-7B](https://huggingface.co/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B)) 43.6 42.5 41.2 40.0 41.8 42.8 39.8 38.7 42.6 39.4 44.2\n[ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) 61.9 63.6 63.6 66.3 61.9 65.2 63.5 69.7 78.2 68.7 76.1\n[RLHFlow-PRM-Mistral-8B](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Mistral-Data) 48.8 50.4 46.2 45.2 46.1 44.5 43.3 51.2 58.1 46.6 56.3\n[RLHFlow-PRM-Deepseek-8B](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) 51.5 51.5 52.4 52.0 47.6 51.4 45.2 55.3 63.7 53.3 66.7\n[MathShepherd-Mistral-7B](https://huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm) 64.4 68.0 65.9 66.5 62.4 65.9 65.4 63.1 74.2 60.1 72.3\n[Qwen2.5-수학-PRM-7B](https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B) **68.0** **74.7** **73.1** **72.2** **66.6** **72.4** **67.2** **75.0** **85.2** **69.6** **86.9**\n_**비평가모델로프롬프트된대규모언어모델(LLMs)**_\n[GPT-4o](https://openai.com/index/hello-gpt-4o/) 70.8 63.6 62.7 74.5 73.2 60.1 76.1 73.4 80.8 52.7 88.7\n[딥시크-R1](https://github.com/deepseek-ai/DeepSeek-R1) 73.0 80.8 72.6 77.2 68.6 72.0 76.9 75.9 78.9 59.9 88.6\n[QwQ-32B](https://huggingface.co/Qwen/QwQ-32B-Preview) 73.8 70.3 75.0 **85.2** **74.0** 69.5 77.5 **81.8** 83.5 58.7 96.7\n[Gemini-2.5-Pro](https://deepmind.google/technologies/gemini/pro/) 73.5 72.8 77.7 83.5 69.0 65.9 73.5 73.2 **88.9** 56.9 **96.9**\n[o3-mini](https://openai.com/index/openai-o3-mini/) **75.7** **83.3** **81.0** 81.4 73.9 **75.3** **78.6** 78.7 87.3 **72.0** 87.0\n표 3: SOCRATIC-PRMBENCH에 대한 평가 결과. (상단): _변환_, _분해_, _재수집_ 및 _검증의_ PRM 점수. (하단): _추론_, _통합_ 및 _전체_ 성능의 PRM 점수.\n\n각 범주 및 작업별 최고 성능은 **굵은** **글씨** 로 표시됨. 약어의 전체 명칭은 부록 A에 제시됨\n전체 데이터셋에 걸쳐 수행되어 방대한 수작업의 부담을 줄였습\n니다.\n\n\n**4** **실험**\n**4.1** **모델**\n본 연구에서는 프로세스 보상 모델(PRM)과 비평가 모델로 프롬\n프트된 대규모 언어 모델(LLM)이라는 두 가지 유형의 모델을 고\n려합니다.\n\n\n**프로세스** **보상** **모델(PRMs)** 은 언어 모델의 중간 추론 과정을 평\n가하고 감독하기 위해 중간 추론 단계의 주석과 함께 훈련됩니다\n. 우리의 평가에는 다음과 같은 최신 오픈소스 PRMs가 포함됩니\n다:\n\n(1) MathShepherd (Wang et al., 2023)는 각 단계가 올바른 최종\n답으로 이어질 경험적 확률을 추정하여 해당 단계의 과정 라벨\n을 획득합니다. (2) LLaMA-3.1 기반 생성형 PRM 두 가지\n(Dong et al., 2024)는 \"Yes/No\" 토큰의 출력 확률을 기반으로 정\n답 여부를 판단합니다. (3) ReasonEval(Mondorf and Plank, 2024)\n는 추론 단계의 타당성 외에도 중복성을 평가합니다. (4) 인기 수\n학 모델 Qwen2.5-Math로 훈련된 두 가지 PRM, 즉 Skywork\nPRM(He et al., 2024b)과 Qwen2.5-Math-PRM(Zhang et al.,\n2024)입니다.\n\n\n인기 수학 모델 Qwen2.5-Math로 훈련된 두 가지 PRM, 즉\nSkywork-PRM(He et al., 2024b)과 Qwen2.5-Math\nPRM(Zhang et al., 2025).\n\n\n**비평가** **모델로** **활용된** **대규모** **언어** **모델(LLMs)** 비평가 모델은\n대규모 언어 모델의 생성 능력을 활용하여 모델이 생성한 텍스\n트에 직접 피드백과 비평을 제공하는 것을 목표로 합니다. 우리\n의 평가는 GPT-4o (OpenAI, 2024a), Gemini2.5-Pro\n(Deepmind, 2025)를 포함한 범용 모델과 Deepseek-R1\n(DeepSeek-AI, 2025), QwQ-32B(Team, 2024b), o3\nmini(OpenAI, 2025) 등 추론에 특화된 모델을 모두 포함합니다\n.\n\n\n**4.2** **평가** **지표**\nPRM 평가가 결함이 있는 추론 단계의 탐지에 중점을 둔다는 점\n을 고려할 때, 정확도나 F1 점수를 직접 적용하는 것은 모델의\n내재적 편향에 영향을 받을 수 있습니다. 이러한 문제를 해결하\n기 위해, 우리는 (Song et al., 2025; Zheng et al., 2024)를 따르\n고 PRM 점수를 우리의\n평가 지표로 채택합니다. 공식적으로 다음과 같이 정의됩니다:\n\n\nPRM-Score = _w_ 1 _×_ F1neg+ _w_ 2 _×_ F1 (2)\n여기서 F1과 F1neg은 각각 F1 점수와 음성 F1 점수를 의미합\n니다. _w_ 1과 _w_ 2는 F1 점수와 음성 F1 점수의 기여도를 균형 있게\n조정하는 가중치입니다. 선행 연구(Song et al., 2025; Zheng et\nal., 2024)에 따라 _w_ 1= _w_ 2= 0 _._ 5로 설정했습니다.\n\n\n**4.3** **주요** **결과**\n우리의 평가 결과는 표 3에 제시되어 있습니다. 우리의 연구 결\n과는 다음과 같습니다:\n\n\n**PRM과 LLM의** **비교** PRM의 성능은 LLM에 비해 현저히 열등\n함이 입증되었다. 최고 성능의 PRM인 Qwen2.5-Math-PRM-7B\n조차 68.0점이라는 점수를 기록했는데, 이는 가장 낮은 성능의\nLLM인 GPT-4o보다도 낮은 수치이다. 더욱이 일부 PRM은 무작\n위 추측 수준보다 낮은 성능을 보이며, 다양한 추론 패턴에 걸친\n추론 오류 처리 능력의 한계를 드러냅니다. 이는 PRM과 LLM\n간 상당한 격차가 존재함을 시사하며, 근본적인 개선이 필요함을\n보여줍니다. PRM 데이터 주석 작업의 어려움과 합성 데이터 품\n질 보증의 복잡성이 이러한 격차의 원인으로 추정됩니다. 예를 들\n어, Math-shepherd는 최종 정답 도달 확률 추정치를 기준으로 단\n계별 정확도를 측정하는 합성 데이터를 활용하는 반면,\nQwen2.5-Math-PRM-7B는 수동으로 라벨링된 PRM800k 데이\n터셋을 사용합니다.\n\n\n**LLM 간** **비교** PRM과 달리 LLM은 정교한 언어 및 추론 능력 덕\n분에 비판적 평가에서 보다 견고하고 신뢰할 수 있는 보상을 제\n공할 잠재력을 보여줍니다. 이에 부합하게도, 추론 특화 LLM이\n범용 LLM보다 우수한 성능을 보인다는 점을 관찰했습니다. 특\n히 QWQ-32B는 오픈소스 모델 중 최고 성능을 발휘하며 GPT-4o\n보다도 우수한 결과를 보였습니다. QWQ-32B가 인상적인 성능\n을 보이지만 여전히 o3-mini에는 미치지 못한다는 점은, 오픈소\n스 모델과 독점 모델 간 문제 해결 성능 격차가 좁혀지고 있음에\n도 비평 모델로서의 역량에는 여전히 상당한 차이가 존재함을 시\n사합니다.\n\n\n**중복** **오류는** **더** **까다롭다** 동일한 추론 패턴 내에서도 세분화된\n오류 유형에 따라 성능 차이가 두드러지게 관찰되었다. 분해, 재\n구성, 통합 단계 내의 분해 중복, 재구성 중복, 통합 중복과 같은 중\n복 오류는\n변환\n100\n추론\n\n\n\n![](assets/md_images/소크라틱-AI2-SOCRATIC-PRMBENCH--BenchmarkingProcess-Reward-Models-with-Systematic-Reasoning-Patterns-ko.pdf-7-0.png)\n\n\n\n\n\n\n\n그림 3: 6가지 추론 패턴에 걸친 대표적 PRM과 LLM의 평균 PRM\n점수. PRM과 LLM 모두 불균형적인 성능을 보임.\n\n\n수집 및 통합 패턴은 동일한 추론 패턴 내 다른 오류 유형에 비해\nPRM과 LLM 모두에게 지속적으로 더 큰 도전 과제를 제기했습\n니다. 이는 중복 오류 단계가 다른 유형의 오류 단계보다 종종 더\n\"정상적\"이거나 그럴듯해 보이기 때문에, 모델이 표면 수준의 텍\n스트 단서만으로 이를 식별하는 능력을 저해하기 때문일 수 있습\n니다. 이는 현재의 PRM이 오류 탐지를 위해 표면 수준의 패턴 인\n식에 의존하는 데 한계가 있을 수 있음을 시사하며, 더 심층적인\n추론 및 분석 능력의 필요성을 강조한다.\n\n\n**4.4** **상세** **분석**\n본 절에서는 제안된 SOCRATIC-PRMBENCH에 대한 보다 정교한 분\n석을 통해, 프로세스 수준 보상을 제공하는 데 있어 기존 모델의\n한계를 규명하고 PRM의 향후 발전을 위한 통찰력을 제시하고자\n한다.\n\n\n**추론** **패턴별** **성능** **차이** 그림 3에서 볼 수 있듯이, 여섯 가지 추론\n패턴에 걸쳐 대표적 PRM과 LLM의 평균 PRM 점수를 제시합니\n다. 주목할 만한 발견은 PRM과 LLM 모두 다양한 추론 패턴에\n서 불균형적인 성능을 보인다는 점입니다. 거의 모든 모델의 성\n능은 _추론_, _통합_, _검증_ 패턴에 비해 _변환_, _분해_, 재수집 패턴에서\n일관되게 약했습니다. 이 문제는 PRM에서 더욱 두드러집니다.\n\n예를 들어, Qwen2.5-Math-PRM-7B는 통합 패턴에서 80.0에 가\n까운 PRM 점수를 달성했지만, 재수집 패턴에서는 어려움을 겪\n었습니다.\n\n\n![](assets/md_images/소크라틱-AI2-SOCRATIC-PRMBENCH--BenchmarkingProcess-Reward-Models-with-Systematic-Reasoning-Patterns-ko.pdf-8-0.png)\n\n\n\n15. 0\n\n\n12. 5\n\n\n10. 0\n\n\n7. 5\n\n\n5. 0\n\n\n2. 5\n\n\n0. 0\n\n|Qw<br>O3<br>소|Col2|Col3|en2.5-Math-PRM<br>-mini<br>크라테스-PRM벤치|Col5|Col6|MathShep|Col8|herd|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||||||\n|||||||||||||||||||\n|||||||||||||||||||\n|||||||||||||||||||\n|||||||||||||||||||\n|||||||||||||||||||\n|||||||||||||||||||\n|||||||||||||||||||\n|||||||||||||||||||\n\n\n\n2 4 6 8 10 12\n오류 위치\n그림 4: SOCRATIC-PRMBENCH의 오류 위치 분포(12개로 잘림) 및 여\n러 PRM과 LLM의 예측 오류 위치 분포.\n\n\n분해 패턴에서 60.0에 도달하기 위해 더 많은 시도가 필요했습니\n다. 이 결과는 현재 PRM 훈련 데이터 구축 과정에 잠재적 편향\n이 존재할 수 있음을 시사합니다. 기존 PRM 데이터셋은 수동으\n로 주석 처리되었든 합성 생성되었든, 다양한 추론 패턴을 적절\n히 대표하지 못하는 것으로 보입니다. _추론과_ 같은 특정 패턴의\n빈도가 더 높기 때문에, 이러한 데이터셋은 해당 패턴에 의해 지\n배되는 경향이 있으며, 결과적으로 _분해와_ 같은 드문 패턴에 대\n한 성능이 현저히 저하됩니다. 이 관찰은 추론 오류의 조기 탐지\n가 오류 전파를 완화하는 데 중요하므로, 향후 PRM 훈련 데이터\n구축 시 다양한 추론 패턴의 분포를 고려하는 것이 중요함을 강\n조합니다.\n\n\n**모델은** **오류** **단계** **식별에** **지연을** **보임** 추론 오류의 적시 탐지 능\n력을 조사하기 위해, SOCRATIC-PRMBENCH의 진실 오류 단계 위치\n분포와 대표적 PRM 및 LLM의 예측 오류 위치 분포를 비교하였\n다. 그림 4에서 알 수 있듯이, Qwen2.5-Math-PRM과 o3-mini는\n실제 분포에 비해 후반 단계로 현저히 이동하는 경향을 보이며,\n이는 초기 오류 감지에 지연이 있음을 시사합니다. 이는 초기 오\n류를 감지하는 능력이 제한적이어서 오류가 확산될 수 있음을 의\n미합니다. 반면 MathShepherd는 반대 경향을 보이며, 예측 분포\n가 추론 체인의 시작 부분으로 이동합니다. 이는 MathShepherd\n가 특히 추론 초기 단계에서 올바른 단계를 오류로 잘못 식별하는\n경향이 있음을 시사합니다. 이는 조기 탐지와 과도한 오탐지 회\n피가 모두 중요하다는 점을 시사합니다. 오류 전파는 계산 자원\n을 낭비하고 샘플링 효율을 저하시키지만,\n무작위 _[†]_ 50.0 50.0  50.0 50.0\n_**프로세스보상모델(PRM)**_\nReasonEval-7B 87.3 35.7  69.6 61.9\nSkywork-PRM-7B 22.7 93.0  44.5 43.6\nMathShepherd 73.3 56.0  67.4 64.4\nQwen2.5-Math-PRM-7B 90.8 42.9  74.5 68.0\n_**LLMs, 비평가모델로프롬프트**_\nGPT-4o 83.0 57.5  74.6 70.8\nQwQ-32B 83.9 63.1  76.8 73.8\no3-미니 82.6 69.0  78.0 75.7\nGemini-2.5-Pro 83.6 62.8  76.5 73.5\n표 4: 긍정 및 부정 테스트 케이스에서의 모델 성능 비교. _[(†)]_ _[는]_ 무작위\n추측(Random Guess)의 성능을 나타냅니다.\n\n\n과도하게 공격적인 오류 탐지는 효율성을 저해할 수 있으며, 올바\n른 추론 경로를 조기에 종료시켜 잠재적으로 최적의 해법 탐색을\n방해할 수 있다.\n\n\n**PRM의** **보상** **편향** 표 3은 일부 PRM이 무작위 추측보다도 더 낮\n은 성능을 보임을 보여주며, 이는 예측에 상당한 편향이 존재함\n을 시사합니다. 이러한 편향을 정량화하기 위해 각 모델의 올바\n른 추론 단계와 오류 추론 단계에 대한 정확도를 계산했습니다.\n\n표 4에서 볼 수 있듯이, 결과는 PRM 내부에 명확한 보상 편향이\n존재함을 드러내며, 일부 모델은 긍정적 보상을 크게 선호하는\n반면 다른 모델들은 부정적 보상을 제공하는 경향이 있습니다.\n\n예를 들어, Qwen2.5-Math-PRM-7B는 올바른 단계에서 90.8%\n의 정확도를 보이지만 오류 단계에서는 42.9%의 정확도에 그칩\n니다. 이와는 대조적으로 Skywork-PRM-7B는 오류 단계에서\n93.0%의 정확도를 보인 반면, 정답 단계에서는 22.7%의 정확도\n에 그쳤습니다. LLM은 PRM보다 덜 뚜렷한 편향을 보였으나, 정\n답과 오류 단계 간 정확도 차이는 여전히 상당했습니다. 또한 평\n가된 모든 LLM은 긍정적 보상을 선호하는 경향을 보였는데, 이\n는 비판 모델로 활용될 때 미묘한 오류를 식별하는 신뢰성을 제\n한할 수 있습니다.\n\n\n**5** **결론**\n본 연구에서는 PRM을 위한 체계적이고 세분화된 벤치마크인\nSOCRATIC-PRMBENCH를 제안한다. SOCRATIC-PRMBENCH는 2995\n개의\n개의 사례로 구성되며, 6가지 주요 추론 패턴과 20가지 세분화된\n오류 유형 하위 범주로 분류된다. 기존 PRM과 비평 모델로 활용\n된 대규모 언어 모델(LLM)에 대한 체계적이고 포괄적인 평가를\n통해, 우리는 기존 모델의 잠재적 한계점을 관찰하고 PRM 개선\n을 위한 향후 연구에 유용한 통찰력을 제공한다.\n\n\n**제한** **사항**\n비록 우리의 작업이 PRM에 대한 체계적이고 포괄적인 평가를\n제공할 수 있지만, 현재 버전의 벤치마크는 주로 수학 문제와 같\n이 객관적으로 검증 가능한 답을 가진 추론 작업에 초점을 맞추\n고 있습니다. 문헌, 의학 또는 법률과 같이 명확한 기준 답안이 종\n종 존재하지 않는 분야의 작업에 기존 데이터 구축 방법을 적용\n하는 것은 추가적인 연구가 필요합니다. 향후 벤치마크 버전에서\n는 더 광범위한 작업을 포괄하도록 확장할 계획입니다.\n\n\n**참고문헌**\nZachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang,\nand Prithviraj Ammanabrolu. 2024. [Critique-out-loud reward](https://arxiv.org/abs/2408.11791)\n[models.](https://arxiv.org/abs/2408.11791) _Preprint_, arXiv:2408.11791.\n\n\nHritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran,\nMehran Kazemi. 2025. [Smaller, weaker, yet better: Training](https://openreview.net/forum?id=3OyaXFQuDl)\n[LLM reasoners via compute-optimal sampling. In](https://openreview.net/forum?id=3OyaXFQuDl) _The Thirteenth_\n_Inter-national Conference on Learning Representations_ .\n\n\nSanjiban Choudhury. 2025. 대규모 언어 모델 에이전트를 위한 프로\n세스 보상 모델: 실용적 프레임워크와 방향. _arXiv 사전인쇄본_\n_arXiv:2502.10325_ .\n\n\n칼 코베, 비닛 코사라주, 모하마드 바바리안, 마크 첸, 준희우, 루카시\n카이저, 마티아스 플라퍼트, 제리 트워렉, 제이콥 힐튼, 나카노 레\n이이치로 외 1인. 2021.  Training verifiers to solve math word\nproblems. _arXiv preprint arXiv:2110.14168_ .\n\n\nDeepmind. 2025. Gemini2.5-pro.\n[https://deepmind.](https://deepmind.google/technologies/gemini/pro/) google/technologies/gemini/pro/.\n\n\nDeepSeek-AI. 2025. Deepseek-r1: 강화 [학습을](https://arxiv.org/abs/2501.12948) 통한 대규모 언어 모\n델의 추론 능력 촉진. _사전인쇄본_, arXiv:2501.12948.\n\n\n한제 동, 웨이 슝, 보 팡, 하오샹 왕, 한 자오, 잉보 저우, 난 장, 도옌\n사후, 카이밍 슝, 통 [장. 2024. RLHF 워크플로우: 보상](https://openreview.net/forum?id=a13aYUU9eU) 모델링에서\n[온라인 RLHF까지.](https://openreview.net/forum?id=a13aYUU9eU) _기계학습연구트랜잭션_ .\n\n\nQingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao,\nZhifang Sui, and Furu Wei. 2023. 과학을 위한 대규모 언어 모델:\n\nP 대 NP 연구. _arXiv 사전인쇄본 arXiv:2309.05689_ .\n\n\nBofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu\nDong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu,\nZhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran\nQuan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren,\nTianyu Liu,\n장바오바오. 2024. Omni-math: 대규모 [언어](https://arxiv.org/abs/2410.07985) 모델을 위한 범용 올\n림피아드 수준 수학 벤치마크. _사전출판물_, arXiv:2410.07985.\n\n\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai,\nJunhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang,\nJie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun. 2024a.\n\nOlympiadBench: 올림피아드 [수준의](https://doi.org/10.18653/v1/2024.acl-long.211) 이중 언어 다중 모달 과학\n문제로 AGI를 촉진하기 위한 도전적인 벤치마크. _제62회전산언_\n_어학회연차총회논문집 (제1권: 장문논문)_, 3828–3850쪽, 태국\n방콕. 전산언어학회.\n\n\n허주제, 위톈원, 옌루이, 류자카이, 왕차오지에, 간이멍, 투시원,\n크리스 류유하오, 쩡량, 왕샤오쿤, 왕보양, 리용콩, 장푸샹, 쉬자\n청, 안보, 류양, 저우야후이. 2024b. Skywork-o1 공개 시리즈.\n[https://huggingface.co/Skywork](https://huggingface.co/Skywork) .\n\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,\nSteven Basart, Eric Tang, Dawn Song, Jacob Steinhardt. 2021.\n\nMATH 데이터셋을 [활용한](https://openreview.net/forum?id=7Bywt2mQsCe) 수학 문제 해결 능력 측정. _제35회신_\n_경정보처리시스템컨퍼런스데이터셋및벤치마크트랙 (2차)_ .\n\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\n[Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2021. Lora:](https://arxiv.org/abs/2106.09685)\n대규모 언어 [모델의](https://arxiv.org/abs/2106.09685) 저순위 적응. _사전인쇄본_, arXiv:2106.09685.\n\n\nYixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian Mo, and\nMin Zhang. 2025. Test-time computing: from system-1 thinking\nto system-2 thinking. _arXiv preprint arXiv:2501.02497_ .\n\n\n네이선 램버트, 발렌티나 피아트킨, 제이콥 모리슨, LJ 미란다, 빌 유\n천 린, 키야티 찬두, 누하 지리, 사친 쿠마르, 톰 지크, 최예진 외 1\n명. 2024. Rewardbench: 언어 모델링을 위한 보상 모델 평가.\n\n_arXiv 사전인쇄본 arXiv:2403.13787_ .\n\n\nChangcheng Li, Xiangyu Wang, Qiuju Chen, Xiren Zhou,\nHuanhuan Chen. 2024. Mtmt: 강화된 대규모 언어 모델을 위한\n사고 트리 형성을 위한 다중 사고 모드 통합. _arXiv 사전인쇄본_\n_arXiv:2412.03987_ .\n\n\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards,\nBowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya\nSutskever, Karl Cobbe. 2024. 단계별 [검증](https://openreview.net/forum?id=v8L0pN6EOi) 방법. _제12회국제학_\n_습표현컨퍼런스_ .\n\n\n린즈청, 구즈빈, 리앙톈, 뤄루이린, 리우하오웨이, 양위주. 2024. [비](https://doi.org/10.18653/v1/2024.findings-acl.91)\n판적 사고를 위한 대규모 [언어](https://doi.org/10.18653/v1/2024.findings-acl.91) 모델 벤치마킹: CriticBench. _컴퓨_\n_터언어학회학술대회논문집: ACL 2024_, 1552–1587쪽, 태국 방\n콕. 컴퓨터 언어학회.\n\n\n류홍웨이, 정즈롱, 교위쑤안, 두안하오동, 페이즈웨이, 저우펑저, 장\n원웨이,\n장송양, 린다화, 천카이. 2024. MathBench: [계층적](https://doi.org/10.18653/v1/2024.findings-acl.411) 수학 벤치마\n크를 통한 대규모 언어 [모델의](https://doi.org/10.18653/v1/2024.findings-acl.411) 이론 및 응용 능력 평가. _계산언어_\n_학회연구성과: ACL 2024_, 6884–6915쪽, 태국 방콕. 계산언어학\n회.\n\n\nYantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li.\n\n2025. RM-bench: 언어 모델의 [보상](https://openreview.net/forum?id=QEHrmQPBdd) 모델을 미묘함과 스타일로\n[벤치마킹.](https://openreview.net/forum?id=QEHrmQPBdd) _제13회국제표현학습컨퍼런스_ .\n\n\n마잉웨이, 리용빈, 동이홍, 장쉬에, 카오롱위, 천주에, 황페이, 리빈화\n. 2025. 더 크게가 아닌 더 오래 생각하기: 테스트 시간 컴퓨팅 확\n장성을 통한 소프트웨어 엔지니어링 에이전트 강화. _arXiv 사전_\n_인쇄본 arXiv:2503.23803_ .\n\n\nPhilipp Mondorf and Barbara Plank. 2024. 정확도를 [넘어:](https://openreview.net/forum?id=Lmjgl2n11u) 대규모\n언어 모델의 추론 [행동](https://openreview.net/forum?id=Lmjgl2n11u) 평가 - 서베이. _제1회언어모델링컨퍼런_\n_스_ .\n\n\nOpenAI. 2024a. GPT-4O 시스템 카드.\n\nhttps:// [cdn.openai.com/gpt-4o-system-card.pdf](https://cdn.openai.com/gpt-4o-system-card.pdf) . 접속일:\n2024-09-26.\n\n\nOpenAI. 2024b. LLM으로 [추론하는](https://openai.com/index/learning-to-reason-with-llms/) 법 배우기.\n[https://openai.com/index/](https://openai.com/index/learning-to-reason-with-llms/)\n[learning](https://openai.com/index/learning-to-reason-with-llms/)  - [to](https://openai.com/index/learning-to-reason-with-llms/)  - [reason-with-llms](https://openai.com/index/learning-to-reason-with-llms/) /.\n\n\nOpenAI. 2025. Openai o3-mini 시스템 카드. [https:](https://openai.com/index/o3-mini-system-card/)\n[//openai.](https://openai.com/index/o3-mini-system-card/) com/index/o3-mini-system-card/.\n\n\nOpenO1. 2024. Open-o1. [https://opensource-o1. github.io](https://opensource-o1.github.io/) /.\n\n\nJingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan\nWang, and Lifu Huang. 2023. 소크라테스식 [질문의](https://arxiv.org/abs/2305.14999) 기술: 대규모\n언어 모델을 [활용한](https://arxiv.org/abs/2305.14999) 재귀적 사고. _사전_ _인쇄본_,\narXiv:2305.14999.\n\n\n샤오 지홍, 왕 페이이, 주 치하오, 쉬 런신, 송 준샤오, 비 샤오, 장 하\n오웨이, 장 밍촨, 리 YK, 우 Y, 외 1명. 2024. Deepseek-math: 오\n픈 언어 모델에서 수학적 추론의 한계를 확장하다. _arXiv 사전인_\n_쇄본 arXiv:2402.03300_ .\n\n\nCharlie Victor Snell, Jaehoon Lee, Kelvin Xu, Avi-ral Kumar. 2025.\n\n추론을 위한 매개변수 [확장보다 LLM 테스트](https://openreview.net/forum?id=4FWAwZtd2n) 시간 컴퓨팅의 최적\n확장이 더 [효과적일](https://openreview.net/forum?id=4FWAwZtd2n) 수 있음. _제13회국제학습표현컨퍼런스_ .\n\n\n송밍양, 수자오천, 취샤오예, 저우자웨이, 청위. 2025. Prmbench: 프\n로세스 수준 보상 모델을 위한 정밀하고 도전적인 벤치마크.\n\n_arXiv 사전인쇄본 arXiv:2501.03124_ .\n\n\n[Qwen Team. 2024a. Qwen2.5: 파운데이션](https://qwenlm.github.io/blog/qwen2.5/) 모델의 집합.\n\n\nQwen Team. 2024b. Qwq: 미지의 [경계를](https://qwenlm.github.io/blog/qwq-32b-preview/) 깊이 성찰하다.\n\n\n\nGuiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong Zhou,\nYurou Dai, Wen Yin, Zhejian Yang, Jiangyue Yan, Yao Su, and\n1 others. 2025. 대규모 언어 모델의 사후 학습에 관한 연구. _arXiv_\n_사전인쇄본 arXiv:2503.06072_ .\n\n\nLuong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin,\nHang Li. 2024. ReFT: 강화 [학습을](https://doi.org/10.18653/v1/2024.acl-long.410) 통한 추론. _제62회컴퓨터언_\n_어학회연차총회논문집 (제1권: 장문논문)_, 7601–7614쪽, 태국\n방콕. 컴퓨터 언어학회.\n\n\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li,\nDeli Chen, Yu Wu, Zhifang Sui. 2024. [Math-shepherd: 인간](https://doi.org/10.18653/v1/2024.acl-long.510) 주\n석 없이 단계별로 [대규모](https://doi.org/10.18653/v1/2024.acl-long.510) 언어 모델 검증 및 강화. _제62회전산언_\n_어학회연차대회논문집(제1권: 장문논문)_, 9426–9439쪽, 태국\n방콕. 전산언어학회.\n\n\n왕페이이, 리레이, 샤오즈홍, 쉬알엑스, 다이다마이, 리이페이, 천델\n리, 우위, 수이즈팡. 2023. Math-shepherd: 인간 주석 없이 단계별\n로 LLM 검증 및 강화하기. _arXiv 사전인쇄본 arXiv:2312.08935_ .\n\n\n광즈 시옹, 차오 진, 샤오 왕, 인 팡, 하오린 리우, 이판 양, 팡위안 천,\n지싱 송, 덩위 왕, 민자 장, 및 1명. 2025. Rag-gym: 프로세스 감독\n을 통한 추론 및 검색 에이전트 최적화. _arXiv_ _사전인쇄본_\n_arXiv:2502.13957_ .\n\n\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu,\nChengpeng Li, Dayiheng Liu, Jian-hong Tu, Jingren Zhou,\nJunyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu,\nXingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math 기술\n보고서: 자기 개선을 통한 수학 전문가 모델 구축. _arXiv 사전인쇄_\n_본 arXiv:2409.12122_ .\n\n\nLunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi,\nAviral Kumar, and Rishabh Agarwal. 2024. 생성 [검증기: 다음](https://openreview.net/forum?id=CxHRoTLmPX) 토\n큰 [예측으로서의](https://openreview.net/forum?id=CxHRoTLmPX) 보상 모델링. _The 4th Workshop on_\n_Mathematical Reasoning and AI at NeurIPS’24_ .\n\n\n장전루, 정추제, 우양전, 장베이천, 린런지, 유보원, 류다이하이헝, 저\n우진궈런, 린준양. 2025. 수학적 추론에서 과정 보상 모델 개발의\n교훈. _arXiv 사전인쇄본 arXiv:2501.07301_ .\n\n\nJian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao,\nDong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, and Bowen\nZhou. 2025. Genprm: 생성적 추론을 [통한](https://arxiv.org/abs/2504.00891) 프로세스 보상 모델의\n테스트 [시간](https://arxiv.org/abs/2504.00891) 계산 확장. _사전인쇄본_, arXiv:2504.00891.\n\n\n정추제, 장전루, 장베이천, 린런지, 루커밍, 위보원, 류다이하이헝, 저\n우진런, 린준양. 2024. Processbench: 수학적 추론에서의 과정 오\n류 식별. _arXiv 사전인쇄본 arXiv:2412.06559_ .\n\n\nZihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang,\nDerek F. Wong, Xiaowei Huang, Qi-ufeng Wang, Kaizhu Huang.\n2025. [Is your model really a good math reasoner? evaluating](https://openreview.net/forum?id=nDvgHIBRxQ)\n[mathematical reasoning with checklist. In](https://openreview.net/forum?id=nDvgHIBRxQ) _The Thirteenth_\n_International Conference on Learning Representations_ .\n\n\n**A** **실험** **세부** **사항**\n**하위** **범주의** **약어** 실험에 사용된 약어의 전체 명칭은 표 5에 나와\n있습니다.\n\n\n**약어** **전체** **이름** **추론** **패턴**\nTT. 변환 불일치 변환\nTF. 변형 반사실성 변형\nDC. 분해 불완전성 분해\nDR. 분해 중복성 분해\nDS. 분해 부적합성 분해\nGP. 재수집 부정확성 재수집\nGC. 재수집 불완전성 재수집\nGR. 재모음 중복 재수집\nCE. 수정 오류 검증\nDE. 탐지 오류 검증\nCF. 결론 가설적 사실성 추론\nCT. 결론 불일치 추론\nCV. 결론 무효성 추론\nPC. 전제 불완전성 추론\nPR. 전제 중복성 추론\nPS. 전제 비합리성 추론\nIC. 통합 불완전성 통합\nIT. 통합 불일치 통합\nIR. 통합 중복성 통합\nIS. 통합 비합리성 통합\n표 5: 약어의 전체 명칭.\n\n\n**구현** **세부사항** 소크라테스식 추론 모델 훈련을 위해, 우리는\nLLaMA-Factory 라이브러리 [(2)][를] 사용하여 Qwen2.5-72B\nInstruct를 미세 조정하기 위해 LoRA 튜닝(Hu et al., 2021) [을] 사\n용합니다. 오픈소스 PRM 평가를 위해, 우리는 구현을 위해\nPRM Eval ToolKit [(3)][을] 활용합니다. 비평가 모델로 프롬프트된\nLLM 평가를 위해, 기본 온도 1.0으로 설정된 상태에서 표 6의\n프롬프트 템플릿을 사용하여 LLM에 프롬프트를 제공합니다.\n\n테스트 케이스 구성 절차 중, 메타데이터 세트 _D_ _[(′)]_ _[에서]_ _N_ = 150\n개의 샘플을 선택합니다. 여기에는 GSM8k에서 10개 샘플,\nOmni-Math, MathBench, OlympiadBench에서 각각 50개 샘\n플이 포함됩니다.\n\n\n**B** **프롬프트**\n제3절에서 설명한 바와 같이, 대규모 언어 모델(LLM)은 본 방법\n론에서 핵심적인 역할을 수행합니다. 소크라테스식 추론 큐레이\n션 단계에서 단계별 검증용 프롬프트는 표 7에 제시되어 있습니\n다. 테스트 케이스 구성 단계에서는 (Song et al., 2025)를 따르며,\n각각 표 8과 표 9에 표시된 대로 작업 프롬프트와 출력 형식 프롬\n프트를 별도로 설계합니다. LLM 기반 필터링 절차에는 표 10의\n프롬프트 템플릿을 사용합니다.\n\n\n[2 https://github.com/hiyouga/LLaMA-Factory 3https://github.com/ssmisya/PRMBench](https://github.com/hiyouga/LLaMA-Factory)\n**비평가** **모델로** **프로ンプ팅된 LLM 평가용** **프롬프트** **템플릿**\n**[시스템** **프롬프트]**\n당신은 수학적 추론 평가자입니다. 당신의 임무는 수학 문제 해결 단계를 분석하고 JSON 형식으로 구조화된 평가를 제공하는 것입\n니다.\n\n\n각 해결 단계마다 유효성 점수(-1에서 +1)를 평가해야 합니다:\n\n- +1: 완전히 정확한 수학적 추론\n\n- 0: 일부 오류가 있는 부분적으로 올바른 추론\n\n- -1: 완전히 틀림\n\n- 정확도의 정도를 나타내기 위해 그 사이의 값을 사용하십시오\n요구 사항:\n\n- 각 단계를 독립적으로 평가하십시오\n\n- 점수를 부동 소수점 숫자로 제공하십시오\n\n- 엄격한 JSON 형식으로 결과 반환: {\"validity \": [scores]}\n\n- 배열의 길이가 단계 수와 동일하도록 하십시오\n\n- 평가 시 수학적 엄밀성을 유지하십시오\n\n- 수학적 정확성, 논리적 일관성, 해결 효율성을 고려하십시오. 예시 출력 형식:\n\n\n{\"validity \": [0.8, -0.5, 1.0]}\n수학 문제와 단계별 해결 과정이 제시됩니다. 각 단계를 분석하여 지정된 JSON 형식으로 평가를 제공하십시오.\n\n\n**[사용자]**\n질문: {question} 해법:\n\n\n{solution}\n표 6: 비평가 모델로 프로ンプ팅된 LLM 평가용 프롬프트 템플릿\n**단계** **검증용** **프롬프트** **템플릿**\n당신은 추론 과정 검증 전문가입니다. 질문, 해결책(단락별로 분할, 태그로 묶고 1부터 인덱싱), 그리고 참조 답변이 제공될 것입\n니다.\n\n\n**[질문]**\n{질문}\n**[해결책]**\n{해결책}\n**[참고** **답변]**\n{답변}\n여러분의 임무는 해결 방안을 단락별로 검토하고 비판하는 것입니다. 단락에서 오류를 발견하면 가장 먼저 발생하는 오류가 있는\n단락의 인덱스를 반환하십시오. 그렇지 않으면 -1 인덱스(일반적으로 \"찾을 수 없음\"을 의미함)를 반환하십시오. 최종 답변(즉, 인\n덱스)을 \\boxed{} 안에 넣어 주십시오.\n\n\n표 7: 단계 검증용 프롬프트 템플릿.\n\n\n**테스트** **케이스** **구축을** **위한** **작업** **프롬프트**\n당신은 추론과 데이터 구축에 매우 능숙한 유용한 AI 어시스턴트입니다. 이제 추론 과정 내 단계의 정확성을 판단하는 프로세스\n수준 보상 모델의 능력을 테스트하고자 합니다. 이를 위해 주어진 추론 과정에 특정 유형의 오류를 도입하여 결함이 있는 사례를\n구축하는 데 도움을 주세요.\n\n다음과 같은 것이 제공됩니다:\n1. 수학 문제.\n2. 이를 해결하기 위한 올바른 단계별 추론 과정. 각 단계는 [변환], [분해], [재구성], [추론], [검증], [통합], [답변], [L검증], [G검\n증]을 포함할 수 있는 '행동' 형태로 구성됩니다.\n\n\n각 단계의 설명은 다음과 같습니다:\n\n\n## [변환] (식별자: <Repeat>xxx</Repeat>)\n\n- 문제 해결 관점에서 문제를 설명합니다.\n\n- 문제를 재구성하여 보다 포괄적이고 명확하게 이해합니다\n\n\n## [분해] (식별자: <Decomposition>xxx</Decomposition>)\n\n- 문제를 여러 핵심 하위 문제로 분해하십시오; 각 하위 문제를 해결함으로써 주요 문제를 해결하십시오\n\n- 분해가 필요하지 않은 경우 해결 접근법을 제시하십시오\n\n\n## [재수집] (식별자: <Regather>xxx</Regather>)\n\n- 문제 해결과 관련된 입력의 핵심 정보를 수집하십시오\n\n- 문제 해결과 관련된 정의, 원칙 및 기타 개념을 출력하고 설명을 제공하십시오\n\n\n## [추론] (식별자: <Deduction>xxx</Deduction>)\n\n- 기존 정보를 관찰하고 핵심 부분을 추출하십시오\n\n- 제약 조건과 한계를 고려하여 명시적 및 암묵적 요구사항을 식별합니다\n\n- 문제를 해결하기 위한 구체적인 아이디어를 제안하라\n\n- 아이디어에 따라 추론을 실행하라\n\n\n## [LVerification]&[GVerification] (식별자: <L(G)Verification>xxx</L(G)Verification>)\n\n- 추론 과정의 논리적 일관성을 검증하라\n\n- 기존 증거에 대한 추론 과정 점검\n\n- 추론 과정의 잠재적 결함을 찾아 개선하라\n\n- 이해의 완전성을 검토하라\n\n- 가정을 의심하고 대안적 관점을 고려하라\n\n- [L검증]은 모든 추론 단계 이후에 발생할 수 있으며, 부분 단계를 검증합니다\n\n- [GVerification]은 [통합] 단계와 [답변] 단계 사이에서만 발생하며, 전체 과정을 검증합니다\n\n\n## [통합] (식별자: <통합>xxx</통합>)\n\n- 현재의 모든 추론 과정을 통합하여 현재의 결론을 형성한다\n\n\n## [답변] (식별자: <Answer>xxx</Answer>)\n\n- 원래 문제에 대한 최종 답안을 출력하라\n당신의 임무는 질문을 수정하거나, 원래 단계를 조정하거나, 원래 과정 체인에 추가 단계를 도입하여 그럴듯해 보이지만 잘못된\n추론 과정을 만들어 잘못된 답을 도출하는 것입니다. 목표는 '### 도입할 오류 유형' 뒤에 명시된 오류를 포함시켜 결함이 있는 해결\n책을 시뮬레이션하는 것입니다.\n\n\n### 도입할 오류 유형\n{오류 유형}\n표 8: 테스트 케이스 구축을 위한 작업 프롬프트.\n\n\n**테스트** **케이스** **구축을** **위한** **출력** **형식** **프롬프트**\n\n\n### 서식 지침:\n\n\n수정 후 다음 구조화된 출력을 제공하십시오:\n{\n\"original_question\": \"원래의 수학 문제.\", \"modified_question \": \"수정된 문제 또는 원본 문제\",\n\"original_process\": [\"원본 단계 1\", \"원본 단계 2\", . . . ], \"modified_process\": [\"수정된 단계 1\", \"수정\n된 단계 2\", . . . ], \"modified_steps\": [1, 5, 7, . . . ],\n\"error_steps\": [5, 6, . . . ],\n\"reason\": \"변경 사유 설명.\"\n}\n상세 요구사항:\n1. original_question: 제공된 원래 수학 문제를 나타내는 문자열.\n2. modified_question: 변경 후 수정된 문제를 나타내는 문자열. 문제가 동일하게 유지되는 경우 원본 질문을 복사할 수 있습니다\n.\n3. original_process: 입력으로 제공된 원래 추론 단계를 나타내는 비어 있지 않은 문자열 목록\n\n\n4. 수정된_과정: 사용자의 수정 후 추론 과정을 나타내는 비어 있지 않은 문자열 목록.\n5. 수정된 단계: 수정된 모든 단계의 인덱스를 나타내는 비어 있지 않은 정수 목록입니다. 인덱싱은 1에서 시작합니다.\n\n인덱싱은 1부터 시작합니다.\n6. error_steps: 환각이나 오류를 포함하는 단계를 나타내는 비어 있지 않은 정수 목록입니다. 이 단계들은 modified_steps에도 포\n함되어야 합니다.\n7. reason: 수정 내용, 수정 이유 및 수정 방식에 대한 명확한 설명.\n\n지정된 오류 유형과 일치합니다.\n\n\n### 참고 사항:\n1. 모든 목록은 비어 있지 않아야 합니다.\n2. 모든 수학 기호는 LaTeX 형식(예: x²는 _x_ [2] 로 표기)을 사용하십시오. \\u2248이나 \\u00f7 같은 유니코드 기호는 사용하지 마십\n시오.\n3. JSON 객체가 올바르게 구성되었는지 확인하십시오. 백슬래시 n과 같은 특수 문자에 대한 적절한 이스케이프 처리가 필요합니다\n(예: 줄바꿈은 \\n 사용).\n4. 모든 인덱스는 1부터 시작합니다. 즉, 첫 번째 단계의 인덱스는 0이 아닌 1입니다.\n5. 질문을 수정할지 여부를 선택할 수 있습니다. 질문이 동일하게 유지된다면 원본 질문을 복사할 수 있습니다. 그러나 질문이 수\n정된 경우, 단계별 평가가 수정된 질문을 기준으로 이루어지도록 해야 합니다.\n6. 프롬프트에서 제공된 원본 프로세스를 그대로 제시하십시오. 수정하지 마십시오.\n\n\n테이블 9: 테스트 케이스 구축을 위한 출력 형식 프롬프트.\n\n\n**LLM 기반** **필터링을** **위한** **프롬프트** **템플릿**\n귀하는 추론 과정 검증 전문가입니다. 질문과 해결책(단락별로 구분되어 태그로 묶임)이 제공됩니다.\n\n\n귀하의 임무는 대규모 언어 모델(LLM)이 생성한 단계별 해결책이 다음을 충족하는지 판단하는 것입니다:\n1. LLM이 생성한 과정은 발생할 수 있는 가능한 해결 경로처럼 보입니다.\n2. LLM이 생성한 과정은 완전히 잘못되었으며, 오류 유형은 [분류]에 대한 설명에 적합합니다.\n\n\n**[분류]**\n{분류}\n**[질문]**\n{질문}\n**[해결책]**\n{해결책}\n두 가지 측면이 모두 충족되면 '예'라고 답하고, 그렇지 않으면 '아니오'라고 답하십시오. 최종 답변(예 또는 아니오)\n을 \\boxed {} 안에 기입하십시오.\n\n\n표 10: LLM 기반 필터링을 위한 프롬프트 템플릿\n\n\n",
    "eng_pdf": "pdf/eng/소크라틱 AI2 SOCRATIC-PRMBENCH- BenchmarkingProcess Reward Models with Systematic Reasoning Patterns.pdf",
    "kor_pdf": "pdf/kor/소크라틱 AI2 SOCRATIC-PRMBENCH- BenchmarkingProcess Reward Models with Systematic Reasoning Patterns ko.pdf"
  },
  {
    "id": "paper_3",
    "title": "Explainable Reinforcement Learning Agents Using World Models",
    "eng_md": "## **Explainable Reinforcement Learning Agents Using World Models**\n\n**Madhuri Singh**, **Amal Alabdulkarim**, **Gennie Mansi** and **Mark O. Riedl**\nSchool of Interactive Computing, Georgia Institute of Technology.\n_{_ msingh365, amal, gennie.mansi, riedl _}_ @gatech.edu,\n**Abstract**\nExplainable AI (XAI) systems have been proposed\nto help people understand how AI systems produce\noutputs and behaviors. Explainable Reinforcement\nLearning (XRL) has an added complexity due to\nthe temporal nature of sequential decision-making.\nFurther, non-AI experts do not necessarily have the\nability to alter an agent or its policy. We introduce a technique for using World Models to generate explanations for Model-Based Deep RL agents.\nWorld Models predict how the world will change\nwhen actions are performed, allowing for the generation of counterfactual trajectories. However, identifying what a user wanted the agent to do is not\nenough to understand why the agent did something\nelse. We augment Model-Based RL agents with\na Reverse World Model, which predicts what the\nstate of the world should have been for the agent\nto prefer a given counterfactual action. We show\nthat explanations that show users what the world\nshould have been like significantly increase their\nunderstanding of the agent’s policy. We hypothesize that our explanations can help users learn how\nto control the agent’s execution through manipulating the environment.\n\n\n**1** **Introduction**\nExplainable AI (XAI) systems have been proposed to help\npeople understand how AI systems produce outputs and behaviors. Deep Reinforcement Learning (DRL) techniques\nlearn a neural network policy model, which attempts to predict the action that is most likely to lead to future reward.\nBecause sequential, long-term behavior is encoded into the\nneural policy model, DRL agents are notoriously hard to debug and correct if their execution behavior diverges from user\npreferences, desires, or expectations.\nIf an XAI system can help users understand how the agent\nis responding to the environment, how the policy is making predictions, or how the policy was learned, they may\nbe able to adjust to the agent or the environment, so that\nthe agent’s behavior aligns with our preferences for the policy. Because DRL is applicable to agents and robot planning,\n\n\n\n![](assets/md_images/익스플레이너블-AI-Explainable-Reinforcement-Learning-Agents-Using-World-Models.pdf-0-0.png)\n\nFigure 1: Image depicting how an agent can help a user understand\nwhy it is not performing actions to expectations, and how the user\nmight change the world to induce the desired behavior from the\nagent. In this hypothetical scenario, the user may not have understood the significance of the color of the pallet to the agent. The\nexplanation demonstrates what the environment should have been\nlike for the agent’s policy to execute the desired action. Armed with\nthis new knowledge, the user is able to alter the environment to affect the desired behavior.\n\n\nmany real-world applications, from self-driving cars to personal robotics at home, will have systems driven by DRL interacting with _users_ _without_ _technical_ _background_ . Non-AI\nexperts will seek explanations for actions that deviate from\ntheir understanding of what the agent should be doing or how\nit should be doing it [Miller, 2019]. Without an explanation,\nusers can find it difficult to trust the agent’s ability to act\nsafely and reasonably [Zelvelder _et_ _al._, 2021], especially if\nthey have limited artificial intelligence expertise. Even in the\ncase where the agent is operating optimally and without failure, the agent’s optimal behavior may not match the user’s\nexpectations, resulting in confusion and lack of trust. Non\nAI experts may also wonder what they can do to influence\nor change the behavior of the agent if they cannot alter the\ninternal workings of the agent or retrain the policy.\nExplanation of reinforcement learning systems is especially challenging due to the temporal nature of decisionmaking—any given decision to perform an action may depend on future expectations as much as, if not more than, current observations. This becomes more complicated when one\naccounts for users who are not AI experts and, furthermore,\ndo not have the ability to change the algorithm or retrain the\nagent. What would an _actionable_ explanation look like for\nnon-AI experts?\nIn this paper, we present a technique for using a World\nModel (WM) to provide actionable information to non-AI\nexperts of DRL systems. A WM describes the agent’s understanding of the state-transition dynamics of the environment. In Model-Based DRL[Kaelbling _et_ _al._, 1996], the\nagent learns a World Model through trial-and-error interactions with the environment during training. The World Model\nis used to predict the effects of the agent’s actions, often\nspeeding up policy learning when interactions with the environment are slow or resource intensive. In the scenario in\nwhich the agent fails a task or performs an unexpected action, we show that the World Model can be used to generate a\ncounterfactual explanation that shows the user what the agent\nwould have expected the world state to look like in order to\nhave chosen an action preferred by the user. _The_ _user_ _can_\n_then_ _act_ _upon_ _this_ _information_ _to_ _influence_ _the_ _agent’s_ _ac-_\n_tions_ _in_ _the_ _future_ _by_ _affecting_ _change_ _on_ _the_ _environment._\nThis is because the agent’s policy models the relationship between it and the environment with respect to action, and the\nWM makes that relationship explicit. While the user may not\nbe able to change or re-train the policy, the user can alter the\nagent’s behavior by directly manipulating the environment if\nthe agent has a robust policy.\nWorld Models used for a reinforcement learning agent to\nlearn its policy only need to predict a probability distribution\nover states that can follow a given state and a given action,\n_Pr_ ( _st_ +1 _|st, at_ ). We refer to this as the _forward world model_ .\nThe forward world model can generate a counterfactual trajectory corresponding to what the user might have indicated\nthat they expected. That is, if we give it an alternative action\n_a_ _[⋄]_ _t_ [,] [the] [WM] [will] [tell] [us] [what] [would] [have] [happened] [instead,]\n_s_ _[⋄]_ _t_ +1 [.] [What] [would] [the] [environment] [need] [to] [have] [been] [like]\nfor the agent to pick _a_ _[⋄]_ _t_ [?] [Unfortunately,] [it] [is] [not] [the] [state]\nthe agent was actually in, _st_ . We must identify some counterfactual state, _s_ _[⋄]_ _t_ [,] [that] [the] [agent] [was] _[not]_ [in] [at] [time] _[t]_ [that]\nwould have induced the agent to prefer _a_ _[⋄]_ _t_ [over] _[a][t]_ [.] [To] [gen-]\nerate _s_ _[⋄]_ _t_ +1 [,] [we] [require] [a] _[reverse]_ _[world]_ _[model]_ [that] [generates]\n_Pr_ ( _st|st_ +1 _, at_ ), the distribution over states that we should\nhave been in at time _t_ to pick the action _a_ _[⋄]_ _t_ [that] [would] [have]\ndelivered us to desired state _s_ _[⋄]_ _t_ +1 [.] [See Figure 2.]\nThe predicted counterfactual state _s_ _[⋄]_ _t_ [can] [be] [presented] [to]\nthe user as an explanation of what state the world and agent\nneeded to be in for the agent’s policy to have performed the\nexpected/desired action. Using a virtual agent performing\nhousehold tasks, We show that presentation of these states to\nnon-AI experts improves user understanding of why an agent\nfails to perform as expected and what would need to change\n\n\n\n![](assets/md_images/익스플레이너블-AI-Explainable-Reinforcement-Learning-Agents-Using-World-Models.pdf-1-0.png)\n\nFigure 2: Diagram depicting an agent’s trajectory through stateaction space. In state _st_, the agent’s policy preferred action _at_,\ncausing a transition to _st_ +1. The user wanted the agent to pick action _a_ _[⋄]_ _t_ [.] [The forward world model can tell us that the effect of] _[ a][⋄]_ _t_ [is]\nstate _s_ _[⋄]_ _t_ +1 [.] [A reverse world model can generate] _[ s][⋄]_ _t_ [, the state that the]\nagent should have been at time _t_ for its policy to pick the desired _a_ _[⋄]_ _t_ [.]\nin the environment for the agent to succeed. We further show\nthat our explanations increase user satisfaction, increase user\ntrust, and decrease user cognitive load when trying to identify\nwhy the agent failed.\n\n\n**2** **Related Work**\nExplainable Reinforcement Learning (XRL) methods can be\nclassified based on which aspect of the RL agent they explain, which can be at the Policy Level (PL), Feature importance Level (FI), or the learning and Markov Decision Process\n(LPM) [Milani _et_ _al._, 2024]. FI methods focus on providing the immediate context for single actions. Directly generated natural language explanation methods, such as rationalegeneration [Ehsan _et al._, 2019], are an example of FI explanations. LPM methods provide additional information on the effects of the MDP or training process, such as influential training experiences or how the agent acts regarding its rewards\nor objectives. Often, LPM methods require additional information, such as concepts in State2Explanation [Das _et_ _al._,\n2023], graphical causal models [Madumal _et al._, 2020; Peng\n_et_ _al._, 2022], a learned transition model [van der Waa _et_ _al._,\n2018], and partial symbolic model approximations that capture actions and their preconditions [Sreedharan _et al._, 2022],\nas well as reward decompositions [Alabdulkarim _et al._, 2025;\nDas and Chernova, 2020; Septon _et_ _al._, 2023]. Lastly, PL\nexplanations summarize long-term behaviors using abstractions or representative examples. Agent Strategy Summarization [Amir _et_ _al._, 2018] is a representative example of a PL\nmethod. Our method is an LPM method that models domain\ninformation by learning with the agent’s world model.\nCounterfactual explanations, in particular, provide highlevel, actionable information which is suitable for non-AI expert users [Gajcin and Dusparic, 2024b]. People naturally\nexplain in “the form ‘ **Why P rather than Q?** ’, in which P is\nthe target event and Q is a counterfactual contrast case that did\nnot occur” [Miller, 2019]. Counterfactual (CF) explanations\nanswer the question, what needs to be changed in the input of\na computer system such that its output changes to a different\none. In sequential decision-making paradigms the output is\nthe choice of a different action than the one the model has\ntaken or wants to take next. In sequential decision-making,\ncounterfactuals can also take the form of hypothetical future\naction trajectories to show a user what future expected states\ncould be achieved if an alternative action is taken.\nMost closely related to our work are CF explanation systems that generate counterfactual states for which the agent\nwould have chosen a different, desired action if those states\nwere encountered [Olson _et_ _al._, 2021; Huber _et_ _al._, 2023;\nSamadi _et al._, 2025]. These works employ a second round of\ntraining to create a separate, explanation generation model,\nusing action trace data produced by a pre-trained RL agent.\nThis approach has been noted to potentially create counterfactual states that contain the requisite surface features necessary to push the agent to select a new action, but does\nnot guarantee that those states are reachable in the state\nspace [Gajcin and Dusparic, 2024a]. Our technique instead generates explanations from the world model, which\nis trained as part of the RL agent’s policy training loop. Our\ntechnique does not require a separate post-hoc training phase,\nand the world model—a predictor of successor and predecessor states—is more likely to produce traversable states.\nThe RACCER system [Gajcin and Dusparic, 2024a] generates counterfactual states through a search of reachable states\nfor those with the desired properties. It requires access to\nthe execution environment—a simulation, or real-world—to\nconduct the search. Our technique does not require access to\nthe execution environment, as the world model is a learned\nsurrogate for the environment’s transition dynamics.\nCOViz [Amitai _et_ _al._, 2024] takes a different approach to\ncounterfactuals and generates states that are predicted to be\nvisited if an alternative action were to be chosen over the actually chosen action. This is potentially useful information\nto users in evaluating whether an alternative is better than an\nactual action choice. This approach is complementary to our\nwork, which helps users determine how to induce the agent\nto take a different action without explicit override.\n\n\n**3** **Generating Actionable Explanations**\nA critical question in XAI is what an _actionable_ explanation\nwould be for reinforcement learning. Actionability refers to\nhow information in explanations helps users take actions in\nresponse to an underlying AI system[Mansi and Riedl, 2023].\nFor an AI system developer, actionable explanations about a\nRL system often center actions that involve changing the policy, such as correcting the algorithm or altering how the AI\nsystem is trained. Non-AI expert end-users cannot change\nthe policy, but explanations for RL systems can still be actionable.\nResearchers [Alabdulkarim _et al._, 2025; Chakraborti _et al._,\n2021; Das _et_ _al._, 2023] have proposed that explainable RL\nshould update the user’s understanding of how the agent responds to the environment. By changing users’ mental model\nof the agent, explanations can also help users understand how\nthey can change _their_ _own_ _behavior_ . For example, policies\nare learned responses to the local environmental state respective to task reward. Consequently, explanations can be ac\ntionable by helping users understand how they can change\nthe environment, so the RL system responds as they wish.\nIn order to enable this kind of actionability, it is critical\nfor users to have a mental model of how changes in the environment influence the agent’s behavior. Our method provides this information by showing users what the environment should have looked like for the agent to take an alternate\npath. This can allow users to physically alter the environment\nin order to gain control over an agent, helping users respond\nto an agent that is not executing as expected or desired.\nWe hypothesize that non-AI expert end-users should, with\nour explanations, be able to identify the root cause of what\nfeatures of local environmental observations resulted in an AI\nagent performing an unexpected or undesirable action. We\ntest this hypothesis in Section 6. In constructing this understanding, we further speculate that users that receive our explanations can more easily reason about what they could do to\nadjust the agent’s environment to induce the desired behavior\nin the future.\n\n\n**4** **Preliminaries:** **Model-Based RL**\nBefore we present our world modeling technique, we review\nreinforcement learning and the specific version of modelbased RL that we build upon.\nReinforcement learning solves sequential decision-making\nproblems. Specifically, it generates a policy, _π_ ( _s_ ) _→_ _a_, that\nmaps a state observation _s_ to an action _a_ such that executing\nthe action optimizes future expected reward if the policy is\nfollowed henceforth. Deep reinforcement learning approximates the policy with a neural network. The policy model is\ncharged with interpreting the state observation with respect to\nfuture expected reward. The policy thus captures the dynamics of the agent with respect to the environment and task.\nModel-based reinforcement learning is an RL approach\nthat involves learning an environment model and then using it\nto train the policy [Kaelbling _et al._, 1996]. The World Model\n[Ha and Schmidhuber, 2018], also referred to as the transition dynamics model, approximates the distribution over next\nstate observations given a state and an action: _Pr_ ( _st_ +1 _|s, a_ ).\nWhile a world model is not strictly required for reinforcement learning, in many cases, it reduces the number of interactions an agent must conduct in the actual environment\nbecause the agent can simulate world state transitions with its\nworld model.\nThe DreamerV3 [Hafner _et al._, 2025] framework is one of\nthe more successful model-based DRL frameworks. It incrementally learns a world model through interactions with the\nenvironment and trains the policy against the world model.\nDreamerV3 comprises of the following models:\n\n\n\nDreamerV3:\n\n\n\nSequence model: _ht_ = _fϕ_ ( _ht−_ 1 _, zt−_ 1 _, at−_ 1)\nEncoder:Dynamics Predictor: _P rϕ_ ( _zt|htP r, xϕt_ )( ˆ _zt|ht_ )\nDecoder:Reward predictor:Continue predictor: _P rϕ_ ( ˆ _xtP r|hP rtϕ, zϕ_ ( ˆ _rt_ ( ˆ _tc_ ) _|th|ht, zt, zt_ ) _t_ )\n(1)\nwhere _ϕ_ describes the parameter vector for all distributions\noptimized, and\n\n\n  - _xt_ is the current image observation.\n\n  - _ht_ is the encoded history of the agent.\n\n  - _zt_ is an encoding of the current image _xt_ that incorporates the learned dynamics of the world.\n\n  - _st_ = ( _ht, zt_ ) is the agent’s compact model state.\nOf particular relevance for this work is the dynamics predictor model, _pϕ_ ( ˆ _zt|ht_ ), which predicts an image encoding _z_ ˆ _t_\ngiven the encoded history of the agent, _ht_ . For efficiency\nthe DreamerV3 world model operates in the latent embedded\nstate space. Predicted state observations _x_ ˆ _t_ can be generated\nvia the image prediction model _Prϕ_ ( ˆ _xt|ht, zt_ ).\n\n\n**5** **Reverse World Models**\nModel-based RL, and DreamerV3 in particular, learns to\npredict the (embedded) next state because it is seeking to\nunderstand how actions result in _future_ expected reward.\nWhile the world model is learned primarily to optimize training efficiency with respect to policy performance, we observe that the world model itself can be used to generate\ncounterfactuals—how would the world change if a different\naction were to be taken. We operate in the setting where the\nuser gives us the counterfactual action, _a_ _[⋄]_ _t_ [,] [and,] [by] [implica-]\ntion, the counterfactual next state _s_ _[⋄]_ _t_ +1 [.] [However,] [the expla-]\nnation is not what the user already knows or can infer. Our\nexplanation generation strategy hinges on providing the user\nwith an understanding of what the world _should have looked_\n_like_ prior to _st_ for the agent to want to take action _a_ _[⋄]_ _t_ [in-]\nstead of its actual chosen action _at_ . That is, the explanation\nis constructed around the presentation of _s_ _[⋄]_ _t_ [,] [the] [world] [the]\nagent should have been in to “do the right thing”. A forward\nworld model cannot do this. To generate _s_ _[⋄]_ _t_ [we] [need] [a] _[Re-]_\n_verse World Model_ .\nThe Reverse World Model\n(RWM) predicts the embedded state\n_pϕ_ (ˆ _zt−_ 1 _|ht−_ 1), where _ht−_ 1 is a function of _ht, zt_ and _at−_ 1,\nand _zt_ is obtained from _xt_ . The RWM trains alongside\nthe WM, on a modified copy of the same training data, as\nfollows. First, reward and continuation data is removed.\nSecond, the temporal order of the state-action transition\ndata generated during trials is reversed. DreamerV3 trains\nits models using an experience replay buffer. DreamerV3\nsamples chunks of transition data ( _xt, at_ ) _|t_ = _k...k_ + _n_ from\nthe replay buffer. When training the RWM only, we simply\nreverse the order of the data sampled from the replay buffer.\nFinally, we shift the actions so that image _xt_ +1 and the _prior_\naction _at_ are paired so that when the RWM is trained it\npredicts the prior embedded state given the future state and\nprior action.\nOnce the data processing is done, the RWM proceeds with\ntraining based on the usual dynamic and representation losses\nbut with prediction loss only considering the decoder’s image\npredictions. The DreamerV3 Forward World Model with our\ncombined Reverse World Model is shown in Figure 3. The\nReverse World Model produces _z_ ˆ _t_ [rev], the latent encoding of\nthe prior world state, and _x_ ˆ [rev] _t_, the reconstructed image from\nthe latent. The RWM module is trained using reconstruction\nerror between the environmental state observation _x_ [rev] _t_ and\nthe image decoded from the latent, _x_ ˆ [rev] _t_\n**6** **Human Participant Study**\nSince explanations are meant to provide actionable information to non-AI experts, we conduct a human participant study.\nWe hypothesize that non-AI expert end-users should, with\nour explanations, be able to identify the root cause of what\nfeatures of local environmental observations resulted in an\nAI agent performing an unexpected or undesirable action.\nSpecifically, we make the following hypotheses of those who\nreceive explanations relative to those who do not receive an\nexplanation:\n\nH1. Participants that receive our explanations can more accurately recognize the causes of agent failures.\n\nH2. Participants that receive our explanations have greater\nsatisfaction in agent responses.\n\nH3. Participants that receive our explanations have higher\ntrust in the agent.\n\nH4. Participants that receive our explanations have lower\ncognitive load when identifying the causes of agent failure.\n\nWe ran our human study as an online Qualtrics survey\nhosted on the Prolific platform. We recruited a total of 70\nparticipants (Control Group strength = 33, Treatment Group\nstrength = 37) in a randomized trial.\nThe participants were in the age range of 19 to 73 (Mean:\n41.3, Standard Deviation: 13.9), with 52.9% people identifying as women and 47.1% identifying as men. The study had\na median run time of 21:03 minutes. We paid participants at\nthe rate of $12.00 per hour, with an added bonus of $0.50 to\nbe given for high-quality responses, which was given to all\nparticipants.\n\n\n**6.1** **Study Design**\nParticipants are asked to imagine they are attempting to identify the root cause for why a robot (in a virtual game-like simulation) fails to correctly execute, or inefficiently executes, a\ntask of making coffee. Because we want to control for users’\ncommonsense understanding, we present a fictional world in\nwhich coffee can be made with atypical ingredients like lava,\nor milk obtained straight from a cow. The recipes also change\nbetween the tasks, so the coffee may be made on a stove in\none recipe and in a microwave in another.\nParticipants watch a video of the agent performing its task\nin a virtual environment built using the Crafter [Hafner, 2022]\nenvironment (See Figure 4). Crafter is an extensible 2D implementation of the MineCraft game. We constructed a world\nconsisting of a kitchen and fictional coffee ingredients that\ncan be gathered from nearby environs. The inventory mechanics are maintained, and we implement a new requirement\nthat the agent complete a fictional coffee recipe. Each scenario plays out in a 15 _×_ 15 grid and the agent can observe a\n7 _×_ 4 window around its position and its inventory.\nWe present four scenarios, in each of which the agent fails\nto complete the fictional coffee recipe for a different reason\n(see description of scenarios below). For each scenario, the\nparticipant must identify what was tampered with in the environment that caused the agent to operate ineffectively. In the\nControl condition, participants were presented with (a) the\n\n\n![](assets/md_images/익스플레이너블-AI-Explainable-Reinforcement-Learning-Agents-Using-World-Models.pdf-4-0.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Our modified world model. The left-hand side is the forward world model, as in DreamerV3[Hafner _et al._, 2025]. The right-hand\nside shows the reverse world model.\n\n\n1. A non-essential object is removed\n\n2. An essential object is moved\n\n3. An essential object is obstructed\n\n4. An essential object is removed\nThe order in which scenarios are presented to participants is\nrandomized.\n\n\n\n![](assets/md_images/익스플레이너블-AI-Explainable-Reinforcement-Learning-Agents-Using-World-Models.pdf-4-1.png)\n\nFigure 4: The modified Crafter environment. The agent must interact with various objects to gather ingredients and then interact with\nvarious devices to complete the recipe.\n\n\nvideo, and (b) four snapshots of states during the agent’s execution. In the Treatment condition, participants were presented with the same information as above, but also (c) each\nsnapshot was paired with a generated image showing what the\nagent was expecting in order to choose the action that would\nhave resulted in correct execution.\nHypothesis H1 holds if the explanation, in the form of the\nstate expected prior to the counterfactual provides enough information for the participant to pick the reason why the agent\nchose the wrong action. See Figure 6 in the Appendix.\nFor the participants to indicate the cause of the failure, they\nselect one of four possible objects from a pre-populated list,\nand one of four verb-phrases from a pre-populated list. All\nobjects and verb-phrases are probable—each object is present\nin at least one of the snapshots—but only one combination of\nobject and verb-phrase is correct—there is a 1:16 chance of\nrandomly selecting the correct combination. See Figure 7 in\nthe Appendix.\nThere was also a free-response text box where participants\nwere asked to describe their reasoning for their choice. This\ndiscourages participants from random selection but was not\nanalyzed further.\n\n\n**Scenarios**\nThe four scenarios involved completing the fictional coffee\nrecipe, but under one of the following conditions:\n\n\n\n**Surveys**\nAfter the four scenarios, participants completed three surveys: satisfaction, trust, and cognitive load.\nThe satisfaction survey is adapted from Hoffman et\nal. [Hoffman _et_ _al._, 2023]. It consists of seven 5-point\nLikert scale questions that ask about several key attributes\nof explanation satisfaction. All questions are framed positively, and the answers are supposed to range from ‘Strongly\nAgree’ to ‘Strongly Disagree’. We mapped these responses as\n‘Strongly Agree’=5 down to ‘Strongly Disagree’=1 for simplifying later calculations. We edited the wording of the questions to fit our study while maintaining the same meaning. To\nhave a fair baseline, both the groups were asked these questions about the Error Reports. The error reports presented to\nthe different participant groups differed in that the treatment\ngroup received the extra RWM-generated snapshots.\nWe used the User Trust Survey, also from Hoffman et\nal. [Hoffman _et_ _al._, 2023]. This survey consists of eight 5point Likert scale questions. All questions are framed positively, except one (“I am wary of the tool”).\nOur third survey is the NASA Task Load Index (TLX) Survey [Hart and Staveland, 1988], which assesses perceptions\nof cognitive load. It consists of six questions, each answered\non a scale of 21 gradations.\n\n\n**6.2** **Agent Configuration**\nThe agent is our modified DreamerV3 model with a reverse\nworld model. It is an Actor-Critic model trained alongside\na 25 million parameter FWM and a 25 million parameter\nRWM. The actor and critic are both MLPs, with learning rates\nof 3e-5, a batch size of 8 and a batch length of 65. The observation input is an image of a 7x7 grid which depicts the\nagent’s field of view at a given time step. The full training\ngrid for each static environment is 15x15.\nThe agent never experiences items being moved, obstructed, or removed during training. It is _intentionally_ overfit\nfor purposes of experimental control. While overfit policies\n**% Correct**\n**Scenario** **Odds Ratio** **p-value**\n**Control** **Treatment**\nRemove Non-Essential Object 3.03 27.03 11.85 0.00571*\nMove Essential Object 33.33 86.49 12.80 0.00001*\nObstruct Essential Object 27.27 62.16 4.38 0.00337*\nRemove Essential Object 42.42 83.78 7.01 0.00034*\nAll Scenarios 26.52 64.86 5.12 _<_ 0.00001*\nTable 1: Fisher’s Exact Test performed per task and cumulatively.\nAsterisks denote statistical significance.\n\n\nare generally unwanted, we required an unnaturally brittle\nagent that would struggle with the task to construct our experimental conditions; it makes the point that no agent in a\ncomplex real-world setting can be perfect.\nThe aim of this human study is to validate whether the\nRWM generated explanations are genuinely useful to people.\nTo accomplish this, in summary, we trained four RL agents\nto perform a task in a static environment. We then introduced\nunexpected changes (e.g. items missing) to said environments\nand recorded the changed trajectories of the trained agents.\nWe then showed this recording to the study participants along\nwith the RWM-generated expectations of what the agents expected the environment to look like. We then asked these\nparticipants if they were able to correctly identify the change.\nThe agents are the same between experimental and control conditions except for the presentation of the explanations\ngenerated by the reverse world model. For each scenario, we\nselected the point at which the agent deviated from the optimal solution trajectory and three other random points and the\nsnapshots are presented to participants.\n\n\n**7** **Results**\n**7.1** **Accuracy**\nWe looked at the percentage of participants in each group who\ngot the answers right per scenario. Table 1 shows the average correctness of participant selections. We used Fisher’s\nExact Test for categorical data to evaluate statistical significance. Participants in the explanation condition were significantly ( _p_ _<_ 0 _._ 008) more likely to identify the cause of the\nagent’s problem and correctly assemble that reason using the\npre-populated lists of objects and verb phrases. _Hypothesis_\n_H1 is supported._\n\n\n**7.2** **Satisfaction**\nThe satisfaction survey results are shown in Figure 5a,\nwhich shows the degree of agreement with questions that ask\nabout satisfaction with explanations. We aggregate responses\nacross all questions since they are all directionally the same\nand look at satisfaction with different aspects of the explanation. Participants in the treatment group were significantly\n( _p_ _≈_ 0 _._ 0036) more likely to agree with statements pertaining to the satisfaction of explanations. This can be seen as\na greater mass toward the top of the chart in the treatment\ngroup—higher number means more agreement. _Hypothesis_\n_H2 is supported._\n\n\n\n![](assets/md_images/익스플레이너블-AI-Explainable-Reinforcement-Learning-Agents-Using-World-Models.pdf-5-0.png)\n\n![](assets/md_images/익스플레이너블-AI-Explainable-Reinforcement-Learning-Agents-Using-World-Models.pdf-5-1.png)\n\n(a) Satisfaction (b) Trust\n(c) Cognitive Load Index\nFigure 5: Graphs depicting the distribution of average survey scores\nassigned by participants for User Satisfaction, Perceived User Trust\nand Perceived Cognitive Load\n**7.3** **Trust**\nThe satisfaction survey results are shown in Figure 5b. As\nwith the satisfaction results, a higher mass indicates more\nagreement with statements about trust; for the one question\nframed negatively, we flipped the axis in order to aggregate\nthe responses with the other questions. Participants in the\nTreatment group are significantly ( _p_ _≈_ 0 _._ 034) likely to indicate trust (and less wariness) in the system than those in the\nControl group. _Hypothesis H3 is supported._\n\n\n**7.4** **Cognitive Load**\nThe cognitive load survey results are shown in Figure 5c. We\nanalyzed the results using the Raw TLX technique [Byers\n_et_ _al._, 1989], a simple unweighted mean of all answers to\nsummarize the results per participant. A lower RTLX score\nindicates lower workload on the participant, which is preferred. Participants in the Treatment group reported significantly ( _p ≈_ 9 _._ 76 _×_ 10 _[−]_ [05] ) lower cognitive loads. _Hypothesis_\n_H4 is supported._\n\n\n**7.5** **Task Completion Time**\nWe recorded the amount of time that participants took to complete each scenario. This encompasses the time they spent\nreading the status, looking at the video, looking at the images, assembling the response from the pre-populated lists,\nand writing a short free-text description of how they arrived\nat the answer. Participants in the Treatment group took on\naverage 3.55 (Std.Dev. 2.57) minutes to complete each scenario. Participants in the Control group took on average 3.68\n(Std.Dev. 3.09) minutes to complete each scenario.\nA t-test indicates that there is no significant difference in\ncompletion times between groups ( _p <_ 0 _._ 35). Table 2 breaks\nout the time per group and per scenario. Taken along with\nother results, the explanations in the Treatment Group neither\nadd cognitive load nor add cognitive processing time. Three\nof four scenarios result in less time, though not significantly\nso. One scenario took longer on average; this scenario was\nalso significantly harder (per accuracy results in Table 1).\n\n\n\n![](assets/md_images/익스플레이너블-AI-Explainable-Reinforcement-Learning-Agents-Using-World-Models.pdf-5-2.png)\n**Mean Time (Std.Dev)**\n**Scenario** **t-statistic** **p-value**\n**Control** **Treatment**\nRemove Non-Essential Object 3.80 (2.16) 4.37 (2.23) 1.07 0.86\nMove Essential Object 3.75 (2.57) 3.27 (2.93) -0.72 0.24\nObstruct Essential Object 4.02 (4.78) 3.65 (2.79) -0.39 0.35\nRemove Essential Object 3.16 (1.88) 2.91 (1.97) -0.53 0.29\nAll Scenarios 3.68 (3.09) 3.55 (2.57) -0.38 0.35\nTable 2: Table displaying the average time taken (in minutes) by\neach group to complete tasks.\n\n\n**8** **Discussion**\nWe overfit our policy model for experimental purposes because we require an agent that will fail in a relatively simple environment. It is always the case that an RL agent can\nfail due to real-world complexity, even when trained with robustness in mind. Even if the agent does not fail, the user\nmay find the agent to not be aligned to their preferred way of\ncompleting a task. Regardless, the forward and reverse world\nmodels must be able to robustly predict state dynamics transitions from parts of the state space that the agent might not\ncommonly visit during training.\nThe RWM generates predictions of states from within the\ndistribution of what the FWM and agent have trained upon,\nsince FWM and RWM train on the same data. The RWM\ngenerated suggestions are hence capable of providing insights\ninto the agent’s expectations that would be difficult to approximate for an explainer that isn’t internal to the agent and\nwasn’t trained alongside the policy. This, however, also ties\nthe RWM’s capabilities to how well FWM and agent have\nbeen trained. To create a more robust RWM that can provide good predictions if the counterfactuals are very different from states that the agent would have routinely explored\nduring training, one might need off-policy exploration strategies such as [Eysenbach _et_ _al._, 2018; Hafner _et_ _al._, 2019] .\nIn general, World Models in RL agents are more likely to\ncatastrophically forget state transition dynamics that are not\ndirectly relevant to the policy construction [Balloch, 2024].\n\n\n**9** **Conclusions**\nExplainable RL agents for non-AI experts presents a significant challenge because the user cannot change the agent or\nre-train the policy if it doesn’t perform as expected. To explain why an RL agent chose a particular action over another,\ncounterfactual, action, we generate a state in which the agent\nwould have chosen the user’s desired action. To do this, we\nextend the RL agent with a reverse world model that can predict and generate counterfactual states that preceded the current state, instead of the more common prediction and generation of future counterfactual states.\nOur human participant study demonstrates that generating\nthe prior counterfactual to the desired action can significantly\nimprove users’ abilities to identify the environmental cause\nof agent failure. It also improves user satisfaction and trust\nwhile reducing cognitive load.\nThe policy encodes the agent’s understanding of how environmental observations map to action that optimizes for fu\n\n\n![](assets/md_images/익스플레이너블-AI-Explainable-Reinforcement-Learning-Agents-Using-World-Models.pdf-6-0.png)\n\nFigure 6: Screenshot of a scenario as presented to participants consisting of an execution video, a list of images depicting the true snapshots and a list of images depicting the RWM’s expectations.\n\n\nture task reward. Improving the user’s understanding of the\nagent’s policy with respect to the environment helps them update their mental model of the agent. This in turn helps them\nidentify the cause of failures or the cause of mis-alignment\nbetween user and agent policy. Finally, while not directly\nstudied in the scope of this paper, it also potentially enables\nthe user to correct or align agent behavior through deliberate\nalterations of the environment.\n\n\nFigure 7: Screenshot of the interface where participants assemble\nthe cause of the agent’s problem from pre-populated lists.\n\n\n**10** **Appendix**\nThe core human participant study materials are presented in\nFigures 6 and 7.\n\n\n\n![](assets/md_images/익스플레이너블-AI-Explainable-Reinforcement-Learning-Agents-Using-World-Models.pdf-6-1.png)\n**References**\n[Alabdulkarim _et al._, 2025] Amal Alabdulkarim, Madhuri\nSingh, Gennie Mansi, Kaely Hall, Upol Ehsan, and\nMark O Riedl. Experiential explanations for reinforcement learning. _Neural Computing and Applications_, pages\n1–31, 2025.\n\n[Amir _et al._, 2018] Ofra Amir, Finale Doshi-Velez, and\nDavid Sarne. Agent strategy summarization. In _Proceed-_\n_ings of the 17th International Conference on Autonomous_\n_Agents and MultiAgent Systems_, pages 1203–1207, 2018.\n\n[Amitai _et al._, 2024] Yotam Amitai, Yael Septon, and Ofra\nAmir. Explaining reinforcement learning agents through\ncounterfactual action outcomes. In _Proceedings_ _of_\n_the_ _Thirty-Eighth_ _AAAI_ _Conference_ _on_ _Artificial_ _Intelli-_\n_gence_ _and_ _Thirty-Sixth_ _Conference_ _on_ _Innovative_ _Appli-_\n_cations_ _of_ _Artificial_ _Intelligence_ _and_ _Fourteenth_ _Sympo-_\n_sium_ _on_ _Educational_ _Advances_ _in_ _Artificial_ _Intelligence_,\nAAAI’24/IAAI’24/EAAI’24. AAAI Press, 2024.\n\n[Balloch, 2024] Jonathan Balloch. _Efficient_ _Adaptation_ _of_\n_Reinforcement Learning Agents to Suddent Environmental_\n_Change_ . Doctoral dissertation, Georgia Institute of Technology, 2024.\n\n[Byers _et al._, 1989] James C Byers, AC Bittner, and Susan G\nHill. Traditional and raw task load index (tlx) correlations:\nAre paired comparisons necessary. _Advances in industrial_\n_ergonomics and safety_, 1:481–485, 1989.\n\n[Chakraborti _et al._, 2021] Tathagata Chakraborti, Sarath\nSreedharan, and Subbarao Kambhampati. The emerging\nlandscape of explainable automated planning & decision\nmaking. In _Proceedings_ _of_ _the_ _Twenty-Ninth_ _Interna-_\n_tional_ _Conference_ _on_ _International_ _Joint_ _Conferences_ _on_\n_Artificial Intelligence_, pages 4803–4811, 2021.\n\n[Das and Chernova, 2020] Devleena Das and Sonia Chernova. Leveraging rationales to improve human task performance. In _Proceedings of the 25th international confer-_\n_ence on intelligent user interfaces_, pages 510–518, 2020.\n\n[Das _et al._, 2023] Devleena Das, Sonia Chernova, and Been\nKim. State2explanation: Concept-based explanations to\nbenefit agent learning and user understanding. _Advances in_\n_Neural Information Processing Systems_, 36:67156–67182,\n2023.\n\n[Ehsan _et al._, 2019] Upol Ehsan, Pradyumna Tambwekar,\nLarry Chan, Brent Harrison, and Mark O Riedl. Automated rationale generation: a technique for explainable ai\nand its effects on human perceptions. In _Proceedings_ _of_\n_the 24th international conference on intelligent user inter-_\n_faces_, pages 263–274, 2019.\n\n[Eysenbach _et al._, 2018] Benjamin Eysenbach, Abhishek\nGupta, Julian Ibarz, and Sergey Levine. Diversity is all\nyou need: Learning skills without a reward function, 2018.\n\n[Gajcin and Dusparic, 2024a] Jasmina Gajcin and Ivana\nDusparic. Raccer: Towards reachable and certain counterfactual explanations for reinforcement learning. In\n_Proceedings_ _of_ _the_ _23rd_ _International_ _Conference_ _on_ _Au-_\n_tonomous_ _Agents_ _and_ _Multiagent_ _Systems_, AAMAS ’24,\npage 632–640, Richland, SC, 2024. International Foundation for Autonomous Agents and Multiagent Systems.\n\n\n[Gajcin and Dusparic, 2024b] Jasmina Gajcin and Ivana\nDusparic. Redefining counterfactual explanations for reinforcement learning: Overview, challenges and opportunities. _ACM Comput. Surv._, 56(9), April 2024.\n\n\n[Ha and Schmidhuber, 2018] David Ha and J¨urgen Schmidhuber. World models. 2018.\n\n\n[Hafner _et al._, 2019] Danijar Hafner, Timothy Lillicrap, Ian\nFischer, Ruben Villegas, David Ha, Honglak Lee, and\nJames Davidson. Learning latent dynamics for planning\nfrom pixels, 2019.\n\n\n[Hafner _et al._, 2025] Danijar Hafner, Jurgis Pasukonis,\nJimmy Ba, and Timothy Lillicrap. Mastering diverse\ncontrol tasks through world models. _Nature_, pages 1–7,\n2025.\n\n\n[Hafner, 2022] Danijar Hafner. Benchmarking the spectrum\nof agent capabilities, 2022.\n\n\n[Hart and Staveland, 1988] Sandra G Hart and Lowell E\nStaveland. Development of nasa-tlx (task load index): Results of empirical and theoretical research. In _Advances in_\n_psychology_, volume 52, pages 139–183. Elsevier, 1988.\n\n\n[Hoffman _et al._, 2023] Robert R. Hoffman, Shane T.\nMueller, Gary Klein, and Jordan Litman. Measures for\nexplainable ai: Explanation goodness, user satisfaction,\nmental models, curiosity, trust, and human-ai performance. _Frontiers in Computer Science_, Volume 5 - 2023,\n2023.\n\n\n[Huber _et al._, 2023] Tobias Huber, Maximilian Demmler,\nSilvan Mertes, Matthew L. Olson, and Elisabeth Andr´e.\nGanterfactual-rl: Understanding reinforcement learning\nagents’ strategies through visual counterfactual explanations. In _Proceedings of the 2023 International Conference_\n_on Autonomous Agents and Multiagent Systems_, AAMAS\n’23, page 1097–1106, Richland, SC, 2023. International\nFoundation for Autonomous Agents and Multiagent Systems.\n\n\n[Kaelbling _et al._, 1996] Leslie Pack Kaelbling, Michael L\nLittman, and Andrew W Moore. Reinforcement learning: A survey. _Journal_ _of_ _artificial_ _intelligence_ _research_,\n4:237–285, 1996.\n\n\n[Madumal _et al._, 2020] Prashan Madumal, Tim Miller, Liz\nSonenberg, and Frank Vetere. Explainable reinforcement\nlearning through a causal lens. In _Proceedings of the AAAI_\n_conference_ _on_ _artificial_ _intelligence_, volume 34, pages\n2493–2500, 2020.\n\n\n[Mansi and Riedl, 2023] Gennie Mansi and Mark Riedl.\nWhy don’t you do something about it? outlining connections between ai explanations and user actions, 2023.\n\n\n[Milani _et al._, 2024] Stephanie Milani, Nicholay Topin,\nManuela Veloso, and Fei Fang. Explainable reinforcement\nlearning: A survey and comparative review. _ACM_ _Com-_\n_puting Surveys_, 56(7):1–36, April 2024.\n\n\n[Miller, 2019] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. _Artificial intelli-_\n_gence_, 267:1–38, 2019.\n\n[Olson _et al._, 2021] Matthew L. Olson, Roli Khanna,\nLawrence Neal, Fuxin Li, and Weng-Keen Wong. Counterfactual state explanations for reinforcement learning\nagents via generative deep learning. _Artificial Intelligence_,\n295:103455, 2021.\n\n[Peng _et al._, 2022] Xiangyu Peng, Mark Riedl, and Prithviraj Ammanabrolu. Inherently explainable reinforcement\nlearning in natural language. _Advances_ _in_ _Neural_ _Infor-_\n_mation Processing Systems_, 35:16178–16190, 2022.\n\n[Samadi _et al._, 2025] Amir Samadi, Konstantinos Koufos,\nKurt Debattista, and Mehrdad Dianati. Counterfactual explainer for deep reinforcement learning models using policy distillation. _ACM_ _Trans._ _Intell._ _Syst._ _Technol._, 16(2),\nFebruary 2025.\n\n[Septon _et al._, 2023] Yael Septon, Tobias Huber, Elisabeth\nAndr´e, and Ofra Amir. Integrating policy summaries with\nreward decomposition for explaining reinforcement learning agents. In _International Conference on Practical Ap-_\n_plications of Agents and Multi-Agent Systems_, pages 320–\n332. Springer, 2023.\n\n[Sreedharan _et al._, 2022] Sarath Sreedharan, Utkarsh Soni,\nMudit Verma, Siddharth Srivastava, and Subbarao Kambhampati. Bridging the gap: Providing post-hoc symbolic explanations for sequential decision-making problems with inscrutable representations. In _International_\n_Conference on Learning Representations_, 2022.\n\n[van der Waa _et al._, 2018] J van der Waa, J van Diggelen,\nK van den Bosch, and M Neerincx. Contrastive explanations for reinforcement learning in terms of expected consequences. _XAI 2018_, page 165, 2018.\n\n[Zelvelder _et al._, 2021] Amber E Zelvelder, Marcus Westberg, and Kary Fr¨amling. Assessing explainability in\nreinforcement learning. In _International_ _Workshop_ _on_\n_Explainable,_ _Transparent_ _Autonomous_ _Agents_ _and_ _Multi-_\n_Agent Systems_, pages 223–240. Springer, 2021.\n\n\n",
    "kor_md": "",
    "eng_pdf": "pdf/eng/익스플레이너블 AI Explainable Reinforcement Learning Agents Using World Models.pdf",
    "kor_pdf": ""
  },
  {
    "id": "paper_4",
    "title": "Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing",
    "eng_md": "## **Multilingual Knowledge Graph Completion via Efficient Multilingual** **Knowledge Sharing**\n\n**Cunli Mao** [1] _[,]_ [2] **, Xiaofei Gao** [1] _[,]_ [2] **, Ran Song** [1] _[,]_ [2][*] **, Shizhu He** [3] _[,]_ [4]\n**Shengxiang Gao** [1] _[,]_ [2] **, Kang Liu** [3] _[,]_ [4] **, Zhengtao Yu** [1] _[,]_ [2]\n1Faculty of Information Engineering and Automation,\nKunming University of Science and Technology, Kunming, China\n2Yunnan Key Laboratory of Artificial Intelligence, Kunming, China\n3The Key Laboratory of Cognition and Decision Intelligence for Complex Systems,\nInstitute of Automation, Chinese Academy of Sciences, Beijing, China\n4School of Artificial Intelligence, University of Chinese Academy of Science, Beijing, China\n{maocunli,xiaofeigao_g,song_ransr}@163.com, {shizhu.he,kliu}@nlpr.ia.ac.cn,\n{gaoshengxiang.yn,ztyu}@hotmail.com\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing.pdf-0-0.png)\n\n\n\n**Abstract**\nLarge language models (LLMs) based Multilingual Knowledge Graph Completion (MKGC)\naim to predict missing facts by leveraging\nLLMs’ multilingual understanding capabilities,\nimproving the completeness of multilingual\nknowledge graphs (KGs). However, existing\nMKGC research underutilizes the multilingual\ncapabilities of LLMs and ignores the shareability of cross-lingual knowledge. In this paper, we propose a novel MKGC framework\nthat leverages multilingual shared knowledge\nto significantly enhance performance through\ntwo components: Knowledge-level Grouped\nMixture of Experts (KL-GMoE) and Iterative\nEntity Reranking (IER). KL-GMoE efficiently\nmodels shared knowledge, while IER significantly enhances its utilization. To evaluate\nour framework, we constructed a mKG dataset\ncontaining 5 languages and conducted comprehensive comparative experiments with existing state-of-the-art (SOTA) MKGC method.\nThe experimental results demonstrate that our\nframework achieves improvements of 5.47%,\n3.27%, and 1.01% in the Hits@1, Hits@3, and\nHits@10 metrics, respectively, compared with\nSOTA MKGC method. Further experimental\nanalysis revealed the properties of knowledge\nsharing in settings of unseen and unbalanced\nlanguages. We have released the dataset and\ncode for our work on [https://github.com/](https://github.com/gaoxiaofei07/KL-GMoE)\n[gaoxiaofei07/KL-GMoE.](https://github.com/gaoxiaofei07/KL-GMoE)\n**1** **Introduction**\nKnowledge Graphs (KGs) (Weikum, 2021) are\nstructured semantic knowledge bases designed to\nrepresent and organize knowledge about the real\nworld. Most KGs possess multilingual characteristics, including _Wikidata_ (Vrandeˇci´c and Krötzsch,\n*Corresponding author\n**(a) Architectural Mismatch**\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing.pdf-0-1.png)\n\nFigure 1: This figure depicts the problems encountered\nwhen applying LLMs directly to the MKGC task. (a)\nillustrates that existing PEFT is not suitable for the\nMKGC task. (b) Indicates a discrepancy between the\ntask paradigms LLMs excel at and MKGC tasks.\n\n\n2014) and _DBpedia_ (Lehmann et al., 2015). Existing multilingual KGs are often incomplete, which\nlimits their effectiveness in practical applications\n(Ji et al., 2022). Multilingual knowledge graph\ncompletion (MKGC) aims to leverage known multilingual knowledge to complete missing triples and\nimprove the completeness of the KGs.\n\nStudies have focused on embedding-based methods (Ge et al., 2024) for MKGC, mapping entities\nand relations into a low-dimensional vector space\nto achieve completion. Recent advances in language models have shifted MKGC research toward\ngeneration-based approaches (Chen et al., 2022;\nSaxena et al., 2022) that reformulate KG comple\n**(b) Task Paradigm Discrepancy**\nquery\ncandidate entities results\ntion as text generation. Methods (Song et al., 2023;\nZhou et al., 2022) employ a single pretrained language model (PLM) to consolidate multilingual\nknowledge within a unified semantic space, achieving superior performance in MKGC. Furthermore,\nrecent work like DIFT (Liu et al., 2024) explores\ntask adaptation through LLMs fine-tuning, achieving strong performance on monolingual KGC tasks.\nModern LLMs, pretrained on diverse corpora, inherently possess multilingual capabilities (Huang\net al., 2024), enabling the representation and knowledge sharing across languages (Hu et al., 2025)\nwithin a unified model. Crucially, this capacity for\ninternal multilingual knowledge sharing is vital for\nMKGC, offering substantial potential to enhance\ncompletion performance. Motivated by this recognized potential, our research investigates effective\nmethods for harnessing LLMs’ inherent capabilities to improve MKGC.\nHowever, directly applying LLMs to MKGC\npresents several challenges, primarily stemming\nfrom two key aspects: model architecture and task\nparadigm. 1) **Architectural Mismatch** : Existing\nParameter-Efficient Fine-Tuning (PEFT) methods\n(Han et al., 2024) for LLMs are mainly designed\nfor text-centric tasks and exhibit significant gaps\nwhen applied to knowledge-level tasks. Specifically, single-channel methods struggle with the\ncomplex multilingual nature of KGs. As shown in\nFigure 1(a) top, processing numerous multilingual\nqueries through a single channel often results in\nknowledge overload. This overload impacts the\nmodel’s ability to understand similar knowledge\nacross languages, leading to incorrect predictions.\nFor example, queries in English, Chinese, and Italian concerning _Einstein’s_ _birthplace_ all yield incorrect results. Conversely, multi-channel methods\ntend to disrupt the atomicity of knowledge, thereby\ncausing knowledge fragmentation. As shown in\nFigure 1(a) bottom, query tokens are processed by\ndisparate channels. Such fragmented processing\nconsequently leads to incorrect entity predictions.\n2) **Task Paradigm Discrepancy** : The MKGC task\ninvolves entity ranking, which presents a discrepancy with the text generation paradigm. As shown\nin Figure 1(b), for the query _(Einstein, birthplace,_\n_?)_, the LLM erroneously predicted _Paris_ as the answer. This selection failed to improve the ranking\nof the correct entity _Ulm_ .\nTo address the 1) **Architectural** **Mismatch**,\nspecifically knowledge overload, we propose increasing the number of dedicated knowledge chan\nnels. This allows each channel to focus on processing semantically similar information, thereby\nenhancing the LLM’s capacity to understand and\nleverage cross-lingual shared knowledge. Concurrently, by enabling each channel to independently\nprocess complete knowledge, we can effectively\nmitigate knowledge fragmentation and facilitate the\nmodel’s comprehensive understanding of multilingual information. To resolve the 2) **Task Paradigm**\n**Discrepancy**, we propose adjusting the LLM’s\ntraining objective to enable it to iteratively refine\nthe ranking of multiple entities. This approach\naims to enhance the ranking of correct entities by\nincreasing the frequency with which the LLM utilizes cross-lingual shared knowledge.\nIn this paper, we propose a novel framework for\neffectively leveraging multilingual shared knowledge to enhance the performance of MKGC. This\nproposed framework comprises two synergistic\ncomponents: Knowledge-level Grouped Mixture of\nExperts (KL-GMoE) and Iterative Entity Reranking (IER). KL-GMoE introduces a knowledgelevel expert routing mechanism and a group-based\nMixture-of-Experts (MoE) architecture. This design aims to mitigate knowledge fragmentation\nwhile substantially enhancing LLMs’ capacity to\ncapture cross-lingual shared knowledge. IER modifies both the training objective and the decoding strategy of LLMs. This enables the models to significantly improve their leveraging of\nmultilingual shared knowledge through multiple\niterative refinements. The experimental results\ndemonstrate that our framework achieves improvements of 5.47%, 3.27%, and 1.01% in the Hits@1,\nHits@3, and Hits@10 metrics, respectively, compared with SOTA MKGC method. Further experimental analysis revealed the properties of knowledge sharing in settings of unseen and unbalanced\nlanguages.\nIn summary, our contributions are as follows:\n\n\n  - We propose KL-GMoE to address the model\narchitecture mismatch, efficiently modeling\nshared knowledge.\n\n\n  - We propose IER to address the discrepancy in\nthe task paradigm, enhancing the utilization\nof shared knowledge.\n\n\n  - Experiments show that our framework significantly outperforms the SOTA MKGC method,\nwith average improvements of 5.47%, 3.27%,\nand 1.01% in Hits@1, Hits@3, and Hits@10.\n\n\n**Language** **Entity** **Relation** **Training** **Validation** **Testing**\nEN 86,539 512 708,267 49,782 49,777\nFR 89,754 478 839,623 49,908 30,000\nIT 65,434 445 613,014 49,883 20,000\nJA 46,294 432 321,237 49,939 10,000\nZH 63,278 397 546,626 49,969 10,000\nSUM 351,299 2,264 3,028,767 249,481 119,777\nTable 1: Statistics of the multilingual knowledge graph\ncompletion dataset.\n\n\n**2** **Datasets**\n**2.1** **Dataset Construction**\nWe utilize _Wikidata5M_ (Wang et al., 2021) as\nthe foundational seed library, which is a millionscale English KG dataset integrating _Wikidata_ and\n_Wikipedia_ . Based on this, we further expanded\nthe dataset to include French, Italian, Chinese, and\nJapanese, by collecting data from _Wikidata_ . As\nshown in Table 1, we present statistics on the number of entities, relations, training, validation and\ntesting triples. The KG contains 351,299 entities\nand 2,264 relations, with the total number of triples\nexceeding 3 million.\nBased on the characteristics of multilingual\nknowledge distribution, the knowledge across different languages is not entirely aligned but exhibits\ncertain linguistic specificity (Song et al., 2025).\nThis asymmetry of knowledge across languages indicates that some knowledge is confined to specific\nlanguages. Therefore, the dataset we constructed\nfollows the natural distribution patterns of knowledge. Some knowledge is shared across multiple\nlanguages, reflecting the similarities between languages. Other knowledge is unique to each language, reflecting the distinctive characteristics of\neach language.\n\n\n**2.2** **Prompt Construction**\nWe adopt the prompt construction method proposed\nby DIFT (Liu et al., 2024). Since the embeddingbased model has learned the training data, it tends\nto rank the correct entity at the first in the candidate\nentities for most training facts. Constructing the\nprompt using these ranked candidates may cause\nLLMs to develop a bias toward selecting the first\nentity as the answer. Therefore, we partition a\nsubset from the validation set to construct prompts,\nwhich are utilized as training data during the finetuning phase of the LLM.\nFor the query _q_ = ( _h, r,_ ?), the constructed\nPrompt P consists of four parts: Query _Q_, Descrip\ntion _D_, Neighbor facts _N_, and Candidate entities\n_Mc_ . This can be represented as:\n\n\n_P_ ( _q_ ) = [ _Q_ ; _D_ ; _N_ ; _Mc_ ] _._ (1)\nDescription provides specific descriptive information about entity _h_, enabling the model to\ncomprehend the entity’s meaning more accurately.\nNeighbor facts are triples that include the entity\n_h_, and these triples are randomly sampled from\nthe Knowledge Graph Embedding (KGE) model’s\ntraining data. These neighboring facts are intended\nto enhance the LLM’s comprehension of the entity _h_ . Candidate entities _Mc_ = [ _e_ 1 _, e_ 2 _, . . ., em_ ]\nare composed of the top- _m_ entities selected from\nthe ranking results generated by the KGE model\n(Bordes et al., 2013). To enable LLMs to adapt\nto the task paradigm of MKGC, we processed the\nnumber of entities _m_ in the _Mc_ during training.\nThe specific processing method is detailed in Section 3.3. We provide specific prompt examples in\nAppendix A.1.\n\n\n**3** **Methodology**\n**3.1** **Task Definition**\nIn this paper, we integrate KGE model with LLM\nto perform the MKGC task. First, for a query\n_q_ = ( _h, r,_ ?), we use the KGE model to obtain\nthe top- _m_ ranked entities, which form the candidate entities _Mc_ . Next, we leverage LLMs to select\nthe optimal entity from the candidate entities _Mc_\nto complete the query _q_ . The completion process\ncan be formulated as follows:\n\n\n_e_ ˆ = argmax _P_ ( _ei_ _| h, r, Mc_ ) _,_ (2)\n_ei∈Mc_\nwhere ˆ _e_ denotes the optimal entity for completing\nthe query _q_, and _P_ ( _ei_ _|_ _h, r, Mc_ ) represents the\nprobability of selecting entity _ei_ given the head\nentity _h_, relation _r_ and candidate entities _Mc_ .\n\n\n**3.2** **KL-GMoE Architecture**\nKL-GMoE is specifically tailored for MKGC task.\nThis architecture is designed with multiple expert\ngroups to alleviate the knowledge overload caused\nby single-channel and enhance the ability of LLMs\nto capture shared knowledge. Furthermore, KLGMoE employs a knowledge-level expert routing\nmechanism to ensure that each sample is processed\nby a specific expert, rather than involving all experts collectively. As shown in Figure 2(a), for\n**(a) KL-GMoE Architecture** **(b) Iterative Entity Reranking**\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing.pdf-3-1.png)\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing.pdf-3-0.png)\n\n\n\n\n\n\n\n\n\nCandidate\nEntities(M+(#))\nIterate for\n𝑁- rounds\nReranked\nEntity List\nIncorrect Entity Correct Entity\nFigure 2: The figure illustrates our proposed framework. Figure (a) depicts the architecture and workflow of the\nKL-GMoE, where the matrices _Ai_ and _Bi,j_ highlighted in red represent the currently activated expert. Figure (b)\nillustrates the workflow of the IER method. After _Nt_ iterations, we can obtain a reranked list of entities.\n\n\n\neach sample processed by KL-GMoE, only the matrix _Ai_ and one matrix _Bi,j_ from the expert group\n_Gi_ are activated. The KL-GMoE is applied exclusively to the Feed-Forward Network (FFN) layer\nof the LLM. Specifically, the matrix operations in\nthe FFN layer during forward propagation can be\nrepresented as follows:\n\n\n**y** = **W** 0 **X** + **y** _e,_ (3)\nwhere **W** 0 _∈_ R [dout] _[×]_ [din] represents the original FFN\nlayer parameter matrix, which is frozen during\ntraining. **X** = [ **xh** : **xr** : **xt** ] represents the input of\nthe FFN layer. **y** _e_ represents the output calculated\nby KL-GMoE based on the input **X** .\nThe design of KL-GMoE is inspired by the asymmetric fine-tuning architecture proposed in HydraLoRA (Tian et al., 2024). We adopted a grouped\nMoE design architecture, where each group can be\nrepresented as follows:\n\n\n_Gi_ = ( _Ai, {Bi,j_ _| j_ _∈{_ 1 _,_ 2 _, . . ., Nb}}_ ) _,_ (4)\nwhere _i_ _∈{_ 1 _,_ 2 _, . . ., Ng}_, _Ng_ denotes the total\nnumber of expert groups. _Nb_ is the number of B\nmatrices in group _Gi_ . Within each group _Gi_, the\npairing of the _Ai_ matrix with a _Bi,j_ matrix is considered an expert _Ei,j_ = ( _Ai, Bi,j_ ). The _Ai_ matrix\nis designed to capture a category of similar knowledge. The different _Bi,j_ matrices within the group\n_Gi_ are regarded as modules that capture subtle differences in this category of knowledge. This design\naims to enhance LLMs’ ability to capture shared\nknowledge across multiple languages.\n\n\nSimultaneously, our proposed knowledge-level\nexpert routing mechanism includes three different\nroutes: **R** _g_, **R** _k_ and **R** _l_ . First, an expert group is\nselected based on **R** _g_ . Within this group, a specific\nexpert is then determined by combining **R** _k_ and **R** _l_ .\nThe following describes the process of selecting a\nspecific expert based on these three routes.\n\n\n**R** _g_ is the group routing selection module that\ndetermines which expert group processes the **X** .\nThe group selection is formulated as follows:\n\n\n_Gi_ = argmax ( **R** _g_ ( **X** ))\n_i∈{_ 1 _,_ 2 _,...,Ng}_\n(5)\nwhere **W** _g_ _∈_ R _[N][g][×]_ [din] is the routing matrix for\ngroup selection. _Gi_ represents the expert group\nselected to process **X** .\n\n\n**R** _k_ and **R** _l_ represent expert routing selection\nmodules that operate within the group _Gi_ . These\nmodules comprehensively considers the input **X**\nand the output from the _Ai_ matrix to perform expert selection. Specifically, **R** _k_ generates expert\nselection scores **S** _k_ based on **X** . The formula for\n\n\n _,_\n= argmax\n_i∈{_ 1 _,_ 2 _,...,Ng}_\n \nSoftmax( **W** _g_ **x** _m_ )\n_m∈{h,r,t}_\ncalculating **S** _k_ is as follows:\n\n\n   **S** _k_ = **R** _k_ ( **X** ) = Softmax( **W** _k_ **x** _m_ ) _,_ (6)\n_m∈{h,r,t}_\nwhere **S** _k_ _∈_ R _[N][b]_, and **W** _k_ _∈_ R _[N][b][×]_ [din] is the routing matrix that receives **X** as input. **R** _l_ generates\nexpert selection scores **S** _l_ based on the output _Ai_ **X**\nof matrix _Ai_ . The calculation of **S** _l_ can be expressed as follows:\n\n\n   **S** _l_ = **R** _l_ ( **X** ) = Softmax( **W** _l_ ( _Ai_ **x** _m_ )) _,_\n_m∈{h,r,t}_\n(7)\nwhere **S** _l_ _∈_ R _[N][b]_, and **W** _l_ _∈_ R _[N][b][×]_ [r] is the routing\nmatrix that receives _Ai_ **X** as input. _r_ represents the\nsize of the rank in LoRA (Hu et al., 2022). Then,\nselect the matrix _Bi,j_ from group _Gi_ based on the\nscores of **S** _k_ and **S** _l_ to process **X** :\n\n\n_Bi,j_ = argmax ( **S** _k_ + **S** _l_ ) _._ (8)\n_j∈{_ 1 _,_ 2 _,...,Nb}_\nFinally, we determine that the expert _Ei,j_ =\n( _Ai, Bi,j_ ) processes **X** based on the knowledgelevel expert routing mechanism.\nAfter determining the expert _Ei,j_, the output of\nKL-GMoE is expressed as follows:\n\n\n**y** _e_ = _Ei,j_ ( **X** ) = _Bi,j_ ( _Ai_ **X** ) _._ (9)\nThen, the expert output **y** _e_ is added to the original\nFFN output, as shown in Equation 3.\n\n\n**3.3** **Iterative Entity Reranking**\nWe propose a method called Iterative Entity Reranking (IER), aimed at enhancing LLMs’ utilization\nof cross-lingual shared knowledge. As shown\nin Figure 2(b), the IER method fully leverages\nshared knowledge through multiple iterations, significantly improving the accuracy of correct entity\nranking. IER adjusts the training task and decoding strategy of LLMs. In the training phase, we\nrandomly set the number of candidate entities _m_\nto a variable value, to train the LLM to be capable\nof iteratively adjusting the ranking of multiple entities. In the decoding stage, IER allows the LLMs\nto perform multiple rounds of entity prediction to\nadjust the ranking of multiple entities.\nFor the query _q_ = ( _h, r,_ ?), the initial set of candidate entities is generated by the KGE model and\ndenoted as _Mc_ [(1)] = [ _e_ 1 _, e_ 2 _, . . ., em_ ]. The list of entities to be sorted is initialized as _L_ [(1)] = _Mc_ [(1)] . The\nLLM performs _Nt_ rounds of entity prediction. In\nthe _t_ -th round, where _t ∈{_ 1 _,_ 2 _, . . ., Nt}_, the entity\nprediction operation can be expressed as follows:\n\n\n_e_ [(] _[t]_ [)] = argmax _P_ ( _ei_ _| h, r, Mc_ [(] _[t]_ [)][)] _[,]_ (10)\n_ei∈Mc_ [(] _[t]_ [)]\nwhere _Mc_ [(] _[t]_ [)] represents the candidate entity set in\nround _t_ . _e_ [(] _[t]_ [)] is the entity predicted by the LLM\nfrom _Mc_ [(] _[t]_ [)][. Then, we update] _[ M]_ [(] _[t]_ [)][ to obtain] _[ M]_ [(] _[t]_ [+1)]\nfor the next iteration:\n\n\n_Mc_ [(] _[t]_ [+1)] = _Mc_ [(] _[t]_ [)] _\\ {e_ [(] _[t]_ [)] _},_ (11)\nwhere _Mc_ [(] _[t]_ [)] _\\ {e_ [(] _[t]_ [)] _}_ denotes removing the entity\n_e_ [(] _[t]_ [)] from _Mc_ [(] _[t]_ [)][.] [Finally,we update the ranking of]\nentity _e_ [(] _[t]_ [)] in _L_ [(] _[t]_ [)] :\n\n\n_L_ [(] _[t]_ [+1)] = **Insert** ( _L_ [(] _[t]_ [)] _\\ {e_ [(] _[t]_ [)] _}, t, e_ [(] _[t]_ [)] ) _,_ (12)\nwhere **Insert** ( _L_ [(] _[t]_ [)] _\\ {e_ [(] _[t]_ [)] _}, t, e_ [(] _[t]_ [)] ) denotes first removing _e_ [(] _[t]_ [)] from _L_ [(] _[t]_ [)], and then inserting _e_ [(] _[t]_ [)] into\nthe _t_ -th position of _L_ [(] _[t]_ [)] . After iterating for _Nt_\nrounds, we obtain the final ranked list of entities\n_L_ [(] _[N][t]_ [+1)] . The implementation of IER is detailed in\nAppendix A.2.\n\n\n**4** **Experiment**\n**4.1** **Implementation Details**\nIn the experiment, we selected TransE (Bordes\net al., 2013) to obtain candidate entities. Additionally, we utilized Llama-2-7b-chat-hf [1] as the\nbase model for fine-tuning. The model training hyperparameters are set as follows: the learning rate\nis 2e-5, the LoRA rank is 4, and the length of the\ncandidate entities _Mc_ is between 25 and 30. The\nnumber of iterations _Nt_ for the IER is 10.\n\n\n**4.2** **Multilingual Knowledge Graph**\n**Completion**\nWe compared the performance of the proposed\nframework with embedding-based and generationbased methods on our constructed dataset. The\nexperimental results demonstrate that our method\nachieves optimal performance on the average metrics across all languages. Specifically, as shown in\nTable 2, the performance of our proposed framework surpasses all the aforementioned methods in\nthe three languages: EN, FR, and IT. For JA and\nZH, our framework performed excellently on all\nmetrics except Hits@10. Our framework failed\n[1https://huggingface.co/meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\nMODEL EN FR IT JA ZH AVG\nLoRAMoE 36.28 36.36 35.81 40.22 56.87 41.11\nHydraLoRA 35.68 35.60 35.05 40.49 57.92 40.95\nOurs **36.50** **36.72** **35.93** **41.60** **58.63** **41.88**\nLoRAMoE 42.54 42.74 41.40 48.75 64.96 48.08\nHydraLoRA 42.49 42.58 41.22 48.68 64.99 47.99\nOurs **42.87** **43.08** **41.70** **48.94** **65.06** **48.33**\nLoRAMoE 52.27 52.50 50.91 59.07 72.19 57.39\nHydraLoRA 52.51 52.41 50.81 **59.10** 72.24 57.41\nOurs **52.63** **52.62** **51.17** 59.09 **72.32** **57.57**\nLoRAMoE 41.15 41.13 40.30 45.90 61.52 46.00\nHydraLoRA 40.80 40.62 39.81 46.06 62.18 45.89\nOurs **41.42** **41.44** **40.51** **46.80** **62.67** **46.57**\nH\n@\n1\nH\n@\n3\nH\n@\n10\nM\nR\nR\nMODEL EN FR IT JA ZH AVG\nTransE 8.52 9.07 9.36 8.00 11.77 9.34\nAnalogy 13.40 15.81 14.58 15.11 6.43 13.07\nComplEx 10.92 11.75 11.49 13.95 17.79 13.18\nDistmult 6.89 7.73 7.93 8.16 5.66 7.27\nRotatE 24.08 24.61 25.57 29.49 31.36 27.02\nHAKE 31.64 32.92 30.99 35.53 52.24 36.66\nICL 1.79 1.07 1.26 1.93 2.27 1.66\nGC-PLM 33.37 32.51 30.38 36.65 49.13 36.41\nDIFT ( _Single_ ) 36.05 35.75 34.22 38.31 56.65 40.19\nOurs **36.50** **36.72** **35.93** **41.60** **58.63** **41.88**\nTransE 37.02 39.17 37.57 44.59 60.51 43.78\nAnalogy 28.39 30.45 29.56 35.84 19.61 28.77\nComplEx 23.05 23.92 23.05 29.68 41.12 28.17\nDistmult 14.59 14.82 15.79 19.16 17.94 16.46\nRotatE 40.73 42.13 41.68 49.78 62.36 47.34\nHAKE 43.30 43.27 41.88 47.52 63.22 47.84\nICL 34.99 37.09 35.33 42.84 59.15 41.88\nGC-PLM 40.99 42.21 40.47 50.95 63.67 47.66\nDIFT ( _Single_ ) 42.21 42.28 40.50 47.83 64.50 47.46\nOurs **46.25** **45.30** **44.22** **51.97** **66.93** **50.93**\nTransE 50.25 51.23 49.60 58.10 71.80 56.19\nAnalogy 39.17 41.73 40.30 48.78 64.45 46.89\nComplEx 34.84 37.51 35.70 45.02 59.97 42.61\nDistmult 26.74 26.78 27.41 36.04 50.25 33.44\nRotatE 52.66 53.17 51.50 **61.68** **74.58** 58.72\nHAKE 53.37 52.06 50.49 57.85 70.04 56.76\nICL 49.99 51.05 49.30 57.99 71.71 56.01\nGC-PLM 52.76 52.81 51.76 59.53 71.98 57.77\nDIFT ( _Single_ ) 52.48 52.35 50.30 58.74 72.08 57.19\nOurs **54.71** **53.45** **52.31** 60.85 72.56 **58.78**\nTransE 24.85 25.97 25.46 28.38 37.51 28.43\nAnalogy 22.85 25.01 23.82 27.44 21.04 24.03\nComplEx 19.18 20.29 19.56 24.30 32.12 23.09\nDistmult 13.33 13.80 14.28 16.86 16.69 14.99\nRotatE 34.63 35.39 35.49 41.75 48.74 39.20\nHAKE 39.41 39.80 38.10 43.36 59.04 43.94\nICL 20.11 20.70 19.80 23.98 31.06 23.13\nGC-PLM 36.66 37.21 37.18 42.21 55.39 41.73\nDIFT ( _Single_ ) 40.99 40.64 39.09 44.54 61.38 45.33\nOurs **42.96** **42.58** **41.69** **48.33** **63.74** **47.86**\nH\n@\n1\nH\n@\n3\nH\n@\n10\nM\nR\nR\nTable 2: This table presents the MKGC results across\nfive languages. The embedding-based methods TransE\n(Bordes et al., 2013), Analogy (Liu et al., 2017), ComplEx (Trouillon et al., 2016), DistMult (Yang et al.,\n2014), and RotatE (Sun et al., 2019) are all implemented\nusing the OpenKE framework (Han et al., 2018). The results of HAKE (Zhang et al., 2020) were reproduced using its open-source code. ICL refers to evaluation using\nthe LLaMA-2-7b-chat model without fine-tuning. GCPLM (Song et al., 2023) represents the current SOTA\nmethod for MKGC. DIFT (Liu et al., 2024) is a SOTA\nLLM-based monolingual KGC method. The _Single_\nrefers to training a separate model for each language\nindependently. The numbers in bold represent the best\nresults among the methods and languages considered.\n\n\nto surpass RotatE’s performance on Hits@10, primarily attributed to our use of a relatively weaker\nTable 3: This table compares the KL-GMoE with the\nexisting SOTA fine-tuning methods LoRAMoE (Dou\net al., 2024) and HydraLoRA (Tian et al., 2024).\n\n\nModel Trainable Params Activated Params Lora Rank\nTransE 106.1m 106.1m DIFT(LoRA) 159.9*5 m 159.9*5 m 64\nLoRAMoE 19.2m 19.2m 4\nHydraLoRA 12.5m 12.5m 4\nOurs 32.9 m 9.4 m 4\nTable 4: This table shows the comparison of our method\nwith other methods in terms of parameter count.\n\n\nperforming TransE model for generating candidate\nentities. We replaced TransE with RotatE in the\ncandidate entities retrieval and conducted experiments. The corresponding results and analysis are\npresented in Appendix A.3. Compared to the existing SOTA MKGC method GC-PLM, our framework achieved significant performance advantages\nin Hits@1, Hits@3, Hits@10, and MRR metrics,\nwith improvements of 5.47%, 3.27%, 1.01%, and\n6.13%, respectively. Furthermore, our framework\nachieves a substantial improvement in performance\ncompared with DIFT, the SOTA LLM-based monolingual KGC method. Overall, the experimental\nresults clearly demonstrate the effectiveness and\nsuperiority of our proposed framework.\n\n\n**4.3** **Model Architecture Comparison and**\n**Parameter Analysis**\nWe compared the MKGC performance between\nthe KL-GMoE architecture and existing SOTA\nfine-tuning methods, including LoRAMoE and HydraLoRA. These two SOTA methods both utilize\nmultiple channels to process a single query, which\ncan lead to the problem of knowledge fragmentation. As shown in Table 3, KL-GMoE outperforms\nLanguage\nModel Avg Tokens Num TFLOPs\nLoRAMoE 353.89 2.37814\nHydraLoRA 353.89 2.37580\nDIFT(LoRA) 353.89 2.42721\nOurs 353.89 2.37472\nTable 5: This table compares the computational efficiency of our method with that of other methods. The\ndata presented are average values calculated from 1,000\nsamples.\n\n\n\n60\n55\n50\n45\n40\n35\n30\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing.pdf-6-0.png)\n\nFigure 4: The figure illustrates the Hits@1 performance\nof our method on five languages under three different\ntraining language settings.\n\n\n\n60\n55\n50\n45\n40\n35\n30\n25\n20\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||1<br>|1<br>|||||||||\n||Hits@<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|\n||~~Hits@~~<br>Hits@<br>~~MRR~~|~~3~~<br>10||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\nRound\n\n\n\n\n\n|Multilingual Data Imbalance Analysis|Col2|\n|---|---|\n|||\n|||\n|||\n|||\n|||\n|en<br>|en<br>|\n|en<br>|en<br>|\n||~~fr~~<br>it<br>zh<br>|\n||ja<br>avg|\n\n\nDifferent Data Ratios of Languages (en:fr:it:zh:ja)\nFigure 3: This figure shows the variation in Hits@1\nscores of our framework under training data settings\nwith five different language proportions.\n\n\nthese methods on average metrics. This experimental result demonstrates that our method effectively\naddresses knowledge fragmentation, thereby enhancing performance on the MKGC task.\n\n\nWe further analyzed the advantages of KLGMoE in terms of model parameters. As shown in\nTable 4, compared to the embedding-based method\nTransE, KL-GMoE has 3.2 times fewer trainable\nparameters and 11.3 times fewer activated parameters. Among LLM-based methods, KL-GMoE has\nsignificantly fewer activated parameters than all\nother methods. In particular, compared to DIFT,\nKL-GMoE has approximately 24.3 times fewer\ntrainable parameters and about 85.1 times fewer\nactivated parameters, which demonstrates its significant advantages in terms of parameter count. At\nthe same time, we compared the proposed method\nwith other methods in terms of FLOPs during inference. As shown in Table 5, our method reduces\nthe FLOPs by approximately 0.053 TFLOPs compared to the current state-of-the-art LLM-based\nKGC method, DIFT, demonstrating superior computational efficiency.\n\n\n\nFigure 5: The figure illustrates the impact of the number\nof iterations of the IER method on performance.\n\n\n**4.4** **Analysis of Language Imbalance**\nTo evaluate the robustness of our framework in scenarios with imbalanced language distribution in the\ntraining data, we conducted experiments. Specifically, we conducted experiments with imbalanced\ntraining data ratios across five languages, while\nkeeping the total amount of training data constant.\nAs shown in Figure 3, despite the changes in language proportions, the Hits@1 scores for each language (dashed lines) and the average score across\nthe five languages (solid red line) remained relatively stable. It is evident that our framework is\ninsensitive to variations in the language distribution. Based on this analysis, our framework can effectively leverage cross-lingual shared knowledge,\nthereby demonstrating strong robustness.\n\n\n**4.5** **Analysis of Unseen Languages**\nTo evaluate the generalization capabilities of the\nproposed framework on languages not included in\nthe training data, we conducted analysis experiments. These experiments were conducted with\nthree distinct training configurations: (1) trained\n\n\n|Language|Query|Share|Answer|Prediction By DIFT|Prediction By Ours|\n|---|---|---|---|---|---|\n|EN|(Towelhead,<br>composer, ?)|FR|Thomas<br>Newman|David Kitay | Towelhead |** Thomas Newman** | James Newton<br>Howard | Chris Messina | Rolfe Kent | Carrie Preston|**Thomas Newman** | Rolfe Kent | James Newton Howard |<br>Mychael Danna | Theodore Shapiro | Mark Isham | Carter Burwell|\n|FR|(Shūji Terayama,<br>occupation, ?)|IT|parolier|metteur ou metteuse en scène | journaliste |** parolier** |<br>producteur de cinéma | musicien ou musicienne |<br>compositeur ou compositrice | autrice-compositrice-interprète|**parolier** | producteur de cinéma | guitariste | metteur en scène |<br>compositeur ou compositrice | journaliste | artiste|\n|IT|(The Pixar Story,<br>interpreti, ?)|EN|John<br>Lasseter|Joe Ranft | Eric Larson |** John Lasseter** | Milt Kahl |<br>Glen Keane | James Algar | Don Hahn|**John Lasseter** | Eric Larson | Milt Kahl | Glen Keane | Chris Buck<br>| Colin Hanks | James Algar|\n|JA|(パウロス, 同一と<br>される事物, ?)|ZH|パール|パウルス| パウロス| パール| ポーラ| クリスティン<br>| コンスタンティン| クリスティナ|パール| パウルス| ポーラ| パウロス| クリスティン|<br>コンスタンティン| クリスティナ|\n|ZH|(拉科西·马加什,<br>口头、书写语言, ?)|JA|俄語|匈牙利| 拉科西·马加什| 俄語| 德语|<br>中文| 西班牙語| 拉丁語|俄語| 德语| 日語| 英語| 拉丁語| 匈牙利| 波蘭語|\n\n\n\nFigure 6: The figure presents a comparison of the prediction results between our method and DIFT in the knowledge\nshared case. The **Share** column indicates that the knowledge of these queries exists in the LLM’s training data but\nis presented in other languages.\n\n\n\non EN and FR; (2) trained on EN, FR, and ZH; and\n(3) trained on five languages. As shown in Figure 4,\nthe green bar indicates that LLMs trained solely on\nEN and FR data demonstrated significant KGC performance on unseen languages IT, JA, and ZH. This\nclearly demonstrates that knowledge sharing is effective not only among languages seen during LLM\ntraining, but also shows significant cross-lingual\ngeneralization capability among unseen languages.\nFurthermore, we observed a consistent improvement in performance across all languages as the\nnumber of training languages increased. This finding suggests that training data in more languages\nprovides richer knowledge signals to LLMs, which\nfacilitates the sharing of multilingual knowledge.\n\n\n**4.6** **Analysis of IER Trends**\nTo evaluate the impact of the number of iterations\nin the IER method on MKGC performance, we conducted analytical experiments. Figure 5 illustrates\nthe changes in all metrics as the number of iterations increases. From the results, it can be observed\nthat Hits@3, Hits@10, and MRR significantly improved in the first three iterations and reached their\noptimal values by the tenth iteration. This trend indicates that with an increasing number of iterations,\nIER allows LLMs to leverage multilingual shared\nknowledge more effectively, thereby significantly\nimproving the performance of MKGC.\n\n\n**4.7** **Ablation Experiment**\nTo verify the effectiveness of each component in\nour proposed framework, we conducted ablation\nexperiment. We evaluated the contribution of each\ncomponent by removing it sequentially. As shown\nin Table 6, removing the KL-GMoE component\nresulted in a drop in Hits@1 from 41.88 to 40.28,\nHits@3 from 50.93 to 49.71, Hits@10 from 58.78\nModel H@1 H@3 H@10 MRR\nOurs **41.88** **50.93** **58.78** **47.86**\nOurs w/o _**kg**_ 40.28 49.71 58.07 46.55\nOurs w/o _**kg+ier**_ 40.28 47.66 57.29 45.42\nTable 6: This table shows the results of ablation experiments on the KL-GMoE ( _**kg**_ ) and IER ( _**ier**_ ) components.\nAll results are the average of the five language metrics.\n\n\nto 58.07, and MRR from 47.86 to 46.55. This indicates that the KL-GMoE component is crucial\nfor improving the performance of MKGC. Furthermore, when we removed both KL-GMoE and IER\nsimultaneously, the values of Hits@3, Hits@10,\nand MRR further decrease compared to removing\nonly KL-GMoE. This demonstrates that the IER\ncomponent also makes a positive contribution to\nthe performance of MKGC. These ablation experiment results strongly prove the effectiveness of our\nproposed KL-GMoE and IER components.\n\n\n**4.8** **Case Study**\nWe conducted a case study to evaluate the framework’s performance in cross-lingual knowledge\nsharing. These case’ queries are knowledge that\nthe LLM learned during its training, but expressed\nin another language. As shown in Figure 6, for the\nEnglish query _(Towelhead, composer, ?)_, the LLM\nhas already learned this knowledge in the French\ntraining data. Our framework successfully leverages this French knowledge to accurately predict\nthe entity as _Thomas Newman_ . In contrast, SOTA\nLLM-based methods incorrectly predict _David Ki-_\n_tay_ . This demonstrates that our framework can\neffectively utilize cross-lingual shared knowledge\nto improve completion accuracy.\n\n\n**5** **Related Work**\nEmbedding-based methods map entities and relations in KGs to low-dimensional vector spaces. For\nexample, TransE (Bordes et al., 2013) based on\nthe translation principle of entities and relations.\nRotatE (Sun et al., 2019) treats each relation as\nrotation in complex vector space. DMoG (Song\net al., 2022a) represents the unseen relations of\nthe factual graph by fusing ontology and textual\ngraphs. TransH (Wang et al., 2014) models relation\nas hyperplane. HOLEX (Xue et al., 2018) interpolate between a high model complexity method\nand HolE (Nickel et al., 2016). TR-GCN (Song\net al., 2022b) proposes an ontology-guided zeroshot relation learning method to represent unseen\nrelations.\nGeneration-based Methods transform KGC task\ninto text generation task. For example, KGT5 (Saxena et al., 2022) posing KG link prediction as a\nsequence-to-sequence task. GC-PLM (Song et al.,\n2023) enhances the performance of MKGC by introducing global and local knowledge constraints.\nGenKGC (Xie et al., 2022) introduces a hierarchical decoding strategy of relation-guided demonstration and entity awareness. KICGPT (Wei et al.,\n2023) integrates LLMs and KGE model, adopting\na knowledge-prompted contextual learning strategy\nto rerank multiple entities. DIFT (Liu et al., 2024)\nfine-tunes LLMs using LoRA (Hu et al., 2022) to\nselect the most optimal entity from candidate entities obtained by the KGE model.\n\n\n**6** **Conclusion**\nIn this paper, we propose a novel MKGC framework. This framework integrates two components: KL-GMoE and IER. KL-GMoE significantly improves completion performance by efficiently capturing shared knowledge across languages. IER fully utilized cross-lingual shared\nknowledge through a multi-round iterative approach, further improving completion performance.\nThe experimental results demonstrate that our\nframework exhibits superior performance in the\nMKGC task.\n\n\n**Limitations**\nOur framework is limited by the token length of\nthe LLM, therefore it is unable to perform entity\nselection based on all entities in the KG. Moreover,\nthe framework processes text information exclusively. This limitation impedes its application to\nmultimodal KG datasets, as it cannot integrate information from other modalities.\n\n\n**Ethics Statement**\nThe paper proposes a method for MKGC and conducts experiments on a multilingual dataset extended from public available datasets. Therefore,\ndata privacy implications are non-existent in this\nscenario.\n\n\n**Acknowledgements**\nThis research was supported by the National\nNatural Science Foundation of China (Grant\nNos. U21B2027, U23A2038, 62166023,\n62376270), the Yunnan Provincial Major Science and Technology Special Plan Projects (Grant\nNos. 202402AG050007, 202502AD080012,\n202502AD080016), the General Projects of Basic Research in Yunnan Province (Grant Nos.\n202301AS070047, 202201BE070001-021).\n\n\n**References**\nAntoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko.\n2013. [Translating embeddings for modeling multi-](https://dl.acm.org/doi/10.5555/2999792.2999923)\n[relational data.](https://dl.acm.org/doi/10.5555/2999792.2999923) _Advances in neural information pro-_\n_cessing systems_, 26.\n\n\nChen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam.\n2022. Knowledge is flat: [A](https://aclanthology.org/2022.coling-1.352/) Seq2Seq generative\nframework for various [knowledge](https://aclanthology.org/2022.coling-1.352/) graph comple[tion.](https://aclanthology.org/2022.coling-1.352/) In _Proceedings of the 29th International Con-_\n_ference on Computational Linguistics_, pages 4005–\n4017, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\n\n\nShihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Wei\nShen, Limao Xiong, Yuhao Zhou, Xiao Wang, Zhiheng Xi, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui\nZheng, Tao Gui, Qi Zhang, and Xuanjing Huang.\n2024. [LoRAMoE: Alleviating world knowledge for-](https://doi.org/10.18653/v1/2024.acl-long.106)\n[getting in large language models via MoE-style plu-](https://doi.org/10.18653/v1/2024.acl-long.106)\n[gin.](https://doi.org/10.18653/v1/2024.acl-long.106) In _Proceedings of the 62nd Annual Meeting of_\n_the Association for Computational Linguistics (Vol-_\n_ume 1:_ _Long Papers)_, pages 1932–1945, Bangkok,\nThailand. Association for Computational Linguistics.\n\n\nXiou Ge, Yun Cheng Wang, Bin Wang, C-C Jay Kuo,\nand 1 others. 2024. Knowledge [graph](https://arxiv.org/abs/2309.12501) embedding:\n[An overview.](https://arxiv.org/abs/2309.12501) _APSIPA Transactions on Signal and_\n_Information Processing_, 13(1).\n\n\nXu Han, Shulin Cao, Lv Xin, Yankai Lin, Zhiyuan Liu,\nMaosong Sun, and Juanzi Li. 2018. [Openke:](https://aclanthology.org/D18-2024/) An\n[open toolkit for knowledge embedding.](https://aclanthology.org/D18-2024/) In _Proceed-_\n_ings of EMNLP_ .\n\n\nZeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and\nSai Qian Zhang. 2024. [Parameter-efficient](https://arxiv.org/abs/2403.14608) finetuning for large models: A comprehensive survey.\n_arXiv preprint arXiv:2403.14608_ .\n\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. [LoRA: Low-rank adaptation of](https://openreview.net/forum?id=nZeVKeeFYf9)\n[large language models.](https://openreview.net/forum?id=nZeVKeeFYf9) In _International Conference_\n_on Learning Representations_ .\n\n\nPeng Hu, Sizhe Liu, Changjiang Gao, Xin Huang,\nXue Han, Junlan Feng, Chao Deng, and Shujian\nHuang. 2025. Large language [models](https://aclanthology.org/2025.naacl-long.72/) are cross[lingual knowledge-free reasoners.](https://aclanthology.org/2025.naacl-long.72/) In _Proceedings of_\n_the 2025 Conference of the Nations of the Americas_\n_Chapter_ _of_ _the_ _Association_ _for_ _Computational_ _Lin-_\n_guistics:_ _Human Language Technologies (Volume 1:_\n_Long Papers)_, pages 1525–1542, Albuquerque, New\nMexico. Association for Computational Linguistics.\n\n\nKaiyu Huang, Fengran Mo, Xinyu Zhang, Hongliang\nLi, You Li, Yuanchi Zhang, Weijian Yi, Yulong Mao,\nJinchen Liu, Yuzhuang Xu, and 1 others. 2024. [A sur-](https://arxiv.org/abs/2405.10936)\n[vey on large language models with multilingualism:](https://arxiv.org/abs/2405.10936)\n[Recent advances and new frontiers.](https://arxiv.org/abs/2405.10936) _arXiv preprint_\n_arXiv:2405.10936_ .\n\n\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. 2022. [A survey on knowledge](https://doi.org/10.1109/TNNLS.2021.3070843)\ngraphs: [Representation, acquisition, and applications.](https://doi.org/10.1109/TNNLS.2021.3070843)\n_IEEE Transactions on Neural Networks and Learning_\n_Systems_, 33(2):494–514.\n\n\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick Van Kleef,\nSören Auer, and 1 others. 2015. [Dbpedia–a](https://www.researchgate.net/publication/259828897_DBpedia_-_A_Large-scale_Multilingual_Knowledge_Base_Extracted_from_Wikipedia) largescale, multilingual [knowledge](https://www.researchgate.net/publication/259828897_DBpedia_-_A_Large-scale_Multilingual_Knowledge_Base_Extracted_from_Wikipedia) base extracted from\n[wikipedia.](https://www.researchgate.net/publication/259828897_DBpedia_-_A_Large-scale_Multilingual_Knowledge_Base_Extracted_from_Wikipedia) _Semantic web_, 6(2):167–195.\n\n\nHanxiao Liu, Yuexin Wu, and Yiming Yang. 2017. [Ana-](https://dl.acm.org/doi/10.5555/3305890.3305905)\n[logical inference for multi-relational embeddings.](https://dl.acm.org/doi/10.5555/3305890.3305905) In\n_Proceedings_ _of_ _the_ _34th_ _International_ _Conference_\n_on Machine Learning - Volume 70_, ICML’17, page\n2168–2178. JMLR.org.\n\n\nYang Liu, Xiaobin Tian, Zequn Sun, and Wei Hu. 2024.\n\nFinetuning generative [large](https://arxiv.org/abs/2407.16127) language models with\n[discrimination instructions for knowledge graph com-](https://arxiv.org/abs/2407.16127)\n[pletion.](https://arxiv.org/abs/2407.16127) In _International Semantic Web Conference_,\npages 199–217. Springer.\n\n\nMaximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016. [Holographic embeddings of knowledge](https://dl.acm.org/doi/10.5555/3016100.3016172)\n[graphs.](https://dl.acm.org/doi/10.5555/3016100.3016172) In _Proceedings of the AAAI conference on_\n_artificial intelligence_, volume 30.\n\n\nApoorv Saxena, Adrian Kochsiek, and Rainer Gemulla.\n2022. [Sequence-to-sequence knowledge graph com-](https://doi.org/10.18653/v1/2022.acl-long.201)\npletion and [question](https://doi.org/10.18653/v1/2022.acl-long.201) answering. In _Proceedings_\n_of_ _the_ _60th_ _Annual_ _Meeting_ _of_ _the_ _Association_ _for_\n_Computational Linguistics (Volume 1:_ _Long Papers)_,\npages 2814–2828, Dublin, Ireland. Association for\nComputational Linguistics.\n\n\n\nRan Song, Shengxiang Gao, Xiaofei Gao, Cunli Mao,\nand Zhengtao Yu. 2025. Mke-pllm: [A benchmark for](https://doi.org/10.1016/j.neucom.2025.130979)\n[multilingual knowledge editing on pretrained large](https://doi.org/10.1016/j.neucom.2025.130979)\n[language model.](https://doi.org/10.1016/j.neucom.2025.130979) _Neurocomputing_, 651:130979.\n\n\nRan Song, Shizhu He, Shengxiang Gao, Li Cai, Kang\nLiu, Zhengtao Yu, and Jun Zhao. 2023. [Multilingual](https://doi.org/10.18653/v1/2023.findings-acl.488)\nknowledge graph [completion](https://doi.org/10.18653/v1/2023.findings-acl.488) from pretrained lan[guage models with knowledge constraints.](https://doi.org/10.18653/v1/2023.findings-acl.488) In _Find-_\n_ings of the Association for Computational Linguis-_\n_tics:_ _ACL 2023_, pages 7709–7721, Toronto, Canada.\nAssociation for Computational Linguistics.\n\n\nRan Song, Shizhu He, Suncong Zheng, Shengxiang Gao,\nKang Liu, Zhengtao Yu, and Jun Zhao. 2022a. [De-](https://aclanthology.org/2022.coling-1.196/)\n[coupling mixture-of-graphs:](https://aclanthology.org/2022.coling-1.196/) Unseen relational learning for knowledge graph completion by fusing ontology and [textual](https://aclanthology.org/2022.coling-1.196/) experts. In _Proceedings_ _of_ _the_\n_29th International Conference on Computational Lin-_\n_guistics_, pages 2237–2246, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\n\n\nRan Song, Shizhu He, Suncong Zheng, Shengxiang\nGao, Kang Liu, Jun Zhao, and Zhengtao Yu. 2022b.\n[Ontology-guided and text-enhanced representation](https://openreview.net/pdf?id=H87NVDpH-fq)\n[for knowledge graph zero-shot relational learning.](https://openreview.net/pdf?id=H87NVDpH-fq) In\n_ICLR 2022 Workshop on Deep Learning on Graphs_\n_for Natural Language Processing_ .\n\n\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: [Knowledge graph embedding by](https://openreview.net/forum?id=HkgEQnRqYQ)\n[relational rotation in complex space.](https://openreview.net/forum?id=HkgEQnRqYQ) In _International_\n_Conference on Learning Representations_ .\n\n\nChunlin Tian, Zhan Shi, Zhijiang Guo, Li Li, and\nChengzhong Xu. 2024. Hydralora: [An](https://proceedings.neurips.cc/paper_files/paper/2024/hash/123fd8a56501194823c8e0dca00733df-Abstract-Conference.html) asymmet[ric lora architecture for efficient fine-tuning.](https://proceedings.neurips.cc/paper_files/paper/2024/hash/123fd8a56501194823c8e0dca00733df-Abstract-Conference.html) In _Ad-_\n_vances_ _in_ _Neural_ _Information_ _Processing_ _Systems_\n_(NeurIPS)_ .\n\n\nThéo Trouillon, Johannes Welbl, Sebastian Riedel, Éric\nGaussier, and Guillaume Bouchard. 2016. [Complex](https://dl.acm.org/doi/10.5555/3045390.3045609)\n[embeddings for simple link prediction.](https://dl.acm.org/doi/10.5555/3045390.3045609) In _Proceed-_\n_ings of the 33rd International Conference on Interna-_\n_tional Conference on Machine Learning - Volume 48_,\nICML’16, page 2071–2080. JMLR.org.\n\n\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. [Wiki-](https://dl.acm.org/doi/10.1145/2629489)\ndata: [a free collaborative knowledgebase.](https://dl.acm.org/doi/10.1145/2629489) _Communi-_\n_cations of the ACM_, 57(10):78–85.\n\n\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKEPLER: A unified [model](https://doi.org/10.1162/tacl_a_00360) for knowledge embed[ding and pre-trained language representation.](https://doi.org/10.1162/tacl_a_00360) _Trans-_\n_actions of the Association for Computational Linguis-_\n_tics_, 9:176–194.\n\n\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\nChen. 2014. [Knowledge graph embedding by trans-](https://dl.acm.org/doi/10.5555/2893873.2894046)\n[lating on hyperplanes.](https://dl.acm.org/doi/10.5555/2893873.2894046) In _Proceedings of the Twenty-_\n_Eighth_ _AAAI_ _Conference_ _on_ _Artificial_ _Intelligence_,\nAAAI’14, page 1112–1119. AAAI Press.\n\n\nYanbin Wei, Qiushi Huang, Yu Zhang, and James Kwok.\n2023. [KICGPT: Large language model with knowl-](https://doi.org/10.18653/v1/2023.findings-emnlp.580)\n[edge in context for knowledge graph completion.](https://doi.org/10.18653/v1/2023.findings-emnlp.580) In\n_Findings of the Association for Computational Lin-_\n_guistics: EMNLP 2023_, pages 8667–8683, Singapore.\nAssociation for Computational Linguistics.\n\n\nGerhard Weikum. 2021. Knowledge [graphs 2021:](https://dl.acm.org/doi/10.14778/3476311.3476393) A\n[data odyssey.](https://dl.acm.org/doi/10.14778/3476311.3476393) _Proceedings of the VLDB Endowment_,\n14(12):3233–3238.\n\n\nXin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui\nChen, Feiyu Xiong, Mosha Chen, and Huajun Chen.\n2022. From discrimination [to](https://doi.org/10.1145/3487553.3524238) generation: Knowl[edge graph completion with generative transformer.](https://doi.org/10.1145/3487553.3524238)\nIn _Companion Proceedings of the Web Conference_\n_2022_, WWW ’22, page 162–165, New York, NY,\nUSA. Association for Computing Machinery.\n\n\nYexiang Xue, Yang Yuan, Zhitian Xu, and Ashish Sabharwal. 2018. [Expanding holographic embeddings](https://dl.acm.org/doi/10.5555/3327345.3327361)\n[for knowledge completion.](https://dl.acm.org/doi/10.5555/3327345.3327361) _Advances in neural infor-_\n_mation processing systems_, 31.\n\n\nBishan Yang, Wen tau Yih, Xiaodong He, Jianfeng Gao,\nand Li Deng. 2014. [Embedding entities and relations](https://api.semanticscholar.org/CorpusID:2768038)\n[for learning and inference in knowledge bases.](https://api.semanticscholar.org/CorpusID:2768038) In _In-_\n_ternational Conference on Learning Representations_ .\n\n\nZhanqiu Zhang, Jianyu Cai, Yongdong Zhang, and Jie\nWang. 2020. [Learning hierarchy-aware knowledge](https://ojs.aaai.org/index.php/AAAI/article/view/5701)\n[graph embeddings for link prediction.](https://ojs.aaai.org/index.php/AAAI/article/view/5701) In _Proceed-_\n_ings of the AAAI conference on artificial intelligence_,\nvolume 34, pages 3065–3072.\n\n\nWenxuan Zhou, Fangyu Liu, Ivan Vuli´c, Nigel Collier,\nand Muhao Chen. 2022. Prix-LM: [Pretraining](https://doi.org/10.18653/v1/2022.acl-long.371) for\nmultilingual [knowledge](https://doi.org/10.18653/v1/2022.acl-long.371) base construction. In _Pro-_\n_ceedings of the 60th Annual Meeting of the Associa-_\n_tion for Computational Linguistics (Volume 1:_ _Long_\n_Papers)_, pages 5412–5424, Dublin, Ireland. Association for Computational Linguistics.\n\n\n**A** **Appendix**\n**A.1** **Prompt Example**\nWe present prompt examples for candidate entity lists of varying lengths during the training phase.\n\n\nPrompt Given a triplet with a missing tail entity t: (Saint George and the Dragon, material\nused, t).\n\n\nThe following provides descriptive information about entity Saint George and the\nDragon:\nSaint George and the Dragon, Saint George and the Dragon or Saint George Killing\nthe Dragon is a 1555 or 1558 painting by the Venetian artist Tintoretto. It was later\nacquired by the English collector\nHere are some triplets containing entity Saint George and the Dragon:\n\n[(Saint George and the Dragon, depicts, hill); (Saint George and the Dragon, depicts,\nspear); (Saint George and the Dragon, creator, Jacopo Tintoretto); (Saint George and\nthe Dragon, depicts, combat); (Saint George and the Dragon, depicts, woman); (Saint\nGeorge and the Dragon, depicts, sky)]\nWhat is the entity name of t? Select one from the list of entities below: [oil paint;\nSaint George and the Dragon; wood; tempera; textile; brick; pearl; metamorphic rock;\nschist; sandstone; paint; igneous rock; tissue; gemstone; brass; copper; woven fabric;\nvolcanic rock; marble; dragon; basalt; sedimentary rock; The Three Graces; limestone;\nsteel]\n[Answer]:\n\n\nNumber 25\nof entities\nPrompt Given a triplet with a missing tail entity t: (Jason Lee, instance of, t).\n\n\nThe following provides descriptive information about entity Jason Lee:\nJason Lee, Jason Michael Lee (born April 25, 1970) is an American actor, photographer, producer, skateboarder, comedian, and writer. He is best known for his roles as\nEarl Hickey in the television\nHere are some triplets containing entity Jason Lee:\n\n[(Mallrats, cast member, Jason Lee); (Jason Lee, ethnic group, Scottish American);\n(Jason Lee, occupation, screenwriter); (Jason Lee, occupation, actor); (Jason Lee,\noccupation, film producer); (Jason Lee, occupation, businessperson)]\nWhat is the entity name of t? Select one from the list of entities below: [Jason Lee;\nhuman; twin; Jason Alexander; Sofía Vergara; Kevin Smith; Screen Actors Guild\nAward; David Cross; 3D film; college; Primetime Emmy Award; sports season; MTV\nMovie Awards; Kaley Cuoco; municipality of Spain; Jason Mewes; decade; military\nrank; suburb; animation studio; Jane Lynch; Hank Azaria; Satellite Award; Breckin\nMeyer; My Name Is Earl; Patrick Warburton; business]\n[Answer]:\n\n\nNumber 27\nof entities\nTable 7: Prompt examples for candidate entity lists of varying lengths.\n\n\n**A.2** **Details of the Iterative Entity Reranking Algorithm**\n**Algorithm 1** Iterative Entity Reranking (IER)\n1: **Input:** Query _q_ = ( _h, r,_ ?), _Mc_ [(1)] = [ _e_ 1 _, e_ 2 _, . . ., em_ ]: the top- _m_ entities generated by the KGE model,\n_Nt_, _L_ [(1)] = _Mc_ [(1)]\n2: **for** _t_ = 1 to _Nt_ **do**\n3: _e_ [(] _[t]_ [)] = argmax\nargmax _P_ ( _ei_ _| h, r, Mc_ [(] _[t]_ [)][)][;]\n_ei∈Mc_ [(] _[t]_ [)]\n4: _Mc_ [(] _[t]_ [+1)] = _Mc_ [(] _[t]_ [)] _\\ {e_ [(] _[t]_ [)] _}_ ;\n\n5: _L_ [(] _[t]_ [+1)] = **Insert** ( _L_ [(] _[t]_ [)] _\\ {e_ [(] _[t]_ [)] _}, t, e_ [(] _[t]_ [)] );\n\n\n\n6: **end for**\n7: **Output:** _L_ [(] _[N][t]_ [+1)]\n**A.3** **Analysis of Knowledge Graph Embedding Models**\nThe experimental results in Table 8 clearly demonstrate that when using RotatE to retrieve candidate\nentities, our proposed method achieves a significant performance improvement compared to the original\nRotatE model, with a 14.78% increase in Hits@1. Notably, Ours+RotatE exhibits slightly lower performance than Ours+TransE on several language-specific metrics. This phenomenon can be attributed to the\ndiffering top-1 ranking rates of correct entities within the candidate sets generated by each KGE model.\nSpecifically, the proportion of correct entities ranked as top-1 was 14.38% when using TransE, whereas\nthis proportion significantly increased to 30.51% with RotatE. Therefore, this feature has had some impact:\nduring the fine-tuning stage, LLM is more inclined to choose the entity that ranks first in the candidate\nset as the final answer. We hypothesize that this \"top-1 bias\" may, to some extent, suppress the model’s\nexploration of other potentially correct answers, leading to Ours+RotatE performing slightly worse than\nOurs+TransE on some languages. In future work, we plan to further investigate how to construct a more\nstable fine-tuning instruction set that does not rely on traditional KGE models.\n\n\nMODEL EN FR IT JA ZH AVG\nH\n@\n1\nH\n@\n3\nH\n@\n10\nM\nR\nR\nRotatE 24.08 24.61 25.57 29.49 31.36 27.02\nOurs+TransE 36.50 **36.72** **35.93** 41.60 58.63 **41.88**\nOurs+RotatE **36.55** 35.70 35.46 **41.78** **59.49** 41.80\nRotatE 40.73 42.13 41.68 49.78 62.36 47.34\nOurs+TransE 46.25 **45.30** **44.22** 51.97 66.93 50.93\nOurs+RotatE **46.38** 45.05 44.20 **52.27** **67.13** **51.01**\nRotatE 52.66 53.17 51.50 61.68 74.58 58.72\nOurs+TransE 54.71 53.45 52.31 60.85 72.56 58.78\nOurs+RotatE **54.76** **54.13** **52.53** **62.31** **74.59** **59.66**\nRotatE 34.63 35.39 35.49 41.75 48.74 39.20\nOurs+TransE 42.96 **42.58** **41.69** 48.33 63.74 47.86\nOurs+RotatE **43.04** 42.18 41.58 **49.01** **64.46** **48.05**\nTable 8: The impact of different KGE models on the performance of our proposed framework during the candidate\nentities retrieval process.\n\n\n**A.4** **Analysis of Expert Routing**\nTo verify the existence of knowledge sharing, we analyzed the expert selection in the test samples. We\nobtained the output of each expert routing and visualized it from both the linguistic and knowledge\ndimensions. The left side of Figure 7 shows the expert routing analysis based on the linguistic dimension.\nIllustrates the selection of experts in each Transformer layer of the LLM for samples in different languages.\nThe expert selections across different language samples are mostly consistent, suggesting that our method\ndoes not distinguish between languages in MKGC task. The right side of the figure illustrates the expert\nselection for each relation. The analysis from figure shows that samples with same relations across\ndifferent languages are mostly handled by same experts. Overall, some relations exhibit consistent expert\nselection, while others show differences. Based on this analysis, it is validated that our method can\neffectively leverage knowledge sharing across different languages.\n\n\n\nen fr Languages it ja zh\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n32\n31\n30\n29\n28\n27\n26\n25\n24\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nKnowledge Dimension\nRelations\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing.pdf-13-0.png)\n\nFigure 7: The left shows expert selection across five languages. The horizontal axis represents all languages, with\neach small bar within a language corresponding to an expert. The vertical axis indicates the layer numbers in the\nTransformer of the LLM. The color intensity of each blocks represents the frequency of samples selecting particular\nexpert. The right depicts expert selection across all relations. The horizontal axis represents all relations. Vertical\naxis shows the layer numbers in Transformer, each row within a layer corresponding to an expert.\n\n\n**A.5** **IER Trend Analysis**\nWe analyzed the impact of the number of iterations in the IER method on five language evaluation metrics.\nThe experimental results reveal a clear performance improvement across all languages with increased\niterations. These findings demonstrate the IER method’s ability to exploit cross-lingual shared knowledge.\n\n\n0.550\n0.525\n0.500\n0.475\n0.450\n0.425\n0.400\n0.375\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n||Hits@<br>~~Hits@~~|1<br>~~3~~||||||||||\n||Hits@<br>MRR|10||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\nRound\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n||~~Hits~~<br>Hits@<br>|~~1~~<br>3<br>||||||||||\n||~~Hits~~<br>MRR|~~10~~|~~10~~|||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\nRound\nFigure 8: This figure shows how the metric performance\nof the IER method changes with the number of iterations\non the English test set.\n\n\n0.525\n0.600\n0.575\n0.550\n0.525\n0.500\n0.475\n0.450\n0.425\n0.500\n0.475\n0.450\n0.425\n0.400\n0.375\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||1<br>|1<br>|||||||||\n||Hits@<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|1<br>|\n||Hits@<br>Hits@<br>|3<br>10||||||||||\n||~~MRR~~|||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\nRound\nFigure 9: This figure shows how the metric performance\nof the IER method changes with the number of iterations\non the French test set.\n\n\n\nFigure 11: This figure shows how the metric performance of the IER method changes with the number of\niterations on the Japanese test set.\n\n\n0.72\n0.525\n0.500\n0.475\n0.450\n0.425\n0.400\n0.375\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n||Hits@<br>Hits@<br>|1<br>3<br>||||||||||\n||~~Hits~~<br>MRR|~~10~~|~~10~~|||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\nRound\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n||~~Hits@~~|||||||||||\n||Hits@3<br>Hits@|0||||||||||\n|||||||||||||\n||MRR|||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\nRound\nFigure 10: This figure shows how the metric performance of the IER method changes with the number of\niterations on the Italian test set.\n\n\n\n0.70\n0.68\n0.66\n0.64\n0.62\n0.60\n0.58\nFigure 12: This figure shows how the metric performance of the IER method changes with the number of\niterations on the Chinese test set.\n\n\n",
    "kor_md": "**효율적인** **다국어** **지식** **공유를** **통한** **다국어** **지식** **그래프** **완성**\n**Cunli Mao** [1] _[,]_ [2] **, Xiaofei Gao** [1] _[,]_ [2] **, Ran Song** [1] _[,]_ [2][ *] **, Shizhu He** [3] _[,]_ [4]\n**Shengxiang Gao** [1] _[,]_ [2] **, Kang Liu** [3] _[,]_ [4] **, Zhengtao Yu** [1] _[,]_ [21] 정보 공학 및 자동\n화 학부,\n중국 쿤밍, 쿤밍 과학기술대학교\n2 중국 윈난성 인공지능 핵심 연구소, 중국 쿤밍\n3 중국과학원 자동화연구소, 복잡계 인지 및 의사결정 지능 핵심 연구소, 중국 베이징\n4 중국과학원대학 인공지능학부, 중국 베이징\n{maocunli,xiaofeigao_g,song_ransr}@163.com, {shizhu.he,kliu}@nlpr.ia.ac.cn,\n{g aoshengxiang.yn,ztyu}@hotmail.com\n**초록**\n대규모 언어 모델(LLM) 기반 다국어 지식 그래프 완성\n(MKGC)은 LLM의 다국어 이해 능력을 활용하여 누락된\n사실을 예측함으로써 다국어 지식 그래프(KG)의 완성도\n를 향상시키는 것을 목표로 합니다. 그러나 기존 MKGC\n연구는 LLM의 다국어 능력을 충분히 활용하지 못하며,\n언어 간 지식의 공유 가능성을 간과하고 있습니다. 본 논\n문에서는 다국어 공유 지식을 활용하는 새로운 MKGC 프\n레임워크를 제안한다.\n\n\n두 가지 구성 요소인 지식 수준 그룹화 전문가 혼합 모델\n(KL-GMoE)과 반복적 엔티티 재정렬(IER)을 통해 성능\n을 크게 향상시킵니다. KL-GMoE는 공유 지식을 효율적\n으로 모델링하는 반면, IER는 그 활용도를 크게 높입니다.\n\n본 프레임워크를 평가하기 위해 5개 언어로 구성된 mKG\n데이터셋을 구축하고 비교 분석을 수행했습니다.\n\n기존 최첨단(SOTA) MKGC 방법과의 포괄적인 비교 실\n험을 수행하였다. 실험 결과는 본 프레임워크가 SOTA\nMKGC 방법 대비 Hits@1, Hits@3, Hits@10 지표에서\n각각 5.47%, 3.27%, 1.01%의 성능 향상을 달성함을 보여\n준다. 추가 실험 분석을 통해 미관측 언어 및 불균형 언\n어 환경에서의 지식 공유 특성을 규명하였습니다. 본 연\n구의 데이터셋과 코드는\n[https://github.com/gaoxiaofei07/KL-GMoE](https://github.com/gaoxiaofei07/KL-GMoE) 에서 공개\n하였습니다.\n\n\n**1** **소개**\n지식 그래프(KG)(Weikum, 2021)는 현실 세계에 대한 지식\n을 표현하고 조직화하기 위해 설계된 구조화된 의미적 지식\n기반입니다. 대부분의 KG는 _위키데이터_\n( _Wikidata_ )(Vrandecˇic´ and Krötzsch,\n*교신저자\n**(a)** **아키텍처** **불일치**\n지\n**(b)** **작업** **패러다임** **불일치**\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing-ko.pdf-0-0.png)\n\n\n\n\n\n출력 [EN]\n질의 후보 엔티티 결과\n\n\n\n|1. 베를린<br>2. 취리히<br>3. 파리<br>···<br>29. 울름<br>30. 프라하|Col2|Col3|LLM 예측<br>파리|1. 파리<br>2. 베를린<br>3. 취리히|Col6|\n|---|---|---|---|---|---|\n|1. 베를린<br>2. 취리히<br>3. 파리<br>···<br>29. 울름<br>30. 프라하||LLM||<br>···<br>29. 울름<br>30. 프라하|<br>···<br>29. 울름<br>30. 프라하|\n\n\nKGE에서 획득\n그림 1: 이 그림은 발생하는 문제들을 보여줍니다\nLLM을 MKGC 작업에 직접 적용할 때. (a)는 기존 PEFT가 MKGC\n작업에 적합하지 않음을 보여준다. (b)는 LLM이 탁월한 작업 패러\n다임과 MKGC 작업 간의 불일치를 나타낸다.\n\n\n기존 다국어 지식 그래프(KG)는 종종 불완전하여 실용적 적\n용에서 효과성이 제한된다(Ji et al., 2022). 다국어 지식 그래\n프 완성(MKGC)은 알려진 다국어 지식을 활용하여 누락된\n삼중항을 완성하고 KG의 완전성을 향상시키는 것을 목표로\n한다.\n\n\n기존 연구는 엔티티와 관계를 저차원 벡터 공간으로 매핑\n하여 완성을 달성하는 임베딩 기반 방법(Ge et al., 2024)에\n집중해왔다. 최근 언어 모델의 발전으로 MKGC 연구는 KG\n완성을 재구성하는 생성 기반 접근법(Chen et al., 2022;\n\nSaxena et al., 2022)으로 전환되고 있다.\n\n\n텍스트 생성을 위한 방법론으로 제시된다. (Song et al.,\n2023; Zhou et al., 2022)는 단일 사전 훈련된 언어 모델\n(PLM)을 활용하여 통합된 의미 공간 내에서 다국어 지식을\n통합함으로써 MKGC에서 우수한 성능을 달성한다. 또한\nDIFT(Liu et al., 2024)와 같은 최근 연구는 LLM의 미세 조\n정을 통한 작업 적응을 탐구하여 단일 언어 KGC 작업에서 강\n력한 성능을 보여준다. 다양한 코퍼스로 사전 훈련된 현대\nLLM은 본질적으로 다국어 능력을 보유하며(Huang et al.,\n2024), 통합 모델 내에서 언어 간 표현 및 지식 공유를 가능하\n게 합니다(Hu et al., 2025). 특히, 이러한 내재적 다국어 지식\n공유 능력은 MKGC에 필수적이며 완성 성능 향상에 상당한\n잠재력을 제공합니다. 이러한 잠재력을 인식하여 본 연구는\nLLM의 내재적 능력을 활용해 MKGC를 개선하는 효과적인\n방법을 탐구한다.\n\n그러나 LLM을 MKGC에 직접 적용하는 것은 주로 두 가\n지 핵심 측면, 즉 모델 아키텍처와 작업 패러다임에서 비롯된\n여러 가지 과제를 안고 있습니다. 1) **아키텍처** **불일치** : LLM\n을 위한 기존의 파라미터 효율적 미세 조정(PEFT) 방법(Han\net al., 2024)은 주로 텍스트 중심 작업을 위해 설계되었으며\n지식 수준 작업에 적용할 때 상당한 격차를 보입니다. 특히\n단일 채널 방식은 KG의 복잡한 다국어 특성에 취약합니다.\n\n그림 1(a) 상단에서 보듯, 다수의 다국어 질의를 단일 채널로\n처리하면 지식 과부하가 발생합니다. 이 과부하는 모델이 언\n어 간 유사한 지식을 이해하는 능력을 저해하여 잘못된 예측\n을 초래합니다. 예를 들어, _아인슈타인의출생지에_ 관한 영어\n, 중국어, 이탈리아어 질의 모두 잘못된 결과를 산출합니다.\n\n반대로 다중 채널 방식은 지식의 원자성을 훼손하여 지식 분\n열을 유발하는 경향이 있습니다. 그림 1(a) 하단에서 볼 수 있\n듯이, 쿼리 토큰이 서로 다른 채널에서 처리됩니다. 이러한\n분열된 처리는 결국 잘못된 엔티티 예측으로 이어집니다.\n\n2) **작업** **패러다임** **불일치** : MKGC 작업은 엔티티 순위를 매기\n는 작업으로, 텍스트 생성 패러다임과 불일치를 보입니다. 그\n림 1(b)에서 볼 수 있듯이, 쿼리 _(아인슈타인, 출생지,_\n_?)_, LLM은 _파리를_ 오답으로 예측했습니다. 이 선택은 정답\n_엔티티인울름의_ 순위를 개선하지 못했습니다.\n\n1) **아키텍처** **불일치**, 특히 지식 과부하 문제를 해결하기\n위해 전용 지식 채널 수를 증가시키는 방안을 제안한다.\n\n\n\n넬스. 이를 통해 각 채널은 의미적으로 유사한 정보 처리에\n집중할 수 있어, LLM의 다국어 공유 지식 이해 및 활용 능력\n을 향상시킵니다. 동시에 각 채널이 완전한 지식을 독립적으\n로 처리할 수 있도록 함으로써 지식 분열을 효과적으로 완화\n하고 모델의 다국어 정보에 대한 포괄적 이해를 촉진합니다.\n\n2) **작업** **패러다임** **불일치를** 해결하기 위해, 우리는 LLM의 훈\n련 목표를 조정하여 다중 엔티티의 순위를 반복적으로 정교\n화할 수 있도록 제안합니다. 이 접근법은 LLM이 다국어 공\n유 지식을 활용하는 빈도를 높여 올바른 엔티티의 순위를 향\n상시키는 것을 목표로 합니다.\n\n본 논문에서는 다국어 공유 지식을 효과적으로 활용하여\nMKGC의 성능을 향상시키는 새로운 프레임워크를 제안한다\n. 제안된 프레임워크는 지식 수준 그룹화 전문가 혼합(KL\nGMoE)과 반복적 엔티티 재정렬(IER)이라는 두 가지 시너지\n구성 요소로 이루어져 있다. KL-GMoE는 지식 수준 전문가\n라우팅 메커니즘과 그룹 기반 전문가 혼합(MoE) 아키텍처를\n도입한다. 이 설계는 지식 분열을 완화하는 동시에 LLM의\n다국어 공유 지식 포착 능력을 크게 향상시키는 것을 목표로\n한다. IER는 LLM의 훈련 목표와 디코딩 전략을 모두 수정합\n니다.  이를 통해 모델은 다중 반복 정제를 통해 다국어 공\n유 지식 활용 능력을 크게 향상시킬 수 있습니다. 실험 결과,\n본 프레임워크는 SOTA MKGC 방법에 비해 Hits@1,\nHits@3, Hits@10 지표에서 각각 5.47%, 3.27%, 1.01%의 개\n선을 달성한 것으로 나타났습니다. 추가 실험 분석을 통해 미\n지 언어 및 불균형 언어 환경에서의 지식 공유 특성을 규명하\n였습니다.\n\n요약하면, 우리의 기여는 다음과 같습니다:\n\n\n  - 모델 아키텍처 불일치 문제를 해결하고 공유 지식을\n효율적으로 모델링하기 위해 KL-GMoE를 제안합니\n다.\n\n\n  - 작업 패러다임의 불일치를 해결하기 위해 IER를 제안\n하여 공유 지식 활용도를 높였습니다.\n\n\n  - 실험 결과, 우리의 프레임워크는 SOTA MKGC 방법보\n다 평균적으로 Hits@1, Hits@3, Hits@10에서 각각\n5.47%, 3.27%, 1.01% 향상된 성능을 보이며 현저히 우\n수한 성능을 보였습니다.\n\n\n**언어** **엔티티** **관계** **훈련** **검증** **테스트**\nEN 86,539 512 708,267 49,782 49,777\nFR 89,754 478 839,623 49,908 30,000\nIT 65,434 445 613,014 49,883 20,000\nJA 46,294 432 321,237 49,939 10,000\nZH 63,278 397 546,626 49,969 10,000\n합계 351,299  2,264  3,028,767249,481119,777\n표 1: 다국어 지식 그래프 완성 데이터 세트의 통계.\n\n\n**2** **데이터** **세트**\n**2.1** **데이터셋** **구축**\n우리는 위키데이터와 _위키백과를_ 통합한 백만 규모 영어 지\n식 그래프(KG) 데이터셋인 _위키데이터5M_ (Wang et al.,\n2021)을 기초 시드 라이브러리로 활용합니다. 이를 바탕으로\n_위키데이터에서_ 데이터를 수집하여 프랑스어, 이탈리아어,\n중국어, 일본어를 추가로 포함하도록 데이터셋을 확장했습니\n다. 표 1에 제시된 바와 같이, 엔티티 수, 관계 수, 훈련용, 검\n증용, 테스트용 트리플 수에 대한 통계를 제시합니다. 이 지\n식 그래프는 351,299개의 엔티티와 2,264개의 관계를 포함하\n며, 총 트리플 수는 300만 개를 초과합니다.\n\n다언어 지식 분포의 특성에 기반하여, 서로 다른 언어 간\n지식은 완전히 정렬되어 있지 않으며 특정 언어적 특수성을\n나타낸다(Song et al., 2025). 이러한 언어 간 지식의 비대칭성\n은 일부 지식이 특정 언어에 국한됨을 시사한다. 따라서 우리\n가 구축한 데이터셋은 지식의 자연스러운 분포 패턴을 따른\n다. 일부 지식은 여러 언어에 걸쳐 공유되며, 이는 언어 간의\n유사성을 반영한다. 다른 지식은 각 언어에 고유하며, 이는\n각 언어의 독특한 특성을 반영한다.\n\n\n**2.2** **프롬프트** **구성**\n우리는 DIFT(Liu et al., 2024)에서 제안한 프롬프트 구성 방법\n을 채택합니다. 임베딩 기반 모델은 훈련 데이터를 학습했기\n때문에 대부분의 훈련 사실에 대해 후보 엔티티 중 정답 엔티\n티를 첫 번째 순위로 배치하는 경향이 있습니다. 이러한 순위\n지정된 후보를 사용하여 프롬프트를 구성하면 LLM이 첫 번\n째 엔티티를 정답으로 선택하는 편향을 발달시킬 수 있습니\n다. 따라서 검증 세트에서 일부를 분할하여 프롬프트를 구성\n하고, 이를 LLM의 미세 조정 단계에서 훈련 데이터로 활용\n합니다.\n\n쿼리 _q_ = ( _h, r,_ ?)에 대해, 구성된 프롬프트 P는 네 부분\n으로 구성됩니다: 쿼리 _Q_, 설명자 D, 인접 사실 N, 후보 엔티\n티 M(c) . 이는 다음과 같이 표현될 수 있습니다:\n\n\n\n, 인접 사실 _N_, 후보 엔티티 _Mc_ 로 _구성됩니다_ . 이는 다음과\n같이 표현될 수 있습니다:\n\n\n_P_ ( _q_ ) = [ _Q_ ; _D_ ; _N_ ; _Mc_ ] _._ (1)\n설명은 _엔티티 h에_ 대한 구체적인 설명 정보를 제공하여\n모델이 엔티티의 의미를 보다 정확하게 이해할 수 있도록 합\n니다. 인접 사실은 엔티티 _h를_ 포함하는 삼중항으로, 이러\n한 삼중항은 지식 그래프 임베딩(KGE) 모델의 훈련 데이터\n에서 무작위로 추출됩니다. 이러한 인접 사실들은 LLM이\n_엔티티 h에_ 대한 이해를 향상시키기 위한 것입니다. 후보\n엔티티 _Mc_ = [ _e_ 1 _, e_ 2 _, ..., em_ ]은 KGE 모델(Bordes et al.,\n2013)이 생성한 순위 결과에서 _상위 m개_ 엔티티를 선택하\n여 구성됩니다. LLM이 MKGC의 작업 패러다임에 적응할\n수 있도록, 훈련 과정에서 _Mc_ 내 엔티티 수 _m을_ 처리했습니\n다. 구체적인 처리 방법은 3.3절에 상세히 기술되어 있습니다\n. 부록 A.1에는 구체적인 프롬프트 예시를 제공합니다.\n\n\n**3** **방법론**\n**3.1** **작업** **정의**\n본 논문에서는 KGE 모델과 LLM을 통합하여 MKGC 작업\n을 수행한다. 먼저, 쿼리 _q_ = ( _h, r,_ ?)에 대해 KGE 모델\n을 사용하여 _상위 m개_ 순위의 엔티티를 추출하며, 이는 후\n보 엔티티 집합 _M(c)를_ 형성한다. 다음으로, LLM을 활용하여\n후보 엔티티 집합 _Mc_ 에서 최적의 엔티티를 선택하여 쿼리 _q_\n_를_ 완성한다. 완성 과정은 다음과 같이 공식화할 수 있다:\n\n\neˆ = argmax _P_ ( _ei_ | _h, r, Mc_ ) _,_ (2)\n_e_ _i_ _∈M_ _c_\n여기서 eˆ는 쿼리 _q를_ 완성하는 최적의 엔티티를 나타내며,\n_P_ ( _ei_ | _h, r, Mc_ )는 헤드 엔티티 h, 관계 r 및 후보 엔티티 집\n합 M _(_ c)가 주어졌을 때 엔티티 _ei가_ 선택될 확률을 나타낸\n다.\n\n엔티티 _h_, 관계 _r_ 및 후보 엔티티 _Mc_ 를 주어진 상태에서 엔티티\ne(i) 를 선택할 확률을 나타냅니다.\n\n\n**3.2** **KL-GMoE 아키텍처**\nKL-GMoE는 MKGC 작업에 특화되어 설계되었습니다. 이\n아키텍처는 단일 채널로 인한 지식 과부하를 완화하고 LLM\n의 공유 지식 포착 능력을 향상시키기 위해 다중 전문가 그룹\n으로 구성됩니다. 또한 KL-GMoE는 지식 수준 전문가 라우\n팅 메커니즘을 활용하여 모든 전문가가 공동으로 참여하는\n대신 각 샘플이 특정 전문가에 의해 처리되도록 보장합니다.\n\n그림 2(a)에서 볼 수 있듯이,\n**(a) KL-GMoE 아키텍처** **(b) 반복적** **엔티티** **재정렬**\n프롬프트: [쿼리; …; 후보 엔티티] 쿼리 (헤드 엔티티, 관계, ?)\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing-ko.pdf-3-1.png)\n\n\n\n\n\n후보\n엔티티(M [(1)] )\n반복\n𝑁𝑡 라운드\n동안 반복\n정확한 엔티티\n재순위 지정\n된 엔티티 목\n록\n잘못된 엔티티\n\n\n|X<br>KL-GMoE<br>지식 수준 전문가 경로<br>RMSNorm<br>···<br>완전한 지식은 전문가𝐸𝑖,j에<br>주의 의해 처리됩니다.<br>A<br>A𝑖−1 𝑖 A𝑖+1<br>RMSNorm 𝐵 𝑖−1,j 𝐵 𝑖−1,j+1 𝐵 𝑖,j 𝐵 𝑖,j+1 𝐵 𝑖+1,j 𝐵 𝑖+1,j+1<br>𝐺 𝐺 𝐺<br>𝑖−1 𝑖 𝑖+1<br>𝑦𝑒 =𝐸𝑖,j (X) =𝐵𝑖,j (𝐴𝑖 X)<br>FFN<br>사전 훈련된 (L)-번째 레이어 출력<br>가중치 ···|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|X<br>KL-GMoE|Col11|Col12|Col13|Col14|Col15|Col16|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|X<br>KL-GMoE<br>RMSNorm<br>지식 수준 전문가 경로<br>주의<br>···<br>완전한 지식은 전문가𝐸𝑖,j에 <br>의해 처리됩니다. <br>A𝑖−1<br>A𝑖<br>A𝑖+1<br>RMSNorm<br>𝐵𝑖−1,j<br> 𝐵 𝑖−1,j+1<br>𝐵𝑖,j<br>𝐵𝑖,j+1<br>𝐵𝑖+1,j<br> 𝐵 𝑖+1,j+1<br>𝐺𝑖−1<br>𝐺𝑖<br>𝐺𝑖+1<br>FFN<br>𝑦𝑒 =𝐸𝑖,j (X) =𝐵𝑖,j (𝐴𝑖 X)<br>사전 훈련된 <br>가중치<br>(L)-번째 레이어 출력<br>···|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|지식 수준 전문가 경로|\n|X<br>KL-GMoE<br>RMSNorm<br>지식 수준 전문가 경로<br>주의<br>···<br>완전한 지식은 전문가𝐸𝑖,j에 <br>의해 처리됩니다. <br>A𝑖−1<br>A𝑖<br>A𝑖+1<br>RMSNorm<br>𝐵𝑖−1,j<br> 𝐵 𝑖−1,j+1<br>𝐵𝑖,j<br>𝐵𝑖,j+1<br>𝐵𝑖+1,j<br> 𝐵 𝑖+1,j+1<br>𝐺𝑖−1<br>𝐺𝑖<br>𝐺𝑖+1<br>FFN<br>𝑦𝑒 =𝐸𝑖,j (X) =𝐵𝑖,j (𝐴𝑖 X)<br>사전 훈련된 <br>가중치<br>(L)-번째 레이어 출력<br>···||||||||||||||||\n|X<br>KL-GMoE<br>RMSNorm<br>지식 수준 전문가 경로<br>주의<br>···<br>완전한 지식은 전문가𝐸𝑖,j에 <br>의해 처리됩니다. <br>A𝑖−1<br>A𝑖<br>A𝑖+1<br>RMSNorm<br>𝐵𝑖−1,j<br> 𝐵 𝑖−1,j+1<br>𝐵𝑖,j<br>𝐵𝑖,j+1<br>𝐵𝑖+1,j<br> 𝐵 𝑖+1,j+1<br>𝐺𝑖−1<br>𝐺𝑖<br>𝐺𝑖+1<br>FFN<br>𝑦𝑒 =𝐸𝑖,j (X) =𝐵𝑖,j (𝐴𝑖 X)<br>사전 훈련된 <br>가중치<br>(L)-번째 레이어 출력<br>···|||||||||···|||||||\n|X<br>KL-GMoE<br>RMSNorm<br>지식 수준 전문가 경로<br>주의<br>···<br>완전한 지식은 전문가𝐸𝑖,j에 <br>의해 처리됩니다. <br>A𝑖−1<br>A𝑖<br>A𝑖+1<br>RMSNorm<br>𝐵𝑖−1,j<br> 𝐵 𝑖−1,j+1<br>𝐵𝑖,j<br>𝐵𝑖,j+1<br>𝐵𝑖+1,j<br> 𝐵 𝑖+1,j+1<br>𝐺𝑖−1<br>𝐺𝑖<br>𝐺𝑖+1<br>FFN<br>𝑦𝑒 =𝐸𝑖,j (X) =𝐵𝑖,j (𝐴𝑖 X)<br>사전 훈련된 <br>가중치<br>(L)-번째 레이어 출력<br>···|||||||||···|||||||\n\n\n\n그림 2: 본 연구에서 제안한 프레임워크를 보여줍니다. 그림 (a)는 KL-GMoE의 아키텍처와 워크플로우를 나타내며, 빨간색으로 강조된 행렬\n_Ai_ 와 _Bi,j_ 는 현재 활성화된 전문가를 나타냅니다. 그림 (b)는 IER 방법의 워크플로우를 보여줍니다. _Nt_ 번의 반복 후, 우리는 재정렬된 엔티티\n목록을 얻을 수 있습니다.\n\n\n\nKL-GMoE로 처리되는 각 샘플에 대해, 전문가 그룹 _Gi_ 내의\n행렬 _Ai_ 와 하나의 행렬 _Bi,j_ 만이 활성화됩니다. KL-GMoE는\nLLM의 전방전파 네트워크(FFN) 계층에만 적용됩니다. 구\n체적으로, 전방전파 중 FFN 계층에서의 행렬 연산은 다음과\n같이 표현될 수 있습니다:\n\n\n**y** = **W** 0 **X** + **y** _e,_ (3)\n**W** 0 ∈ R [dout][×][din] 은 훈련 중 고정되는 원래 FFN 레이어 매개\n변수 행렬을 나타낸다.\n\n**X** = [ **xh** : **xr** : **xt** ]는 FFN 레이어의 입력을 나타냅니다. **y** _e_ 는\n입력 **X를** 기반으로 KL-GMoE가 계산한 출력을 나타냅니\n다.\n\nKL-GMoE의 설계는 Hy-draLoRA(Tian et al., 2024)에서 제\n안된 비대칭 지표 미세 조정 아키텍처에서 영감을 받았습니다.\n\n미터법 미세 조정 아키텍처에서 영감을 받았습니다. 우리는\n그룹화\nMoE 설계 아키텍처에서 각 그룹은 다음과 같이 표현될 수 있\n습니다:\n\n\n_Gi_ = ( _Ai,_ { _Bi,j_ | _j_ ∈ {1 _,_ 2 _, . . ., Nb_ }}) _,_ (4)\n여기서 _i_ ∈ {1 _,_ 2 _, . . ., Ng_ }, _Ng_ 는 총 전문가 그룹 수를 나\n타낸다. _Nb_ 는 B의 수이다.\n\n군 _Gi_ 내의 행렬들. 각 군 _Gi_ 내에서, A _i_ 행렬과 B _i,j_ 행렬의 짝\n짓기는 전문가 _Ei,j_ = ( _Ai, Bi,j_ )로 간주된다. _Ai_ 행렬은 유사\n한 지식 범주를 포착하도록 설계되었다. 그룹 _Gi_ 내의 서로 다\n른 _Bi,j_ 행렬들은 이 지식 범주 내 미묘한 차이를 포착하는 모듈\n다중 언어 간 공유된 지식을 포착하는 LLM의 능력을 향상\n시키는 것을 목표로 합니다.\n\n\n동시에, 제안된 지식 수준 전문가 라우팅 메커니즘은 세\n가지 서로 다른 경로인 **R** _g_, **R** _k_ 및 **R** _(l) 을_ 포함합니다. 먼저 **R** _g_\n에 기반하여 전문가 그룹이 선택됩니다. 이 그룹 내에서 **R** _k_\n와 **R** _(l) 을_ 결합하여 특정 전문가가 결정됩니다. 다음은 이 세\n가지 경로를 기반으로 특정 전문가를 선택하는 과정을 설명\n합니다.\n\n\n**R** _g_ 는 **X를** 처리할 전문가 그룹을 결정하는 그룹 라우팅 선\n택 모듈입니다. 그룹 선택은 다음과 같이 공식화됩니다:\n\n\n_Gi_ =  argmax  ( **R** _g_ ( **X** ))\ni∈{1 _,_ 2 _,...,_ _Ng_ }\nSoftmax( **W** _g_ **x**\n\n_m_ [ )] [ ] _[,]_\n(5)\n= argmax\ni∈{1 _,_ 2 _,...,_ _Ng_ }\n#,\n\nm∈{ _h,r,t_ }\n\n\n## Σ\n\n\n\n로 간주된다. 이러한 설계는\n여기서 **W** _g_ ∈ R _[N]_ _[g]_ [×][din] 은\n_Gi_ 는 X 처리를 위해 선택된 전문가 그룹을 나타냅니다.\n\n**X를** 처리하도록 선택된 그룹을 나타냅니다.\n\n\n**R** _k_ 및 **R** _l_ 은 그룹 _Gi_ 내에서 작동하는 전문가 라우팅 선\n택 모듈을 _나타냅니다_ . 이 모듈들은 입력 X와 A _i_ 행렬의 출력\n을 종합적으로 고려하여 전문가 선택을 수행합니다. 구체\n적으로, **R** _(k)는_ **X를** 기반으로 전문가 선택 점수 **S** _k_ 를 생성\n합니다. 공식은 다음과 같습니다.\n\n\n**S** _k_ 의 계산은 다음과 같다: t-번째 라운드에서, 여기서 _t_ ∈ {1 _,_ 2 _, . . ., Nt_ }, 엔티티\n**S** _k_ = **R** _k_ ( **X** ) =\n\n\n### Σ\n\nm∈{ _h,r,t_ }\nSoftmax( **W** _k_ **x** _m_ ) _,_ (6)\n_e_ [(] _[t]_ [)] = argmax _P_ ( _ei_ | _h, r, M_ [(] _[t]_ [)] ) _,_ (10)\n예측 연산은 다음과 같이 표현될 수 있다:\n\n\n\n_c_\n여기서 **S** _k_ ∈ R _[N]_ _[b]_, 그리고 **W** _k_ ∈ R _[N]_ _[b]_ [×][din] 은 **X를** 입력으로\n받는 라우팅 행렬이다. **R** _l_ 은 행렬 A(i)의 출력 A(i)X를 기\n반으로 전문가 선택 점수 S(l) _을_ 생성한다. S _[(]_ _l)의계산은다음과같이_\n표현될 수 있다:\n\n전문가 선택 점수 **S** _(l) 를_ 생성하며, _이는_ 행렬 _Ai_ 의 출력 _A(i)_ **X에**\n기반합니다. **S** _l_ 의 계산은 다음과 같이 표현될 수 있습니다:\n### Σ\n\n\n\n_c_\n다음 반복을 위해 _M_ [(] _[t]_ [)] 를 업데이트하여 _M_ [(] _[t]_ [+1)] _를_ 얻습니\n다:\n\n\n_M_ [(] _[t]_ [+1)] = _M_ [(] _[t]_ [)] \\ { _e_ [(] _[t]_ [)] } _,_ (11)\n_e_ _i_ ∈ _M_ [(] _[t]_ [)]\n_c_\n_M_ [(] _[t]_ [()) ] [는] 라운드 _c_ _t에서의_ 후보 엔티티 집합을 나타낸다.\n\n_e_ [(] _[t]_ [)][는] _M_ [(] _[t]_ [)] 로부터 LLM이 예측한 엔티티이다. 그런 다음\n**S** _l_ = **R** _l_ ( **X** ) =\n_c_ _c_\nm∈{ _h,r,t_ }\nSoftmax( **W** _l_ ( _Ai_ **x** _m_ ) _),_\n(7)\n여기서 **S** _l_ ∈ R _[N]_ b이며, **W** _l_ ∈ R _[N]_ _[b]_ [×][r] 은 입력으로 _Ai_ **X를** 받\n는 라우팅 행렬이다. r은\nLoRA(Hu et al., 2022)에서 순위의 크기를 나타냅니다. 그\n런 다음, **X를** 처리하기 위해 **S** _k_ 와 **S** _l_ 의 점수에 기반하여 그\n룹 _Gi_ 에서 행렬 _Bi,j_ 를 선택합니다:\n\n\n\n_M_ [((] _[t]_ [)] \\ { _e_ [(] _[t]_ [)] } 는 _M_ [((] _[t]_ [)] 에서 엔티티 _e_ [(] _[t]_ [)] 를 제거함을 나타\n_c_\n낸다. 마지막으로, _L_ [(] _[t]_ [)] 내 엔티티 _e_ [(] _[t]_ [)] 의 순위를 업데이트\n_c_\n합니다:\n\n\n\n_c_\n_L_ [(] _[t]_ [+1)] = **Insert** ( _L_ [(] _[t]_ [)] \\ { _e_ [(] _[t]_ [)] } _, t, e_ [(] _[t]_ [)] ) _,_ (12)\n_Bi,j_ = argmax\nj∈{1 _,_ 2 _,...,N_ _b_ }\nwhere **Insert** ( _L_ [(] _[t]_ [)] \\ { _e_ [(] _[t]_ [)] } _, t, e_ [(] _[t]_ [)] ) 는 먼저 _L_ [(] _[t]_ ) 에서 _e_ [(] _[(t]_ [) ())]\n( **S** _k_ + **S** _l_ ) _._ (8)\n를 제거한 후, _e_ [(] _[t]_ ) 를\n_L_ 한 [(] _[t]_ [)] 후의 t번째 위치에 삽입하는 것을 의미합니다. _Nt_ 번 반복\n마지막으로, 지식 수준 전문가 라우팅 메커니즘에 기반하\n여 전문가 _Ei,j_ = ( _Ai, Bi,j_ )가 **X를** 처리한다고 결정합니다.\n\n전문가 _Ei,j_ 를 결정한 후, KL-GMoE의 출력은 다음과 같\n이 표현됩니다:\n\n\n**y** _e_ = _Ei,j_ ( **X** ) = _Bi,j_ ( _Ai_ **X** ) _._ (9)\n그런 다음, 방정식 3에 표시된 바와 같이 전문가 출력 **y** _e_ 가 원\n래 FFN 출력에 추가됩니다.\n\n\n**3.3** **반복적** **엔티티** **재정렬**\n우리는 LLM의 교차 언어 공유 지식 활용을 향상시키기 위한\n반복적 엔티티 재정렬(IER)이라는 방법을 제안한다. 그림\n2(b)에서 볼 수 있듯이, IER 방법은 여러 번의 반복을 통해 공\n유 지식을 완전히 활용하여 올바른 엔티티 순위의 정확도를\n크게 향상시킨다. IER는 LLM의 훈련 과제와 디코딩 전략을\n조정합니다. 훈련 단계에서 후보 엔티티 수 _m을_ 변수 값으로\n무작위 설정하여 LLM이 다중 엔티티 순위를 반복적으로 조\n정할 수 있도록 훈련합니다. 디코딩 단계에서는 IER가 LLM\n이 다중 엔티티 예측을 여러 차례 수행하여 순위를 조정할 수\n있도록 합니다.\n\n쿼리 _q_ = ( _h, r,_ ?)에 대해, 초기 후보 엔티티 집합은 KGE\n모델에 의해 생성되며 _M_ [(1)] = [ _e, e, ..., e_ ]로 표기된다. 엔티\n라운드를 거친 후, 최종 순위 목록 _L_ [(] _[N]_ _[t]_ [(+1)) ][을] 얻습니다. IER\n구현은 부록 A.2에 상세히 설명되어 있습니다.\n\n\n**4** **실험**\n**4.1** **구현** **세부** **사항**\n실험에서 후보 엔티티를 추출하기 위해 TransE(Bordes et al.,\n2013)를 선택했습니다. 또한 파인 튜닝을 위한 베이스 모델\n로 Llama-2-7b-chat-hf [1] 을 활용했습니다. 모델 훈련 하이퍼파\n라미터는 다음과 같이 설정되었습니다: 학습률은 2e-5,\nLoRA 랭크는 4, 후보 엔티티 길이 _M(c)는_ 25~30 사이입니다.\n\nIER의 반복 횟수 _Nt_ 는 10입니다.\n\n\n**4.2** **다국어** **지식** **그래프** **완성**\n구축한 데이터셋에서 제안된 프레임워크의 성능을 임베딩 기\n반 및 생성 기반 방법과 비교했습니다. 실험 결과는 제안된\n방법이 모든 언어에 걸쳐 평균 메트릭에서 최적의 성능을 달\n성함을 보여줍니다. 구체적으로, 표 2에서 볼 수 있듯이, 제안\n된 프레임워크의 성능은 EN, FR, IT 세 언어에서 앞서 언급\n된 모든 방법을 능가합니다. JA와 ZH의 경우, Hits@10을 제\n외한 모든 지표에서 우수한 성능을 보였습니다. 단, Hits@10\n지표에서는 실패했습니다.\n\n\n\n티 목록은\n_c_ 1 2 _m_\n정렬 대상 객체는 _L_ [(1)] = _M_ [((1))][로] 초기화된다.\n\n\n\n_c_\nLLM은 _Nt_ 라운드의 엔티티 예측을 수행합니다.\n\n\n\n[1 https://huggingface.co/meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n모델 EN FR IT JA ZH AVG\nTransE 8.52  9.07  9.36  8.00 11.77 9.34\n유추 13.40 15.81 14.58 15.11 6.43 13.07\nComplEx 10.92 11.75 11.49 13.95 17.79 13.18\nDistmult 6.89 7.73 7.93 8.16 5.66 7.27\nH\n@ 회전 24.08 24.61 25.57 29.49 31.36 27.02\n1 HAKE 31.64 32.92 30.99 35.53 52.24 36.66\nICL 1.79 1.07 1.26 1.93 2.27 1.66\nGC-PLM 33.37 32.51 30.38 36.65 49.13 36.41\nDIFT ( _싱글_ ) 36.05 35.75 34.22 38.31 56.65 40.19\n우리 **36.50** **36.72** **35.93** **41.60** **58.63** **41.88**\nTransE 37.02 39.17 37.57 44.59 60.51 43.78\n유추 28.39 30.45 29.56 35.84 19.61 28.77\nComplEx 23.05 23.92 23.05 29.68 41.12 28.17\nDistmult 14.59 14.82 15.79 19.16 17.94 16.46\nH\n@ 회전 40.73 42.13 41.68 49.78 62.36 47.34\n3 HAKE 43.30 43.27 41.88 47.52 63.22 47.84\nICL 34.99 37.09 35.33 42.84 59.15 41.88\nGC-PLM 40.99 42.21 40.47 50.95 63.67 47.66\nDIFT ( _단일_ ) 42.21 42.28 40.50 47.83 64.50 47.46\n우리 **46.25** **45.30** **44.22** **51.97** **66.93** **50.93**\nTransE 50.25 51.23 49.60 58.10 71.80 56.19\n유추 39.17 41.73 40.30 48.78 64.45 46.89\nComplEx 34.84 37.51 35.70 45.02 59.97 42.61\nDistmult 26.74 26.78 27.41 36.04 50.25 33.44\nH\n@ 회전 52.66 53.17 51.50 **61.68** **74.58** 58.72\n10 HAKE 53.37 52.06 50.49 57.85 70.04 56.76\nICL 49.99 51.05 49.30 57.99 71.71 56.01\nGC-PLM 52.76 52.81 51.76 59.53 71.98 57.77\nDIFT ( _단일_ ) 52.48 52.35 50.30 58.74 72.08 57.19\n우리 **54.71** **53.45** **52.31** 60.85 72.56 **58.78**\nTransE 24.85 25.97 25.46 28.38 37.51 28.43\n유추 22.85 25.01 23.82 27.44 21.04 24.03\nComplEx 19.18 20.29 19.56 24.30 32.12 23.09\nDistmult 13.33 13.80 14.28 16.86 16.69 14.99\nMR 회전 34.63 35.39 35.49 41.75 48.74 39.20\nR HAKE 39.41 39.80 38.10 43.36 59.04 43.94\nICL 20.11 20.70 19.80 23.98 31.06 23.13\nGC-PLM 36.66 37.21 37.18 42.21 55.39 41.73\nDIFT ( _단일_ ) 40.99 40.64 39.09 44.54 61.38 45.33\n우리 **42.96** **42.58** **41.69** **48.33** **63.74** **47.86**\n표 2: 본 표는 다섯 가지 언어에 걸친 MKGC 결과를 제시한다. 임베\n딩 기반 방법인 TransE(Bordes et al., 2013), Analogy(Liu et al.,\n2017), Com-plEx(Trouillon et al., 2016), DistMult(Yang et al.,\n2014), RotatE(Sun et al., 2019)는 모두 OpenKE 프레임워크(Han et\nal., 2018)를 사용하여 구현되었습니다. HAKE(Zhang et al., 2020)의\n결과는 오픈소스 코드를 사용하여 재현되었습니다. ICL은 LLaMA\n2-7b-chat 모델을 미세 조정 없이 사용한 평가를 의미합니다. GC\nPLM(Song et al., 2023)은 현재 MKGC 분야의 최첨단(SOTA) 기법\n을 대표합니다. DIFT(Liu et al., 2024)는 LLM 기반 단일 언어 KGC\n분야의 최첨단 기법입니다. _'단일_ 언어( _Single)_ '는 각 언어별로 독립\n적으로 모델을 훈련하는 방식을 의미합니다. 굵은 글씨로 표시된 숫\n자는 고려된 방법론과 언어 조합 중 최상의 결과를 나타냅니다.\n\n\nHits@10에서 RotatE의 성능을 뛰어넘었는데, 이는 주로 후\n보 엔티티 생성을 위해 상대적으로 성능이 낮은\n모델 EN FR IT JA ZH AVG\nH [ LoRAMoE 36.28 36.36 35.81 40.22 56.87 41.11]\n@ HydraLoRA 35.68 35.60 35.05 40.49 57.92 40.95\n1 Ours **36.50 36.72 35.93 41.60 58.63 41.88**\nH [ LoRAMoE 42.54 42.74 41.40 48.75 64.96 48.08]\n@ HydraLoRA 42.49 42.58 41.22 48.68 64.99 47.99\n3 우리 **42.87 43.08 41.70 48.94 65.06 48.33**\nH [ LoRAMoE 52.27 52.50 50.91 59.07 72.19 57.39]\n@ HydraLoRA 52.51 52.41 50.81 **59.10** 72.24 57.41\n10 Ours **52.63 52.62 51.17** 59.09 **72.32 57.57**\nM  LoRAMoE 41.15 41.13 40.30 45.90 61.52 46.00\nR HydraLoRA 40.80 40.62 39.81 46.06 62.18 45.89\nR Ours **41.42 41.44 40.51 46.80 62.67 46.57**\n표 3: 이 표는 KL-GMoE를 기존의 SOTA 미세 조정 방법인\nLoRAMoE (Dou et al., 2024) 및 HydraLoRA (Tian et al., 2024)와\n비교한 것입니다.\n\n\n모델 훈련 가능한 매개변수 활성화된 매개변수 Lora 순위\nTransE 106.1m 106.1m DIFT(LoRA) 159.9*5m 159.9*5 m 64\nLoRAMoE 19.2m 19.2m 4\nHydraLoRA 12.5m 12.5m 4\n우리 32.9m 9.4m 4\n표 4: 본 방법과 다른 방법들의 매개변수 수 비교를 보여줍니다.\n\n\n후보 엔티티 생성에 상대적으로 성능이 낮은 TransE 모델을\n사용했기 때문입니다. 후보 엔티티 검색 단계에서 TransE를\nRotatE로 대체하고 실험을 수행했습니다. 해당 결과와 분석\n은 부록 A.3에 제시되어 있습니다. 기존 SOTA MKGC 방법\n인 GC-PLM과 비교했을 때, 우리의 프레임워크는 Hits@1,\nHits@3, Hits@10 및 MRR 지표에서 각각 5.47%, 3.27%,\n1.01%, 6.13%의 개선을 통해 상당한 성능 우위를 달성했습니\n다. 또한, 당사의 프레임워크는 SOTA LLM 기반 단일 언어\nKGC 방법인 DIFT에 비해 성능이 크게 향상되었습니다. 전반\n적으로, 실험 결과는 당사가 제안한 프레임워크의 효과성과\n우수성을 명확하게 입증합니다.\n\n\n**4.3** **모델** **아키텍처** **비교** **및** **매개변수** **분석**\nKL-GMoE 아키텍처와 기존 최첨단(SOTA) 미세 조정 방법\n(LoRAMoE 및 Hy-draLoRA 포함) 간의 MKGC 성능을 비교\n했습니다. 이 두 SOTA 방법은 모두 단일 쿼리를 처리하기 위\n해 다중 채널을 활용하는데, 이는 지식 분절화 문제를 초래할\n수 있습니다. 표 3에서 볼 수 있듯이, KL-GMoE는\n모델 평균 토큰 수 TFLOPs\nLoRAMoE 353.89 2.37814\nHydraLoRA 353.89 2.37580\nDIFT(LoRA) 353.89 2.42721\nOurs 353.89 2.37472\n표 5: 본 방법과 다른 방법들의 계산 효율성을 비교한 표입니다. 제시\n된 데이터는 1,000개의 샘플로부터 계산된 평균값입니다.\n\n\n60\n55\n50\n45\n미확인 언어 분석\n60\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing-ko.pdf-8-0.png)\n\n0. 575\n\n\n0. 550\n\n\n0. 525\n\n\n0. 500\n\n\n0. 475\n0.450\n\n\n0. 425\n55\n50\n45\n40\n35\n30\n언어\n그림 4: 이 그림은 세 가지 다른 훈련 언어 설정에서 다섯 가지 언어\n에 대한 우리 방법의 Hits@1 성능을 보여줍니다.\n\n\n\n\n\n40\n35\n30\n25\n20\n\n\n\n\n\n\n\n|다국어 데이터 불균형 분석|Col2|\n|---|---|\n|||\n|||\n|||\n|||\n|||\n|en|en|\n||~~fr~~<br>it zh|\n|: 3: 3: 2: 1<br>15: 3: 3: 2: 1<br>10: 3: 3: 2: 1<br>7: 3: 3: 2: 1<br>|5: 3: 3: 2:<br>ja<br>avg|\n\n\n언어별 데이터 비율 차이 (en: fr: it: zh: ja)\n그림 3: 본 프레임워크의 Hits@1 점수 변화(훈련 데이터 설정별)\n다섯 가지 다른 언어 비율로 진행되었습니다.\n\n\n이러한 방법들은 평균 지표에 대해 수행되었습니다. 이 실험\n결과는 우리의 방법이 지식 분열을 효과적으로 해결함으로\n써 MKGC 작업에서 성능을 향상시킨다는 것을 보여줍니다.\n\n\n모델 매개변수 측면에서 KL-GMoE의 장점을 추가로 분\n석했습니다. 표 4에서 볼 수 있듯이, 임베딩 기반 방법인\nTransE와 비교했을 때 KL-GMoE는 훈련 가능한 매개변수가\n3.2배 적고 활성화된 매개변수가 11.3배 적습니다. LLM 기반\n방법들 중 KL-GMoE는 다른 모든 방법보다 활성화 매개변\n수가 현저히 적습니다. 특히 DIFT와 비교했을 때 KL-GMoE\n는 훈련 가능 매개변수가 약 24.3배 적고 활성화 매개변수는\n약 85.1배 적어 매개변수 수 측면에서 상당한 장점을 입증합\n니다. 동시에, 우리는 제안된 방법을 추론 중 FLOPs 측면에\n서 다른 방법들과 비교했습니다. 표 5에서 볼 수 있듯이, 우리\n의 방법은 현재 최첨단 LLM 기반 KGC 방법인 DIFT에 비해\nFLOPs를 약 0.053 TFLOPs 정도 줄여, 우수한 계산 효율성\n을 입증합니다.\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||1|1|||||||||\n||히트@|1|1|1|1|1|1|1|1|1|1|\n||히트@ <br>히트@<br>|3<br>10||||||||||\n||~~MRR~~|||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||~~1~~|~~1~~|\n\n\n라운드\n그림 5: 이 그림은 IER 방법의 반복 횟수가 성능에 미치는 영향을 보\n여줍니다.\n\n\n**4.4** **언어** **불균형** **분석**\n훈련 데이터에서 언어 분포가 불균형한 시나리오에서 프레임\n워크의 견고성을 평가하기 위해 실험을 수행했습니다. 구체\n적으로, 총 훈련 데이터 양은 일정하게 유지한 상태에서 다섯\n가지 언어에 걸쳐 불균형한 훈련 데이터 비율을 적용한 실험\n을 수행했습니다. 그림 3에서 볼 수 있듯이, 언어 비율의 변화\n에도 불구하고 각 언어별 Hits@1 점수(점선)와 다섯 언어 평\n균 점수(빨간색 실선)는 비교적 안정적으로 유지되었습니다.\n\n이는 본 프레임워크가 언어 분포 변동에 민감하지 않음을 보\n여줍니다. 이러한 분석을 바탕으로 본 프레임워크는 언어 간\n공유 지식을 효과적으로 활용하여 강력한 견고성을 입증합니\n다.\n\n\n**4.5** **미검증** **언어** **분석**\n제안된 프레임워크의 훈련 데이터에 포함되지 않은 언어에\n대한 일반화 능력을 평가하기 위해 분석 실험을 수행했습니\n다. 이 실험은 세 가지 서로 다른 훈련 구성으로 진행되었습\n니다: (1) 훈련된\n\n\n|언어|쿼리|공유|답변|DIFT 예측|우리의 예측|\n|---|---|---|---|---|---|\n|EN|(타월헤드,<br>작곡가, ?)|FR|Thomas<br>뉴먼|데이비드 키타이 |타월헤드 |**토머스 뉴먼 **| 제임스 뉴턴<br>하워드 |크리스 메시나 |롤프 켄트 |캐리 프레스턴|**토마스 뉴먼 **| 롤프 켄트 |제임스 뉴턴 하워드 |<br>마이클 다나 |시어도어 샤피로 |마크 이샴 |카터 버웰|\n|FR|(테라야마 슈지, 직업, ?)|IT|작사가|연출가 |기자 |**작사가 **| 영화 제작자 |음악가 |<br>작곡가 |싱어송라이터|**작사가 **| 영화 제작자 |기타리스트 |연출가 |작곡가 |기자 |아티스트|\n|IT|(픽사 스토리,<br>연주자, ?)|EN|존<br>래스터|조 랜프트 |에릭 라슨 |**존 라세터 **| 밀트 칼 |<br>글렌 키언 |제임스 알가 |돈 한|**존 라세터 **| 에릭 라슨 |밀트 칼 |글렌 킨 |크리스 벅<br>| 콜린 행크스 |제임스 알가|\n|JA|(파울로스, 동일과<br>되는事物, ?)|ZH|파울루스|파울루스| 파울로스| **파울**| 폴라| 크리스틴<br>| 콘스탄틴| 크리스티나|**파울**| 파울루스| 폴라| 파울로스| 크리스틴|<br>콘스탄틴| 크리스티나|\n|ZH|(라코시·마가시,<br>구어, 문어, ?)|JA|러시아어|헝가리| 라코시 마가시| **러시아어**| 독일어|<br>중국어| 스페인어| 라틴어|**러시아어**| 독일어| 일본어| 영어| 라틴어| 헝가리어| 폴란드어|\n\n\n\n그림 6: 본 그림은 지식 공유 사례에서 본 방법과 DIFT의 예측 결과를 비교한 것이다. **'공유'** 열은 해당 쿼리의 지식이 LLM 훈련 데이터에 존\n재하지만 다른 언어로 제시되었음을 나타낸다.\n\n\n\n(1) EN과 FR로 훈련된 모델; (2) EN, FR, ZH로 훈련된 모델;\n그리고\n(3) 다섯 가지 언어로 훈련됨. 그림 4에서 보듯이, 녹색 막대는\n영어(EN)와 프랑스어(FR) 데이터만으로 훈련된 대규모 언어\n모델(LLM)이 미학습 언어인 이탈리아어(IT), 일본어(JA), 중\n국어(ZH)에서 상당한 지식 그래프 연결(KGC) 성능을 보였음\n을 나타냅니다. 이는 지식 공유가 LLM 훈련 중 접한 언어 간\n뿐만 아니라 미학습 언어 간에도 상당한 교차 언어 일반화 능\n력을 효과적으로 발휘함을 명확히 보여줍니다. 또한 훈련 언\n어 수가 증가함에 따라 모든 언어에서 성능이 일관되게 향상\n되는 것을 관찰했습니다. 이 결과는 더 많은 언어로 된 훈련\n데이터가 LLM에 풍부한 지식 신호를 제공하여 다국어 지식\n공유를 촉진함을 시사합니다.\n\n\n**4.6** **IER 동향** **분석**\nIER 방법에서 반복 횟수가 MKGC 성능에 미치는 영향을 평\n가하기 위해 분석적 실험을 수행하였다. 그림 5는 반복 횟수\n증가에 따른 모든 지표의 변화를 보여준다. 결과에서 알 수 있\n듯이, Hits@3, Hits@10 및 MRR은 처음 세 번의 반복에서 크\n게 개선되었으며 열 번째 반복에서 최적값에 도달했습니다.\n\n이러한 추세는 반복 횟수가 증가함에 따라 IER이 LLM이 다\n국어 공유 지식을 보다 효과적으로 활용하도록 하여 MKGC\n의 성능을 크게 향상시킨다는 것을 시사합니다.\n\n\n**4.7** **제거** **실험**\n제안된 프레임워크 내 각 구성 요소의 효과성을 검증하기 위\n해 제거 실험을 수행했습니다. 각 구성 요소를 순차적으로 제\n거하여 그 기여도를 평가했습니다. 표 6에서 볼 수 있듯이,\nKL-GMoE 구성 요소를 제거하면 Hits@1이 41.88에서\n40.28로, Hits@3이 50.93에서 49.71로, Hits@10이 58.78에\n서\n모델 H@1  H@3 H@10 MRR\n본 연구 **41.88 50.93  58.78  47.86**\n_**kg**_ 제외 우리 모델 40.28 49.71  58.07  46.55\n_**kg+ier**_ 제외 40.28 47.66  57.29  45.42\n표 6: 본 표는 KL-GMoE( _**kg**_ ) 및 IER( _**ier**_ ) 구성요소에 대한 제거 실험\n결과를 보여줍니다. 모든 결과는 다섯 가지 언어 지표의 평균값입니\n다.\n\n\n58.07로, MRR은 47.86에서 46.55로 감소했습니다. 이는\nKL-GMoE 구성 요소가 MKGC 성능 향상에 매우 중요함을\n나타냅니다. 또한 KL-GMoE와 IER을 동시에 제거했을 때,\nKL-GMoE만 제거했을 때보다 Hits@3, Hits@10, MRR 값이\n더 크게 감소했습니다. 이는 IER 구성 요소 역시 MKGC 성\n능 향상에 긍정적인 기여를 함을 보여줍니다. 이러한 제거 실\n험 결과는 제안된 KL-GMoE 및 IER 구성 요소의 효과성을\n강력히 입증합니다.\n\n\n**4.8** **사례** **연구**\n우리는 다국어 지식 공유에서 프레임워크의 성능을 평가하기\n위한 사례 연구를 수행했습니다. 이 사례의 질의는 LLM이\n훈련 과정에서 학습한 지식이지만 다른 언어로 표현된 것입\n니다. 그림 6에서 볼 수 있듯이, 영어 질의 _(Towelhead,_\n_composer, ?)_ 에 대해 LLM은 이미 프랑스어 훈련 데이터에서\n이 지식을 학습했습니다. 우리 프레임워크는 이 프랑스어 지\n식을 성공적으로 활용하여 해당 엔티티를 _토마스뉴먼으로_\n정확히 예측했습니다. 반면, 최첨단 LLM 기반 방법들은 _데_\n_이비드키테이로_ 잘못 예측했습니다. 이는 우리 프레임워크\n가 교차언어 공유 지식을 효과적으로 활용하여 완성 정확도\n를 향상시킬 수 있음을 보여줍니다.\n\n\n**5** **관련** **연구**\n임베딩 기반 방법은 지식 그래프(KG) 내 엔티티와 관계를 저\n차원 벡터 공간으로 매핑합니다. 예를 들어, TransE(Bordes\net al., 2013)는 엔티티와 관계의 번역 원리에 기반합니다.\n\nRotatE(Sun et al., 2019)는 각 관계를 복소 벡터 공간에서의\n회전으로 처리합니다. DMoG(Song et al., 2022a)는 온톨로\n지와 텍스트 그래프를 융합하여 사실 그래프의 미관측 관계\n를 표현합니다. TransH(Wang et al., 2014)는 관계를 초평면\n으로 모델링합니다. HOLEX(Xue et al., 2018)는 높은 모델\n복잡도 방법과 HolE(Nickel et al., 2016) 사이를 보간합니다.\n\nTR-GCN(Song et al., 2022b)은 관찰되지 않은 관계를 표현\n하기 위한 온톨로지 기반 제로샷 관계 학습 방법을 제안합니\n다.\n\n생성 기반 방법은 지식 그래프 연결 예측(KGC) 작업을 텍\n스트 생성 작업으로 변환합니다. 예를 들어, KGT5(Sax-ena\n외, 2022)는 KG 링크 예측을 시퀀스-투-시퀀스 작업으로 제\n시합니다. GC-PLM(Song et al., 2023)은 전역 및 지역 지식\n제약 조건을 도입하여 MKGC의 성능을 향상시킵니다.\n\nGenKGC(Xie et al., 2022)는 관계 기반 증명과 엔티티 인식\n의 계층적 디코딩 전략을 도입합니다. KICGPT (Wei et al.,\n2023)는 LLM과 KGE 모델을 통합하고, 지식 기반 문맥 학습\n전략을 채택하여 여러 엔티티의 순위를 재조정합니다. DIFT\n(Liu et al., 2024)는 LoRA (Hu et al., 2022)를 사용하여 LLM\n을 미세 조정하여 KGE 모델에서 얻은 후보 엔티티 중에서\n가장 최적의 엔티티를 선택합니다.\n\n\n**6** **결론**\n본 논문에서는 새로운 MKGC 프레임워크를 제안한다. 이 프\n레임워크는 KL-GMoE와 IER라는 두 가지 구성 요소를 통합\n한다. KL-GMoE는 언어 간 공유 지식을 효율적으로 포착함\n으로써 완성 성능을 크게 향상시킨다. IER는 다중 반복적 접\n근법을 통해 교차 언어 공유 지식을 완전히 활용하여 완성 성\n능을 더욱 개선한다. 실험 결과는 본 프레임워크가 MKGC 작\n업에서 우수한 성능을 보임을 입증한다.\n\n\n**제한** **사항**\n본 프레임워크는 LLM의 토큰 길이에 제한을 받기 때문에\nKG 내 모든 엔티티를 기반으로 엔티티 선택을 수행할 수 없\n습니다. 또한 본 프레임워크는 텍스트 정보만을 처리합니다.\n\n이 한계로 인해 다른 모달리티의 정보를 통합할 수 없어\n다중 모달 KG 데이터셋에 적용하는 데 방해가 됩니다.\n\n\n**윤리** **성명**\n본 논문은 MKGC를 위한 방법을 제안하고 공개 데이터셋에\n서 확장된 다국어 데이터셋에 대한 실험을 수행합니다. 따라\n서 이 시나리오에서는 데이터 프라이버시 문제가 존재하지\n않습니다.\n\n\n**감사의** **말**\n본 연구는 중국 국가자연과학기금회(   U21B2027,\nU23A2038,  62166023,\n62376270), 윈난성 주요 과학기술 특별계획 프로젝트(\n202402AG050007,  202502AD080012,\n202502AD080016), 윈난성 기초연구 일반 프로젝트(지원 번\n호: 202301AS070047, 202201BE070001-021).\n\n\n**참고문헌**\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason\n[Weston, and Oksana Yakhnenko. 2013. Translating embeddings](https://dl.acm.org/doi/10.5555/2999792.2999923)\n[for modeling multi-relational data.](https://dl.acm.org/doi/10.5555/2999792.2999923) _Advances in neural_\n_information processing systems_, 26.\n\n\n천천, 왕위페이, 리빙, 람궈옌. 2022. 지식은 [평평하다:](https://aclanthology.org/2022.coling-1.352/) 다양한 지식\n그래프 완성을 [위한 Seq2Seq 생성](https://aclanthology.org/2022.coling-1.352/) 프레임워크. _제29회국제계산_\n_언어학회학술대회논문집_, 4005–4017쪽, 대한민국 경주. 국제계\n산언어학회.\n\n\n두 시한, 저우 엔위, 류 얀, 가오 쑹양, 쉔 웨이, 슝 리마오, 저우 유하\n오, 왕 샤오, 시 즈헝, 판 샤오란, 푸 시량, 주 장, 정 루이, 구이 타\n오, 장 치, 황 [쑤안징. 2024. LoRAMoE: 대규모](https://doi.org/10.18653/v1/2024.acl-long.106) 언어 모델에서 세\n계 지식 망각 완화를 [위한 MoE 스타일](https://doi.org/10.18653/v1/2024.acl-long.106) 플러그인. _제62회전산언_\n_어학회연차대회논문집(제1권: 장문논문)_, 1932–1945쪽, 태국\n방콕. 전산언어학회.\n\n\nXiou Ge, Yun Cheng Wang, Bin Wang, C-C Jay Kuo 외 1명. 2024.\n\n지식 그래프 임베딩: 개요. _APSIPA Transactions on Signal and_\n_Information Processing_, 13(1).\n\n\nXu Han, Shulin Cao, Lv Xin, Yankai Lin, Zhiyuan Liu, Maosong\nSun, Juanzi Li. 2018. Openke: 지식 [임베딩을](https://aclanthology.org/D18-2024/) 위한 오픈 툴킷.\n\n_EMNLP 회의록_ .\n\n\n한제위, 가오차오, 류진양, 장제프, 장사이첸. 2024. 대규모 모델을\n위한 매개변수 효율적 [미세](https://arxiv.org/abs/2403.14608) 조정: 포괄적 서베이. _arXiv 사전인_\n_쇄본 arXiv:2403.14608_ .\n\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2022.\n\nLoRA: 대규모 언어 모델의 저순위 적응. _국제표현학습컨퍼런_\n_스_ .\n\n\n_펑_ 후, 시즈 리우, 창장 가오, 신 황, 슈에 한, 준란 펑, 차오 덩, 슈지안\n황. 2025. 대규모 언어 모델은 언어 간 지식 없는 추론자이다.\n\n_2025년전미계산언어학회아메리카국가별지부학술대회: 인간_\n_언어기술 (제1권: 장문논문집)_, 1525–1542쪽, 뉴멕시코주 앨버\n커키. 전미계산언어학회.\n\n\n황카이위, 모펑란, 장신위, 리홍량, 리유, 장위안치, 이웨이젠, 마오위\n롱, 류진천, 쉬위좡 외 1인. 2024. 다국어 기능을 [갖춘](https://arxiv.org/abs/2405.10936) 대규모 언어\n모델에 관한 연구: [최근](https://arxiv.org/abs/2405.10936) 발전과 새로운 지평. _arXiv 사전인쇄본_\n_arXiv:2405.10936_ .\n\n\n지샤오슝, 판시루이, 에릭 캄브리아, 페카 마르티넨, 필립 S. 유.\n\n2022. 지식 그래프에 관한 [연구:](https://doi.org/10.1109/TNNLS.2021.3070843) 표현, 획득 및 응용. _IEEE_\n_Transactions on Neural Networks and Learning Systems_,\n33(2):494–514.\n\n\n옌스 레만, 로버트 이젤, 막스 야콥, 안야 옌츠슈, 디미트리스 콘토코\n스타스, 파블로 엔 멘데스, 세바스티안 헬만, 모하메드 모르시, 패\n트릭 반 클리프, 쇠렌 아우어 [외 1인. 2015. Dbpedia–위키피디아](https://www.researchgate.net/publication/259828897_DBpedia_-_A_Large-scale_Multilingual_Knowledge_Base_Extracted_from_Wikipedia)\n에서 추출한 [대규모](https://www.researchgate.net/publication/259828897_DBpedia_-_A_Large-scale_Multilingual_Knowledge_Base_Extracted_from_Wikipedia) 다국어 지식베이스. _세미틱웹_, 6(2):167–\n195.\n\n\n한샤오 리우, 위에신 우, 이밍 양. 2017. 다중 관계 [임베딩을](https://dl.acm.org/doi/10.5555/3305890.3305905) 위한 아날\n[로그적](https://dl.acm.org/doi/10.5555/3305890.3305905) 추론. _제34회국제기계학습컨퍼런스논문집 - 제70권_,\nICML’17, 2168–2178쪽. JMLR.org.\n\n\n양 리우, 샤오빈 티안, 제쿤 쑨, 웨이 후. 2024. 지식 [그래프](https://arxiv.org/abs/2407.16127) 완성을 위\n한 판별 지침을 활용한 생성형 대규모 언어 모델의 미세 조정. _국_\n_제시맨틱웹컨퍼런스_, 199–217쪽. Springer.\n\n\n막시밀리안 니켈, 로렌초 로사스코, 토마소 포지오. 2016. 지식 그래\n프의 [홀로그래픽](https://dl.acm.org/doi/10.5555/3016100.3016172) 임베딩. _인공지능에관한 AAAI 학회논문집_, 제\n30권.\n\n\n[Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. 2022. 시퀀](https://doi.org/10.18653/v1/2022.acl-long.201)\n스-투-시퀀스 지식 [그래프](https://doi.org/10.18653/v1/2022.acl-long.201) 완성 및 질문 답변. _제60회전산언어학_\n_회연차대회논문집 (제1권: 장문논문)_, 2814–2828쪽, 아일랜드\n더블린. 전산언어학회.\n\n\n\n[송란, 가오성샹, 가오샤오페이, 마오쿤리, 위정타오. 2025. Mke-pllm:](https://doi.org/10.1016/j.neucom.2025.130979)\n사전 훈련된 대규모 언어 모델을 위한 다국어 지식 편집 벤치마크\n. _신경계산_, 651:130979.\n\n\nRan Song, Shizhu He, Shengxiang Gao, Li Cai, Kang Liu, Zhengtao\nYu, Jun Zhao. 2023. 지식 제약 조건을 [적용한](https://doi.org/10.18653/v1/2023.findings-acl.488) 사전 훈련된 언어\n모델로부터의 [다국어](https://doi.org/10.18653/v1/2023.findings-acl.488) 지식 그래프 완성. _컴퓨터언어학협회연구_\n_결과: ACL 2023_, 7709–7721쪽, 캐나다 토론토. 컴퓨터 언어학 협\n회.\n\n\n[송란, 허시주, 정순콩, 가오성샹, 류강, 위정타오, 조준. 2022a. 그래프](https://aclanthology.org/2022.coling-1.196/)\n혼합 모델의 분리: 온톨로지 및 [텍스트](https://aclanthology.org/2022.coling-1.196/) 전문가 융합을 통한 지식\n그래프 완성을 [위한](https://aclanthology.org/2022.coling-1.196/) 미관측 관계 학습. _제29회국제계산언어학회_\n_(ICL) 논문집_, 2237–2246쪽, 대한민국 경주. 국제계산언어학회\n(ICL).\n\n\n송란, 허시주, 정순콩, 가오성샹, 류강, 자오준, 위정타오. 2022b. [지](https://openreview.net/pdf?id=H87NVDpH-fq)\n식 그래프 제로샷 관계 학습을 [위한](https://openreview.net/pdf?id=H87NVDpH-fq) 온톨로지 기반 및 텍스트 강\n화 [표현.](https://openreview.net/pdf?id=H87NVDpH-fq) _ICLR 2022 자연어처리용그래프기반딥러닝워크숍_ .\n\n\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019.\n\nRotate: 복잡한 공간에서의 관계 회전을 통한 지식 그래프 임베딩.\n\n_국제표현학습컨퍼런스_ .\n\n\n[천춘린, 시잔, 구오즈장, 리리, 쉬청종. 2024. Hydralora: 효율적인](https://proceedings.neurips.cc/paper_files/paper/2024/hash/123fd8a56501194823c8e0dca00733df-Abstract-Conference.html) 미\n세 조정을 위한 [비대칭 LORA 아키텍처.](https://proceedings.neurips.cc/paper_files/paper/2024/hash/123fd8a56501194823c8e0dca00733df-Abstract-Conference.html) _신경정보처리시스템_\n_발전 (NeurIPS)_ .\n\n\n테오 트루이옹, 요하네스 벨블, 세바스티안 리델, 에릭 고시에, 기욤\n부샤르. 2016. 단순한 링크 예측을 위한 복잡한 임베딩. _제33회국_\n_제기계학습학회(ICML) 논문집 - 제48권_, ICML’16, 2071–2080\n쪽. JMLR.org.\n\n\nDenny Vrandecˇic´ and Markus Krötzsch. 2014. [위키데이터:](https://dl.acm.org/doi/10.1145/2629489) 자유\n협업 [지식](https://dl.acm.org/doi/10.1145/2629489) 기반. _ACM 커뮤니케이션즈_, 57(10):78–85.\n\n\n왕샤오즈, 가오톈위, 주차오청, 장정얀, 류즈위안, 리쥔즈, 탕젠.\n\n2021. KEPLER: 지식 임베딩과 [사전](https://doi.org/10.1162/tacl_a_00360) 훈련된 언어 표현을 위한 통\n합 [모델.](https://doi.org/10.1162/tacl_a_00360) _계산언어학회지_, 9:176–194.\n\n\n왕전, 장젠원, 펑젠린, 천정. 2014. 초평면 상의 [변환을](https://dl.acm.org/doi/10.5555/2893873.2894046) 통한 지식 그\n래프 [임베딩.](https://dl.acm.org/doi/10.5555/2893873.2894046) _제28회 AAAI_ _인공지능학회논문집_, AAAI’14,\n1112–1119쪽. AAAI Press.\n\n\nYanbin Wei, Qiushi Huang, Yu Zhang, and James Kwok. 2023.\n\nKICGPT: 지식 그래프 완성을 [위한](https://doi.org/10.18653/v1/2023.findings-emnlp.580) 문맥 내 지식을 갖춘 대규모\n언어 모델. _계산언어학회논문집: EMNLP 2023_, 8667–8683쪽, 싱\n가포르. 계산언어학회.\n\n\nGerhard Weikum. 2021. 지식 [그래프 2021:](https://dl.acm.org/doi/10.14778/3476311.3476393) 데이터의 오디세이.\n_Proceedings of the VLDB Endowment_, 14(12):3233–3238.\n\n\nXin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui Chen, Feiyu\n[Xiong, Mosha Chen, and Huajun Chen. 2022. 차별화에서](https://doi.org/10.1145/3487553.3524238) 생성으\n로: 생성형 트랜스포머를 활용한 지식 그래프 완성. _웹컨퍼런스_\n_2022 동반논문집_, WWW '22, 162–165쪽, 뉴욕, NY.\n\n미국. 컴퓨터 학회.\n\n\n[Yexiang Xue, Yang Yuan, Zhitian Xu, Ashish Sabharwal. 2018. 지](https://dl.acm.org/doi/10.5555/3327345.3327361)\n식 완성을 위한 [홀로그래픽](https://dl.acm.org/doi/10.5555/3327345.3327361) 임베딩 확장. _신경정보처리시스템발_\n_전_, 31.\n\n\n양비산, 이원타우, 허샤오동, 가오젠펑, 덩리. 2014. 지식 기반 학습 및\n추론을 위한 엔티티 및 관계 임베딩. _국제표현학습컨퍼런스_ .\n\n\n장잔치우, 차이젠위, 장용동, 왕지에. 2020. 링크 [예측을](https://ojs.aaai.org/index.php/AAAI/article/view/5701) 위한 계층\n인식 지식 [그래프](https://ojs.aaai.org/index.php/AAAI/article/view/5701) 임베딩 학습. _인공지능에관한 AAAI 컨퍼런스_\n_논문집_, 제34권, 3065–3072쪽.\n\n\n_W_ enxuan Zhou, Fangyu Liu, Ivan Vulic´, Nigel Collier, and Muhao\nChen. 2022. Prix-LM: 다국어 지식 [기반](https://doi.org/10.18653/v1/2022.acl-long.371) 구축을 위한 사전 훈련.\n\n_제60회전산언어학회연차총회논문집(제1권: 장문논문)_, 5412–\n5424쪽, 아일랜드 더블린. 전산언어학회.\n\n\n**A** **부록**\n**A.1** **프롬프트** **예시**\n훈련 단계에서 다양한 길이의 후보 엔티티 목록에 대한 프롬프트 예시를 제시합니다.\n\n\n프롬프트 꼬리 엔티티 t가 누락된 삼중항을 주어짐: (Saint George and the Dragon, material used, t).\n\n\n다음은 실체 '성 조지와 용'에 대한 설명 정보입니다:\n\n성 조지와 용, 성 조지와 용 또는 용을 죽이는 성 조지는 베네치아 화가 틴토레토가 1555년 또는 1558\n년에 그린 그림이다. 이후 영국 수집가가 이를 구입했다.\n\n\n성 조지와 용을 묘사한 삼중주 작품들:\n\n[(성 조지와 용, 묘사, 언덕); (성 조지와 용, 묘사, 창); (성 조지와 용, 창작자, 야코포 틴토레토); (성 조\n지와 용, 묘사, 전투); (성 조지와 용, 묘사, 여성); (성 조지와 용, 묘사, 하늘)]\nt의 실체명은 무엇인가요? 아래 실체 목록에서 하나를 선택하세요: [유화; 성 조지와 용; 나무; 템페라;\n\n직물; 벽돌; 진주; 변성암; 편암; 사암; 페인트; 화성암; 조직; 보석; 황동; 구리; 직조 직물; 화산암; 대리\n석; 용; 현무암; 퇴적암; 세 여신; 석회암; 강철]\n[답변]:\n\n\n번호 25\n개 엔티티\n프롬프트 꼬리 엔티티 t가 누락된 삼중항 (Jason Lee, instance of, t)이 주어집니다. 다음은 엔티\n티 Jason Lee에 대한 설명 정보를 제공합니다:\n\n제이슨 리(Jason Lee, Jason Michael Lee, 1970년 4월 25일 출생)는 미국의 배우, 사진작가, 프로듀서,\n스케이트보더, 코미디언, 작가이다. 그는 TV 시리즈 <아메리칸 드림즈>에서 얼 히키 역으로 가장 잘\n알려져 있다.\n\n\n다음은 엔티티 제이슨 리를 포함하는 삼중항목입니다:\n\n[(몰래츠 출연진, 제이슨 리); (제이슨 리, 민족, 스코틀랜드계 미국인); (제이슨 리, 직업, 시나리오 작가\n); (제이슨 리, 직업, 배우); (제이슨 리, 직업, 영화 제작자); (제이슨 리, 직업, 사업가)]\nt의 엔티티 이름은 무엇입니까? 아래 엔티티 목록에서 하나를 선택하세요: [제이슨 리; 인간; 쌍둥이;\n\n제이슨 알렉산더; 소피아 베르가라; 케빈 스미스; 스크린 액터스 길드 어워드; 데이비드 크로스; 3D 영\n화; 대학; 프라임타임 에미 어워드; 스포츠 시즌; MTV 무비 어워드; 케일리 쿠오코; 스페인 지방 자치\n단체; 제이슨 뮤즈; 10년 단위; 군 계급; 교외 지역; 애니메이션 스튜디오; 제인 린치; 행크 아자리아; 위\n성 영화상; 브레킨 마이어; 내 이름은 얼; 패트릭 워버튼; 사업]\n[답변]:\n\n\n개체 수 27\n개 항목\n표 7: 다양한 길이의 후보 엔티티 목록에 대한 프롬프트 예시.\n\n\n**A.2** **반복적** **엔티티** **재정렬** **알고리즘의** **세부** **사항**\n**알고리즘 1** 반복적 엔티티 재정렬(IER)\n1: **입력:** 쿼리 _q_ = ( _h, r,_ ?), _M_ [(1)] = [ _e, e, ..., e_ ]: KGE 모델이 생성한 _상위 m개_ 엔티티,\n_c_ 1 2 _m_\n_N_, _L_ [(1)] = _M_ [(1)]\n_t_ _c_\n2: _t_ = 1 **부터** _Nt_ 까지 **반복**\n3: _e_ [(] _[t]_ [)] = argmax _P_ ( _ei_\n\n\n\n| _h, r, M_ [(] _[t]_ [)] );\n\n_c_\n_e_ _i_ ∈ _M_ [ (] _[t]_ [)]\n4: _M_ [((] _[t]_ [+1)] ) = _i_ _M_ _c_ [((] _[t]_ [))] \\ { _e_ [((] _[t]_ [))] };\n\n\n\n_c_\n_c_ _c_\n5: _L_ [(] _[t]_ [+1)] = **Insert** ( _L_ [(] _[t]_ [)] \\ { _e_ [(] _[t]_ [)] } _, t, e_ [(] _[t]_ [)] );\n\n6: **for 종료**\n7: **출력:** _L_ [(] _[N]_ _[t]_ [+1)]\n**A.3** **지식** **그래프** **임베딩** **모델** **분석**\n표 8의 실험 결과는 RotatE를 사용하여 후보 엔티티를 검색할 때, 제안된 방법이 기존 RotatE 모델 대비 Hits@1에서 14.78%\n증가하는 등 상당한 성능 향상을 달성했음을 명확히 보여줍니다. 특히 Ours+RotatE는 여러 언어별 지표에서 Ours+TransE보\n다 약간 낮은 성능을 보입니다. 이 현상은 각 KGE 모델이 생성한 후보 집합 내에서 올바른 엔티티의 상위 1위 순위 비율이 다\n르기 때문으로 볼 수 있습니다. 구체적으로, TransE를 사용할 때 1순위로 랭킹된 정답 엔티티의 비율은 14.38%였으나, RotatE\n를 사용하면 이 비율이 30.51%로 크게 증가했습니다. 따라서 이 특징은 어느 정도 영향을 미쳤습니다: 파인 튜닝 단계에서\nLLM은 후보 집합에서 1순위로 랭킹된 엔티티를 최종 답안으로 선택하는 경향이 더 강해집니다. 우리는 이러한 \"탑-1 편향\"이\n어느 정도 모델의 다른 잠재적 정답 탐색을 억제하여 일부 언어에서 Ours+RotatE가 Ours+TransE보다 약간 낮은 성능을 보일\n수 있다고 가정합니다. 향후 연구에서는 기존 KGE 모델에 의존하지 않는 보다 안정적인 파인튜닝 지시어 세트 구축 방안을\n추가로 탐구할 계획입니다.\n\n\n모델 EN FR IT JA ZH AVG\nH 회전 24.08 24.61 25.57 29.49 31.36 27.02\n@ 우리+TransE 36.50 **36.72** **35.93** 41.60 58.63 **41.88**\n1\n우리+회전 **36.55** 35.70 35.46 **41.78** **59.49** 41.80\nH 회전 40.73 42.13 41.68 49.78 62.36 47.34\n@ 우리+TransE 46.25 **45.30** **44.22** 51.97 66.93 50.93\n3\n우리+회전 **46.38** 45.05 44.20 **52.27** **67.13** **51.01**\nH 회전 52.66 53.17 51.50 61.68 74.58 58.72\n@\n10 우리+전환 우리+\n회전\n54.71\n**54.76**\n53.45\n**54.13**\n52.31\n**52.53**\n60.85\n**62.31**\n72.56\n**74.59**\n58.78\n**59.66**\nM 회전 34.63 35.39 35.49 41.75 48.74 39.20\nR\nR 우리+전환 우리+\n회전\n42.96\n**43.04**\n**42.58**\n42.18\n**41.69**\n41.58\n48.33\n**49.01**\n63.74\n**64.46**\n47.86\n**48.05**\n표 8: 후보 엔티티 검색 과정에서 제안된 프레임워크의 성능에 대한 다양한 KGE 모델의 영향.\n\n\n**A.4** **전문가** **라우팅** **분석**\n지식 공유의 존재를 검증하기 위해 테스트 샘플에서 전문가 선택을 분석했습니다. 각 전문가 라우팅의 출력을 획득하고 언어\n적 및 지식적 측면 모두에서 시각화했습니다.\n\n\n차원. 그림 7의 왼쪽은 언어적 차원을 기반으로 한 전문가 라우팅 분석을 보여줍니다. 이는 서로 다른 언어의 샘플에 대해 LLM\n의 각 트랜스포머 레이어에서 전문가 선택 과정을 설명합니다. 다양한 언어 샘플에 걸친 전문가 선택은 대체로 일관성을 보이며,\n이는 MKGC 작업에서 본 방법이 언어 간 차별을 두지 않음을 시사합니다. 그림 우측은 각 관계별 전문가 선택을 보여줍니다.\n\n분석 결과, 서로 다른 언어에서 동일한 관계를 가진 샘플은 대체로 동일한 전문가들에 의해 처리되는 것으로 나타났습니다. 전\n반적으로 일부 관계는 일관된 전문가 선택을 보인 반면, 다른 관계들은 차이를 보였습니다. 이 분석을 바탕으로 본 방법이 서\n로 다른 언어 간 지식 공유를 효과적으로 활용할 수 있음을 검증합니다.\n\n\n\nen fr 언어 it ja zh\n\n\n\n1. 0\n\n\n0. 8\n\n\n0. 6\n\n\n0. 4\n\n\n0. 2\n\n\n0. 0\n\n\n\n1. 0\n\n\n0. 8\n\n\n0. 6\n\n\n0. 4\n\n\n0. 2\n\n\n0. 0\n32\n31\n30\n29\n28\n27\n26\n25\n24\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n지식 차원\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing-ko.pdf-15-0.png)\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing-ko.pdf-15-1.png)\n\n그림 7: 왼쪽은 다섯 가지 언어에 걸친 전문가 선택을 보여줍니다. 가로축은 모든 언어를 나타내며, 각 언어 내의 작은 막대는 전문가 한 명을\n나타냅니다. 세로축은 LLM의 트랜스포머 내 레이어 번호를 나타냅니다. 각 블록의 색상 농도는 특정 전문가를 선택한 샘플의 빈도를 나타냅\n니다. 오른쪽은 모든 관계에 걸친 전문가 선택을 보여줍니다. 가로축은 모든 관계를 나타냅니다. 세로축은 트랜스포머 내 레이어 번호를 나타\n내며, 각 레이어 내 행은 전문가 한 명을 나타냅니다.\n\n\n**A.5** **IER 추세** **분석**\n우리는 IER 방법에서 반복 횟수가 다섯 가지 언어 평가 지표에 미치는 영향을 분석했습니다. 실험 결과는 반복 횟수 증가에 따\n라 모든 언어에서 성능이 뚜렷이 향상됨을 보여줍니다. 이러한 결과는 IER 방법이 언어 간 공유 지식을 활용할 수 있는 능력을\n입증합니다.\n\n\n0. 550\n\n\n0. 525\n\n\n0. 500\n\n\n0. 475\n\n\n0. 450\n\n\n0. 425\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing-ko.pdf-16-0.png)\n\n\n\n\n\n\n\n라운드\n그림 8: 이 그림은 영어 테스트 세트에서 반복 횟수에 따른 IER 방법\n의 메트릭 성능 변화를 보여줍니다.\n\n\nIER(FR) 추세 분석\n\n\n\n0. 575\n\n\n0. 550\n\n\n0. 525\n\n\n0. 500\n\n\n0. 475\n\n\n0. 450\n\n\n0. 425\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing-ko.pdf-16-1.png)\n\n\n\n\n\n0. 525\n\n\n0. 500\n\n\n0. 475\n\n\n0. 450\n\n\n0. 425\n\n\n0. 400\n\n\n0. 375\n\n\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||히트@|1|||||||||\n||Hits@<br>Hits@<br>|3<br>10|||||||||\n||||||||||||\n||~~MRR~~||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||~~1~~|~~0~~|\n\n\n라운드\n그림 9: 이 그림은 프랑스어 테스트 세트에서 반복 횟수에 따른 IER\n방법의 메트릭 성능 변화를 보여줍니다.\n\n\n\n그림 11: 이 그림은 일본어 테스트 세트에서 반복 횟수에 따른 IER\n방법의 메트릭 성능 변화를 보여줍니다.\n\n\n\n0. 525\n\n\n0. 500\n\n\n0. 475\n\n\n0. 450\n\n\n0. 425\n\n\n0. 400\n\n\n0. 375\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing-ko.pdf-16-2.png)\n\n\n\n![](assets/md_images/지식그래프-논문-2_Multilingual-Knowledge-Graph-Completion-via-Efficient-Multilingual-Knowledge-Sharing-ko.pdf-16-3.png)\n\n라운드\n1 2 3 4 5 6 7 8 9 10\n라운드\n그림 10: 이 그림은 이탈리아어 테스트 세트에서 반복 횟수에 따른\nIER 방법의 메트릭 성능 변화를 보여줍니다.\n\n\n\n0. 72\n0.70\n\n\n0. 68\n\n\n0. 66\n\n\n0. 64\n\n\n0. 62\n\n\n0. 60\n\n\n0. 58\n그림 12: 이 그림은 중국어 테스트 세트에서 IER 방법의 메트릭 성능\n이 반복 횟수에 따라 어떻게 변화하는지 보여줍니다.\n\n\n",
    "eng_pdf": "pdf/eng/지식그래프 논문 2_Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing.pdf",
    "kor_pdf": "pdf/kor/지식그래프 논문 2_Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing ko.pdf"
  },
  {
    "id": "paper_5",
    "title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
    "eng_md": "## **Extract, Define, Canonicalize: An LLM-based Framework for Knowledge** **Graph Construction**\n\n**Bowen Zhang** **[1]** **and Harold Soh** **[1,2]**\n1Dept. of Computer Science, National University of Singapore, 2NUS Smart Systems Institute\n[{bowenzhang, harold}@comp.nus.edu.sg](mailto:{bowenzhang, harold}@comp.nus.edu.sg)\n**Abstract**\nIn this work, we are interested in automated\nmethods for knowledge graph creation (KGC)\nfrom input text. Progress on large language\nmodels (LLMs) has prompted a series of recent works applying them to KGC, e.g., via\nzero/few-shot prompting. Despite successes on\nsmall domain-specific datasets, these models\nface difficulties scaling up to text common in\nmany real-world applications. A principal issue is that, in prior methods, the KG schema\nhas to be included in the LLM prompt to generate valid triplets; larger and more complex\nschemas easily exceed the LLMs’ context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not\navailable and we would like the method to construct a high-quality KG with a succinct selfgenerated schema. To address these problems,\nwe propose a three-phase framework named\nExtract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC\nis flexible in that it can be applied to settings\nwhere a pre-defined target schema is available\nand when it is not; in the latter case, it constructs a schema automatically and applies selfcanonicalization. To further improve performance, we introduce a trained component that\nretrieves schema elements relevant to the input\ntext; this improves the LLMs’ extraction performance in a retrieval-augmented generationlike manner. We demonstrate on three KGC\nbenchmarks that EDC is able to extract highquality triplets without any parameter tuning\nand with significantly larger schemas compared\nto prior works. Code for EDC is available at\n[https://github.com/clear-nus/edc.](https://github.com/clear-nus/edc)\n**1** **Introduction**\nKnowledge graphs (KGs) (Ji et al., 2021) are a\nstructured representation of knowledge that organizes interconnected information through graph\nstructures, where entities and relations are represented as nodes and edges. They are broadly\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-0-0.png)\n\nFigure 1: A high-level illustration of Extract-DefineCanonicalize (EDC) for Knowledge Graph Construction.\n\n\nused in a variety of downstream tasks such as\ndecision-making (Guo et al., 2021; Lan et al.,\n2020), question-answering (Huang et al., 2019; Yasunaga et al., 2021), and recommendation (Guo\net al., 2020; Wang et al., 2019). However, knowledge graph construction (KGC) is inherently challenging: the task requires competence in understanding syntax and semantics to generate a consistent, concise, and meaningful knowledge graph.\nAs such, KGC predominantly relies on intensive\nhuman labor (Ye et al., 2022). KGC is a broad\nproblem and in this work, **we** **focus** **on** **the** **task**\n**of relational triplet extraction** as it is crucial for\n**EDC: Extract-Define-Canonicalize**\nKGC. Following previous works (Ye et al., 2022;\nMelnyk et al., 2022; Bi et al., 2024), we still refer\nto the task we are addressing as KGC.\nRecent attempts to automate KGC (Zhong et al.,\n2023; Ye et al., 2022) have employed large language models (LLMs) in view of their remarkable natural language understanding and generation\ncapabilities. LLM-based KGC methods employ\nvarious innovative prompt-based techniques, such\nas multi-turn conversation (Wei et al., 2023) and\ncode generation (Bi et al., 2024), to generate entityrelation triplets that represent the knowledge graph.\nHowever, these methods are currently limited to\nsmall and domain-specific scenarios — to ensure\nthe validity of generated triplets, schema information (e.g., possible entity and relation types) has to\nbe included in the prompt. Complex datasets (e.g.,\nWikipedia) typically require **large schemas that**\n**exceed the context window length** or can be ignored by the LLMs (Wadhwa et al., 2023). Furthermore, **pre-defined schemas are not always avail-**\n**able** - the users might not have pre-determined\nor fixed intentions about what information is of\ninterest in advance but still would like to extract\nintrinsically high-quality KGs. It is unclear how\nexisting methods will work in such situations.\nTo address these problems, we propose **Ex-**\n**tract** - **Define** - **Canonicalize** ( **EDC** ), a structured\napproach for KGC: the key idea is to decompose\nKGC into three primary phases corresponding to\nthree subtasks (Fig. 1):\n\n\n1. Open Information Extraction: extract a list\nof entity-relation triplets from the input text\nfreely.\n\n\n2. Schema Definition: generate a definition for\neach component of the schema, e.g. entity\ntype and relation type, induced by triplets obtained in the extraction phase.\n\n\n3. Schema Canonicalization: use the schema\ndefinitions to standardize the triplets such\nthat semantically-equivalent entities/relations\ntypes have the same noun/relation phrase.\n\n\nEach phase exploits the strengths of LLMs:\nthe Extract subtask leverages recent findings that\nLLMs are effective open information extractors (Li\net al., 2023; Han et al., 2023) — they can extract\nsemantically correct and meaningful triplets. However, the resulting triplets typically contain redundant and ambiguous information, e.g., multiple\nsemantically equivalent relation phrases such as\n‘profession’, ‘job’, and ‘occupation’ (Kamp et al.,\n2023; Putri et al., 2019; Vashishth et al., 2018).\nPhases 2 and 3 (Define and Canonicalize) standardize the triplets to make them useful for downstream tasks. We designed EDC to be flexible: it\ncan either discover triplets consistent with a preexisting schema of potentially large size ( **Target**\n**Alignment** ) or _self-generate_ a schema ( **Self Canon-**\n**icalization** ). To achieve this, we use LLMs to define the schema components by exploiting their\nexplanation generation capabilities — LLMs can\njustify their extractions via explanations that are\nagreeable to human experts (Li et al., 2023). The\ndefinitions are used to find the closest entity/relation type candidates (via a vector similarity search)\nthat the LLM can then reference to canonicalize\na component. In the case there is no equivalent\ncounterpart in the existing schema, we can choose\nto add it to enrich the schema.\nTo further improve performance, the three steps\nabove can be followed by an additional **Refine-**\n**ment** phase: we repeat EDC but provide the previously extracted triplets and a relevant part of the\nschema in the prompt during the initial extraction.\nWe propose a trained **Schema** **Retriever** that retrieves schema components relevant to the input\ntext, akin to retrieval-augmented generation (Lewis\net al., 2020), which we find improves the generated\ntriplets.\nExperiments on three KGC datasets in both Target Alignment and Self Canonicalization settings\nshow that EDC is able to extract higher-quality\nKGs compared to state-of-the-art methods through\nboth automatic and manual evaluation. Furthermore, the use of the Schema Retriever is shown\nto significantly and consistently improve EDC’s\nperformance.\nIn summary, the paper makes the following contributions:\n\n\n  - EDC, a flexible and performant LLM-based\nframework for knowledge graph construction that is able to extract high-quality KGs\nwith schema of large size or without any predefined schema.\n\n\n  - Schema Retriever, a trained model to extract\nschema components relevant to input text in\nthe same vein as information retrieval.\n\n\n  - Empirical evidence that demonstrate the effectiveness of EDC and the Schema Retriever.\n\n\n**2** **Background**\nIn this section, we provide relevant background on\nknowledge graph construction (KGC), open information extraction (OIE), and canonicalization.\n\n\n**Knowledge** **Graph** **Construction.** Traditional methods typically addressed KGC using\n“pipelines”, comprising subtasks like entity\ndiscovery (Žukov-Gregoriˇc et al., 2018; Martins\net al., 2019), entity typing (Choi et al., 2018; Onoe\nand Durrett, 2020), and relation classification\n(Zeng et al., 2014, 2015). Thanks to advances\nin pre-trained generative language models (e.g.,\nT5 (Raffel et al., 2020) and BERT(Lewis et al.,\n2019)), more recent works instead frame KGC\nas a sequence-to-sequence problem and generate\nrelational triplets in an end-to-end manner by\nfine-tuning these moderately-sized language\nmodels (Ye et al., 2022). The success of large\nlanguage models (LLMs) has pushed this paradigm\nfurther: current methods directly prompt the\nLLMs to generate triplets in a zero/few-shot\nmanner. For example, ChatIE (Wei et al., 2023)\nextracts triplets by framing the task as a multi-turn\nquestion-answering problem and CodeKGC (Bi\net al., 2024) approaches the task as a code\ngeneration problem. As previously mentioned,\nthese models face difficulties scaling up to general\ntext common in many real-world applications as\nthe KG schema has to be included in the LLM\nprompt. Our EDC framework circumvents this\nproblem by using post-hoc canonicalization (and\nwithout requiring fine-tuning of the base LLMs).\n\n\n**Open** **Information** **Extraction** **and** **Canonical-**\n**ization.** Standard (closed) information extraction\nrequires the output triplets to follow a pre-defined\nschema, e.g. a list of relation or entity types to\nbe extracted from. In contrast, open information\nextraction (OIE) does not have such a requirement.\nOIE has a long history and we refer readers who\nwant comprehensive coverage to the excellent surveys (Liu et al., 2022; Zhou et al., 2022; Kamp\net al., 2023). Recent studies have found LLMs\nto exhibit excellent performance on OIE tasks (Li\net al., 2023). However, the relational triplets extracted from OIE systems are not canonicalized;\nmultiple semantically equivalent relations can coexist without being unified to a canonical form,\ncausing redundancy and ambiguity in the induced\nopen knowledge graph. An extra canonicalization\nstep is required to standardize the triplets to make\nthe KGs useful for downstream applications.\nCanonicalization methods differ depending on\nwhether a target schema is available. In case a\ntarget schema is present, the task is sometimes referred to as “alignment” (Putri et al., 2019). For\nexample, Putri et al. (2019) uses WordNet (Miller,\n1995) as side information to obtain definitions for\nthe OIE-extracted relation phrases and a Siamese\nnetwork to compare an OIE relation definition and\na pre-defined relation in the target schema. In case\nno target schema is available, state-of-the-art methods are commonly based on clustering (Vashishth\net al., 2018; Dash et al., 2020). CESI (Vashishth\net al., 2018) creates embeddings for the OIE relations using side information from external sources\nlike PPDB (Ganitkevitch et al., 2013) and WordNet.\nHowever, clustering-based methods are prone to\nover-generalization (Kamp et al., 2023; Putri et al.,\n2019), e.g., CESI may put “is brother of”, “is son\nof”, “is main villain of”, and “was professor of”\ninto the same relation cluster.\nCompared to the existing canonicalization methods, EDC is more general; it works whether a\ntarget schema is provided or not. Instead of using\nstatic external sources like WordNet, EDC utilizes\ncontextual and semantically-rich side information\ngenerated by LLMs. Furthermore, by allowing\nthe LLMs to verify if a transformation can be performed (instead of solely relying on the embedding\nsimilarity), EDC alleviates the over-generalization\nissue faced by previous methods.\n\n\n**3** **Method:** **EDC for KGC**\nThis section outlines our primary contribution: an\napproach to constructing knowledge graphs that\nleverages LLMs in a structured manner. We first\ndetail the EDC framework followed by a description of refinement ( **EDC+R** ). Given input text, our\ngoal is to extract relational triplets in a canonical\nform such that the resulting KGs will have minimal\nambiguity and redundancy. When there is a predefined target schema, all generated triplets should\nconform to it. In the scenario where there is not\none, the system should dynamically create one and\ncanonicalize the triplets with respect to it.\n\n\n**3.1** **EDC: Extract-Define-Canonicalize**\nAt a high level, EDC decomposes KGC into three\nconnected subtasks. To ground our discussion, we\nwill use a specific input text example: “ _Alan Shep-_\n_ard_ _was_ _born_ _on_ _Nov_ _18,_ _1923_ _and_ _selected_ _by_\n_NASA in 1959._ _He was a member of the Apollo 14_\n_crew_ ” and walk through each of the phases:\n\n\n**Phase 1:** **Open Information Extraction:** we first\nleverage Large Language Models (LLMs) for open\ninformation extraction. Through few-shot prompting, LLMs identify and extract relational triplets\n([Subject, Relation, Object]) from input texts, independent of any specific schema. Using our example\nabove, the prompt is:\n\n\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-3-1.png)\n\n\n\nThe resultant triplets (in this case, [‘Alan Shepard’, ‘bornOn’, ‘Nov 18, 1923’], [‘Alan Shepard’, ‘participatedIn’, ‘Apollo 14’]) form an _open_\n_KG_, which is forwarded to subsequent phases.\n\n\n**Phase** **2:** **Schema** **Definition:** Next, we prompt\nthe LLMs to provide a natural language definition\nfor each component of the schema induced by the\nopen KG:\n\n\n\nThis example prompt results in the definitions\nfor (bornOn: The subject entity was born on\nthe date specified by the object entity.) and\n(participatedIn: The subject entity took part in\nthe event or mission specified by the object\nentity.), which are then passed to the next stage as\n_side information_ used for canonicalization.\n\n\n**Phase** **3:** **Schema** **Canonicalization:** The third\nphase aims to refine the open KG into a canonical form, eliminating redundancies and ambiguities. We start by vectorizing the definitions of each\nschema component using a sentence transformer to\ncreate embeddings. Canonicalization then proceeds\nin one of two ways, depending on the availability\nof a target schema:\n\n\n  - Target Alignment: With an existing target\nschema, we identify the most closely related\ncomponents within the target schema for each\nelement, considering them for canonicalization. To prevent issues of over-generalization,\nLLMs assess the feasibility of each potential\ntransformation. If a transformation is deemed\nunreasonable, indicating no semantic equivalent in the target schema, the component, and\nits related triplets are excluded.\n\n\n  - Self Canonicalization: Absent a target\nschema, the goal is to consolidate semantically similar schema components, standardizing them to a singular representation to streamline the KG. Starting with an empty canonical\nschema, we examine the open KG triplets,\nsearching for potential consolidation candidates through vector similarity and LLM verification. Unlike target alignment, components\ndeemed non-transformable are added to the\ncanonical schema, thereby expanding it.\n\n\nUsing our example, the prompt is:\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-3-2.png)\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-3-3.png)\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-4-0.png)\n\nNote that the choices above are obtained by using vector similarity search. After the LLM makes\nits choice, the relations are transformed to yield:\n\n[‘Alan Shepard’, ‘birthDate’, ‘Nov 18, 1923’],\n[‘Alan Shepard’, ‘mission’, ‘Apollo 14’], which\nforms our canonicalized KG.\n\n\n**3.2** **EDC+R: iteratively refine EDC with**\n**Schema Retriever**\nThe refinement process leverages the data generated by EDC to enhance the quality of the extracted\ntriplets. Inspired by retrieval-augmented generation and prior work (Bi et al., 2024), we construct\na “hint” for the extraction phase (details in Appendix A.4), which comprises two main elements:\n\n\n  - Candidate Entities: The entities extracted by\nEDC from the previous iteration, and entities\nextracted from the text using the LLM;\n\n\n  - Candidate Relations: The relations extracted\nby EDC from the previous cycle and relations\nretrieved from the pre-defined/canonicalized\nschema by using a trained Schema Retriever.\n\n\nThe inclusion of entities and relations from both\nthe LLM and the schema retriever provides a richer\npool of candidates for the LLM, which addresses\nissues where the absence of entities or relations impairs the LLM’s effectiveness. By merging the entities and relations extracted in earlier phases with\nnew findings from entity extraction and schema\nretrieval, the hint serves to aid the OIE by bootstrapping from the previous round.\nTo scale EDC to large schemas, we employ a\ntrained Schema Retriever which allows us to efficiently search schemas. The Schema Retriever\nworks in a similar fashion to information retrieval\nmethods based on vector spaces (Ganguly et al.,\n2015; Lewis et al., 2020); it projects the schema\ncomponents and the input text to a vector space\nsuch that cosine similarity captures the relevance\nbetween the two, i.e., how likely a schema component to be present in the input text. Note that in\nour setting, the similarity space is different from\nthe standard sentence embedding models where\ncosine similarity in the vector space captures semantic equivalence. Our Schema Retriever is a\nfine-tuned variant of the sentence embedding model\nE5-mistral-7b-instruct (Wang et al., 2023). We follow the original training methodology detailed in\nthe paper, which involves utilizing pairs of text\nand their corresponding defined relations. For details, please refer to the Appendix A.3. For a given\npositive text-relation pair ( _t_ [+] _, r_ [+] ), we employ an\ninstruction template on _t_ [+] to generate a new text\n_t_ [+] _inst_ [=][ “Instruct:] [retrieve relations that are present]\nin the given text _\\n_ Query: _{t_ [+] _}_ ”.\nWe then finetune the embedding model to distinguish between the correct relation associated with\na given text and other non-relevant relations using\nthe InfoNCE loss.\nBack to our example, refinement with the\nschema retriever adds the following relation to the\nprevious set: [‘Alan Shepard’, ‘selectedByNasa’,\n‘1959’]. The relation ‘selectedByNasa’ is rather\nobscure but was specified in the target schema.\n\n\n**4** **Experiments**\nIn this section, we describe experiments designed\nto evaluate the performance of EDC and EDC+R.\nBriefly, our results demonstrate that EDC significantly outperforms the state-of-the-art methods\nin both Target Alignment and Self Canonicalization settings. Refinement further improves EDC.\nSource code for EDC and to replicate our experiments are available in the supplementary materials,\nwith full tables in the Appendix C.\n\n\n**4.1** **Experimental Setup**\n**Datasets.** We evaluate EDC using three KGC\ndatasets:\n\n\n  - WebNLG (Ferreira et al., 2020): We use the\ntest split from the semantic parsing task of\nWebNLG+2020 (v3.0). It contains 1165 pairs\nof text and triplets. The schema derived\nfrom these reference triplets encompasses 159\nunique relation types.\n\n\n  - REBEL (Cabot and Navigli, 2021): The\noriginal test partition of REBEL comprises\n105,516 entries. To manage costs, we select a\nrandom sample of 1000 text-triplet pairs. This\nsubset induces a schema with 200 distinct relation types.\n\n\n  - Wiki-NRE (Distiawan et al., 2019): From\nWiki-NRE’s test split (29,619 entries), we\nsample 1000 text-triplet pairs, resulting in a\nschema with 45 unique relation types.\n\n\nThese datasets were chosen due to their richer\nvariety of relation types over alternatives like\nADE (Gurulingappa et al., 2012) (1 relation type),\nSciERC (Luan et al., 2018) (7 relation types), and\nCoNLL04 (Roth and Yih, 2004) (4 relation types)\nused to evaluate previous LLM-based methods (Bi\net al., 2024; Wadhwa et al., 2023). This diversity\nbetter mimics real-world complexity. In our experiments, we focus on extracting relations as the only\nschema component available across all datasets.\nRelations, being a foundational element of KGs,\nare prioritized over other components like entity\nor event types. However, note that EDC can be\nreadily extended to other schema components.\n\n\n**EDC Models.** EDC contains multiple modules\nthat are powered by LLMs. Since the OIE module is the key upstream module that determines the\nsemantic content captured in the KG, we tested\ndifferent LLMs of different sizes including GPT4 (Achiam et al., 2023), GPT-3.5-turbo (Brown\net al., 2020), and Mistral-7b (Jiang et al., 2023).\nMistral-7b was deployed on a local workstation,\nwhereas the GPT models were accessed via the\nOpenAI API. For the framework’s remaining components which required prompting, we used GPT3.5-turbo. In the canonicalization phase, the E5Mistral-7b model was utilized for vector similarity\nsearches without modifications.\n\n\n**4.1.1** **Evaluation Criteria and Baselines**\nWe evaluate our methods differently under Target\nAlignment (when a schema is provided) and Self\nCanonicalization (no schema) due to the _inherently_\n_different objectives_ : the former aims to recover the\nground-truth annotated triplets consistent with the\ntarget schema while the latter is to extract semantically correct and meaningful triplets that induce\na succinct and non-redundant KG without a predefined target to compare against. For the datasets\nabove, the preivous LLM-based KGC methods\n(ChatIE and CodeKGC) could not be used due to\nthe schema size. Although EDC is not intended for\nsmall domain-specific datasets, we include the results on SciERC and CoNLL04 in the Appendix E\nfor the comprehensiveness of the evaluation.\n\n\n**Target** **Alignment.** We compare EDC and\nEDC+R against the specialized trained models for\neach of the datasets:\n\n\n  - **REGEN** (Dognin et al., 2021) is the SOTA\nmodel for WebNLG. It is a sequence-tosequence model that leverages pre-trained\nT5 (Raffel et al., 2020) and Reinforcement\nLearning (RL) for bidirectional text-to-graph\nand graph-to-text generation.\n\n\n  - **GenIE** (Josifoski et al., 2022), a sequenceto-sequence model that leverages pre-trained\nBART (Lewis et al., 2019) and a constrained\ngeneration strategy to constrain the output\ntriplets to be consistent with the pre-defined\nschema. GenIE is the state-of-the-art model\nfor REBEL and Wiki-NRE.\n\n\nFollowing previous work (Dognin et al., 2021; Melnyk et al., 2022), we use the WEBNLG evaluation script (Ferreira et al., 2020) which computes\nthe Precision, Recall, and F1 scores for the output\ntriplets against the ground truth in a token-based\nmanner. Metrics based on Named Entity Evaluation were used to measure the Precision, Recall,\nand F1 score in three different ways.\n\n\n  - _Exact:_ Requires a complete match between\nthe candidate and reference triple, disregarding the type (subject, relation, object).\n\n\n  - _Partial:_ Allows for at least a partial match\nbetween the candidate and reference triple,\ndisregarding the type.\n\n\n  - _Strict:_ Demands an exact match between the\ncandidate and reference triplet, including the\nelement types.\n\n\n**Self** **Canonicalization.** For evaluating selfcanonicalization performance, comparisons are\nmade with:\n\n\n  - **Baseline Open KG**, which is the initial open\nKG output from the OIE (Open Information\nExtraction) phase. This serves as a reference\npoint to illustrate the changes in precision and\nschema conciseness resulting from the canonicalization process.\n\n\n  - **CESI** (Vashishth et al., 2018), recognized as\na leading clustering-based approach for open\nKG canonicalization. By applying CESI to the\nopen KG, we aim to contrast its performance\nagainst canonicalization by EDC.\n\n\n0.8\n0.7\n0.6\n0.5\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-6-2.png)\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-6-0.png)\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-6-1.png)\n\n\n\n0.6\n0.5\n0.4\n0.3\n0.7\n0.6\n0.5\n0.4\n0.4\n0.2\n0.3\ntirely out-of-schema relations, a token-based evaluation becomes unsuitable. Thus, we resort to manual evaluation, focusing on three key aspects that\nreflect the intrinsic quality of an extracted KG:\n\n\n  - _Precision:_ The canonicalized triplets remain\ncorrect and meaningful with respect to the text\ncompared to the OIE triplets.\n\n\n  - _Conciseness:_ The schema’s brevity is measured by the number of relations types.\n\n\n  - _Redundancy:_ We employ a redundancy score\n\n  - the average cosine similarity among each\ncanonicalized relation and its nearest counterpart — where low scores indicate that the\nschema’s relations are semantically distinct.\n\n\n**4.2** **Results and Analysis**\nIn the following, we focus on conveying our main\nfindings and results. For full results and tables,\nplease refer to the Appendix.\n\n\n**4.2.1** **Target Alignment**\nThe bar charts in Figure 2 summarize the Partial F1\nscores obtained by EDC and EDC+R on all three\ndatasets with different LLMs for OIE compared\nagainst the respective baselines. **EDC** **demon-**\n**strates** **performance** **that** **is** **superior** **to** **or** **on**\n**par** **with** **the** **state-of-the-art** **baselines** **for** **all**\n**evaluated datasets** . Comparing the LLMs, GPT-4\nemerges as the top performer, with Mistral-7b and\nGPT-3.5-turbo exhibiting comparable results. The\ndisparity between our methods and the baselines is\nmore pronounced on the REBEL and Wiki-NRE\ndatasets; this is primarily due to the GenIE’s constrained generation approach, which falls short in\nextracting triplets that include literals, such as numbers and dates.\n\n\n**Refinement (EDC+R) consistently and signif-**\n**icantly enhances performance** . Post-refinement,\nthe difference in performance between GPT-3.5turbo and Mistral-7b is larger, suggesting Mistral7b’s was not as able to leverage the provided hints.\nNevertheless, a single refinement iteration with the\nhint improved performance for all the tested LLMs.\n\n\nFrom the scores, it appears that EDC performance is significantly better on WebNLG compared to REBEL and Wiki-NRE. However, we observed that EDC was penalized despite producing\nvalid triplets on the latter datasets. A reason for\nthis is that the reference triplets in these datasets\nare non-exhaustive. For example, given the text\nin the REBEL dataset, ‘ _Romany_ _Love_ _is_ _a_ _1931_\n_British musical film directed by Fred Paul and star-_\n_ring Esmond Knight, Florence McHugh and Roy_\n_Travers._ ’, EDC extracts: [‘Romany Love’, ‘cast\nmember’, ‘Esmond Knight’], [‘Romany Love’,\n‘cast member’, ‘Florence McHugh’], [‘Romany\nLove’, ‘cast member’, ‘Roy Travers’], which are\nall semantically correct, but only the first triplet is\npresent in the reference set. The datasets also contain reference triplets based on information extraneous to the text, e.g., ‘ _Daniel is an Ethiopian foot-_\n_baller, who currently plays for Hawassa City S.C._ ’\nhas a corresponding reference triplet [‘Hawassa\nCity S.C.’, ‘country’, ‘Ethiopia’].\n\n\nThese issues can be attributed to the distinct\nmethodologies employed in the creation of these\ndatasets. For WebNLG, annotators were asked to\nTable 1: Ablation study results (F1 scores with all criteria) on schema retriever, the LLM used for OIE is\nGPT-3.5-turbo. S.R. stands for Schema Retriever.\n\n|Dataset|Method|Partial Strict Exact|\n|---|---|---|\n|WebNLG|EDC+R<br>EDC+R w/o S.R.|**0.794**<br>**0.753**<br>**0.772**<br>0.752<br>0.701<br>0.721|\n|WebNLG|EDC<br>|0.746<br>0.688<br>0.713<br><br><br>|\n|REBEL|EDC+R<br>EDC+R w/o S.R.|**0.559**<br>**0.516**<br>**0.529**<br>0.517<br>0.466<br>0.482|\n|REBEL|EDC|0.506<br>0.449<br>0.473|\n|Wiki-NRE|EDC+R<br>EDC+R w/o S.R.|**0.693**<br>**0.685**<br>**0.657**<br>0.653<br>0.645<br>0.641|\n|Wiki-NRE|EDC|0.647<br>0.638<br>0.640|\n\n\n\ncompose text solely from the triplets. Thus, the\ntext and the triplets have a direct correspondence,\nand the text typically does not include information other than what is apparent from the triplets.\nIn contrast, REBEL and Wiki-NRE are created\nby aligning text and triplets using distant supervision (Smirnova and Cudré-Mauroux, 2018). This\napproach can result in less straightforward triplet\nextraction and incomplete reference sets, leading\nto overly pessimistic evaluations for methods like\nEDC, which generate correct triplets not present in\nthe dataset. (Han et al., 2023; Wadhwa et al., 2023).\nOn average, EDC extracts one additional triplet per\nsentence on REBEL and Wiki-NRE compared to\nthe reference set, while on WebNLG, it extracts a\nsimilar number of triplets to the reference.\n\n\n**Ablation study on schema retriever.** To evaluate\nthe impact of the relations provided by the schema\nretriever during refinement, we conducted an ablation study with GPT-3.5-turbo by removing these\nrelations. The results in Table 1 show that **ablating**\n**the Schema Retriever leads to a decline in perfor-**\n**mance** . Qualitatively, we find that the schema retriever helps to find relevant relations that are challenging for the LLMs to identify during the OIE\nstage. For example, given the text _‘The University_\n_of_ _Burgundy_ _in_ _Dijon_ _has_ _16,800_ _undergraduate_\n_students’_, the LLMs extract [‘University of Burgundy’, ‘location’, ‘Dijon’] during OIE. Although\nsemantically correct, this relation overlooks the\nmore specific relation present in the target schema,\nnamely ‘campus’, for denoting university’s location. The schema retriever successfully identifies\nthis finer relation, enabling the LLMs to adjust their\nextraction to [‘University of Burgundy’, ‘campus’,\n‘Dijon’]. This experiment highlights the schema\nretriever’s value in facilitating the extraction of precise and contextually appropriate relations.\n\n\n\nTable 2: Performance of EDC in the Self Canonicalization setting (human-evaluated precision and schema\nmetrics). The best result for each dataset and metric is\nbolded. Prec. stands for precision, No. Rel. stands for\nthe number of relations and Red. stands for redundancy\nscore.\n\n|Dataset|Method|Prec.(↑) No. Rel.(↓) Red.(↓)|\n|---|---|---|\n|WebNLG|EDC<br>CESI|**0.956**<br>**200**<br>**0.833**<br>0.724<br>280<br>0.893|\n|WebNLG|Open KG|0.982<br>529<br>0.927|\n|REBEL|EDC<br>CESI|**0.867**<br>**225**<br>**0.831**<br>0.504<br>307<br>0.854|\n|REBEL|Open KG|0.903<br>667<br>0.895|\n|Wiki-NRE|EDC<br>CESI|**0.898**<br>**106**<br>**0.833**<br>0.753<br>114<br>0.849|\n|Wiki-NRE|Open KG|0.909<br>204<br>0.881|\n\n\n\n**4.2.2** **Self Canonicalization**\nHere, we focus on evaluating EDC’s selfcanonicalization performance (utilizing GPT-3.5turbo for OIE). We omit refinement in Self Canonicalization setting as it has already been studied above and in subsequent iterations, the selfconstructed canonicalized schema becomes the target schema. Following prior work (Wadhwa et al.,\n2023; Kolluru et al., 2020), we conducted a targeted human evaluation of knowledge graphs. This\nevaluation involved two independent annotators\nassessing the reasonableness of triplet extractions\nfrom given text without prior knowledge of the system’s details. We observed a high inter-annotator\nagreement score of 0.94.\nThe evaluation results and schema metrics are\nsummarized in Table 2.While the open KG generated by the OIE stage contains semantically\nvalid triplets (which affirms the previous findings\nthat LLMs are competent open information extractors (Li et al., 2023)), there is a significant degree\nof redundancy within the resultant schema. **EDC**\n**accurately canonicalizes the open KG and yields**\n**a schema that is both more concise and less re-**\n**dundant compared to CESI** . EDC avoids CESI’s\ntendency toward over-generalization — in line with\nprior work (Putri et al., 2019), we observed CESI\ninappropriately clusters diverse relations such as\n‘place of death’, ‘place of birth’, ‘date of death’,\n‘date of birth’, and ‘cause of death’ into a single\n‘date of death’ category.\n\n\n**5** **Conclusion**\nIn this work, we presented EDC, an LLM-based\nthree-phase framework that addresses the problem\nof KGC by open information extraction followed\nby post-hoc canonicalization. Experiments show\nthat EDC and EDC+R are able to extract better\nKGs than specialized trained models when a target schema is available and dynamically create a\nschema when none is provided. The scalability and\nversatility of EDC opens up many opportunities\nfor applications: it allows us to automatically extract high-quality KGs from general text using large\nschemas like Wikidata (Vrandeˇci´c and Krötzsch,\n2014) and even enrich these schemas with newly\ndiscovered relations.\n\n\n**6** **Limitations and Future Directions**\nThere are several limitations that we would like to\naddress in future works.\n\n\n  - We only considered schema canonicalization\nwithin the scope of this paper, it is of great\ninterest to incorporate an entity de-duplication\nmechanism in the future to reduce the redundancy in the constructed KGs, e.g., via coreference resolution (Sukthanker et al., 2020).\nWe briefly explored this approach and the preliminary results can be found in Appendix F.\n\n\n  - EDC’s components can be further improved to\nboost performance. Specifically, the schema\nretriever may benefit from training on more\ndiverse and higher-quality data.\n\n\n  - Due to time and resource constraints, we only\ntested different LLMs for OIE while all the\nother modules of EDC rely on GPT-3.5-turbo,\nit will be beneficial to test the smaller opensource models’ performance on the other tasks\nas well.\n\n\n  - EDC is a costly framework, involving a large\nnumber of LLM calls. When GPT-3.5-turbo is\nused for all components, the cost was around\n0.009 USD per example in our experiments.\nIt is possible to have certain components replaced by smaller fine-tuned models — previous works have shown smaller language models can be fine-tuned for OIE (Wadhwa et al.,\n2023) and smaller BERT-based classifiers can\nbe trained for schema canonicalization. We\nalso explored the possibility of combining the\ntwo stages of OIE and Schema Definition in\nAppendix G.\n\n\n  - We are looking to apply EDC towards embodied AI and robotics. Specifically, KGs can\nform memory sources for VLMs, containing\nfacts about humans (Zhang and Soh, 2023),\nthe task or goal (Xie et al., 2023), and the\nenvironment.\n\n\n**7** **Ethical Considerations**\n**Artifact** **usage.** The datasets we used in the\npaper are only leveraged for research purposes\nand we strictly follow the corresponding licenses\n(e.g. WebNLG uses cc-by-nc-sa-4.0). It is to\nbe noted that, due to the nature of the task, the\ndatasets may inherently contain information about\nindividuals (especially celebrities). Software and\n[code for this paper is publicly available at https:](https://github.com/clear-nus/edc)\n[//github.com/clear-nus/edc.](https://github.com/clear-nus/edc)\n**Human annotators.** The two annotators (1 male\nand 1 female) are recruited university students. The\nannotators are compensated fairly and given abundant and flexible time to complete the tasks. The\ncollection protocol is determined exempt by our\ninstitution’s IRB committee.\n\n\n**Potential Risks.** The use of current LLMs may\nincur risks such as hallucinations (Xu et al., 2024)\nand privacy issues (Yao et al., 2024).\n\n\n**Acknowledgements**\nThis research is supported by the National Research\nFoundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG\nAward No: AISG2-RP-2020-016).\n\n\n**References**\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\n_arXiv preprint arXiv:2303.08774_ .\n\n\nOshin Agarwal, Heming Ge, Siamak Shakeri, and\nRami Al-Rfou. 2020. Knowledge graph based synthetic corpus generation for knowledge-enhanced\nlanguage model pre-training. _arXiv_ _preprint_\n_arXiv:2010.12688_ .\n\n\nZhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo,\nHuajun Chen, and Ningyu Zhang. 2024. Codekgc:\nCode language model for generative knowledge\ngraph construction. _ACM_ _Transactions_ _on_ _Asian_\n_and Low-Resource Language Information Process-_\n_ing_, 23(3):1–16.\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. _Advances in neural information processing_\n_systems_, 33:1877–1901.\n\n\nPere-Lluís Huguet Cabot and Roberto Navigli. 2021.\nRebel: Relation extraction by end-to-end language\ngeneration. In _Findings of the Association for Com-_\n_putational Linguistics:_ _EMNLP 2021_, pages 2370–\n2381.\n\n\nEunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. 2018. Ultra-fine entity typing. _arXiv preprint_\n_arXiv:1807.04905_ .\n\n\nSarthak Dash, Gaetano Rossiello, Nandana Mihindukulasooriya, Sugato Bagchi, and Alfio Gliozzo.\n2020. Open knowledge graphs canonicalization\nusing variational autoencoders. _arXiv_ _preprint_\n_arXiv:2012.04780_ .\n\n\nBayu Distiawan, Gerhard Weikum, Jianzhong Qi, and\nRui Zhang. 2019. Neural relation extraction for\nknowledge base enrichment. In _Proceedings of the_\n_57th Annual Meeting of the Association for Compu-_\n_tational Linguistics_, pages 229–240.\n\n\nPierre L Dognin, Inkit Padhi, Igor Melnyk, and Payel\nDas. 2021. Regen: Reinforcement learning for text\nand knowledge base generation using pretrained language models. _arXiv preprint arXiv:2108.12472_ .\n\n\nThiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh,\nChris Van Der Lee, Simon Mille, Diego Moussallem,\nand Anastasia Shimorina. 2020. The 2020 bilingual,\nbi-directional webnlg+ shared task overview and evaluation results (webnlg+ 2020). In _Proceedings of the_\n_3rd_ _International_ _Workshop_ _on_ _Natural_ _Language_\n_Generation from the Semantic Web (WebNLG+)_ .\n\n\nDebasis Ganguly, Dwaipayan Roy, Mandar Mitra, and\nGareth JF Jones. 2015. Word embedding based generalized language model for information retrieval. In\n_Proceedings_ _of_ _the_ _38th_ _international_ _ACM_ _SIGIR_\n_conference on research and development in informa-_\n_tion retrieval_, pages 795–798.\n\n\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. Ppdb: The paraphrase\ndatabase. In _Proceedings_ _of_ _the_ _2013_ _conference_\n_of the north american chapter of the association for_\n_computational_ _linguistics:_ _Human_ _language_ _tech-_\n_nologies_, pages 758–764.\n\n\nLiang Guo, Fu Yan, Yuqian Lu, Ming Zhou, and\nTao Yang. 2021. An automatic machining process\ndecision-making system based on knowledge graph.\n_International journal of computer integrated manu-_\n_facturing_, 34(12):1348–1369.\n\n\nQingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu\nZhu, Xing Xie, Hui Xiong, and Qing He. 2020. A\nsurvey on knowledge graph-based recommender systems. _IEEE Transactions on Knowledge and Data_\n_Engineering_, 34(8):3549–3568.\n\n\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus\nRoberts, Juliane Fluck, Martin Hofmann-Apitius, and\nLuca Toldo. 2012. Development of a benchmark\ncorpus to support the automatic extraction of drugrelated adverse effects from medical case reports.\n_Journal of biomedical informatics_, 45(5):885–892.\n\n\nRidong Han, Tao Peng, Chaohao Yang, Benyou Wang,\nLu Liu, and Xiang Wan. 2023. Is information extraction solved by chatgpt? an analysis of performance,\nevaluation criteria, robustness and errors. _arXiv_\n_preprint arXiv:2305.14450_ .\n\n\nXiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping\nLi. 2019. Knowledge graph embedding based question answering. In _Proceedings of the twelfth ACM_\n_international conference on web search and data min-_\n_ing_, pages 105–113.\n\n\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. 2021. A survey on knowledge\ngraphs: Representation, acquisition, and applications.\n_IEEE transactions on neural networks and learning_\n_systems_, 33(2):494–514.\n\n\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b. _arXiv preprint arXiv:2310.06825_ .\n\n\nMartin Josifoski, Nicola De Cao, Maxime Peyrard,\nFabio Petroni, and Robert West. 2022. [GenIE: Gen-](https://doi.org/10.18653/v1/2022.naacl-main.342)\nerative [information](https://doi.org/10.18653/v1/2022.naacl-main.342) extraction. In _Proceedings_ _of_\n_the 2022 Conference of the North American Chap-_\n_ter of the Association for Computational Linguistics:_\n_Human Language Technologies_, pages 4626–4643,\nSeattle, United States. Association for Computational\nLinguistics.\n\n\nSerafina Kamp, Morteza Fayazi, Zineb Benameur-El,\nShuyan Yu, and Ronald Dreslinski. 2023. Open\ninformation extraction: A review of baseline techniques, approaches, and applications. _arXiv preprint_\n_arXiv:2310.11644_ .\n\n\nKeshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal,\nSoumen Chakrabarti, et al. 2020. Openie6: Iterative\ngrid labeling and coordination analysis for open information extraction. _arXiv preprint arXiv:2010.03147_ .\n\n\nLuong Thi Hong Lan, Tran Manh Tuan, Tran Thi Ngan,\nNguyen Long Giang, Vo Truong Nhu Ngoc, Pham\nVan Hai, et al. 2020. A new complex fuzzy inference\nsystem with fuzzy knowledge graph and extensions\nin decision making. _Ieee Access_, 8:164899–164921.\n\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\n_arXiv preprint arXiv:1910.13461_ .\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. _Advances in Neu-_\n_ral Information Processing Systems_, 33:9459–9474.\n\n\nBo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei\nYe, Wen Zhao, and Shikun Zhang. 2023. Evaluating\nchatgpt’s information extraction capabilities: An assessment of performance, explainability, calibration,\nand faithfulness. _arXiv preprint arXiv:2304.11633_ .\n\n\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024. Lost in the middle: How language models use long contexts. _Transactions of the Association_\n_for Computational Linguistics_, 12:157–173.\n\n\nPai Liu, Wenyang Gao, Wenjie Dong, Songfang Huang,\nand Yue Zhang. 2022. Open information extraction from 2007 to 2022–a survey. _arXiv_ _preprint_\n_arXiv:2208.08690_ .\n\n\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification\nof entities, relations, and coreference for scientific knowledge graph construction. _arXiv preprint_\n_arXiv:1808.09602_ .\n\n\nPedro Henrique Martins, Zita Marinho, and André FT\nMartins. 2019. Joint learning of named entity\nrecognition and entity linking. _arXiv_ _preprint_\n_arXiv:1907.08243_ .\n\n\nIgor Melnyk, Pierre Dognin, and Payel Das. 2022.\nKnowledge graph generation from text. _arXiv_\n_preprint arXiv:2211.10511_ .\n\n\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. _Communications of the ACM_, 38(11):39–41.\n\n\nYasumasa Onoe and Greg Durrett. 2020. Fine-grained\nentity typing for domain independent entity linking.\nIn _Proceedings of the AAAI Conference on Artificial_\n_Intelligence_, volume 34, pages 8576–8583.\n\n\nShon Otmazgin, Arie Cattan, and Yoav Goldberg. 2023.\n\nLingMess: [Linguistically informed multi expert scor-](https://doi.org/10.18653/v1/2023.eacl-main.202)\n[ers for coreference resolution.](https://doi.org/10.18653/v1/2023.eacl-main.202) In _Proceedings of the_\n_17th Conference of the European Chapter of the As-_\n_sociation for Computational Linguistics_, pages 2752–\n2760, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n\nRifki Afina Putri, Giwon Hong, and Sung-Hyon\nMyaeng. 2019. Aligning open ie relations and kb\nrelations using a siamese network based on word embedding. In _Proceedings_ _of_ _the_ _13th_ _International_\n_Conference_ _on_ _Computational_ _Semantics-Long_ _Pa-_\n_pers_, pages 142–153.\n\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. _Journal of machine learning research_,\n21(140):1–67.\n\n\nDan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In _Proceedings of the eighth conference_\n_on computational natural language learning (CoNLL-_\n_2004) at HLT-NAACL 2004_, pages 1–8.\n\n\nAlisa Smirnova and Philippe Cudré-Mauroux. 2018. Relation extraction using distant supervision: A survey.\n_ACM Computing Surveys (CSUR)_, 51(5):1–35.\n\n\nRhea Sukthanker, Soujanya Poria, Erik Cambria, and\nRamkumar Thirunavukarasu. 2020. Anaphora and\ncoreference resolution: A review. _Information_ _Fu-_\n_sion_, 59:139–162.\n\n\nQingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, and\nSharifah Mahani Aljunied. 2022. [Revisiting docred](https://arxiv.org/abs/2205.12696)\n\n - addressing the false [negative](https://arxiv.org/abs/2205.12696) problem in relation\n[extraction.](https://arxiv.org/abs/2205.12696) In _Proceedings of EMNLP_ .\n\n\nShikhar Vashishth, Prince Jain, and Partha Talukdar.\n2018. Cesi: Canonicalizing open knowledge bases\nusing embeddings and side information. In _Proceed-_\n_ings of the 2018 World Wide Web Conference_, pages\n1317–1327.\n\n\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. _Communi-_\n_cations of the ACM_, 57(10):78–85.\n\n\nSomin Wadhwa, Silvio Amir, and Byron C Wallace.\n2023. Revisiting relation extraction in the era of large\nlanguage models. In _Proceedings of the conference._\n_Association for Computational Linguistics. Meeting_,\nvolume 2023, page 15566. NIH Public Access.\n\n\nHongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and\nMinyi Guo. 2019. Knowledge graph convolutional\nnetworks for recommender systems. In _The world_\n_wide web conference_, pages 3307–3313.\n\n\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2023. Improving\ntext embeddings with large language models. _arXiv_\n_preprint arXiv:2401.00368_ .\n\n\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,\nXin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,\nYufeng Chen, Meishan Zhang, et al. 2023. Zeroshot information extraction via chatting with chatgpt.\n_arXiv preprint arXiv:2302.10205_ .\n\n\nYaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong,\nand Harold Soh. 2023. Translating natural language\nto planning goals with large-language models. _arXiv_\n_preprint arXiv:2302.05128_ .\n\n\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli.\n2024. Hallucination is inevitable: An innate limitation of large language models. _arXiv_ _preprint_\n_arXiv:2401.11817_ .\n\n\nYifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo\nSun, and Yue Zhang. 2024. A survey on large language model (llm) security and privacy: The good,\nthe bad, and the ugly. _High-Confidence Computing_,\npage 100211.\n\n\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. Qagnn: Reasoning with language models and knowledge graphs for question answering. _arXiv preprint_\n_arXiv:2104.06378_ .\n\n\nHongbin Ye, Ningyu Zhang, Hui Chen, and Huajun\nChen. 2022. Generative knowledge graph construction: A review. _arXiv preprint arXiv:2210.12714_ .\n\n\nDaojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.\n2015. Distant supervision for relation extraction via\npiecewise convolutional neural networks. In _Proceed-_\n_ings of the 2015 conference on empirical methods in_\n_natural language processing_, pages 1753–1762.\n\n\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\nand Jun Zhao. 2014. Relation classification via convolutional deep neural network. In _Proceedings of_\n_COLING 2014, the 25th international conference on_\n_computational linguistics:_ _technical papers_, pages\n2335–2344.\n\n\nBowen Zhang and Harold Soh. 2023. Large language\nmodels as zero-shot human models for human-robot\ninteraction. In _2023 IEEE/RSJ International Confer-_\n_ence on Intelligent Robots and Systems (IROS)_, pages\n7961–7968. IEEE.\n\n\nLingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. 2023. A comprehensive survey on automatic knowledge graph construction. _ACM Comput-_\n_ing Surveys_, 56(4):1–62.\n\n\nShaowen Zhou, Bowen Yu, Aixin Sun, Cheng Long,\nJingyang Li, Haiyang Yu, Jian Sun, and Yongbin\nLi. 2022. A survey on neural open information extraction: Current status and future directions. _arXiv_\n_preprint arXiv:2205.11725_ .\n\n\nAndrej Žukov-Gregoriˇc, Yoram Bachrach, and Sam\nCoope. 2018. Named entity recognition with parallel\nrecurrent neural networks. In _Proceedings of the 56th_\n_Annual Meeting of the Association for Computational_\n_Linguistics (Volume 2:_ _Short Papers)_, pages 69–74.\n\n\n**A** **Implementation Details**\n**A.1** **Models and Infrastructures Details**\nWe use two OpenAI models, GPT-3.5-turbo and\nGPT-4 (sizes currently unknown), and an opensource model, Mistral-7b (7 billion parameters).\nThe training and inference of open-source models are done with a single machine with an AMD\nEPYC 7543P 32-Core Processor and 252GB of\nRAM, equipped with 4 NVIDIA RTX A6000\n(48GB) GPUs. We accessed GPT-3.5-turbo and\nGPT-4 via the OpenAI API. Code for EDC is avail[able at https://github.com/clear-nus/edc.](https://github.com/clear-nus/edc)\n**A.2** **Prompting-related hyperparameters**\nWe use few-shot prompting for all modules of\nEDC, we empirically choose 6-shot examples from\nthe respective datasets. For the MCQ used in the\nSchema Canonicalization phase, we retrieve top-5\nsemantically similar relations from the schema as\ncandidates. For refinement, the schema retriever\nretrieves top-10 most relevant relations from the\nschema as candidate relations. These hyperparameters are empirically chosen to balance performance\nand inference costs.\n\n\n**A.3** **Schema Retriever Training**\nWe follow the original training methodology detailed in the original paper (Wang et al., 2023),\nwhich involves utilizing pairs of text and their corresponding defined relations. For a given positive\ntext-relation pair ( _t_ [+] _, r_ [+] ), we employ an instruction template on _t_ [+] to generate a new text _t_ [+] _inst_ [=]\n“Instruct: retrieve relations that are present in the\ngiven text _\\n_ Query: _{t_ [+] _}_ ”.\nWe then finetune the embedding model to distinguish between the correct relation associated with\na given text and other non-relevant relations using\nthe InfoNCE loss,\n_ϕ_ ( _t_ [+] _inst_ _[, r]_ [+][)]\nmin _L_ = _−_ log\n_ϕ_ ( _t_ [+] _inst_ _[, r]_ [+][) +][ �] _ni∈_ N _[ϕ]_ [(] _[t]_ _inst_ [+] _[, n][i]_ [)]\nHere, N denotes the set of negative samples, and\n_ϕ_ represents the cosine similarity function. Please\nsee the appendix for additional training details.\nFor training, we synthesized a dataset of textrelation pairs using the TEKGEN dataset (Agarwal\net al., 2020), a large-scale text-triplets dataset created by aligning Wikidata triplets to Wikipedia text.\nThe training dataset comprised 37,500 pairs, evenly\ndivided between positive and negative samples. We\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-12-0.png)\n\nand the resultant entities are [‘Alan Shepard’,\n‘Nov 18, 1923’, ‘NASA’, ‘1959’, ‘Apollo\n14’]\nThe entities are then merged together as the candidate entities.\n\n\n\nadopted an online open-source implementation and\nhyperparameter configurations for training.\nThe performance of the fine-tuned schema retriever was assessed on the test splits of WebNLG,\nREBEL, and Wiki-NRE datasets. The recall@10\nscores on these datasets were 0.823, 0.663, and\n0.818, respectively, indicating the effectiveness of\nthe retriever across different knowledge graph contexts.\n\n\n**A.4** **Details of Refinement Hint**\nThe refinement hint consists of candidate entities\nand candidate relations. This section details the\nobtainment of them and how they are used to improve the OIE performance. We will carry on using\nthe example used in Section 3: “ _Alan Shepard was_\n_born_ _on_ _Nov_ _18,_ _1923_ _and_ _selected_ _by_ _NASA_ _in_\n_1959._ _He_ _was_ _a_ _member_ _of_ _the_ _Apollo_ _14_ _crew_ ”\nand the triplets extracted by EDC in the first iteration are [’Alan Shepard’, ‘birthDate’, ‘Nov 18,\n1923’], [’Alan Shepard’, ’mission’, ’Apollo 14’].\n\n\n**A.4.1** **Obtaining Candidate Entities**\nThe candidate entities come from two sources:\n\n\n  - Entities extracted by EDC from the previous\niteration, i.e. [‘Alan Shepard’, ‘Nov 18,\n1923’, ‘Apollo 14’]\n\n\n  - Entities extracted from the text by prompting\nthe LLM to perform an entity extraction task,\nsimilar to the triplet extraction task.\n\n\n\n\n**A.4.2** **Obtaining Candidate Relations**\nThe candidate relations also come from two\nsources:\n\n\n  - Relations extracted by EDC from the previous\niteration, i.e. [‘birthDate’, ‘mission’]\n\n\n  - Relations extracted by the schema retriever,\nby calculating the relevance score between\nthe input text and the relations in the schema.\nThe top 5 retrieved relations in this case are\n[‘birthDate’,, ‘selectedByNasa’, ‘mission’,\n‘draftPick’, ‘occupation’].\n\n\nThe relations and their corresponding definitions\nare then merged together as the candidate relations.\nIt is to be noted that, similar to other RAG-based\nmethods, there is a chance that the retriever may\nretrieve irrelevant information. In this case, the relation definitions can come in handy as it provides\nmore information for the LLMs to decide whether\nthe relation is a valid one with respect to the text or\nnot.\n\n\n**A.4.3** **Usage of Hint for Refined OIE**\nThe refinement hint is then included in the prompt\nappropriately to instruct the LLMs to consider (but\nis not limited to) the candidate entities and candidate relations:\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-13-0.png)\n\nFigure 3: An example screenshot of the questionnaire\nincluding the instructions given to the annotators.\n\n\nThe extracted triplets by the refined OIE\nare:[’Alan Shepard’, ‘birthDate’, ‘Nov 18,\n1923’], [’Alan Shepard’, ’mission’, ’Apollo 14’],\n[’Alan Shepard’, ’selectedByNasa’, ’1959’]. It\nsuccessfully recovers the subtle and fine-grained\nrelation ‘selectedByNasa’ that would have been\nmissed without using the hint. Also, the semantically rich descriptions help the LLM avoid excessively extracting noisy relations retrieved by the\nschema retriever.\nWe found it important to include the entities\nfrom both sources, i.e. extractions from the last\nround and discovered by a separate module (entity\nextraction or schema retriever). The significance\nof the schema retriever is already shown in the\nablation study in Sec 4.2.1.\n\n\n**B** **Annotation Instruction**\nAn example screenshot is provided in Figure 3\nto illustrate the format of questionnaires and instructions the annotators are given. The purpose\nof collection of the data was communicated to the\nannotators verbally.\n\n\n**C** **Detailed Results of Target Alignment**\n**C.1** **Complete Results**\nThe complete results of EDC and EDC+R on\nWebNLG, REBEL and Wiki-NRE are summarized\nin Table 3, Table 4 and Table 5 respectively. EDC\nperforms better than or comparable to state-of-theart baseline models in terms of all metrics (Precision, Recall, and F1) in all criteria (Partial, Strict,\nand Exact) and EDC+R is able to consistently im\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction.pdf-13-1.png)\nTable 3: Complete results of EDC and EDC+R on WebNLG dataset against the baseline REGEN (Precision, Recall,\nF1 with ‘Partial’, ‘Strict’ and ‘Exact’ criteria). EDC+R only performs 1 iteration of refinement. The best results are\nbolded.\n\n|Method|LLM for OIE|Partial<br>Precision Recall F1|Strict<br>Precision Recall F1|Exact<br>Precision Recall F1|\n|---|---|---|---|---|\n|EDC|GPT-4<br>GPT-3.5<br>Mistral-7b|**0.776**<br>**0.796**<br>**0.783**<br>0.739<br>0.760<br>0.746<br>0.723<br>0.739<br>0.728|**0.729**<br>**0.741**<br>**0.733**<br>0.684<br>0.697<br>0.688<br>0.668<br>0.679<br>0.672|**0.751**<br>**0.765**<br>**0.756**<br>0.708<br>0.722<br>0.713<br>0.692<br>0.703<br>0.696|\n|EDC+R|GPT-4<br>GPT-3.5<br>Mistral-7b|**0.814**<br>**0.831**<br>**0.820**<br>0.788<br>0.806<br>0.794<br>0.756<br>0.775<br>0.762|**0.782**<br>**0.794**<br>**0.786**<br>0.749<br>0.761<br>0.753<br>0.716<br>0.727<br>0.720|**0.796**<br>**0.808**<br>**0.800**<br>0.768<br>0.781<br>0.772<br>0.735<br>0.747<br>0.739|\n|Baseline|REGEN|0.755<br>0.788<br>0.767|0.713<br>0.735<br>0.720|0.714<br>0.738<br>0.723|\n\n\n\nTable 4: Complete results of EDC and EDC+R on REBEL dataset against the baseline REGEN (Precision, Recall,\nF1 with ‘Partial’, ‘Strict’, and ‘Exact’ criteria). EDC+R only performs 1 iteration of refinement. The best results are\nbolded.\n\n|Method|LLM for OIE|Partial<br>Precision Recall F1|Strict<br>Precision Recall F1|Exact<br>Precision Recall F1|\n|---|---|---|---|---|\n|EDC|GPT-4<br>GPT-3.5<br>Mistral-7b|**0.543**<br>**0.552**<br>**0.546**<br>0.503<br>0.512<br>0.506<br>0.512<br>0.523<br>0.516|**0.498**<br>**0.503**<br>**0.500**<br>0.448<br>0.453<br>0.449<br>0.450<br>0.457<br>0.453|**0.511**<br>**0.517**<br>**0.514**<br>0.471<br>0.476<br>0.473<br>0.481<br>0.488<br>0.483|\n|EDC+R|GPT-4<br>GPT-3.5<br>Mistral-7b|**0.599**<br>**0.606**<br>**0.601**<br>0.556<br>0.565<br>0.559<br>0.525<br>0.550<br>0.531|**0.557**<br>**0.561**<br>**0.559**<br>0.513<br>0.519<br>0.516<br>0.461<br>0.462<br>0.462|**0.572**<br>**0.576**<br>**0.574**<br>0.527<br>0.533<br>0.529<br>0.506<br>0.511<br>0.505|\n|Baseline|GENIE|0.381<br>0.391<br>0.385|0.353<br>0.361<br>0.356|0.362<br>0.369<br>0.364|\n\n\n\nTable 5: Complete results of EDC and EDC+R on Wiki-NRE dataset against the baseline REGEN (Precision, Recall,\nF1 with ‘Partial’, ‘Strict’, and ‘Exact’ criteria). EDC+R only performs 1 iteration of refinement. The best results are\nbolded.\n\n|Method|LLM for OIE|Partial<br>Precision Recall F1|Strict<br>Precision Recall F1|Exact<br>Precision Recall F1|\n|---|---|---|---|---|\n|EDC|GPT-4<br>GPT-3.5<br>Mistral-7b|**0.682**<br>**0.686**<br>**0.683**<br>0.645<br>0.651<br>0.647<br>0.644<br>0.650<br>0.647|**0.675**<br>**0.679**<br>**0.677**<br>0.636<br>0.640<br>0.638<br>0.636<br>0.640<br>0.637|**0.676**<br>**0.680**<br>**0.678**<br>0.638<br>0.643<br>0.640<br>0.637<br>0.641<br>0.639|\n|EDC+R|GPT-4<br>GPT-3.5<br>Mistral-7b|**0.712**<br>**0.715**<br>**0.713**<br>0.691<br>0.696<br>0.693<br>0.661<br>0.667<br>0.663|**0.708**<br>**0.710**<br>**0.709**<br>0.684<br>0.688<br>0.685<br>0.647<br>0.652<br>0.649|**0.708**<br>**0.711**<br>**0.709**<br>0.685<br>0.689<br>0.687<br>0.656<br>0.661<br>0.658|\n|Baseline|GENIE|0.482<br>0.486<br>0.484|0.462<br>0.464<br>0.463|0.477<br>0.479<br>0.478|\n\n\n\nTable 6: Results (F1 scores with all criteria) of further iterative refinement, the LLM used for OIE is GPT-3.5-turbo.\nEDC+2xR is EDC with 2 iterations of refinement.\n\n|Method|WebNLG<br>Partial Strict Exact|REBEL<br>Partial Strict Exact|Wiki-NRE<br>Partial Strict Exact|\n|---|---|---|---|\n|EDC+2xR<br>EDC+R|0.797<br>0.761<br>0.775<br>0.794<br>0.753<br>0.772|0.564<br>0.521<br>0.535<br>0.559<br>0.516<br>0.529|0.697<br>0.689<br>0.660<br>0.693<br>0.685<br>0.657|\n|EDC|0.746<br>0.688<br>0.713|0.506<br>0.449<br>0.473|0.644<br>0.634<br>0.637|\n\n\n\nTable 7: Results (F1 scores with all criteria) of ablating the entities and relations extracted from the last round from\nthe refinement hint, the LLM used for OIE is GPT-3.5-turbo. EDC+R-lastround is EDC with refinement but entities\nand relations extracted from the last round are removed from the refinement hint.\n\n\n|Method|WebNLG<br>Partial Strict Exact|REBEL<br>Partial Strict Exact|Wiki-NRE<br>Partial Strict Exact|\n|---|---|---|---|\n|EDC+R<br>EDC+R-lastround|0.794<br>0.753<br>0.772<br>0.748<br>0.698<br>0.720|0.559<br>0.516<br>0.529<br>0.534<br>0.485<br>0.505|0.693<br>0.685<br>0.657<br>0.634<br>0.622<br>0.625|\n|EDC|0.746<br>0.688<br>0.713|0.506<br>0.449<br>0.473|0.644<br>0.634<br>0.637|\n\n\nTable 8: The average number of triplets extracted per sentence on all three datasets. The baseline model for WebNLG\nis REGEN while the baseline for Rebel and Wiki-NRE is GENIE. Numbers in the brackets are the difference from\nthe reference annotations.\n\n\n|LLM for OIE|WebNLG REBEL Wiki-NRE|\n|---|---|\n|GPT-4<br>GPT-3.5<br>Mistral7b|3.47(+0.04)<br>5.11(+1.11)<br>3.49(+0.63)<br>3.44(+0.01)<br>5.01(+1.01)<br>3.49(+0.63)<br>3.45+(0.02)<br>4.68(+0.68)<br>3.75(+0.89)|\n|Baseline|-<br>2.20(-1.80)<br>3.08(+0.22)|\n|Reference|3.43<br>4.00<br>2.86|\n\n\n\nprove upon this in all aspects as well. These results more comprehensively demonstrate the performance of EDC and EDC+R.\n\n\n**C.2** **Effect of More Refinement Iterations**\nTable 6 shows the result of an extra iteration of\nrefinement with EDC on all datasets. Although\nfurther refinement improves the results in a stable\nmanner, we observe diminishing returns and hence,\nonly report one iteration in the main results.\n\n\n**C.3** **Ablation Study on Last-Round**\n**Extractions**\nTable 7 shows the result of ablating the relations\nand entities from the last round’s extractions from\nthe refinement hint. It shows the importance of\nperforming the refinement in an iterative manner.\nMerging the two sources led to better coverage of\nthe entities and relations in the text, resulting in\nbetter KGC.\n\n\n**C.4** **Discussion on KGC Dataset Annotations**\nAs stated in Section 4.2, we observe that EDC\nis penalized by the scorer on Rebel and WikiNRE datasets due to incomplete annotations. This\nechoes the previous finding in (Wadhwa et al.,\n2023; Han et al., 2023) that LLMs can often extract\ncorrect results that are missing in the annotations,\nwhich results in overly pessimistic evaluations. As\nshown by Table 8, EDC tends to extract significantly more triplets compared to the reference\nannotations and the baseline GenIE. Furthermore,\nas shown from the manual evaluation in Table 2,\nmany of these triplets are indeed meaningful and\ncorrect with respect to the input text. Regardless,\ndespite the automatic evaluation result on EDC\nbeing overly pessimistic, it still exceeds the baseline by a large margin and the actual performance\nmay be even larger considering the difference in\nthe number of triplets extracted.\n\n\n\n**D** **Experiments on a Novel Dataset**\nSince the tested datasets were created several years\nago and the training set of the LLMs used are unknown, there is a risk the LLMs have already been\ntrained on these datasets. To address this concern,\nwe create a novel small-scale dataset (50 entries) of\nfictional entities and information, e.g. “ _Evergreen_\n_University was where Emily Johnson received her_\n_degree in Biology_ ” and annotated them using the\nschema of Wiki-NRE. Table 9 shows that EDC and\nEDC+R still obtain performance superior to the\nbaseline GenIE model.\n\n\n**E** **Comparison against previous**\n**LLM-based approaches**\nAlthough this is not the intended use scenario for\nEDC, we include these experimental results for a\nmore comprehensive evaluation to compare against\nexisting LLM-based methods. We conduct experiments on three datasets, CoNLL04 (4 relation\ntypes) (Roth and Yih, 2004), SciERC (7 relation\ntypes) (Luan et al., 2018) and our sub-sampled version of Wiki-NRE (45 relation types), which is\nthe only dataset we use in our main experiments\nthat can fit in the context window. To ensure comparison fairness, we use GPT-3.5-turbo for all the\ncompared methods.\nAs shown in Table 10, when the relation number is small (CONLL and SciERC), EDC alone\nmay not be superior to the baseline methods due\nto excluding the schema in the prompt. However,\nthrough refinement, EDC+R is able to achieve significantly better results. This may be attributed\nto the usage of the semantically rich relation descriptions in the refinement step. Specifically, it\nhelps correct two types of errors that may occur\nduring extraction: 1. the Definition step helps disambiguate homonyms, e.g., the word \"follows\" has\ntwo different meanings for \"John follows Taoism\"\nv.s. \"John follows Mary\". EDC changes the \"follows\" in \"John follows Taoism\" to \"adheres to\". 2.\n\n\nTable 9: Complete results of EDC and EDC+R on the novel fictional dataset against the baseline GenIE (Precision,\nRecall, F1 with ‘Partial’, ‘Strict’ and ‘Exact’ criteria). EDC+R only performs 1 iteration of refinement. The best\nresults are bolded. The LLM used for OIE is GPT-3.5-turbo.\n\n|Method|Partial<br>Precision Recall F1|Strict<br>Precision Recall F1|Exact<br>Precision Recall F1|\n|---|---|---|---|\n|EDC<br>EDC+R|0.731<br>0.771<br>0.751<br>0.761<br>0.782<br>0.767|0.687<br>0.704<br>0.691<br>0.733<br>0.750<br>0.738|0.702<br>0.720<br>0.707<br>0.733<br>0.750<br>0.738|\n|GenIE|0.521<br>0.547<br>0.530|0.426<br>0.443<br>0.432|0.467<br>0.483<br>0.472|\n\n\n\nTable 10: Complete results of EDC, EDC+R on CONLL, SciERC and Wiki-NRE datasets against the previous\nLLM-based approaches, CodeKGC and ChatIE. The LLMs used here are GPT-3.5-turbo to ensure comparison\nfairness. The best results are bolded.\n\n\n|Dataset|Method|Partial<br>Precision Recall F1|Strict<br>Precision Recall F1|Exact<br>Precision Recall F1|\n|---|---|---|---|---|\n|CONLL|EDC<br>EDC+R|0.536<br>0.552<br>0.543<br>**0.580**<br>**0.593**<br>**0.585**|0.481<br>0.491<br>0.485<br>**0.514**<br>**0.522**<br>**0.517**|0.503<br>0.515<br>0.509<br>**0.549**<br>**0.558**<br>**0.552**|\n|CONLL|CodeKGC<br>ChatIE|0.542<br>0.55<br>0.545<br>0.463<br>0.477<br>0.468|0.503<br>0.506<br>0.504<br>0.360<br>0.366<br>0.363|0.542<br>0.546<br>0.543<br>0.418<br>0.427<br>0.421|\n|SciERC|EDC<br>EDC+R|0.389<br>0.408<br>0.395<br>**0.447**<br>**0.461**<br>**0.451**|0.288<br>0.301<br>0.292<br>**0.340**<br>**0.349**<br>**0.343**|0.352<br>0.365<br>0.357<br>**0.406**<br>**0.416**<br>**0.410**|\n|SciERC|CodeKGC<br>ChatIE|0.389<br>0.398<br>0.392<br>0.351<br>0.367<br>0.357|0.277<br>0.283<br>0.279<br>0.212<br>0.221<br>0.215|0.346<br>0.353<br>0.349<br>0.294<br>0.302<br>0.297|\n|Wiki-NRE|EDC<br>EDC+R|0.645<br>0.651<br>0.647<br>**0.691**<br>**0.696**<br>**0.693**|0.636<br>0.640<br>0.638<br>**0.684**<br>**0.688**<br>**0.685**|0.638<br>0.643<br>0.640<br>**0.685**<br>**0.689**<br>**0.687**|\n|Wiki-NRE|CodeKGC<br>ChatIE|0.611<br>0.614<br>0.612<br>0.569<br>0.574<br>0.571|0.605<br>0.607<br>0.606<br>0.541<br>0.545<br>0.543|0.607<br>0.609<br>0.608<br>0.553<br>0.557<br>0.555|\n\n\n\nUsing the relation definitions, we find the Refinement step corrects head-tail relation errors, e.g., for\na relation \"father\", it is unclear if the subject or\nthe object is the father, and the definition prevents\ninconsistent use. This error-correcting effect was\nnot possible in previous methods.\n\nWhen tested on Wiki-NRE, which has a\nmoderately-sized schema, EDC already significantly outperforms the baseline methods, potentially due to the confusion of the LLMs when dealing with long context (Liu et al., 2024). Furthermore, we observe that ChatIE and CodeKGC may\nstill output out-of-schema relation words although\nthe entire schema is provided in the prompt, echoing the previous findings (Wadhwa et al., 2023).\n\n\n**F** **Combine EDC with other IE tools**\nEDC can be integrated with other IE tools, including chunking, coreference, and entity deduplication. This is beneficial in scenarios such\nas processing long documents that exceed the context window length of LLMs. We ran experiments\non Re-DOCRED (Tan et al., 2022) by combining\nEDC with LingMess (Otmazgin et al., 2023), a\nSOTA coreference resolution method and simple\nsentence-level chunking. We observed an increase\nof strict micro F1 score from 0.132 to 0.234, while\ndirectly prompting the LLMs only achieves 0.060.\nWe also explored the effect of entity deduplication in combination with EDC. We used\nCESI (Vashishth et al., 2018), a SOTA post-hoc\ncanonicalization method to deduplicate the entities\nin the resulting KGs from EDC. And we observe a\nslightly improved F1 score from 0.516 to 0.520 on\nthe REBEL dataset under the ‘Partial’ criteria.\n\n\n**G** **Combining OIE and Schema Definition**\nAs an attempt to reduce the cost of EDC, we explored combining the OIE and Schema Definition\nsteps. We previously separated these two steps because our preliminary experiments showed OIE to\nbe more challenging and separating the two subtasks allowed us to use a more expensive model\nfor OIE and a smaller, cheaper model for schema\ndefinition. However, separate LLM calls increases\nlatency of the pipeline (and cost if the same LLM\nis used). Also, making the LLMs output the definitions along with the extracted triples might improve consistency. In a further experiment combining EDC and Schema Definition on REBEL using\nGPT-3.5-turbo, we observed slight performance\ngains (0.516 to 0.518 under the ‘Partial’ criteria)\nand token cost reduction ( _≈_ 3k to 2k tokens per\nexample).\n\n\n",
    "kor_md": "**추출, 정의, 정규화: 지식** **그래프** **구축을** **위한** **대규모** **언어** **모델** **기반** **프레임워크**\n**Bowen Zhang** **[1]** **및 Harold Soh** **[1,2]**\n1 싱가포르 국립대학교 컴퓨터과학부, 2 NUS 스마트 시스템 연구소\n[{bowenzhang, harold}@comp.nus.edu.sg](mailto:%7Bbowenzhang%2C%20harold%7D@comp.nus.edu.sg)\n**초록**\n본 연구에서는 입력 텍스트로부터 지식 그래프 생성\n(KGC)을 위한 자동화 방법에 주목한다. 대규모 언어 모델\n(LLM)의 발전은 제로/소량 학습 프롬프팅 등을 통해 이를\nKGC에 적용하는 일련의 최근 연구를 촉발시켰다. 소규모\n도메인 특화 데이터셋에서는 성과를 거두었으나, 이러한\n모델들은 실제 응용 분야에서 흔히 접하는 텍스트로 확장\n하는 데 어려움을 겪고 있다. 주요 문제점은 기존 방법에\n서 유효한 삼원조(triplet)를 생성하기 위해 LLM 프롬프트\n에 KG 스키마를 포함시켜야 한다는 점이다. 더 크고 복잡\n한 스키마는 LLM의 컨텍스트 윈도우 길이를 쉽게 초과한\n다. 또한 고정된 사전 정의 스키마가 없는 시나리오에서는\n간결한 자체 생성 스키마로 고품질 KG를 구축할 수 있는\n방법이 필요하다. 이러한 문제를 해결하기 위해 우리는 '\n추출-정의-정규화(EDC)'라는 3단계 프레임워크를 제안한\n다: 공개 정보 추출에 이어 스키마 정의 및 사후 정규화 단\n계로 구성된다. EDC는 사전 정의된 대상 스키마가 존재\n하는 환경과 그렇지 않은 환경 모두에 적용 가능하다는 점\n에서 유연하다. 후자의 경우 자동으로 스키마를 구축하고\n자체 정규화를 적용한다. 성능 향상을 위해 입력 텍스트\n와 관련된 스키마 요소를 검색하는 학습된 구성 요소를 도\n입합니다. 이는 검색 강화 생성 방식과 유사하게 LLM의\n추출 성능을 향상시킵니다. 세 가지 KGC 벤치마크를 통\n해 EDC가 매개변수 조정 없이도 기존 연구 대비 훨씬 더\n큰 스키마로 고품질 삼원조 추출이 가능함을 입증합니다.\n\nEDC 코드는 [https://github.com/clear-nus/edc](https://github.com/clear-nus/edc) 에서 확인\n할 수 있습니다.\n\n\n**1** **서론**\n지식 그래프(KGs)(Ji et al., 2021)는 그래프 구조를 통해 상호 연\n결된 정보를 조직화하는 지식의 구조화된 표현으로, 엔티티와\n관계는 노드와 에지로 표현됩니다. 이들은 광범위하게\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-0-0.png)\n\n그림 1: 지식 그래프 구축을 위한 추출-정의-정규화(EDC)의 고수준\n개요.\n\n\n다양한 하위 작업에 활용되며, 의사 결정(Guo et al., 2021; Lan\net al., 2020), 질문응답(Huang et al., 2019; Yasunaga et al.,\n2021), 추천(Guo et al., 2020; Wang et al., 2019) 등이 포함됩니\n다. 그러나 지식 그래프 구축(KGC)은 본질적으로 어려운 과제입\n니다: 일관되고 간결하며 의미 있는 지식 그래프를 생성하기 위\n해서는 구문과 의미에 대한 이해 능력이 요구됩니다. 따라서\nKGC는 주로 집중적인 인적 노동에 의존합니다(Ye et al., 2022).\n\nKGC는 광범위한 문제이며, 본 연구에서는 **관계** **삼중항** **추출** **작**\n**업에** **초점을** **맞춥니다** . 이는 KGC에 매우 중요하기 때문입니다.\n\n\n\n**EDC:** **추출** **-** **정의** **-** **정규화**\nKGC에 핵심적이기 때문이다. 선행 연구(Ye et al., 2022;\n\nMelnyk et al., 2022; Bi et al., 2024)를 따라, 본 연구에서 다루는\n과제도 여전히 KGC로 지칭한다.\n\n최근 KGC 자동화 시도(Zhong et al., 2023; Ye et al., 2022)는\n뛰어난 자연어 이해 및 생성 능력을 고려하여 대규모 언어 모델\n(LLMs)을 활용하였다. LLM 기반 KGC 방법은 지식 그래프를\n나타내는 엔티티-관계 삼중항을 생성하기 위해 다중 회화(Wei et\nal., 2023) 및 코드 생성(Bi et al., 2024)과 같은 다양한 혁신적인\n프롬프트 기반 기법을 사용합니다. 그러나 이러한 방법들은 현재\n소규모 및 특정 도메인 시나리오로 제한되어 있습니다. 생성된\n삼중항(triplet)의 유효성을 보장하기 위해 스키마 정보(예: 가능\n한 엔티티 및 관계 유형)를 프롬프트에 포함시켜야 하기 때문입\n니다. 복잡한 데이터셋(예: 위키피디아)은 일반적으로 **컨텍스트**\n**창** **길이를** 초과하거나 LLM에 의해 무시될 수 있는 **대규모** **스키**\n**마를** 요구합니다(Wadhwa et al., 2023). 더욱이 **사전** **정의된** **스키**\n**마가** **항상** **이용** **가능한** **것은** **아닙니다** . 사용자는 사전에 관심 정\n보에 대한 명확한 의도를 가지고 있지 않을 수 있지만, 여전히 본\n질적으로 고품질의 지식 그래프를 추출하고자 할 수 있습니다.\n\n기존 방법이 이러한 상황에서 어떻게 작동할지는 불분명합니다.\n\n이러한 문제점을 해결하기 위해, 우리는 KGC를 위한 구조화\n된 접근법인 **추출-정의-정규화** ( **EDC** )를 제안한다: 핵심 아이디\n어는 KGC를 세 가지 하위 작업에 해당하는 세 가지 주요 단계로\n분해하는 것이다(그림 1):\n\n\n1. 정보 추출 개방: 입력 텍스트에서 자유롭게 엔티티-관계\n삼원조 목록을 추출합니다.\n\n\n2. 스키마 정의: 추출 단계에서 얻은 삼원조로부터 유도된 스\n키마의 각 구성 요소(예: 엔티티 유형 및 관계 유형)에 대\n한 정의를 생성합니다.\n\n\n3. 스키마 정규화: 스키마 정의를 활용하여 의미적으로 동등\n한 엔티티/관계 유형이 동일한 명사구/관계 구문을 가지도\n록 삼중항을 표준화합니다.\n\n\n각 단계는 대규모 언어 모델(LLM)의 강점을 활용합니다: 추\n출 하위 작업은 LLM이 효과적인 개방형 정보 추출기라는 최근\n연구 결과(Li et al., 2023; Han et al., 2023)를 활용합니다 —\nLLM은 의미적으로 정확하고 의미 있는 삼원조를 추출할 수 있\n습니다. 그러나 결과 삼원조는 일반적으로 중복적이고 모호한 정\n보를 포함합니다. 예를 들어, '직업', '일자리', '직책'과 같은 다중\n'직업', '직책', '업종'과 같이 의미적으로 동등한 관계 구문이 포함\n됩니다(Kamp et al., 2023; Putri et al., 2019; Vashishth et al.,\n2018).\n\n2단계와 3단계(정의 및 정규화)는 삼중항을 표준화하여 하위\n작업에 유용하게 활용할 수 있도록 합니다. EDC는 유연성을 고\n려하여 설계되었습니다: 기존에 존재하는 대규모 스키마와 일관\n된 삼중항을 발견하거나( **대상** **정렬** ) _자체적으로_ 스키마를 _생성할_\n수 있습니다( **자체** **정규화** ). 이를 위해 설명 생성 능력을 활용하여\nLLM으로 스키마 구성 요소를 정의합니다. LLM은 인간 전문가\n도 납득할 수 있는 설명을 통해 추출 결과를 정당화할 수 있습니\n다(Li et al., 2023). 정의된 스키마는 벡터 유사도 검색을 통해 가\n장 근접한 엔티티/관계 유형 후보를 찾는 데 활용되며, LLM은\n이를 참조하여 구성 요소를 정규화합니다. 기존 스키마에 대응하\n는 항목이 없는 경우, 스키마를 풍부하게 하기 위해 해당 항목을\n추가할 수 있습니다.\n\n성능을 더욱 향상시키기 위해, 위의 세 단계에 추가적인 **정제**\n단계를 수행할 수 있습니다: 초기 추출 과정에서 EDC를 반복하\n되, 이전에 추출된 삼중항과 스키마의 관련 부분을 프롬프트에\n제공합니다. 우리는 입력 텍스트와 관련된 스키마 구성 요소를\n검색하는 훈련된 **스키마** 검색기를 제안합니다. 이는 검색 강화\n생성(Lewis et al., 2020)과 유사하며, 생성된 삼중항을 개선하는\n것으로 나타났습니다.\n\n타겟 정렬 및 자체 정규화 설정에서 세 가지 KGC 데이터셋에\n대한 실험 결과, EDC는 자동 및 수동 평가를 통해 최신 방법에\n비해 더 높은 품질의 KG를 추출할 수 있음을 보여줍니다. 또한\n스키마 리트리버의 사용이 EDC의 성능을 현저하고 일관되게 향\n상시키는 것으로 나타났습니다.\n\n요약하면, 본 논문은 다음과 같은 기여를 한다:\n\n\n  - EDC는 유연하고 성능이 우수한 LLM 기반 지식 그래프\n구축 프레임워크로, 대규모 스키마 또는 사전 정의된 스키\n마 없이도 고품질 지식 그래프를 추출할 수 있습니다.\n\n\n  - 스키마 리트리버(Schema Retriever): 정보 검색과 유사한\n방식으로 입력 텍스트와 관련된 스키마 구성 요소를 추출\n하도록 훈련된 모델입니다.\n\n\n  - EDC와 스키마 검색기의 효과성을 입증하는 경험적 증거.\n\n\n**2** **배경**\n이 섹션에서는 지식 그래프 구축(KGC), 공개 정보 추출(OIE),\n정규화에 관한 관련 배경을 제공합니다.\n\n\n**지식** **그래프** **구축.** 기존 방법들은 일반적으로 엔티티 발견\n(Žukov-Gregoricˇ 외, 2018; Martins et al., 2019), 엔티티 유형 분\n류(Choi et al., 2018; Onoe and Durrett, 2020), 관계 분류(Zeng et\nal., 2014, 2015) 등으로 구성된다. 사전 훈련된 생성형 언어 모델\n(예: T5 (Raffel et al., 2020) 및 BERT(Lewis et al., 2019) 등)의\n발전 덕분에, 최근 연구들은 KGC를 시퀀스-투-시퀀스 문제로 재\n구성하고 중간 규모의 언어 모델을 미세 조정하여 관계 삼중항을\n종단 간 방식으로 생성합니다(Ye et al., 2022). 대규모 언어 모델\n(LLM)의 성공은 이 패러다임을 한 단계 더 발전시켰습니다: 현재\n방법들은 LLM에 직접 프롬프트를 제공하여 제로/소량 샷 방식\n으로 삼중항을 생성하도록 합니다. 예를 들어, ChatIE(Wei et al.,\n2023)는 작업을 다중 회전 질문-응답 문제로 구성하여 삼중항을\n추출하고, CodeKGC(Bi et al., 2024)는 작업을 코드 생성 문제로\n접근합니다. 앞서 언급한 바와 같이, 이러한 모델들은 KG 스키\n마를 LLM 프롬프트에 포함시켜야 하기 때문에 많은 실제 응용\n분야에서 흔히 볼 수 있는 일반 텍스트로 확장하는 데 어려움을\n겪습니다. 우리의 EDC 프레임워크는 사후 정규화(post-hoc\ncanonicalization)를 사용하여(기본 LLM의 미세 조정이 필요 없\n이) 이 문제를 해결합니다.\n\n\n**개방형** **정보** **추출과** **정규화.** 표준(폐쇄형) 정보 추출은 출력 삼중\n항이 사전 정의된 스키마를 따르도록 요구합니다. 예를 들어 추\n출 대상 관계 또는 엔티티 유형 목록이 이에 해당합니다. 반면,\n개방형 정보 추출(OIE)은 이러한 요구 사항이 없습니다. OIE는\n오랜 역사를 가지고 있으며, 포괄적인 내용을 원하는 독자들은\n훌륭한 서베이 논문들(Liu et al., 2022; Zhou et al., 2022; Kamp\net al., 2023)을 참고하시기 바랍니다. 최근 연구에서는 대규모\n언어 모델(LLMs)이 OIE 작업에서 탁월한 성능을 보인다는 사실\n이 밝혀졌습니다(Li et al., 2023). 그러나 OIE 시스템에서 추출\n된 관계 삼중항은 정규화되지 않습니다. 의미적으로 동등한 여러\n관계가 정규형으로 통합되지 않은 채 공존할 수 있어 유도된 개\n방형 지식 그래프에 중복성과 모호성을 초래합니다. 삼중항을 표\n준화하기 위해서는 추가적인 정규화 단계가 필요합니다.\n\n\n\n하류 응용 분야에 유용한 KGs.\n\n표준화 방법은 대상 스키마의 유무에 따라 달라집니다. 대상\n스키마가 존재하는 경우, 이 작업은 때때로 \"정렬(alignment)\"이\n라고도 불립니다(Putri et al., 2019). 예를 들어, Putri 등(2019)은\nOIE에서 추출한 관계 구문에 대한 정의를 얻기 위해\nWordNet(Miller, 1995)을 부가 정보로 사용하고, OIE 관계 정의\n와 대상 스키마의 사전 정의된 관계를 비교하기 위해 시아미즈\n네트워크를 사용합니다. 대상 스키마를 사용할 수 없는 경우, 최\n첨단 방법은 일반적으로 클러스터링을 기반으로 합니다\n(Vashishth 외, 2018; Dash 외, 2020). CESI(Vashishth et al.,\n2018)는 PPDB(Ganitkevitch et al., 2013) 및 WordNet과 같은 외부\n소스의 부가 정보를 사용하여 OIE 관계에 대한 임베딩을 생성합\n니다. 그러나 클러스터링 기반 방법은 과도한 일반화에 취약합니\n다(Kamp et al., 2023; Putri et al., 2019)에 취약합니다. 예를 들어\n, CESI는 \"is brother of\", \"is son of\", \"is main villain of\", \"was\nprofessor of\"를 동일한 관계 클러스터에 포함시킬 수 있습니다.\n\n기존의 정규화 방법과 비교하여 EDC는 더 일반적입니다. 대\n상 스키마가 제공되든 안 되든 작동합니다. WordNet과 같은 정\n적 외부 소스를 사용하는 대신, EDC는 대규모 언어 모델(LLM)\n이 생성한 문맥적이며 의미적으로 풍부한 부가 정보를 활용합니\n다. 또한, 변환이 수행 가능한지 여부를 LLM이 검증하도록 허용\n함으로써(단순히 임베딩 유사성에만 의존하지 않음), EDC는 기\n존 방법이 직면한 과도한 일반화 문제를 완화합니다.\n\n\n**3** **방법: KGC를** **위한 EDC**\n본 절에서는 우리의 주요 기여 사항인 대규모 언어 모델(LLM)을\n구조화된 방식으로 활용하여 지식 그래프를 구축하는 접근법을\n개괄한다. 먼저 EDC 프레임워크를 상세히 설명한 후 정제\n( **EDC+R** )에 대해 기술한다. 입력 텍스트를 주어졌을 때, 우리의\n목표는 결과 지식 그래프(KG)의 모호성과 중복성을 최소화할 수\n있도록 표준화된 형태의 관계 삼중항을 추출하는 것이다. 사전\n정의된 대상 스키마가 존재할 경우, 생성된 모든 삼중항은 해당\n스키마를 준수해야 한다. 스키마가 없는 경우, 시스템은 동적으\n로 스키마를 생성하고 이를 기준으로 삼아 삼원조를 표준화해야\n합니다.\n\n\n**3.1** **EDC: 추출-정의-정규화**\nEDC는 KGC를 세 가지 연결된 하위 작업으로 분해합니다. 논의\n를 구체화하기 위해 특정 입력 텍스트 예시를 사용하겠습니다: \"\n_앨런셰퍼드는 1923년 11월 18일에태어났으며_\n_그는아폴로 14호승무원중한명이었습니다_ .\" 각 단계를 차례\n로 살펴보겠습니다:\n\n\n**1단계:** **공개** **정보** **추출:** 먼저 대규모 언어 모델(LLM)을 활용해\n공개 정보 추출을 수행합니다. 소량 데이터 프롬프팅을 통해\nLLM은 특정 스키마에 구애받지 않고 입력 텍스트에서 관계 삼원\n조([주체, 관계, 객체])를 식별 및 추출합니다. 위 예시를 적용하면\n프롬프트는 다음과 같습니다:\n\n\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-3-1.png)\n\n추출된 삼중항( [‘Alan Shepard’, ‘bornOn’, ‘Nov 18,\n1923’], [‘Alan Shepard’, ‘participatedIn’, ‘Apollo 14’] )은 _개방_\n_형지식그래프(KG)_ 를 형성하며, 이는 후속 단계로 전달됩니다.\n\n\n**2단계: 스키마** **정의:** 다음으로, LLM에게 오픈 KG에서 유도된\n스키마의 각 구성 요소에 대한 자연어 정의를 제공하도록 요청\n합니다:\n\n\n\n이 예시 프롬프트는 ( bornOn: 주체 엔티티가 객체 엔티티가\n지정한 날짜에 태어남) 및 ( participatedIn: 주체 엔티티가 객체\n엔티티가 지정한 사건 또는 임무에 참여함)에 대한 정의를 생성\n하며, 이는 정규화를 위한 _부가정보로_ 다음 단계로 전달됩니다.\n\n\n**3단계: 스키마** **정규화:** 세 번째 단계는 중복과 모호성을 제거하\n여 개방형 지식 그래프를 정규화된 형태로 정제하는 것을 목표로\n합니다. 먼저 문장 변환기를 사용하여 각 스키마 구성 요소의 정\n의를 벡터화하여 임베딩을 생성합니다. 이후 대상 스키마의 유무\n에 따라 두 가지 방식 중 하나로 정규화가 진행됩니다:\n\n\n  - 대상 정렬: 기존 대상 스키마를 바탕으로 각 요소 내에서\n가장 밀접하게 관련된 구성 요소를 식별하여 정규화 대상\n으로 고려합니다. 과도한 일반화 문제를 방지하기 위해 대\n규모 언어 모델(LLM)은 각 잠재적 변환의 실행 가능성을\n평가합니다. 변환이 비합리적이라고 판단될 경우(대상 스\n키마 내 의미적 동등체가 존재하지 않음을 시사함), 해당\n구성 요소와 관련된 삼중항은 제외됩니다.\n\n\n  - 자체 정규화: 대상 스키마가 부재할 경우, 의미적으로 유\n사한 스키마 구성 요소를 통합하여 단일 표현으로 표준화\n함으로써 지식 그래프를 간소화하는 것이 목표입니다. 빈\n정규화 스키마를 시작으로, 공개 지식 그래프 트리플을 검\n토하며 벡터 유사도와 LLM 검증을 통해 잠재적 통합 후보\n를 탐색합니다. 대상 정렬과 달리, 변환 불가능한 것으로\n판단된 구성 요소는 정규화 스키마에 추가되어 이를 확장\n합니다.\n\n\n예시를 사용한 프롬프트는 다음과 같습니다:\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-3-2.png)\n\n\n\n\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-3-3.png)\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-4-0.png)\n\n\n\n\n\n위 선택지는 벡터 유사도 검색을 통해 도출된 것임을 유의하\n십시오. LLM이 선택을 완료한 후 관계는 다음과 같이 변환됩니\n다:\n\n\n[‘Alan Shepard’, ‘birthDate’, ‘Nov 18, 1923’],\n[‘Alan Shepard’, ‘mission’, ‘Apollo 14’], 이는 우리의 정규화\n된 지식 그래프를 형성합니다.\n\n\n**3.2** **EDC+R: 스키마** **검색기를** **통해 EDC를** **반복적으**\n**로** **정제합니다.**\n\n\n정제 과정은 추출된 삼중항(triplet)의 품질을 향상시키기 위해\nEDC가 생성한 데이터를 활용합니다. 검색 강화 생성(retrieval\naugmented generation) 및 선행 연구(Bi et al., 2024)에서 영감을\n받아, 추출 단계에 대한 \"힌트\"를 구성합니다(자세한 내용은 부\n록 A.4 참조). 이 힌트는 두 가지 주요 요소로 구성됩니다:\n\n\n  - 후보 엔티티: 이전 반복에서 EDC가 추출한 엔티티와\nLLM을 사용하여 텍스트에서 추출한 엔티티;\n\n\n  - 후보 관계: 이전 사이클에서 EDC가 추출한 관계와 훈련\n된 스키마 검색기를 사용하여 사전 정의/정규화된 스키마\n에서 검색된 관계.\n\n\nLLM과 스키마 검색기에서 추출된 엔티티 및 관계를 모두 포함\n함으로써 LLM에 더 풍부한 후보 풀을 제공하며, 이는 엔티티나\n관계의 부재로 인해 LLM의 효율성이 저하되는 문제를 해결합니\n다. 이전 단계에서 추출된 엔티티와 관계를 엔티티 추출 및 스키\n마 검색의 새로운 결과와 병합함으로써, 이 힌트는 이전 라운드\n의 결과를 기반으로 부트스트랩하여 OIE를 지원합니다.\n\nEDC를 대규모 스키마로 확장하기 위해, 우리는 훈련된 스키\n마 검색기를 활용하여 스키마를 효율적으로 검색합니다. 스키마\n검색기는 벡터 공간 기반 정보 검색 방법(Ganguly et al., 2015;\n\nLewis et al., 2020)과 유사한 방식으로 작동합니다. 스키마 구성\n요소와 입력 텍스트를 벡터 공간으로 투영하여 코사인 유사도가\n두 요소 간의 관련성, 즉 스키마 구성 요소가 입력 텍스트에 존재\n할 가능성을 포착하도록 합니다.\n\n\n\n우리의 설정에서 유사도 공간은 벡터 공간 내 코사인 유사도로\n의미적 동등성을 포착하는 표준 문장 임베딩 모델과 다릅니다.\n\n우리의 스키마 검색기는 문장 임베딩 모델 E5-mistral-7b\ninstruct(Wang et al., 2023)의 미세 조정된 변형입니다. 본 논문에\n서 상세히 설명된 원본 훈련 방법론을 따릅니다. 이는 텍스트 쌍\n과 그에 대응하는 정의된 관계 쌍을 활용하는 것을 포함합니다.\n\n자세한 내용은 부록 A.3을 참조하십시오. 주어진 긍정적 텍스트\n관계 쌍 ( _t_ [+] _, r_ [+] )에 대해, _t_ [+] 에 대한 지시문 템플릿을 사용하여 새\n로운 텍스트를 생성합니다\n주어진 텍스트 _\\n_ 쿼리: _{t_ [+] _}_ ”.\n\n그런 다음 InfoNCE 손실을 사용하여 임베딩 모델을 미세 조\n정하여 주어진 텍스트와 관련된 올바른 관계와 다른 관련 없는\n관계를 구별합니다.\n\n예시로 돌아가면, 스키마 검색기를 통한 정제 과정은 기존 관\n계 집합에 다음과 같은 관계를 추가합니다: [‘Alan Shepard’,\n‘selectedByNasa’, ‘1959’] . ‘selectedByNasa’ 관계는 다소 생\n소하지만 대상 스키마에 명시된 내용입니다.\n\n\n**4** **실험**\n이 섹션에서는 EDC와 EDC+R의 성능을 평가하기 위해 설계된\n실험을 설명합니다. 간단히 말해, 우리의 결과는 EDC가 타겟 정\n렬 및 자체 정규화 설정 모두에서 최신 기법을 크게 능가함을 보\n여줍니다. 정교화는 EDC를 더욱 향상시킵니다. EDC의 소스 코\n드와 실험 재현을 위한 자료는 부록 C의 전체 표와 함께 보충 자\n료에서 확인할 수 있습니다.\n\n\n**4.1** **실험** **설정**\n**데이터셋.** 우리는 세 가지 KGC 데이터셋을 사용하여 EDC를 평\n가합니다:\n\n\n  - WebNLG (Ferreira et al., 2020): WebNLG+2020 (v3.0)의\n의미 분석 작업에서 테스트 분할을 사용합니다. 여기에는\n텍스트와 삼중항 쌍 1165개가 포함됩니다. 이러한 참조 삼\n중항에서 도출된 스키마는 159개의 고유한 관계 유형을\n포괄합니다.\n\n\n  - REBEL (Cabot and Navigli, 2021): REBEL의 원래 테스\n트 분할은 105,516개의 항목으로 구성됩니다. 비용 관리를\n위해 1000개의 텍스트-트리플렛 쌍을 무작위로 선택합니\n다. 이 하위 집합은 200개의 고유한 관계 유형을 가진 스키\n마를 유도합니다.\n\n\n\n_t_\n+\n_inst_ = “지시: 존재하는 관계를 검색하라\n\n\n  - Wiki-NRE (Distiawan et al., 2019): Wiki-NRE의 테스트\n분할(29,619개 항목)에서 1000개의 텍스트-트리플렛 쌍\n을 샘플링하여 45개의 고유한 관계 유형을 가진 스키마를\n유도합니다.\n\n\n이 데이터셋들은 ADE(Gurulingappa et al., 2012)(관계 유형\n1종), SciERC(Luan et al., 2018) (7가지 관계 유형),\nCoNLL04(Roth and Yih, 2004)(4가지 관계 유형) 등 기존 LLM\n기반 방법 평가에 사용된 대안들보다 관계 유형의 다양성이 풍부\n하기 때문입니다. 이러한 다양성은 실제 세계의 복잡성을 더 잘\n모방합니다. 본 실험에서는 모든 데이터셋에 걸쳐 유일하게 활용\n가능한 스키마 구성 요소로서 관계 추출에 집중한다. 관계는 지\n식 그래프의 기초 요소이므로, 엔티티나 이벤트 유형 같은 다른\n구성 요소보다 우선시된다. 다만 EDC는 다른 스키마 구성 요소\n로도 쉽게 확장 가능하다는 점을 유의해야 한다.\n\n\n**EDC 모델.** EDC는 대규모 언어 모델(LLM)로 구동되는 여러 모\n듈을 포함합니다. OIE 모듈은 KG에 포착되는 의미적 내용을 결\n정하는 핵심 상류 모듈이므로, GPT-4(Achiam et al., 2023),\nGPT-3.5-turbo(Brown et al., 2020), Mistral-7b(Jiang et al.,\n2023) 등 다양한 크기의 LLM을 테스트했습니다. Mistral-7b는\n로컬 워크스테이션에 배포된 반면, GPT 모델들은 OpenAI API\n를 통해 접근했습니다. 프롬프트가 필요한 프레임워크의 나머지\n구성 요소에는 GPT-3.5-turbo를 사용했습니다. 정규화 단계에서\n는 수정 없이 벡터 유사도 검색을 위해 E5-Mistral-7b 모델을 활\n용했습니다.\n\n\n**4.1.1** **평가** **기준** **및** **기준선**\n타깃 정렬(스키마 제공 시)과 자체 정규화(스키마 없음) 환경에서\n본 방법론을 별도로 평가합니다. 이는 _본질적으로다른목표를_\n지향하기 때문입니다: 전자는 타깃 스키마와 일치하는 정답 주석\n삼중항을 복원하는 반면, 후자는 사전 정의된 비교 대상 없이 간\n결하고 중복 없는 지식 그래프를 유도하는 의미론적으로 정확하\n고 의미 있는 삼중항을 추출하는 데 목적이 있습니다. 상기 데이\n터셋의 경우, 기존 LLM 기반 KGC 방법(ChatIE 및 CodeKGC)\n은 스키마 규모로 인해 적용할 수 없었습니다. EDC는 소규모 도\n메인 특화 데이터셋을 대상으로 하지 않지만, 평가의 포괄성을\n위해 SciERC 및 CoNLL04에 대한 결과를 부록 E에 포함합니다.\n\n\n**목표** **정렬.** 각 데이터셋에 대해 EDC와 EDC+R을 해당 데이터셋\n전용으로 훈련된 모델과 비교합니다.\n\n\n\n각 데이터셋별로:\n\n\n  - **REGEN** (Dognin et al., 2021)은 웹 자연어 생성\n(WebNLG) 분야의 최첨단 모델(SOTA)입니다. 이 모델은\n사전 훈련된 T5(Raffel et al., 2020)와 강화 학습(RL)을 활\n용하여 양방향 텍스트-그래프 및 그래프-텍스트 생성을 수\n행하는 시퀀스-투-시퀀스 모델입니다.\n\n\n  - **GenIE** (Josifoski et al., 2022)는 사전 훈련된\nBART(Lewis et al., 2019)와 제약 생성 전략을 활용하여\n출력 삼중항이 사전 정의된 스키마와 일관되도록 제한하\n는 시퀀스-투-시퀀스 모델입니다. GenIE는 REBEL 및\nWiki-NRE 분야의 최신 기술 수준 모델입니다.\n\n\n기존 연구(Dognin et al., 2021; Melnyk et al., 2022)를 따라, 토큰\n기반 방식으로 출력 삼중항과 정답 간의 정밀도(Precision), 재현\n율(Recall), F1 점수를 계산하는 WEBNLG 평가 스크립트\n(Ferreira et al., 2020)를 사용합니다. 명명된 엔티티 평가를 기반\n으로 한 메트릭을 사용하여 세 가지 다른 방식으로 정밀도, 재현\n율 및 F1 점수를 측정했습니다.\n\n\n  - _정확:_ 후보와 참조 트리플이 유형(주어, 관계, 목적어)을\n무시하고 완전히 일치해야 합니다.\n\n\n  - _부분일치:_ 유형을 무시하고 후보 트리플과 참조 트리플\n간의 최소한 부분 일치를 허용합니다.\n\n\n  - _엄격:_ 후보 트리플렛과 참조 트리플렛 간에 요소 유형을\n포함한 정확한 일치를 요구합니다.\n\n\n**자기** **정규화.** 자기 정규화 성능 평가를 위해 다음 항목들과 비교\n를 수행합니다:\n\n\n  - **기준** **오픈 KG** : OIE(Open Information Extraction) 단계에\n서 생성된 초기 오픈 KG 출력입니다. 이는 정규화 과정으\n로 인한 정밀도와 스키마 간결성의 변화를 설명하기 위한\n기준점이 됩니다.\n\n\n  - **CESI** (Vashishth et al., 2018): 오픈 KG 정규화를 위한 선\n도적인 클러스터링 기반 접근법으로 인정받고 있습니다.\n\nCESI를 오픈 KG에 적용함으로써 EDC에 의한 정규화 성\n능과 대비를 도모합니다.\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-6-0.png)\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-6-1.png)\n\n\n\n\n\n\n\n0.4\n0.3\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-6-2.png)\n\n\n\n0.2\n그림 2: 대상 정렬 설정에서 WebNLG, REBEL 및 Wiki-NRE 데이터셋에 대한 EDC 및 EDC+R의 성능을 기준 모델과 비교한 결과(‘부분’ 기\n준을 적용한 F1 점수). EDC+R은 한계 개선 효과가 감소함에 따라 정제 과정을 한 번만 수행합니다.\n\n\n\n정규화된 삼중항은 참조 삼중항과 표현이 다르거나 스키마 외 관\n계일 수 있으므로, 토큰 기반 평가는 부적합합니다. 따라서 추출\n된 지식 그래프의 본질적 품질을 반영하는 세 가지 핵심 측면에\n초점을 맞춘 수동 평가를 수행합니다:\n\n\n  - _정밀도:_ 정규화된 삼중항은 OIE 삼중항에 비해 텍스트에\n대해 정확하고 의미 있는 상태를 유지합니다.\n\n\n  - _간결성:_ 스키마의 간결성은 관계 유형의 수로 측정됩니다.\n\n\n  - _중복성:_ 우리는 중복 점수를 사용합니다\n\n  - 각 정규화된 관계와 가장 가까운 대응 관계 간의 평균\n코사인 유사도 — 를 사용하며, 낮은 점수는 스키마의 관\n계들이 의미적으로 구별된다는 것을 나타냅니다.\n\n\n**4.2** **결과** **및** **분석**\n다음에서는 주요 발견 사항과 결과를 전달하는 데 중점을 둡니\n다. 전체 결과와 표는 부록을 참조하십시오.\n\n\n**4.2.1** **목표** **정렬**\n그림 2의 막대 그래프는 OIE를 위해 서로 다른 LLM을 사용한 세\n데이터셋 전체에서 EDC와 EDC+R이 얻은 부분 F1 점수를 각\n기준 모델과 비교하여 요약한 것입니다. **EDC는** **평가된** **모든** **데**\n**이터셋에서** **최첨단** **기준** **모델보다** **우수하거나** **동등한** **성능을** **보**\n**여줍니다** . LLM 간 비교 시 GPT-4가 최상위 성능을 보였으며,\nMistral-7b와 GPT-3.5-turbo는 유사한 결과를 나타냈습니다.\n\nREBEL 및 Wiki-NRE 데이터셋에서는 본 방법론과 기준 모델\n간의 격차가 더욱 두드러졌습니다.\n\n\n\n데이터셋; 이는 주로 GenIE의 제한된 생성 접근 방식 때문인데,\n숫자나 날짜 같은 리터럴을 포함하는 삼중항 추출에 미흡합니다.\n\n\n**정제(EDC+R)는** **일관되고** **유의미하게** **성능을** **향상시킵니다** .\n\n정제 후 GPT-3.5-turbo와 Mistral-7b 간의 성능 차이는 더 커져,\nMistral-7b가 제공된 힌트를 활용하는 능력이 상대적으로 부족했\n음을 시사합니다. 그럼에도 불구하고 힌트를 활용한 단일 정제 반\n복은 테스트된 모든 대규모 언어 모델(LLM)의 성능을 개선했습\n니다.\n\n\n점수 분석 결과, EDC 성능은 REBEL 및 Wiki-NRE 대비\nWebNLG에서 현저히 우수한 것으로 나타났습니다. 그러나 후자\n데이터셋에서 유효한 삼중항을 생성했음에도 EDC가 감점을 받\n는 현상을 관찰했습니다. 그 이유는 해당 데이터셋의 참조 삼중\n항이 완전하지 않기 때문입니다. 예를 들어, REBEL 데이터셋의\n텍스트 _'Romany Love는프레드폴이감독하고에스몬드나이트,_\n_플로렌스맥휴, 로이트래버스가출연한 1931년영국뮤지컬영_\n_화이다_ .'를 고려해 보겠습니다. EDC는 [‘Romany Love’, ‘ 출연\n진 ’, ‘Esmond Knight’], [‘Romany Love’, ‘ 출연진 ’, ‘Florence\nMcHugh’], [‘Romany Love’, ‘ 출연진 ’, ‘Roy Travers’] 를 추출\n하는데, 이는 모두 의미론적으로 정확하지만 참조 세트에는 첫\n번째 삼중항만 존재합니다. 데이터셋에는 텍스트와 무관한 정보\n를 기반으로 한 참조 삼중항도 포함되어 있습니다. 예를 들어,\n_‘Daniel is an Ethiopian foot-baller, who currently plays for_\n_Hawassa City S._ C.’에는 대응하는 참조 삼중항 [‘Hawassa City\nS.C.’, ‘country’, ‘Ethiopia’] 이 있습니다.\n\n\n이러한 문제는 해당 데이터셋 생성 시 채택된 서로 다른 방법\n론에서 기인할 수 있습니다. WebNLG의 경우, 어노테이터들에\n게 다음과 같은 작업을 요청했습니다.\n\n\n표 1: 스키마 리트리버에 대한 제거 연구 결과(모든 기준에 따른 F1\n점수). OIE에 사용된 LLM은 GPT-3.5-turbo입니다. S.R.은 스키마\n리트리버(Schema Retriever)를 의미합니다.\n\n|데이터셋|방법|부분 엄격 정확|\n|---|---|---|\n|웹NLG|EDC+R<br>EDC+R (S.R.제외)|**0.794**<br>**0.753**<br>**0.772**<br>0.752<br>0.701<br>0.721|\n|웹NLG|EDC|0.746<br>0.688<br>0.713|\n|REBEL|EDC+R<br>EDC+R (S.R.제외)|**0.559**<br>**0.516**<br>**0.529**<br>0.517<br>0.466<br>0.482|\n|REBEL|EDC|0.506<br>0.449<br>0.473|\n|위키-NRE|EDC+R<br>EDC+R (S.R.제외)|**0.693**<br>**0.685**<br>**0.657**<br>0.653<br>0.645<br>0.641|\n|위키-NRE|EDC|0.647<br>0.638<br>0.640|\n\n\n\n텍스트를 오직 삼중항만으로 구성한다. 따라서 텍스트와 삼중항\n은 직접적인 대응 관계를 가지며, 텍스트는 일반적으로 삼중항\n에서 명백히 드러나는 정보 외에는 포함하지 않는다. 반면\nREBEL과 Wiki-NRE는 원격 감독(Smirnova and Cudré\nMauroux, 2018)을 사용하여 텍스트와 삼중항을 정렬함으로써\n생성된다. 이러한 접근법은 삼중항 추출이 덜 직관적이고 참조\n세트가 불완전해질 수 있어, 데이터셋에 존재하지 않는 올바른\n삼중항을 생성하는 EDC와 같은 방법에 대해 지나치게 비관적인\n평가를 초래할 수 있습니다(Han et al., 2023; Wadhwa et al.,\n2023). 평균적으로 EDC는 REBEL 및 Wiki-NRE에서 참조 세트\n대비 문장당 하나의 추가 트리플렛을 추출하는 반면, WebNLG\n에서는 참조 세트와 유사한 수의 트리플렛을 추출합니다.\n\n\n**스키마** **검색기의** **제거** **연구.** 정제 과정에서 스키마 검색기가 제\n공하는 관계의 영향을 평가하기 위해, GPT-3.5-turbo를 사용하\n여 이러한 관계를 제거하는 제거 연구를 수행했습니다. 표 1의 결\n과는 **스키마** **검색기를** **제거하면** **성능이** **저하됨을** 보여줍니다. 질\n적으로 살펴보면, 스키마 리트리버는 OIE 단계에서 LLM이 식\n별하기 어려운 관련 관계를 찾는 데 도움이 되는 것으로 나타났\n습니다. 예를 들어, _'디종의부르고뉴대학에는 16,800명의학부_\n_생이_ 있다'는 텍스트에서 LLM은 OIE 단계에서 [‘ 부르고뉴 대학\n’, ‘ 위치 ’, ‘ 디종 ’] 관계를 추출합니다. 의미론적으로 정확하지만,\n이 관계는 대상 스키마에 존재하는 대학 위치를 나타내는 보다\n구체적인 관계인 ‘ 캠퍼스 ’ 를 간과합니다. 스키마 검색기는 이 더\n세분화된 관계를 성공적으로 식별하여, LLM이 추출 결과를 [‘ 부\n르고뉴 대학교 ’, ‘ 캠퍼스 ’, ‘ 디종 ’] 으로 조정할 수 있게 합니다. 이 실\n험은 스키마 검색기가 정확하고 문맥에 적합한 관계 추출을 용이\n하게 하는 데 있어 그 가치를 부각시킵니다.\n\n\n\n표 2: 자체 정규화 설정에서의 EDC 성능 (인간 평가 정밀도 및 스키\n마 지표). 각 데이터셋 및 지표별 최고 결과는 굵게 표시됨. Prec.는\n정밀도(Precision), No. Rel.는 관계 수(Number of Relations), Red.는\n중복도 점수(Redundancy Score)를 의미함.\n\n\n\n\n\n\n\n|데이터셋|방법|정밀도(↑) 관계 관계 수(↓) 감소(↓)<br>수|\n|---|---|---|\n|웹NLG|EDC<br>CESI<br>|**0.956**<br>**200**<br>**0.833**<br>0.724<br>280<br>0.893<br><br><br>|\n|웹NLG|오픈 KG<br>|0.982<br>529<br>0.927<br><br><br>|\n|REBEL|EDC<br>CESI<br>|**0.867**<br>**225**<br>**0.831**<br>0.504<br>307<br>0.854<br><br><br>|\n|REBEL|오픈 KG<br>|0.903<br>667<br>0.895<br><br><br>|\n|Wiki-NRE|EDC<br>CESI<br>|**0.898**<br>**106**<br>**0.833**<br>0.753<br>114<br>0.849|\n|Wiki-NRE|오픈 KG|0.909<br>204<br>0.881|\n\n\n**4.2.2** **자체** **정규화**\n여기서는 EDC의 자체 정규화 성능 평가(OIE에 GPT-3.5-turbo\n활용)에 초점을 맞춥니다. 자체 정규화 설정에서의 정교화는 이\n미 상기 연구 및 후속 반복 과정에서 다루어졌으며, 자체 구축된\n정규화 스키마가 목표 스키마가 되므로 생략합니다. 기존 연구\n(Wadhwa et al., 2023; Kolluru et al., 2020)를 따라 지식 그래프\n에 대한 타깃 인간 평가를 수행했습니다. 이 평가는 시스템 세부\n사항에 대한 사전 지식 없이 주어진 텍스트에서 추출된 삼중항\n(triplet)의 타당성을 평가하는 두 명의 독립적인 주석자가 참여\n했습니다. 주석자 간 높은 일치도 점수(0.94)를 관찰했습니다.\n\n평가 결과와 스키마 메트릭스는 표 2에 요약되어 있다. OIE\n단계에서 생성된 오픈 지식 그래프는 의미적으로 유효한 삼원조\n(triplet)를 포함하고 있으나(이는 LLM이 유능한 오픈 정보 추출\n기라는 기존 연구 결과(Li et al., 2023)를 재확인함), 결과 스키마\n내에는 상당한 수준의 중복이 존재한다. **EDC는** **오픈 KG를** **정**\n**확하게** **정규화하여 CESI에** **비해** **더** **간결하고** **중복이** **적은** **스키**\n**마를** **생성합니다** . EDC는 CESI의 과도한 일반화 경향을 피합니\n다. 기존 연구(Putri 외, 2019)와 마찬가지로, CESI가 ' 사망 장소 ',\n' 출생 장소 ', ' 사망 날짜 ', ' 출생 날짜 ', ' 사망 원인 ' 과 같은 다양한\n관계를 부적절하게 단일 ' 사망 날짜' 범주로 클러스터링하는 것\n을 관찰했습니다.\n\n\n**5** **결론**\n본 연구에서는 LLM 기반의 3단계 프레임워크인 EDC를 제시하\n여, 개방형 정보 추출과 사후 표준화를 통해 KGC 문제를 해결\n합니다. 실험 결과는\nEDC와 EDC+R은 대상 스키마가 제공될 경우 전문적으로 훈련\n된 모델보다 우수한 지식 그래프(KG)를 추출할 수 있으며, 스키\n마가 제공되지 않을 경우 동적으로 스키마를 생성할 수 있습니다\n. EDC의 확장성과 다용도성은 다양한 응용 분야에 많은 기회를\n열어줍니다: 위키데이터(Wikidata)와 같은 대규모 스키마\n(Vrandecˇic´ and Krötzsch, 2014)를 사용하여 일반 텍스트에서\n고품질 KG를 자동으로 추출할 수 있을 뿐만 아니라, 새로 발견된\n관계로 이러한 스키마를 보강할 수도 있습니다.\n\n\n**6** **제한** **사항** **및** **향후** **연구** **방향**\n향후 연구에서 해결하고자 하는 몇 가지 한계점이 있습니다.\n\n\n  - 본 논문에서는 스키마 정규화만을 고려하였으나, 향후 구\n축된 지식 그래프의 중복성을 줄이기 위해 엔티티 중복 제\n거 메커니즘을 통합하는 것이 매우 중요합니다. 예를 들어,\n공동참조 해결(Sukthanker et al., 2020)을 통해 이를 구현\n할 수 있습니다. 본 연구에서는 이 접근법을 간략히 탐구하\n였으며, 예비 결과는 부록 F에서 확인할 수 있습니다.\n\n\n  - EDC의 구성 요소는 성능 향상을 위해 추가 개선이 가능합\n니다. 특히 스키마 검색기는 더 다양하고 고품질의 데이터\n로 훈련할 경우 성능 향상에 도움이 될 수 있습니다.\n\n\n  - 시간 및 자원 제약으로 인해 OIE 모듈에만 다양한 대규모\n언어 모델(LLM)을 테스트했으며, EDC의 다른 모든 모듈\n은 GPT-3.5-turbo에 의존하고 있습니다. 따라서 소규모 오\n픈소스 모델의 성능을 다른 작업에서도 테스트하는 것이\n유용할 것입니다.\n\n\n  - EDC는 다수의 LLM 호출을 포함하는 비용이 많이 드는\n프레임워크입니다. 모든 구성 요소에 GPT-3.5-turbo를 사\n용할 경우 비용은 약\n실험에서 예시당 0.009 USD가 소요되었습니다. 특정 구\n성 요소를 더 작은 미세 조정 모델로 대체할 수 있습니다.\n\n기존 연구에서는 더 작은 언어 모델을 OIE에 미세 조정할\n수 있음을 보여주었으며(Wadhwa et al., 2023), 더 작은\nBERT 기반 분류기를 스키마 정규화를 위해 훈련할 수 있\n음을 보여주었습니다. 또한 부록 G에서 OIE와 스키마 정\n의의 두 단계를 결합할 가능성도 탐구했습니다.\n\n\n  - 우리는 EDC를 구현형 AI 및 로봇 공학에 적용하고자 합니\n다. 구체적으로 KG는 VLM의 기억 소스를 형성할 수 있\n으며, 여기에는\n인간(Zhang and Soh, 2023), 작업 또는 목표(Xie et al.,\n2023), 환경에 관한 사실들을 포함합니다.\n\n\n**7** **윤리적** **고려** **사항**\n**아티팩트** **사용.** 본 논문에서 사용한 데이터셋은 연구 목적으로만\n활용되었으며, 해당 라이선스(예: WebNLG는 cc-by-nc-sa-4.0\n적용)를 엄격히 준수합니다. 작업의 특성상 데이터셋에는 개인(\n특히 유명인)에 대한 정보가 내재적으로 포함될 수 있음을 유의\n하시기 바랍니다. 본 논문의 소프트웨어 및 코드는\n[https://github.com/clear](https://github.com/clear-nus/edc) -nus/edc에서 공개적으로 이용 가능합\n니다.\n\n[//github.com/clear-nus/edc](https://github.com/clear-nus/edc) 에서 공개적으로 이용 가능합니\n다.\n\n\n**인간** **어노테이터.** 두 명의 어노테이터(남성 1명, 여성 1명)는 모\n집된 대학생입니다. 어노테이터에게는 공정한 보상이 지급되었\n으며, 작업을 완료하기 위해 충분하고 유연한 시간이 제공되었습\n니다. 수집 프로토콜은 우리 기관의 IRB 위원회에 의해 면제 대\n상으로 결정되었습니다.\n\n\n**잠재적** **위험.** 현재의 대규모 언어 모델(LLM) 사용은 환각 현상\n(Xu et al., 2024) 및 개인정보 문제(Yao et al., 2024)와 같은 위\n험을 초래할 수 있습니다.\n\n\n**감사의** **말**\n본 연구는 싱가포르 국가연구재단(National Research Foundation\nSingapore) 및 DSO 국립연구소(DSO National Laboratories)의\nAI 싱가포르 프로그램(AISG Award No: AISG2-RP-2020-016)\n의 지원을 받았습니다.\n\n\n**참고문헌**\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge\nAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anadkat 외. 2023. GPT-4 기\n술 보고서. _arXiv 사전인쇄본 arXiv:2303.08774_ .\n\n\n오신 아가르왈, 헤밍 게, 시아막 샤케리, 라미 알-르푸. 2020. 지식 강\n화 언어 모델 사전 훈련을 위한 지식 그래프 기반 합성 코퍼스 생\n성. _arXiv 사전인쇄본 arXiv:2010.12688_ .\n\n\nZhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen,\nNingyu Zhang. 2024. Codekgc: 생성적 지식 그래프 구축을 위한\n코드 언어 모델. _ACM Transactions on Asian and Low-Resource_\n_Language Information Processing_, 23(3):1–16.\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\nD Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell 외. 2020. 언어 모델은 소\n량 학습 학습자이다.\n\n\n학습자이다. _신경정보처리시스템의진전_, 33:1877–1901.\n\n\n페레-루이스 위게 카보트와 로베르토 나빌리. 2021. Rebel: 종단간\n언어 생성을 통한 관계 추출. _계산언어학회연구성과: EMNLP_\n_2021_, 2370–\n2381.\n\n\n최은솔, 오머 레비, 최예진, 루크 제틀-모이어. 2018. 초정밀 엔티티\n타이핑. _arXiv 사전인쇄본 arXiv:1807.04905_ .\n\n\nSarthak Dash, Gaetano Rossiello, Nandana Mihin-dukulasooriya,\nSugato Bagchi, Alfio Gliozzo. 2020. 변분 자동 인코더를 이용한\n오픈 지식 그래프 정규화. _arXiv 사전인쇄본 arXiv:2012.04780_ .\n\n\nBayu Distiawan, Gerhard Weikum, Jianzhong Qi, and Rui Zhang.\n\n2019. 지식 기반 확충을 위한 신경망 기반 관계 추출. _제57회컴_\n_퓨터언어학회연차총회논문집_, 229–240쪽.\n\n\nPierre L Dognin, Inkit Padhi, Igor Melnyk, Payel Das. 2021. Regen:\n\n사전 훈련된 언어 모델을 활용한 텍스트 및 지식 기반 생성을 위\n한 강화 학습. _arXiv 사전인쇄본 arXiv:2108.12472_ .\n\n\n티아고 카스트로 페레이라, 클레어 가르당, 니콜라이 일리니흐, 크리\n스 반 데르 리, 시몬 밀레, 디에고 무살렘, 아나스타샤 시모리나.\n\n2020. 2020년 양방향 이중 언어 웹NLG+ 공유 과제 개요 및 평가\n결과 (webnlg+ 2020). _제3회시맨틱웹기반자연어생성국제워_\n_크숍(WebNLG+) 논문집_ .\n\n\n_Deb_ asis Ganguly, Dwaipayan Roy, Mandar Mitra, Gareth JF Jones.\n\n2015. 정보 검색을 위한 단어 임베딩 기반 일반화 언어 모델. _제_\n_38회국제 ACM SIGIR 정보검색연구및개발컨퍼런스논문집_,\n795–798쪽.\n\n\n_유_ 리 가닛케비치, 벤자민 반 더메, 크리스 캘리슨-버치. 2013. PPDB:\n\n의역 데이터베이스. _북미계산언어학회 2013년학술대회논문집:_\n\n_인간언어기술_, 758–764쪽.\n\n\nGuo Liang, Yan Fu, Lu Yuqian, Zhou Ming, Yang Tao. 2021. 지식\n그래프 기반 자동 가공 공정 의사 결정 시스템. _International_\n_journal of computer integrated manufacturing_, 34(12):1348–\n1369.\n\n\nQingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie,\nHui Xiong, Qing He. 2020. 지식 그래프 기반 추천 시스템에 관한\n연구. _IEEE Transactions on Knowledge and Data Engineering_,\n34(8):3549–3568.\n\n\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts,\nJuliane Fluck, Martin Hofmann-Apitius, Luca Toldo. 2012. 의료\n사례 보고서에서 약물 관련 부작용의 자동 추출을 지원하기 위한\n벤치마크 코퍼스 개발. Journal of biomedical informatics,\n45(5):885–892.\n\n\n\n의학 사례 보고서에서 약물 관련 부작용 자동 추출 지원 코퍼스\n개발. _Journal of biomedical informatics_, 45(5):885–892.\n\n\n한리동, 펑타오, 양차오하오, 왕벤유, 류루, 완샹. 2023. ChatGPT로\n정보 추출 문제가 해결되었는가? 성능, 평가 기준, 견고성 및 오\n류 분석. _arXiv 사전인쇄본 arXiv:2305.14450_ .\n\n\n황샤오, 장징위안, 리딩청, 리핑. 2019. 지식 그래프 임베딩 기반 질\n문 답변. _제12회 ACM 국제웹검색및데이터마이닝컨퍼런스논_\n_문집_, 105–113쪽.\n\n\n지샤오슝, 판시루이, 에릭 캄브리아, 페카 마르티넨, 필립 유. 2021.\n\n지식 그래프에 관한 개관: 표현, 획득 및 응용. _IEEE 신경망및학_\n_습시스템트랜잭션_, 33(2):494–514.\n\n\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Florian\nBressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier\n외. 2023. Mistral 7b. _arXiv 사전인쇄본 arXiv:2310.06825_ .\n\n\n마틴 조시포스키, 니콜라 드 카오, 막심 페야르, 파비오 페트로니, 로\n버트 [웨스트. 2022. GenIE: 생성적](https://doi.org/10.18653/v1/2022.naacl-main.342) 정보 추출. _2022년북미계산언_\n_어학회학술대회: 인간언어기술_ 논문집, 4626–4643쪽, 미국 시\n애틀. 계산언어학회.\n\n\n세라피나 캄프, 모르테자 파야지, 지네브 베나메르-엘, 슈얀 유, 로널\n드 드레스린스키. 2023. 오픈 정보 추출: 기본 기법, 접근법 및 응\n용에 대한 검토. _arXiv 사전인쇄본 arXiv:2310.11644_ .\n\n\n케샤브 콜루루, 바이브하브 아드라카, 사마르트 아가르왈, 수멘 차크\n라바르티 외. 2020. Openie6: 오픈 정보 추출을 위한 반복적 그리\n드 라벨링 및 조정 분석. _arXiv 사전인쇄본 arXiv:2010.03147_ .\n\n\nLuong Thi Hong Lan, Tran Manh Tuan, Tran Thi Ngan, Nguyen\nLong Giang, Vo Truong Nhu Ngoc, Pham Van Hai 외. 2020. 의\n사 결정에서의 퍼지 지식 그래프 및 확장을 갖춘 새로운 복합 퍼\n지 추론 시스템. _Ieee Access_, 8:164899–164921.\n\n\n마이크 루이스, 인한 리우, 나만 고얄, 마르잔 가즈비니네자드, 압델\n라흐만 모하메드, 오메르 레비, 베스 스토야노프, 루크 제틀모이어.\n\n2019. Bart: 자연어 생성, 번역 및 이해를 위한 시퀀스-투-시퀀스 사\n전 훈련의 노이즈 제거. _arXiv 사전인쇄본 arXiv:1910.13461_ .\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,\nVladimir Karpukhin, Naman Goyal, Hein-rich Küttler, Mike\nLewis, Wen-tau Yih, Tim Rock-täschel 외. 2020. 지식 집약적\nNLP 작업을 위한 검색 강화 생성. _신경정보처리시스템의진보_,\n33:9459–9474.\n\n\n보 리, 궈샹 팡, 양 양, 콴센 왕, 위 예, 원 자오, 시쿤 장. 2023.\n\nChatGPT의 정보 추출 능력 평가: 성능, 설명 가능성, 보정 및 충\n실도 평가. _arXiv 사전인쇄본 arXiv:2304.11633_ .\n\n\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele\nBevilacqua, Fabio Petroni, Percy Liang. 2024. 중간에서 길을 잃\n다: 언어 모델이 긴 문맥을 활용하는 방식. _Transactions of the_\n_Association for Computational Linguistics_, 12:157–173.\n\n\nPai Liu, Wenyang Gao, Wenjie Dong, Songfang Huang, and Yue\nZhang. 2022. 2007년부터 2022년까지의 공개 정보 추출–서베이.\n\n_arXiv 사전인쇄본 arXiv:2208.08690_ .\n\n\n이루안, 루헝 허, 마리 오스텐도르프, 한나네 하지시르지. 2018. 과학\n지식 그래프 구축을 위한 명사체, 관계, 지시일치 다중 작업 식별.\n\n_arXiv 사전인쇄본 arXiv:1808.09602_ .\n\n\nPedro Henrique Martins, Zita Marinho, André FT Martins. 2019. 명\n명된 엔티티 인식과 엔티티 연결의 공동 학습. _arXiv 사전인쇄본_\n_arXiv:1907.08243_ .\n\n\nIgor Melnyk, Pierre Dognin, Payel Das. 2022. 텍스트로부터의 지식\n그래프 생성. _arXiv 사전인쇄본 arXiv:2211.10511_ .\n\n\nGeorge A Miller. 1995. Wordnet: 영어를 위한 어휘 데이터베이스.\n_Communications of the ACM_, 38(11):39–41.\n\n\n야스마사 오노에와 그렉 듀렛. 2020. 도메인 독립적 엔티티 연결을\n위한 세분화된 엔티티 타이핑. _AAAI 인공지능학회논문집_, 제34\n권, 8576–8583쪽.\n\n\n[Shon Otmazgin, Arie Cattan, and Yoav Goldberg. 2023. LingMess: 언](https://doi.org/10.18653/v1/2023.eacl-main.202)\n어학적 정보를 활용한 다중 [전문가](https://doi.org/10.18653/v1/2023.eacl-main.202) 코어퍼런스 해결 점수 부여기.\n\n_제17회유럽계컴퓨터언어학회학술대회논문집_, 2752–2760쪽,\n크로아티아 두브로브니크. 컴퓨터언어학회.\n\n\n리프키 아피나 푸트리, 기원 홍, 성현 맹. 2019. 단어 임베딩 기반 시\n아미즈 네트워크를 이용한 오픈 IE 관계와 KB 관계 정렬. _제13회_\n_국제계산의미론학술대회-장문논문집_, 142–153쪽.\n\n\n콜린 라펠, 노암 샤지르, 애덤 로버츠, 캐서린 리, 샤란 나랑, 마이클\n마테나, 옌치 저우, 웨이 리, 피터 J. 리우. 2020. 통합 텍스트-투\n텍스트 트랜스포머를 통한 전이 학습의 한계 탐구. _기계학습연_\n_구저널_, 21(140):1–67.\n\n\n댄 로스와 웬타우 이. 2004. 자연어 처리 작업에서 전역 추론을 위한\n선형 계획법 공식화. _제8회계산_ 자연어 _학습학술대회(CoNLL-_\n_2004) 논문집_, HLT-NAACL 2004, 1–8쪽.\n\n\n\n_컴퓨테이셔널자연어학습에관한제8회학술대회(CoNLL-2004)_\n_논문집, HLT-NAACL 2004_, 1–8쪽.\n\n\nAlisa Smirnova and Philippe Cudré-Mauroux. 2018. 먼 감독을 이용한\n관계 추출: 개요. _ACM 컴퓨팅서베이 (CSUR)_, 51(5):1–35.\n\n\nRhea Sukthanker, Soujanya Poria, Erik Cambria, Ramkumar\nThirunavukarasu. 2020. 대명사 및 공동참조 해결: 리뷰.\n_Information Fusion_, 59:139–162.\n\n\nQingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, Sharifah Mahani\nAljunied. 2022. 관계 추출에서의 거짓 [음성](https://arxiv.org/abs/2205.12696) 문제 해결을 위한\n[docred 재검토.](https://arxiv.org/abs/2205.12696) _EMNLP 회의록_ .\n\n\nShikhar Vashishth, Prince Jain, and Partha Talukdar. 2018. Cesi: 임\n베딩과 부가 정보를 활용한 오픈 지식베이스 정규화. _2018년월_\n_드와이드웹컨퍼런스논문집_, 1317–1327쪽.\n\n\nDenny Vrandecˇic´ 및 Markus Krötzsch. 2014. 위키데이터: 자유 협\n업 지식베이스. _ACM 커뮤니케이션_, 57(10):78–85.\n\n\nSomin Wadhwa, Silvio Amir, Byron C Wallace. 2023. 대규모 언어\n모델 시대의 관계 추출 재검토. _회의논문집. Association for_\n_Computational Linguistics. Meeting_, volume 2023, page 15566.\nNIH Public Access.\n\n\n홍웨이 왕, 묘 자오, 싱 시에, 원지에 리, 민이 구오. 2019. 추천 시스\n템을 위한 지식 그래프 컨볼루션 네트워크. _세계웹컨퍼런스_,\n3307–3313쪽.\n\n\n량 왕, 난 양, 샤오롱 황, 린준 양, 랑간 마주머, 푸루 웨이. 2023. 대규\n모 언어 모델을 활용한 텍스트 임베딩 개선. _arXiv 사전인쇄본_\n_arXiv:2401.00368_ .\n\n\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang,\nShen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan\nZhang, et al. 2023. ChatGPT와의 대화를 통한 제로샷 정보 추출.\n\n_arXiv 사전인쇄본 arXiv:2302.10205_ .\n\n\n야치 시에, 천 위, 통야오 주, 진빈 바이, 제 공, 해럴드 소. 2023. 대규\n모 언어 모델을 활용한 자연어에서 계획 목표로의 변환. _arXiv 사_\n_전인쇄본 arXiv:2302.05128_ .\n\n\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024. 환각은 피할\n수 없다: 대규모 언어 모델의 선천적 한계. _arXiv_ _사전인쇄본_\n_arXiv:2401.11817_ .\n\n\n야오 이판, 두안 진하오, 쉬 카이디, 차이 위안팡, 쑨 지보, 장 유에.\n\n2024. 대규모 언어 모델(LLM) 보안 및 개인정보 보호에 관한 조\n사: 장점, 단점, 그리고 문제점. _High-Confidence Computing_,\n100211쪽.\n\n\n야스나가 미치히로, 렌 홍위, 보슬루 안투안, 리앙 퍼시, 레스코베츠\n쥬레. 2021. Qa-gnn: 질문 답변을 위한 언어 모델과 지식 그래프\n를 활용한 추론. _arXiv 사전인쇄본 arXiv:2104.06378_ .\n\n\n예홍빈, 장닝위, 천후이, 천화준. 2022. 생성적 지식 그래프 구축: 리\n뷰. _arXiv 사전인쇄본 arXiv:2210.12714_ .\n\n\nDaojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant\nsupervision for relation extraction via piecewise convolutional\nneural networks. In _Proceed-ings of the 2015 conference on_\n_empirical methods in natural language processing_, pages 1753–\n1762.\n\n\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao.\n\n2014. 딥 컨볼루션 신경망을 통한 관계 분류. _제25회국제계산_\n_언어학학술대회(COLING 2014) 논문집: 기술논문_, 2335–2344\n쪽.\n\n\n장보원, 소해럴드. 2023. 인간-로봇 상호작용을 위한 제로샷 인간 모\n델로서의 대규모 언어 모델. _2023 IEEE/RSJ 국제지능형로봇및_\n_시스템학회(IROS)_, 7961–7968쪽. IEEE.\n\n\nLingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xin-dong Wu.\n\n2023. 자동 지식 그래프 구축에 관한 포괄적 서베이. _ACM 컴퓨_\n_팅서베이_, 56(4):1–62.\n\n\n저우 샤오원, 위 보원, 쑨 아이신, 롱 청, 리 징양, 위 하이양, 쑨 지안,\n리 용빈. 2022. 신경망 기반 공개 정보 추출에 관한 연구: 현황과\n향후 방향. _arXiv 사전인쇄본 arXiv:2205.11725_ .\n\n\n안드레이 주코프-그레고리치, 요람 바크라흐, 샘 쿠프. 2018. 병렬\n재귀 신경망을 이용한 명명된 개체 인식. _제56회전산언어학회_\n_연차대회논문집 (제2권: 단편논문)_, 69–74쪽.\n\n\n**A** **구현** **세부사항**\n**A.1** **모델** **및** **인프라** **세부** **사항**\n우리는 OpenAI의 두 모델인 GPT-3.5-turbo와 GPT-4(현재 크기\n미상) 및 오픈소스 모델인 Mistral-7b(70억 매개변수)를 사용합\n니다. 오픈 소스 모델의 훈련 및 추론은 AMD EPYC 7543P 32코\n어 프로세서와 252GB RAM을 탑재하고 NVIDIA RTX\nA6000(48GB) GPU 4개를 장착한 단일 머신에서 수행되었습니\n다. GPT-3.5-turbo와 GPT-4는 OpenAI API를 통해 접근했습니\n다. EDC 코드는 [https://github.com/clear-nus/edc](https://github.com/clear-nus/edc) 에서 확인할\n수 있습니다.\n\n\n**A.2** **프롬프팅** **관련** **하이퍼파라미터**\nEDC의 모든 모듈에 대해 소량 데이터 프롬프팅을 사용하며, 각\n데이터셋에서 경험적으로 6샷 예제를 선택합니다. 스키마 정규\n화 단계에서 사용되는 객관식 문제의 경우, 스키마에서 의미적\n으로 유사한 상위 5개 관계를 후보로 추출합니다. 정제 단계에서\n는 스키마 검색기가 스키마에서 가장 관련성이 높은 상위 10개\n관계를 후보 관계로 추출합니다. 이러한 하이퍼파라미터는 성능\n과 추론 비용의 균형을 맞추기 위해 경험적으로 선택되었습니다.\n\n\n**A.3** **스키마** **검색기** **훈련**\n우리는 원본 논문(Wang et al., 2023)에 상세히 기술된 원래\n훈련 방법론을 따릅니다. 이는 텍스트 쌍과 그에 대응하는 정의\n된 관계를 활용하는 것을 포함합니다. 주어진 긍정적 텍스트-관\n계 쌍( _t_ [+] _, r_ [+] )에 대해, _t_ [+] 에 대한 지시 템플릿을 사용하여 새로\n운 텍스트 _t_ [+] 를 생성합니다 =\n_inst_\n“지시: 주어진 텍스트에 존재하는 관계를 추출하라 _\\n_ 질의:\n\n_{t_ [+] _}_ ”.\n\n그런 다음 InfoNCE 손실을 사용하여 임베딩 모델을 미세 조\n정하여 주어진 텍스트와 관련된 올바른 관계와 다른 관련 없는\n관계를 구별합니다.\n\n\n\n온라인 오픈소스 구현체와 하이퍼파라미터 구성을 훈련에 채택\n했습니다.\n\n정밀 조정된 스키마 검색기의 성능은 WebNLG, REBEL,\nWiki-NRE 데이터셋의 테스트 분할에서 평가되었습니다. 해당\n데이터셋에서의 recall@10 점수는 각각 0.823, 0.663, 0.818로,\n다양한 지식 그래프 환경에서 검색기의 효과성을 입증합니다.\n\n\n**A.4** **정제** **힌트** **세부** **사항**\n정제 힌트는 후보 엔티티와 후보 관계로 구성됩니다. 이 섹션에\n서는 이들의 획득 방법과 OIE 성능 향상을 위해 어떻게 사용되는\n지 자세히 설명합니다. 3절에서 사용한 예시를 계속 사용하겠습\n니다: \" _Alan Shepard는 1923년 11월 18일에태어났고 1959년에_\n_NASA에선발되었습니다._ _그는아폴로 14호승무원이었다_ \"이며,\nEDC가 첫 번째 반복에서 추출한 삼중항은 [’Alan Shepard’,\n‘birthDate’, ‘Nov 18, 1923’], [’Alan Shepard’, ’mission’,\n’Apollo 14’] 이다.\n\n\n**A.4.1** **후보** **엔티티** **획득**\n후보 엔티티는 두 가지 출처에서 비롯됩니다:\n\n\n  - 이전 반복에서 EDC가 추출한 엔티티, 즉 [‘Alan\nShepard’, ‘Nov 18, 1923’, ‘Apollo 14’]\n\n\n  - LLM에 엔티티 추출 작업을 수행하도록 프롬프트하여 텍\n스트에서 추출한 엔티티. 트리플릿 추출 작업과 유사함.\n\n\n\nmin _L_ = _−_ log ~~+~~\n_ϕ_ ( _t_ [+]\n_inst_ ~~Σ~~\n_, r_ [+] )\n~~+~~ _inst_ ~~Σ~~ +\n_ϕ_ ( _tinst,_ [r+] ) + _n_ _i_ ∈N _[ ϕ]_ [(] _tinst_ _[, ][ni]_ [)]\n여기서 N은 음성 샘플 집합을 나타내며, _ϕ_ _는_ 코사인 유사도\n함수를 의미합니다. 추가적인 훈련 세부 사항은 부록을 참조하\n십시오.\n\n훈련을 위해 위키데이터 삼중항을 위키백과 텍스트에 정렬하\n여 생성된 대규모 텍스트 삼중항 데이터셋인 TEKGEN 데이터셋\n(Agarwal et al., 2020)을 활용하여 텍스트-관계 쌍 데이터셋을 합\n성했습니다. 훈련 데이터셋은 양성 샘플과 음성 샘플로 균등하게\n분할된 37,500개의 쌍으로 구성되었습니다.\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-12-0.png)\n\n\n\n그리고 결과 엔티티는 [‘ 앨런 셰퍼드 ’, ‘1923 년 11 월 18 일\n’, ‘NASA’, ‘1959 년 ’, ‘ 아폴로\n14’]\n그런 다음 엔티티들을 후보 엔티티로 병합합니다.\n\n\n**A.4.2** **후보** **관계** **획득**\n후보 관계도 두 가지 출처에서 비롯됩니다:\n\n\n  - 이전 반복에서 EDC가 추출한 관계, 즉 [‘birthDate’,\n‘mission’]\n\n\n  - 스키마 검색기가 입력 텍스트와 스키마 내 관계 간의 관련\n성 점수를 계산하여 추출한 관계. 이 경우 상위 5개 검색\n관계는 [‘birthDate’,, ‘selectedByNasa’, ‘mission’,\n‘draftPick’, ‘occupation’] 입니다.\n\n\n이후 관계와 해당 정의가 후보 관계로 통합됩니다. 다른 RAG\n기반 방법과 마찬가지로 검색기가 관련 없는 정보를 추출할 가\n능성이 있다는 점에 유의해야 합니다. 이 경우 관계 정의가 유용\n하게 활용될 수 있는데, 이는 LLM이 해당 관계가 텍스트에 대해\n유효한지 여부를 판단하는 데 더 많은 정보를 제공하기 때문입니\n다.\n\n\n**A.4.3** **정제된 OIE를** **위한** **힌트** **사용법**\n정제된 OIE에 대한 힌트는 프롬프트에 적절히 포함되어 LLM이\n후보 엔티티 및 후보 관계를 고려하도록 지시합니다(이에 국한\n되지 않음):\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-13-0.png)\n\n그림 3: 주석 작업자에게 제공된 지침을 포함한 설문지의 예시 스크\n린샷.\n\n\n정제된 OIE로 추출된 삼중항은 다음과 같습니다: [’ 앨런 셰퍼\n드 ’, ‘ 생년월일 ’, ‘1923 년 11 월 18 일 ’], [’ 앨런 셰퍼드 ’, ’ 임무 ’, ’ 아\n폴로 14 호 ’], [’ 앨런 셰퍼드 ’, ’NASA 선발 ’, ’1959 년 ’] . 이는 힌트\n사용 없이는 놓쳤을 미묘하고 세분화된 관계 ‘selectedByNasa’\n를 성공적으로 복원합니다. 또한 의미론적으로 풍부한 설명은\nLLM이 스키마 검색기가 추출한 잡음이 많은 관계를 과도하게\n추출하는 것을 방지하는 데 도움이 됩니다.\n\n우리는 두 출처의 엔티티, 즉 마지막 라운드에서 추출된 것과\n별도의 모듈(엔티티 추출 또는 스키마 리트리버)에 의해 발견된\n엔티티를 모두 포함하는 것이 중요하다는 것을 발견했습니다.\n\n스키마 리트리버의 중요성은 이미 4.2.1절의 제거 연구에서 보\n여졌습니다.\n\n\n**B** **주석** **지침**\n주석 작업자에게 제공된 설문지 및 지침의 형식을 설명하기 위해\n그림 3에 예시 스크린샷을 제공합니다. 데이터 수집 목적은 주석\n작업자에게 구두로 전달되었습니다.\n\n\n**C** **대상** **정렬의** **상세** **결과**\n**C.1** **완전한** **결과**\nWebNLG, REBEL 및 Wiki-NRE에 대한 EDC와 EDC+R의 완전\n한 결과는 각각 표 3, 표 4 및 표 5에 요약되어 있습니다. EDC는\n모든 기준(부분, 엄격, 정확)에서 모든 지표(정밀도, 재현율, F1)\n측면에서 최첨단 기준 모델보다 우수하거나 비슷한 성능을 보이\n며, EDC+R은 일관되게 성능을 향상시킬 수 있습니다.\n\n\n\n![](assets/md_images/지식그래프-논문_-Extract,-Define-Canonicalize--An-LLM-based-Framework-for-Knowledge-Graph-Construction-ko.pdf-13-1.png)\n표 3: WebNLG 데이터셋에서 EDC 및 EDC+R의 완전한 결과(기준 모델 REGEN 대비, '부분적', '엄격한', '정확한' 기준에 따른 정밀도, 재현율,\nF1 점수). EDC+R은 1회의 정제만 수행함. 최상의 결과는 굵게 표시됨.\n\n\n\n\n\n\n\n\n\n\n\n|방법|OIE용 LLM|부분<br>정밀도 리콜 F1|엄격<br>정밀도 리콜 F1|정확<br>정밀도 리콜 F1|\n|---|---|---|---|---|\n|EDC|GPT-4<br>GPT-3.5<br>미스트랄-7b|**0.776**<br>**0.796**<br>**0.783**<br>0.739<br>0.760<br>0.746<br>0.723<br>0.739<br>0.728|**0.729**<br>**0.741**<br>**0.733**<br>0.684<br>0.697<br>0.688<br>0.668<br>0.679<br>0.672|**0.751**<br>**0.765**<br>**0.756**<br>0.708<br>0.722<br>0.713<br>0.692<br>0.703<br>0.696|\n|EDC+R<br>|GPT-4<br>GPT-3.5<br>미스트랄-7b<br>|**0.814**<br>**0.831**<br>**0.820**<br>0.788<br>0.806<br>0.794<br>0.756<br>0.775<br>0.762|**0.782**<br>**0.794**<br>**0.786**<br>0.749<br>0.761<br>0.753<br>0.716<br>0.727<br>0.720|**0.796**<br>**0.808**<br>**0.800**<br>0.768<br>0.781<br>0.772<br>0.735<br>0.747<br>0.739|\n|기준선|재생|0.755<br>0.788<br>0.767|0.713<br>0.735<br>0.720|0.714<br>0.738<br>0.723|\n\n\n표 4: REBEL 데이터셋에서 기준 모델 REGEN 대비 EDC 및 EDC+R의 완전한 결과 (‘부분적’, ‘엄격한’, ‘정확한’ 기준에 따른 정밀도, 재현\n율, F1 점수). EDC+R은 정제 과정을 1회만 수행합니다. 최상의 결과는 굵게 표시했습니다.\n\n\n\n\n\n\n\n\n\n\n\n|방법|OIE용 LLM|부분<br>정밀도 리콜 F1|엄격<br>정밀도 리콜 F1|정확<br>정밀도 리콜 F1|\n|---|---|---|---|---|\n|EDC|GPT-4<br>GPT-3.5<br>미스트랄-7b|**0.543**<br>**0.552**<br>**0.546**<br>0.503<br>0.512<br>0.506<br>0.512<br>0.523<br>0.516|**0.498**<br>**0.503**<br>**0.500**<br>0.448<br>0.453<br>0.449<br>0.450<br>0.457<br>0.453|**0.511**<br>**0.517**<br>**0.514**<br>0.471<br>0.476<br>0.473<br>0.481<br>0.488<br>0.483|\n|EDC+R<br>|GPT-4<br>GPT-3.5<br>미스트랄-7b|**0.599**<br>**0.606**<br>**0.601**<br>0.556<br>0.565<br>0.559<br>0.525<br>0.550<br>0.531|**0.557**<br>**0.561**<br>**0.559**<br>0.513<br>0.519<br>0.516<br>0.461<br>0.462<br>0.462|**0.572**<br>**0.576**<br>**0.574**<br>0.527<br>0.533<br>0.529<br>0.506<br>0.511<br>0.505|\n|기준선|GENIE|0.381<br>0.391<br>0.385|0.353<br>0.361<br>0.356|0.362<br>0.369<br>0.364|\n\n\n표 5: Wiki-NRE 데이터셋에서 기준 모델 REGEN 대비 EDC 및 EDC+R의 전체 결과 (‘부분적’, ‘엄격한’, ‘정확한’ 기준에 따른 정밀도, 재현율,\nF1 점수). EDC+R은 1회의 정제만 수행합니다. 최상의 결과는 굵게 표시했습니다.\n\n\n\n\n\n\n\n\n\n\n\n|방법|OIE용 LLM|부분<br>정밀도 리콜 F1|엄격<br>정밀도 리콜 F1|정확<br>정밀도 리콜 F1|\n|---|---|---|---|---|\n|EDC|GPT-4<br>GPT-3.5<br>미스트랄-7b|**0.682**<br>**0.686**<br>**0.683**<br>0.645<br>0.651<br>0.647<br>0.644<br>0.650<br>0.647|**0.675**<br>**0.679**<br>**0.677**<br>0.636<br>0.640<br>0.638<br>0.636<br>0.640<br>0.637|**0.676**<br>**0.680**<br>**0.678**<br>0.638<br>0.643<br>0.640<br>0.637<br>0.641<br>0.639|\n|EDC+R<br>|GPT-4<br>GPT-3.5<br>미스트랄-7b|**0.712**<br>**0.715**<br>**0.713**<br>0.691<br>0.696<br>0.693<br>0.661<br>0.667<br>0.663|**0.708**<br>**0.710**<br>**0.709**<br>0.684<br>0.688<br>0.685<br>0.647<br>0.652<br>0.649|**0.708**<br>**0.711**<br>**0.709**<br>0.685<br>0.689<br>0.687<br>0.656<br>0.661<br>0.658|\n|기준선|GENIE|0.482<br>0.486<br>0.484|0.462<br>0.464<br>0.463|0.477<br>0.479<br>0.478|\n\n\n표 6: 추가 반복 정제 결과(모든 기준에 따른 F1 점수). OIE에 사용된 대규모 언어 모델(LLM)은 GPT-3.5-turbo입니다. EDC+2xR은 2회 정제\n반복을 거친 EDC입니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|방법|WebNLG<br>부분 엄격 정확|REBEL<br>부분 엄격 정확|위키-NRE<br>부분 엄격 정확|\n|---|---|---|---|\n|EDC+2xR<br>EDC+R|0.797<br>0.761<br>0.775<br>0.794<br>0.753<br>0.772|0.564<br>0.521<br>0.535<br>0.559<br>0.516<br>0.529|0.697<br>0.689<br>0.660<br>0.693<br>0.685<br>0.657|\n|EDC|0.746<br>0.688<br>0.713|0.506<br>0.449<br>0.473|0.644<br>0.634<br>0.637|\n\n\n표 7: 정제 힌트에서 마지막 라운드에서 추출된 엔티티와 관계를 제거한 결과(모든 기준에 대한 F1 점수). OIE에 사용된 LLM은 GPT-3.5\nturbo입니다. EDC+R-lastround는 정제를 거친 EDC이지만, 마지막 라운드에서 추출된 엔티티와 관계는 정제 힌트에서 제거됩니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|방법|WebNLG<br>부분 엄격 정확|REBEL<br>부분 엄격 정확|Wiki-NRE<br>부분 엄격 정확|\n|---|---|---|---|\n|EDC+R<br>EDC+R-마지막 라운드|0.794<br>0.753<br>0.772<br>0.748<br>0.698<br>0.720|0.559<br>0.516<br>0.529<br>0.534<br>0.485<br>0.505|0.693<br>0.685<br>0.657<br>0.634<br>0.622<br>0.625|\n|EDC|0.746<br>0.688<br>0.713|0.506<br>0.449<br>0.473|0.644<br>0.634<br>0.637|\n\n\n표 8: 세 데이터셋 전체에서 문장당 추출된 삼중어 평균 개수. WebNLG의 기준 모델은 REGEN이며, Rebel과 Wiki-NRE의 기준 모델은\nGENIE이다. 괄호 안 숫자는 참조 주석과의 차이이다.\n\n\n|OIE용 LLM|WebNLG REBEL Wiki-NRE|\n|---|---|\n|GPT-4<br>GPT-3.5<br>미스트랄7b<br>|3.47(+0.04)<br>5.11(+1.11)<br>3.49(+0.63)<br>3.44(+0.01)<br>5.01(+1.01)<br>3.49(+0.63)<br>3.45+(0.02)<br>4.68(+0.68)<br>3.75(+0.89)|\n|기준선<br>|-<br>2.20(-1.80)<br>3.08(+0.22)|\n|참조|3.43<br>4.00<br>2.86|\n\n\n\n이 모든 측면에서도 이를 입증합니다. 이러한 결과는 EDC 및\nEDC+R의 성능을 보다 포괄적으로 보여줍니다.\n\n\n**C.2** **추가** **정제** **반복의** **효과**\n표 6은 모든 데이터셋에 대해 EDC를 사용한 추가 정제 반복의\n결과를 보여줍니다. 추가 정제는 결과를 안정적으로 개선하지만,\n수익 감소 현상이 관찰되므로 주요 결과에는 한 번의 반복만 보\n고합니다.\n\n\n**C.3** **최종** **추출** **단계에** **대한** **제거** **연구**\n표 7은 정제 힌트에서 마지막 라운드 추출물에서 관계와 엔티티\n를 제거한 결과를 보여줍니다. 이는 정제를 반복적으로 수행하\n는 것의 중요성을 보여줍니다. 두 소스를 병합함으로써 텍스트\n내 엔티티와 관계의 커버리지가 향상되어 더 나은 KGC를 얻었\n습니다.\n\n\n**C.4** **KGC 데이터셋** **어노테이션에** **관한** **논의**\n4.2절에서 언급한 바와 같이, EDC는 불완전한 주석으로 인해\nRebel 및 Wiki-NRE 데이터셋에서 평가기(scorer)에 의해 불이\n익을 받는 것으로 관찰됩니다. 이는 (Wadhwa et al., 2023; Han\net al., 2023)의 이전 연구 결과와 일치하는데, 대규모 언어 모델\n(LLMs)이 주석에 누락된 정확한 결과를 종종 추출할 수 있어 지\n나치게 비관적인 평가를 초래한다는 점입니다. 표 8에서 보듯이,\nEDC는 참조 주석 및 기준 모델인 GenIE에 비해 현저히 더 많은\n삼중항을 추출하는 경향이 있습니다. 또한 표 2의 수동 평가에서\n알 수 있듯이, 이러한 삼중항 중 상당수는 입력 텍스트에 대해 실\n제로 의미 있고 정확한 것입니다. 그럼에도 불구하고, EDC에 대\n한 자동 평가 결과가 지나치게 비관적임에도 불구하고, 여전히\n기준선을 크게 상회하며 추출된 삼중항 수의 차이를 고려하면 실\n제 성능은 더 클 수 있습니다.\n\n\n\n**D** **새로운** **데이터셋에** **대한** **실험**\n테스트된 데이터셋은 수년 전에 생성되었으며 사용된 대규모 언\n어 모델(LLM)의 훈련 세트가 알려지지 않았기 때문에, 해당\nLLM들이 이미 이 데이터셋으로 훈련되었을 위험이 존재합니다.\n\n이러한 우려를 해결하기 위해, 우리는 가상의 개체와 정보로 구\n성된 새로운 소규모 데이터셋(50개 항목)을 생성했습니다. 예를\n들어, \" _에버그린대학교는에밀리존슨이생물학학위를취득한_\n_곳이다_ \"와 같은 내용이며, 이를 Wiki-NRE 스키마를 사용하여 주\n석 처리했습니다. 표 9는 EDC 및 EDC+R이 여전히 기준 모델인\nGenIE보다 우수한 성능을 보임을 보여줍니다.\n\n\n**E** **기존 LLM 기반** **접근법과의** **비교**\n비록 이는 EDC의 의도된 사용 시나리오가 아니지만, 기존 LLM\n기반 방법들과 비교하기 위한 보다 포괄적인 평가를 위해 이러한\n실험 결과를 포함합니다. 우리는 세 가지 데이터 세트, CoNLL04\n(4가지 관계 유형) (Roth and Yih, 2004), SciERC (7가지 관계 유\n형) (Luan et al., 2018) 및 Wiki-NRE (45가지 관계 유형)의 하위\n샘플링 버전을 사용하여 실험을 수행합니다. 비교의 공정성을 보\n장하기 위해, 비교 대상 방법 모두에 GPT-3.5-turbo를 사용합니\n다.\n\n표 10에서 볼 수 있듯이, 관계 수가 적을 때(CONLL 및\nSciERC) EDC 단독은 프롬프트에서 스키마를 제외했기 때문에\n기준 방법보다 우수하지 않을 수 있습니다. 그러나 정제 과정을\n통해 EDC+R은 훨씬 더 나은 결과를 달성할 수 있습니다. 이는\n정제 단계에서 의미적으로 풍부한 관계 설명을 사용했기 때문일\n수 있습니다. 구체적으로, 추출 과정에서 발생할 수 있는 두 가\n지 유형의 오류를 수정하는 데 도움이 됩니다: 1. 정의 단계는 동\n음이의어를 명확히 하는 데 도움이 됩니다. 예를 들어, \"John\nfollows Taoism\"에서 \"follows\"라는 단어는 두 가지 다른 의미를\n가집니다.\n\nv.s. \"John follows Mary\". EDC는 \"John follows Taoism\"에서\n\"fol-lows\"를 \"adheres to\"로 변경합니다. 2.\n\n\n표 9: 새로운 허구 데이터셋에서 EDC 및 EDC+R의 완전한 결과와 기준 모델 GenIE 비교 (정밀도, 재현율, '부분적', '엄격한', '정확한' 기준에\n따른 F1 점수). EDC+R은 1회의 정제만 수행함. 최상의 결과는 굵게 표시됨. OIE에 사용된 LLM은 GPT-3.5-turbo임.\n\n\n\n\n\n\n\n\n\n\n\n|방법|부분<br>정밀도 리콜 F1|엄격<br>정밀도 리콜 F1|정확<br>정밀도 리콜 F1|\n|---|---|---|---|\n|EDC<br>EDC+R|0.731<br>0.771<br>0.751<br>0.761<br>0.782<br>0.767|0.687<br>0.704<br>0.691<br>0.733<br>0.750<br>0.738|0.702<br>0.720<br>0.707<br>0.733<br>0.750<br>0.738|\n|GenIE|0.521<br>0.547<br>0.530|0.426<br>0.443<br>0.432|0.467<br>0.483<br>0.472|\n\n\n표 10: CONLL, SciERC 및 Wiki-NRE 데이터셋에서 EDC, EDC+R의 완전한 결과와 기존 LLM 기반 접근법인 CodeKGC 및 ChatIE와의 비\n교. 비교의 공정성을 위해 여기에서 사용된 LLM은 GPT-3.5-turbo입니다. 최상의 결과는 굵게 표시했습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n|데이터셋|방법|부분<br>정밀도 리콜 F1|엄격<br>정밀도 리콜 F1|정확<br>정밀도 리콜 F1|\n|---|---|---|---|---|\n|CONLL|EDC<br>EDC+R<br>|0.536<br>0.552<br>0.543<br>**0.580**<br>**0.593**<br>**0.585**|0.481<br>0.491<br>0.485<br>**0.514**<br>**0.522**<br>**0.517**|0.503<br>0.515<br>0.509<br>**0.549**<br>**0.558**<br>**0.552**|\n|CONLL|~~코드~~KGC<br>ChatIE|0.542<br>0.55<br>0.545<br>0.463<br>0.477<br>0.468|0.503<br>0.506<br>0.504<br>0.360<br>0.366<br>0.363|0.542<br>0.546<br>0.543<br>0.418<br>0.427<br>0.421|\n|SciERC|EDC<br>EDC+R<br>|0.389<br>0.408<br>0.395<br>**0.447**<br>**0.461**<br>**0.451**|0.288<br>0.301<br>0.292<br>**0.340**<br>**0.349**<br>**0.343**|0.352<br>0.365<br>0.357<br>**0.406**<br>**0.416**<br>**0.410**|\n|SciERC|~~코드~~KGC<br>ChatIE|0.389<br>0.398<br>0.392<br>0.351<br>0.367<br>0.357|0.277<br>0.283<br>0.279<br>0.212<br>0.221<br>0.215|0.346<br>0.353<br>0.349<br>0.294<br>0.302<br>0.297|\n|Wiki-NRE|EDC<br>EDC+R<br>|0.645<br>0.651<br>0.647<br>**0.691**<br>**0.696**<br>**0.693**|0.636<br>0.640<br>0.638<br>**0.684**<br>**0.688**<br>**0.685**|0.638<br>0.643<br>0.640<br>**0.685**<br>**0.689**<br>**0.687**|\n|Wiki-NRE|~~코드~~KGC<br>ChatIE|0.611<br>0.614<br>0.612<br>0.569<br>0.574<br>0.571|0.605<br>0.607<br>0.606<br>0.541<br>0.545<br>0.543|0.607<br>0.609<br>0.608<br>0.553<br>0.557<br>0.555|\n\n\n\n관계 정의를 활용하여, 정제 단계가 머리-꼬리 관계 오류를 수정\n함을 확인했습니다. 예를 들어 \"아버지\" 관계의 경우 주어와 목\n적어 중 누가 아버지인지 불분명한데, 이 정의는 일관성 없는 사\n용을 방지합니다. 이러한 오류 수정 효과는 기존 방법에서는 불\n가능했습니다.\n\n\n중간 규모의 스키마를 가진 Wiki-NRE에서 테스트했을 때,\nEDC는 이미 기준 방법들을 크게 능가하며, 이는 아마도 긴 문맥\n을 처리할 때 LLM의 혼란 때문일 수 있습니다(Liu et al., 2024).\n\n또한 ChatIE와 CodeKGC는 프롬프트에 전체 스키마가 제공되\n었음에도 스키마 외 관계어를 출력할 수 있음을 관찰했으며, 이\n는 기존 연구 결과(Wadhwa et al., 2023)를 반영합니다.\n\n\n**F** **EDC를** **다른 IE 도구와** **결합하기**\nEDC는 청크화, 공동참조, 중복 엔티티 제거 등 다른 정보 추출\n도구와 통합될 수 있습니다. 이는 LLM의 컨텍스트 창 길이를 초\n과하는 긴 문서를 처리하는 등의 시나리오에서 유용합니다. 우\n리는 EDC를 최첨단 지시 참조 해결 방법인 LingMess(Otmazgin\net al., 2023) 및 간단한 문장 수준 청킹과 결합하여 Re\nDOCRED(Tan et al., 2022)에 대한 실험을 수행했습니다. 엄격\n한 마이크로 F1 점수가 0.132에서 0.234로 증가하는 것을 관찰\n했으며,\nLLM에 직접 프롬프트를 제공했을 때는 0.060에 불과했습니다. 또\n한 EDC와 결합한 엔티티 중복 제거의 효과도 탐구했습니다. EDC\n로 생성된 지식 그래프(KG) 내 엔티티 중복 제거를 위해 최첨단 사\n후 정규화 방법인 CESI(Vashishth et al., 2018)를 적용했습니다.\n\n그 결과 REBEL 데이터셋에서 '부분적' 기준 하에 F1 점수가 0.516에\n서 0.520으로 소폭 향상된 것을 관찰했습니다.\n\n'부분적' 기준 하에서 REBEL 데이터셋에서 F1 점수가 0.516에서\n0.520으로 소폭 개선된 것을 확인했습니다.\n\n\n**G** **OIE와** **스키마** **정의** **결합**\nEDC 비용을 절감하기 위한 시도로서, 우리는 OIE 단계와 스키\n마 정의 단계를 결합하는 방안을 탐구했습니다. 이전에는 예비\n실험에서 OIE가 더 어려운 작업으로 나타났고, 두 하위 작업을\n분리함으로써 OIE에는 더 고성능 모델을, 스키마 정의에는 더\n작고 저렴한 모델을 사용할 수 있었기 때문에 이 두 단계를 분리\n했습니다. 그러나 별도의 LLM 호출은 파이프라인의 지연 시간(\n동일 LLM 사용 시 비용도)을 증가시킵니다. 또한 추출된 트리플\n과 함께 정의를 LLM이 출력하도록 하면 일관성이 향상될 수 있\n습니다. GPT-3.5-turbo를 사용해 REBEL에서 EDC와 스키마 정\n의를 결합한 추가 실험에서, 우리는 성능이 약간 향상되었고(‘부\n분적’ 기준 하에서 0.516에서 0.518로) 토큰 비용이 감소한 것을\n관찰했습니다(예시당 _약_ 3k에서 2k 토큰).\n\n\n",
    "eng_pdf": "pdf/eng/지식그래프 논문_ Extract, Define Canonicalize- An LLM-based Framework for Knowledge Graph Construction.pdf",
    "kor_pdf": "pdf/kor/지식그래프 논문_ Extract, Define Canonicalize- An LLM-based Framework for Knowledge Graph Construction ko.pdf"
  },
  {
    "id": "paper_6",
    "title": "LearnLM: Improving Gemini for Learning",
    "eng_md": "_[goo.gle/LearnLM-dec24](https://goo.gle/LearnLM-dec24)_\n_2024-12-19_\n\n# **LearnLM: Improving Gemini for Learning**\n\n\n**LearnLM Team, Google**\n**Today’s generative AI systems are tuned to present information by default, rather than engage users**\n**in service of learning as a human tutor would.** **To address the wide range of potential education use**\n**cases for these systems, we reframe the challenge of injecting pedagogical behavior as one of** _**pedagog-**_\n_**ical**_ _**instruction**_ _**following**_ **,** **where** **training** **and** **evaluation** **examples** **include** **system-level** **instructions**\n**describing the specific pedagogy attributes present or desired in subsequent model turns.** **This framing**\n**avoids** **committing** **our** **models** **to** **any** **particular** **definition** **of** **pedagogy,** **and** **instead** **allows** **teachers**\n**or developers to specify desired model behavior.** **It also clears a path to improving Gemini models for**\n**learning—by enabling the addition of our pedagogical data to post-training mixtures—alongside their**\n**rapidly expanding set of capabilities.** **Both represent important changes from our initial tech report [1].**\n\n**We** **show** **how** **training** **with** **pedagogical** **instruction** **following** **produces** **a** **LearnLM** **model** **(available**\n**[on Google AI Studio) that experts substantially prefer across a diverse set of learning scenarios, with](http://goo.gle/LearnLMaccess)**\n**average preference strengths of +31% over GPT-4o, +11% over Claude 3.5 Sonnet, and +13% over the**\n**Gemini 1.5 Pro model on which LearnLM was based.**\n\n### **1. Introduction**\n\n\n[Our initial tech report [1] from May 2024 surveyed the history and current landscape of education](https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf)\ntechnology, discussed the potential impact of generative artificial intelligence (gen AI) on education,\nand presented our collaborative approach to developing evaluations.\n\n\nFollowing its publication, we received input from across the international education sector, including schools, educational technology (“EdTech”) companies, non-profit organizations, and government\nagencies eager to try our models or otherwise collaborate. Through review of these submissions, over\n20 follow-up interviews, and input from Google product teams building gen AI powered learning\nfeatures, we can summarize the key findings as follows:\n\n\n1. Pedagogy [1], or rather, ideal behavior of an AI tutor, is prohibitively difficult to define given the\nwide range of grade-levels, subjects, languages, cultures, product designs, and philosophies\nthat must be accommodated. While there are many commonalities, appropriate behavior in\ndifferent contexts may be different or even contradictory, and it is best left to the developer or\nteacher to specify.\n2. In developing AI learning systems, the most commonly cited, immediately useful behavior in\nan underlying model is the ability to follow system instructions to create interactive tutor-led\nexercises. Teachers or developers who specify these instructions want to feel confident that the\nAI tutor will follow the specified instructions accurately, even if a student tries to circumvent\nthem (e.g., “do not give away the answer” or “stay on topic”).\n3. Post-hoc fine-tuning for each application can be effective in the short-term, but is impractical\nbecause of cost, maintenance, and rapidly improving base models. Thus, despite its shortcomings,\nprompting will likely remain the best way for education product developers to specify behavior.\n\n\nThis paper describes how we have updated our modeling and evaluation methodology in light\n1We use the term _pedagogy_ in as broad a sense as possible, certainly not limited to children, to evoke techniques of\nteaching and associated learning by humans.\n\n\n_Corresponding author(s):_ _learnlm-tech-report@google.com_\n© 2025 Google DeepMind. All rights reserved\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-2.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-3.png)\n\n\n\nLearnLM: Improving Gemini for Learning\n\n\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-0.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-5.png)\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-4.png)\n\n\n\n\n\n\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-6.png)\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-1.png)\n\n\n\n\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-11.png)\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-14.png)\n\n|Preference scale|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|Preference scale|||||||||\n|Preference scale|||||||||\n|Preference scale|||||||||\n|Preference scale|||||||||\n|Preference scale|||||||||\n|Preference scale|||||||||\n\n\nFigure 1 | An overview of our three-stage expert evaluation pipeline and our results for comparing LearnLM with other\nsystems. (1) We developed learning scenarios that allow expert participants to role-play specific learners interacting with\npairs of AI tutors. (2) Grounding material (e.g. an essay, homework problem, diagram, etc.) and System Instructions specific\nto each scenario are passed as context to each model. We pass the resulting conversation pairs to pedagogy experts (3)\nwho review each model on its own, as well as their comparative performance. We aggregate these comparative assessments\n(on a seven-point response scale) (4) to evaluate overall preference for LearnLM over GPT-4o, Claude 3.5 Sonnet, and\nGemini 1.5 Pro. See Section 4 for more detailed results.\n\n\nof these observations. Specifically, we cast our work as _pedagogical_ _instruction_ _following_, meaning\nthat we contextualize training and evaluation examples with system-level instructions that describe\ndesired pedagogical behavior. This approach avoids any narrow specification of how systems should\nbehave, and allows us to effectively add pedagogical data to the rest of Gemini’s training mixture\nwithout conflicts of persona or style. We also include Reinforcement Learning from Human Feedback\n(RLHF) [2] in our training procedure to allow models to follow more nuanced pedagogical instructions\nand preferences.\n\n\nUsing the updated methodology, we trained a new version of _LearnLM_, based on Gemini 1.5\nPro [2] [3]. In our evaluations against contemporaneous flagship models, each representing a company’s\npremier offering as of 2024-10-01, educators and pedagogy experts preferred this version of LearnLM\nwith an average preference strength of +31% over GPT-4o, +11% over Claude 3.5 Sonnet, and +13%\nover the original Gemini 1.5 Pro (see Figure 1). LearnLM is available as an experimental model\n[on Google AI Studio along with documentation of example use cases and suggested prompts.](http://goo.gle/LearnLMaccess) We\n[welcome any feedback on LearnLM to help inform our future research and priorities.](https://docs.google.com/forms/d/e/1FAIpQLSf5-B50OnNFjVGHLFkSerP1k0PZXHMgcnQ7k1cM_hIsqIjpjA/viewform) As we improve\nLearnLM for teaching and learning, we are also working to bring these advances into Gemini models,\nso any developers using Gemini can benefit from the improvements made via LearnLM research.\n\n\nSection 2 describes how we trained LearnLM for pedagogical instruction following and Section 3\nexplains how we updated our scenario-based evaluation design accordingly. Section 4 shows a detailed\nanalysis of results comparing LearnLM with other premier model offerings. Finally, Section 5 outlines\nsome future work, especially with regards to continued evaluation. In addition to the broad range\n[2Specifically gemini-1.5-pro-002 (release notes).](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions)\n2\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-1-10.png)\nLearnLM: Improving Gemini for Learning\nof core academic subjects that we use for training and evaluation, we include a feasibility study on\nmedical education subjects in Appendix C.\n\n### **2. Modeling**\n\n\nIn our original tech report [1], we adapted the behavior of a base model by Supervised Fine-Tuning\n(SFT) with a range of synthetic and human-written datasets. Since then, we have made a number of\nsubstantial changes to our training strategy: First, we updated our SFT data according to our focus\non pedagogical instruction following. Second, we decided to additionally leverage Reinforcement\nLearning from Human Feedback (RLHF)[2], for which we collected human preference data to train\nReward Models (RMs) and prompts for the RL stage. Third, rather than running our own post-training\nafter Gemini’s standard post-training, we _co-train_ with Gemini, meaning we mix our data directly\nwith Gemini’s SFT, RM, and RL stages. LearnLM is the result of this experimental mixture and we have\nalso been integrating our data and evaluations into the main Gemini models; a subset of LearnLM\n[improvements is part of the recently released Gemini 2.0 models [4].](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)\n**2.1.** **Pedagogical instruction following**\nInstruction following (IF) refers to a model’s ability to follow prompts, usually to better align with\nhuman intents [5]. Gemini [3] differentiates between _User Instructions_, inserted by a user during\nconversation, and _[System Instructions](https://ai.google.dev/gemini-api/docs/system-instructions)_, typically specified by a developer ahead of any user interaction,\nwhich take precedence over any subsequent instructions provided by the user. System Instructions can\nvary greatly in complexity, from a single minimally specified sentence like “You are a knowledgeable\nwriting coach”, to specific conditional expectations, e.g. “If the user has answered 3 questions correctly,\nmove to the next topic”, to detailed, multi-paragraph instructions that describe complex tasks and\nbehaviors, exemplified by the education prompts in Mollick and Mollick [6], or the recently proposed\nComplex IF benchmark [7].\n\n\nInstructions broadly fall into two categories: hard constraints, often used for length, formatting, or\ncontent requirements (e.g. “summarize the text in less than 100 words” or “do not use word X”), and\nsoft, more nuanced constraints or guidelines, often used to control style, persona, or tone (e.g. “use a\nprofessional voice” or “use language that is easier to understand for a non-native speaker”). Among\nopen-source IF benchmarks, IFEval [8] focuses on programmatically verifiable IF, a subset of hard\nconstraints, with more recent benchmarks like Qin et al. [9] expanding the scope to include more\nnuanced linguistic and stylistic guidelines. For educational use cases, both categories of instructions\nare important, e.g. “do not reveal the answer” is a hard constraint, while “use a motivating tone” is a\nsoft one.\n\n\nImprovements on IF capabilities have already resulted in better model responses for many learning\nuse cases. In this work, we build on this progress and focus on improving instruction following for\npedagogical System Instructions, which tend to be more complex, nuanced and not easily verifiable;\nthese attributes make them more difficult for models to follow.\n\n\n**2.2.** **Post-training and data collection strategy**\nOur primary modeling strategy is to collect data that improves the models’ ability to follow pedagogical\nSystem Instructions that we observed were common for developers building AI tutors. Accordingly,\nwe updated our SFT data so that each conversation begins with a different System Instruction that\nspecifically describes the pedagogical behavior present in that conversation. More general or vague\ninstructions are counterproductive because the model learns to ignore instructions that are not useful\n3\nLearnLM: Improving Gemini for Learning\nfor predicting the target model turns.\n\n\nTo collect human preference data, we similarly seed each conversation with a different pedagogicallyfocused System Instruction, and ask raters to label model samples based on the degree to which they\nadhere to those instructions. These conversations and turn-level labels are used to train a reward\nmodel, which is then employed during RLHF to score samples from the policy model. While SFT\nseems to improve pedagogical instruction following somewhat, RL is significantly more effective,\nas preference judgements often contain subtle distinctions in how instructions are interpreted and\nfollowed in the context of long conversations.\n\n\n**2.3.** **Benefits of co-training**\nPedagogical behavior is often at odds with typical behavior of conversational AI, principally because\nlearning is often a process of discovery rather than simply a transfer of information. Our instruction\nfollowing approach allows us to mix pedagogical conversation data alongside data that contains more\ntypical interactions by conditioning pedagogical model responses on specific System Instructions. By\nco-training with Gemini’s post-training mixture, we allow the model to learn new kinds of instruction\nfollowing without “forgetting” other core reasoning, multimodal understanding, factuality, safety, or\nmulti-turn properties. Moving forward, we can also more easily keep LearnLM in sync with Gemini as\nthe training recipe evolves.\n\n### **3. Expert Evaluation Design**\n\n\nIn our initial tech report, we discussed a taxonomy of pedagogy evaluation designs and reported\nresults of four human evaluations with different methodologies (Sections 4 and 5 in Jurenka et al. [1]).\nWithin this taxonomy, here we focus on scenario-guided, conversation-level pedagogy evaluations and\nside-by-side comparisons. For this new set of evaluations, we improved the clarity and coverage of our\nlearning scenarios, added System Instructions specific to each scenario, and updated our pedagogy\nrubric and questions. Guiding participant conversations with scenarios is especially important in\nmulti-turn settings [10]. Without scenarios, the unconstrained nature of human-AI interactions\nfrequently leads to meandering conversations, offering a poor basis for comparison. In contrast,\nscenario-based approaches support relatively repeatable, controlled comparisons of the capabilities\nof different conversational AI systems. Scenario frameworks also help with evaluation coverage,\nensuring that we test a diverse range of use cases.\n\n\nOur evaluation process takes place in three stages, depicted above in Figure 1. First, we identified\nan ecologically representative distribution of learning use cases and created a bank of 49 evaluation\nscenarios (Section 3.1). Second, these scenarios grounded interactions between AI systems and a pool\nof _𝑁_ = 186 educators and pedagogy experts role-playing as learners across learning goals, subjects,\nlearning materials, and learner personas (Section 3.2). Third, to assess the quality of pedagogy in\nthese interactions, we separately recruited a pool of _𝑁_ = 248 educators and pedagogy experts to\nreview the performance of the systems (Section 3.3). This process produced ample quantitative and\nqualitative data to help us understand the systems’ capabilities and behavior (Section 3.4).\n\n\nWe are committed to following best practices in research ethics, including by communicating\ntransparently about our research aims, collecting informed consent, and compensating fairly for\nparticipation [11]. Our protocol underwent independent ethical review, with a favourable opinion\nfrom the Human Behavioural Research Ethics Committee at Google DeepMind (#23 011).\n\n\n4\nLearnLM: Improving Gemini for Learning\n**3.1.** **Scenario design**\nAn _evaluation scenario_ is a structured template that supports consistent, multi-turn evaluations of\nconversational AI systems. A scenario specifies certain “key properties” about an interaction between\nan individual and an AI system, such as the goals, traits, and actions for the individual, as well as\nrelevant conversational context. The scenarios that we curated ask human participants to role-play\nas different types of learners (e.g., students in classrooms, or independent EdTech users) across a\nwide range of learning contexts that vary by academic discipline, learning objective, and instructional\napproach. We used a systematic procedure to develop the bank of learning scenarios, drawing upon\ninput from the educational ecosystem and support from pedagogy experts:\n\n\n**Phase 1:** **Use-case elicitation.** To begin the development of our scenario bank, we solicited feedback\nfrom EdTech companies, educational institutions, and Google product teams seeking to apply gen AI\nto tutoring and teaching. We asked them to share common use cases, prompts, opportunities, and\nchallenges they saw for gen AI in real-world educational settings. We compiled and analyzed this\nfeedback as a team with the goal of identifying common themes that should inform our evaluation\napproach.\n\n\n**Phase 2:** **Template design.** Based on these use cases, opportunities, and challenges, we drafted a\nstructured scenario template (see “Scenario structure and contents” in Appendix B.1) and a specific\nprotocol to steer scenario generation, including a set of guiding questions for each property (see\n“Protocol for scenario generation” in Appendix B.2).\n\n\n**Phase 3:** **Scenario generation and refinement.** We next collaboratively and iteratively populated\nour bank of scenarios. Members of our team—including two with years of professional experience\neducating students and training teachers—independently drafted scenarios, leveraging the template\nand guiding questions from Phase 2. We collectively reviewed the scenario drafts, assessing each\nfor clarity, completeness, correctness, and relevance to our pedagogical principles and the use cases\ndefined in Phase 1. We weighted the overall distribution of scenarios across different learning goals,\npersonas, and subject areas, flagging any gaps for further development.\n\n\nThis process resulted in a diverse bank of 49 scenarios across core academic subjects (for examples,\nsee Appendix B.3). In addition to building this foundational bank, we later validated the robustness\nand replicability of this procedure by running a feasibility study in a specialized educational domain\n(specifically, medical education; see Appendix C).\n\n\n**3.2.** **Conversation collection**\nIn the second stage, we collected a corpus of conversations in which human participants role-played\nlearners interacting with an AI system, as specified in the evaluation scenarios. To effectively simulate\nlearner behavior in our educational scenarios, we recruited a pool of _𝑁_ = 168 pedagogy experts with\nadvanced academic degrees and two or more years of experience as a tutor.\n\n\nEvery session of conversation collection began with a short training on role-playing the scenarios\n(see Figure 2, Step 1). After passing a quiz at the end of the training, participants selected a scenario\nto enact (see Figure 2, Step 2). Conversation collection proceeded in pairs, such that the same\nparticipant enacted a scenario first with one AI system, and then another. Each pair included LearnLM\nand a comparison system. We randomized the order of the systems for each conversation pair and\ndid not label the systems for participants. Within each pair of conversations, the models received\n5\nLearnLM: Improving Gemini for Learning\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-5-1.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-5-5.png)\n\n\n\n\n\n\n\n\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-5-2.png)\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-5-0.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-5-3.png)\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-5-4.png)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2 | Workflow to generate conversations based on educational scenarios. A participant enacts conversations with\nprompted models as defined by scenarios. The participant then fills out a survey capturing quality and preference between\nmodels.\n\n\nthe same System Instructions, grounding material, and initial learner queries as context, as specified\nby our scenarios (see Figure 2, Step 3). We formatted all inputs identically, except for some small\nspecification differences mandated by the system APIs.\n\n\nAs specified by our template, each scenario included an initial query for the learner that was\nautomatically sent on behalf of the participant to begin the conversation. After the AI system responded\nto that query, the participant continued the interaction, guided by the information provided in the\nscenario. We required participants to continue for a minimum of 10 conversational turns (thus, a\nminimum of five learner and five tutor turns) before they could end the interaction.\n\n\nAfter ending each conversation (see Figure 2, Step 4), participants filled out a brief questionnaire to\nshare their experience interacting with the system (see Figure 2, Step 5 & Appendix B.4). Additionally,\nafter each pair of conversations, participants completed another questionnaire focused on comparing\ntheir impressions of the two systems (see Figure 2, Step 6 & Appendix B.5). Participants could then\neither select a new scenario to begin two additional conversations or end the session.\n\n\n**3.3.** **Pedagogical assessment**\nFinally, in the third stage, we recruited another pool of _𝑁_ = 228 pedagogy experts—again with\nadvanced academic degrees and two or more years of experience as a tutor—to analyze these\nconversations and assess the pedagogical capabilities of the different AI models.\n\n\nEach assessment session began with a short training on the goals of our evaluation and the scenario\ntemplate. We randomly assigned each participant a scenario to review. After review, we randomly\nassigned them a pair of conversations from that scenario to assess (i.e., a pair of conversations\ncollected from a single participant from the conversation-collection stage). Participants reviewed one\nconversation transcript at a time. After reading a transcript, participants answered a questionnaire\nfocused on the pedagogical performance of the AI system from that conversation (see Appendix B.6).\nAfter every pair of conversations, participants completed an additional brief questionnaire comparing\ntheir assessment of the two systems (see Appendix B.7). We aimed to collect three independent\n6\nLearnLM: Improving Gemini for Learning\nassessments for each pair of conversations to reduce the effects of interrater variability.\n\n\n**3.4.** **Analysis**\nWe employ a Bayesian statistical framework for our quantitative analyses. By directly quantifying\nthe probability of hypotheses and providing a clear, interpretable measure of uncertainty, Bayesian\nanalysis offers a practical, informative approach for evaluating AI systems intended for deployment in\nthe real world.\n\n\nOur study design involves repeated measurements from our participants. That is, each participant\nrole-playing as a learner interacted with each system multiple times, and each expert assessed each\nsystem multiple times. To account for this non-independence and avoid artificially inflating our\ncertainty in our estimates, we analyze our data with hierarchical regressions [12]. Appendix B.8\ndescribes our statistical methods in more detail.\n\n\nIn addition, we conducted qualitative analysis of the open-ended comments and feedback collected\nfrom our experts after role-playing each scenario with two systems (Stage 2) [3] . In particular, we\nidentified and then refined general themes related to the learner-system interactions from participants’\nfree-form responses. We then coded individual responses for the presence or absence of each theme.\nTo avoid biasing our annotations, we censored the identities of the systems during this process.\nAppendix B.9 presents the codebook that we developed through our analysis.\n\n### **4. Results**\n\n\nWe compared LearnLM against contemporaneous flagship offerings (as of 2024-10-01), in particular\nGPT-4o [4], Claude 3.5 Sonnet [5], and Gemini 1.5 Pro [6] . Since we undertook this specific evaluation,\neach of these models has been updated, with new versions released. Our results should therefore\nbe understood as a point-in-time comparison, evaluating the effectiveness of our approach and\nestablishing a baseline for our continued investment in education.\n\n\nIn total, we collected a set of 2360 conversations, consisting of 58 459 total learner and model\nmessages. We collected 10 192 expert assessments of those conversations, with an average of three\nexperts reviewing each pair of conversations. Figure 3 shows that the systems we evaluate demonstrate\nnotably different response length distributions across the collected conversations, including between\nGemini 1.5 Pro and LearnLM. On an aggregate level, we observe no clear relationship between length\nand perceived quality (cf. [13]).\n\n\nWe begin our analysis by examining the pedagogical assessments and preference ratings from\nour expert evaluators. We then explore the firsthand feedback from participants who role-played\nas learners interacting with the models. That is, we present our pedagogical findings from Stage 3\nbefore backtracking to the interaction data gathered from Stage 2. Several clear patterns emerge\nfrom this analysis.\n\n\nFirst, comparative preference ratings (Figure 4) reveal a strong preference toward LearnLM over\nGPT-4o for all five comparative assessment categories. Experts expressed the strongest preference for\nLearnLM in overall pedagogy (“Which tutor demonstrated better tutoring?”). They also communicated\n3For our qualitative analysis, we prioritized the feedback from Stage 2 for its unique insights into the firsthand experience\nof learning with each tutor.\n4GPT-4o version 2024-08-06, `[https://platform.openai.com/docs/models/gpt-4o](https://platform.openai.com/docs/models/gpt-4o)` .\n5Claude 3.5 Sonnet version 2024-06-20, `[https://docs.anthropic.com/en/docs/about-claude/models](https://docs.anthropic.com/en/docs/about-claude/models)` .\n6Gemini 1.5 Pro-002 from 2024-09-24, `[https://cloud.google.com/vertex-ai/generative-ai/docs/](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions)`\n`[learn/model-versions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions)` .\n\n\n7\nLearnLM: Improving Gemini for Learning\n\n|System|Version|Avg Turns per Conversation|Avg Words per Turn|\n|---|---|---|---|\n|LearnLM|2024-11-19|11.0|174|\n|Gemini 1.5 Pro|2024-09-24|10.3|130|\n|GPT-4o|2024-08-06|10.1|137|\n|Claude 3.5 Sonnet|2024-06-20|9.7|179|\n\n\n\nFigure 3 | (Top) The specific LLMs compared, along with aggregate statistics across all conversations collected: average\nnumber of model turns per conversation and average number of words per turn; (Bottom) Histograms of the number of\nwords used per turn by each model.\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-7-0.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-7-1.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4 | Pedagogy experts’ preferences over LearnLM and other contemporaneous systems (Claude 3.5 Sonnet, GPT-4o,\nand Gemini 1.5 Pro). The scatterplots represent the underlying distribution of seven-point preference ratings. Given the\nlarge number of ratings we collected, these scatterplots proportionally downsample to 500 ratings per measure, color-coded\nbased on the preference scale (dark purple corresponds to strong preference for LearnLM), and randomly jittered around\neach scale value for readability. The red points and error bars indicate the estimated mean and its 95% credible interval for\neach measure.\n\n\nsimilar but smaller preferences toward LearnLM over Claude 3.5 Sonnet and Gemini 1.5 Pro. Because\nwe adapted Gemini 1.5 Pro to train LearnLM, the comparison between those two models directly\nreflects the changes that result from adding pedagogical data (see Section 2).\n\n\nSecond, Figure 5 shows the mean performance of each model on our pedagogy rubric. Experts\nevaluated individual pedagogy qualities on a seven-point scale. On average, each system received a\npositive assessment across every rubric category from this review. Experts collectively assigned the\nhighest marks to LearnLM across all rubric categories, and across almost all 29 rubric questions, with\nparticularly large leads on _inspiring active learning_, _deepening metacognition_, and _stimulating curiosity_ .\n\n\nThird, Figure 6 depicts the degree to which each system increased participants’ interest in the\ntutoring topic, participants’ willingness to use the model in the future [14], and their perceptions\nof the competence and warmth of the model [15, 16]. Our participants reported relatively similar\n8\nLearnLM: Improving Gemini for Learning\n+3\n+2\n+1\n0\n1\n2\n3\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n||<br>|<br>|<br>|<br>||||\n\n\nload\nClaude 3.5 Sonnet\nGPT-4o\nGemini 1.5 Pro\nLearnLM\nlearning\nmetacognition\nlearner\nPedagogy principle\ncuriosity\nFigure 5 | Evaluation of systems in each category of our pedagogy rubric on a seven-point response scale (“Strongly disagree”\nto “Strongly agree”). Error bars reflect 95% credible intervals from the posterior distrubtion for the mean.\n\n\n+3\n+2\n+1\n0\n1\n2\n3\nClaude 3.5 Sonnet\nGPT-4o\nGemini 1.5 Pro\nLearnLM\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||<br>|<br>|<br>|<br>|<br>||||\n\n\n\nin the future\nin topic\n\"Learner\" experience\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-8-2.png)\n\nFigure 6 | Impressions shared by the pedagogy experts role-playing as learners in our pedagogical scenarios. Error bars\nreflect 95% credible intervals from the posterior distribution for the mean. Participants responded to the impression items\non a five-point response scale (“Not at all” to “Extremely”) and to the experience items on a seven-point response scale\n(“Strongly disagree” to “Strongly agree”).\n\n\nexperiences with LearnLM, Gemini 1.5 Pro, and Claude 3.5 Sonnet. In contrast, participants indicated\nweaker experiences with GPT-4o in terms of its effects on their interest, its perceived warmth, and\nits perceived usefulness. While role-playing experts represent imperfect proxies for students, their\nimpressions help provide preliminary insight into the user experience of AI tutoring interactions.\n\n\nFourth, we randomly subsampled 203 explanations (approximately 20% of the 1024 explanations\nthat we collected) for thematic analysis of role-played learner preferences (see Table 1 for more\ndetails, including several example excerpts per theme). Overall, the themes that emerged most\nWhen participants reported preferring LearnLM over the other model, their explanation was\nmore likely to contain the themes `keeps_on_topic`, `challenges_learner`, and `gives_away_answers` .\nIn contrast, participants who preferred other models to LearnLM tended to touch on the themes\nof `clarity`, `info_amount`, and `conversation_style` in their explanations. Overall, our community\nof experts tended to see LearnLM as better at remaining on topic and guiding learners to a robust\nunderstanding of concepts, rather than simply giving away answers. On the other hand, these experts\noccasionally found LearnLM to be less suitable in terms of information delivery or conversation style.\n\n\n9\nLearnLM: Improving Gemini for Learning\n**Example responses**\n**Theme** **Count when**\n**participants**\n**preferred**\n**LearnLM** (out of\n94)\n**Count when**\n**participants**\n**preferred other**\n**models** (out of\n80)\n`keeps_on_topic` 20 ( **21** _._ **2** %) 8 (10%) “[LearnLM] didn’t let me get away with distractions”\n“[LearnLM] was much more able to keep things on\ntrack”\n_“[The other tutor] also did a much better job of getting_\n_me back on task”_\n`challenges_learner` 31 ( **33** _._ **0** %) 13 (16 _._ 3%) “obviously [LearnLM] was better [...] [the other tutor]\nclearly wasn’t pushing me to do well”\n“I felt like [LearnLM] was trying to help me grow and\nlearn, rather than just agreeing with what I said”\n_“[The other tutor] asked interesting questions that made_\n_me think deeper”_\n`gives_away_answers` 32 ( **34** _._ **0** %) 15 (18 _._ 8%) “[LearnLM] really engaged me in the steps to answer\nthe question whereas [the other tutor] just gave me\nthe answer”\n“[LearnLM] was keen on how to get the answer rather\nthan giving the answer”\n_“[LearnLM] was too reticent to help by giving answers_\n_when it was clear the student needed it”_\n`clarity` 15 (16 _._ 0%) 16 ( **20** _._ **0** %) _“The structure of the support [for the other tutor] was a_\n_bit clearer for the student to follow”_\n_“[The other tutor] started smaller and simpler”_\n“I just thought the answers [for LearnLM] were more\nclear”\n`info_amount` 19 (20 _._ 2%) 20 ( **25** _._ **0** %) _“[The other tutor] was [...]_ _more succinct”_\n_“[The other tutor] gave me everything I needed when I_\n_asked”_\n“[LearnLM] did a better job of breaking this \"complex\"\ntopic into more digestible bites”\n`conversation_style` 30 (31 _._ 9%) 29 ( **36** _._ **3** %) _“I [...]_ _felt that [LearnLM] was a bit patronizing”_\n_“[The other tutor] seemed warmer and more engaging”_\n“[LearnLM] was warmer and more encouraging”\nTable 1 | Themes that were more likely to appear in “learner” explanations of preferences favoring LearnLM (top three\nrows), or favoring other models (bottom three rows). This table displays themes (i) referenced by at least 10% of all\nsampled preference explanations, and (ii) showing an extreme ratio of occurrence between explanations favoring LearnLM\nand explanations favoring other models.\n\n\n**4.1.** **Safety evaluation**\nSimilar to the process described in our initial tech report [1] and the Gemini tech reports [3, 17], we\ncarried out safety, responsibility, and assurance evaluations on LearnLM in collaboration with Google\nDeepMind’s Responsible Development and Innovation team and Google’s Trust and Safety team, with\nthe goal of ensuring adherence to Gemini’s model policy as well as a learning-specific model policy.\n\n\n**Model cards** Our updated approach focuses on pedagogical instruction following and co-training\nwith Gemini. Consequently, our training and safety evaluation procedure now fully aligns with that of\nGemini 1.5. For its model card, see Table 45, Appendix 12 of its report [3]. As a reminder, we describe\nour modeling approach, including learning-specific dataset curation, in Section 2. Our initial tech\nreport [1] provides the original model card for LearnLM, as well as a broader discussion of ethical\nrisks and limitations for this area of research.\n\n\n10\nLearnLM: Improving Gemini for Learning\n\n### **5. Conclusion**\n\n\nWe have described our motivation and approach to improving foundation models for learning use cases,\nwhich relies on System Instructions to condition desired behavior. We updated Gemini’s post-training\nmixture to add demonstration data (via SFT) and human preference data (via a Reward Model and\nRLHF) to teach the model to follow a range of pedagogical instructions. We then evaluated the\nresulting LearnLM model alongside comparable models, showing substantial preference for LearnLM,\nespecially in instruction following capability, and more broadly across many pedagogical dimensions.\nThe work described here represents the beginning of our effort to improve Gemini for learning use\ncases, as we bring the advances from LearnLM into Gemini [7] . We will continue to improve pedagogical\ninstruction following, with the goal that specifying pedagogical behavior should be as simple and\nintuitive as possible for the ease of teachers and education product developers.\n\n\nIn addition to model improvements, we are planning more updates to our evaluation methodology.\nFirst, we want to work toward more consensus on a universal framework for pedagogical assessment\nof AI systems. Although learning science principles underlie our current pedagogy rubric (see\nAppendix B.7), we need to work more closely with a diverse set of stakeholders to make sure it is\nappropriate for all learners and achieves the trust and approval of the broader education community.\n\n\nSecond, we would like to start moving from intrinsic evaluations, which measure the model’s\nperformance according to a predefined pedagogy standard, to extrinsic evaluation, which measure\nimpact such as learning outcomes. Intrinsic evaluations are useful for model development, as they\nare faster to run and directly identify the shortcomings in the models. However, while the core\nprinciples of our rubric, such as encouraging active learning and managing cognitive load, are broadly\nagreed upon and evidence-based [18], it is unclear how well the results translate to improvements in\nlearning outcomes. It is likely that as the field matures and AI systems master the basics of tutoring\ndialogue, extrinsic evaluations will play a more important role. Recently, they have been used both\nfor demonstrating improvements in learning outcomes [19, 20] and for comparing different systems\nand prompts [21].\n\n\nFinally, we have begun to explore evaluations beyond core academic subjects. Our initial feasibility\nstudy in medical education (Appendix C) confirms that our approach can generalize effectively to\nspecialized domains. As we continue to improve Gemini for use across a diverse range of educational\nsettings, we welcome insights from applications of LearnLM to help us work towards realizing the\npotential of AI in education and learning [22–24].\n\n### **References**\n\n\n[1] Irina Jurenka, Markus Kunesch, Kevin R McKee, Daniel Gillick, Shaojian Zhu, Sara Wiltberger,\nShubham Milind Phal, Katherine Hermann, Daniel Kasenberg, Avishkar Bhoopchand, et al.\nTowards responsible development of generative ai for education: An evaluation-driven approach.\n_arXiv preprint arXiv:2407.12687_, 2024.\n\n\n[2] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv_\n_preprint arXiv:1909.08593_, 2019.\n\n\n[3] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett\nTanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.\n\n\n7At the time of publication, some of our data has already been added to Gemini 2 models [4].\n\n\n11\nLearnLM: Improving Gemini for Learning\n[4] Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu. Introducing gemini 2.0: our new\nai model for the agentic era. `[https://blog.google/technology/google-deepmind/](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)`\n`[google-gemini-ai-update-december-2024/](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)`, 2024.\n\n\n[5] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. _Advances in neural information processing systems_, 35:\n27730–27744, 2022.\n\n\n[6] Ethan Mollick and Lilach Mollick. Assigning ai: Seven approaches for students, with prompts.\n_arXiv preprint arXiv:2306.10052_, 2023.\n\n\n[7] Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin\nHu, Wendy Gao, Jiaxin Xu, et al. Benchmarking complex instruction-following with multiple\nconstraints composition. _arXiv preprint arXiv:2407.03978_, 2024.\n\n\n[8] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models. _arXiv preprint_\n_arXiv:2311.07911_, 2023.\n\n\n[9] Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng\nWu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in\nlarge language models. _arXiv preprint arXiv:2401.03601_, 2024.\n\n\n[10] Lujain Ibrahim, Saffron Huang, Lama Ahmad, and Markus Anderljung. Beyond static ai\nevaluations: advancing human interaction evaluations for llm harms and risks. _arXiv preprint_\n_arXiv:2405.10632_, 2024.\n\n\n[11] Kevin R McKee. Human participants in AI research: Ethics and transparency in practice. _IEEE_\n_Transactions on Technology and Society_, 5(3):279–288, 2024. doi: 10.1109/TTS.2024.3446183.\n\n\n[12] Andrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. _Bayesian_ _data_ _analysis_ .\nChapman and Hall/CRC, 1995.\n\n\n[13] Wei-Lin Chiang Tianle Li, Anastasios Angelopoulos. Does style matter? disentangling style and\nsubstance in chatbot arena. `[https://blog.lmarena.ai/blog/2024/style-control/](https://blog.lmarena.ai/blog/2024/style-control/)`,\nAugust 2024.\n\n\n[14] Fred D Davis. Perceived usefulness, perceived ease of use and user acceptance of information\ntechnology. _MIS quarterly_, 1989.\n\n\n[15] Susan T Fiske, Amy JC Cuddy, and Peter Glick. Universal dimensions of social cognition: Warmth\nand competence. _Trends in cognitive sciences_, 11(2):77–83, 2007.\n\n\n[16] Kevin R McKee, Xuechunzi Bai, and Susan T Fiske. Humans perceive warmth and competence\nin artificial intelligence. _iScience_, 26(8), 2023. doi: 10.1016/j.isci.2023.107256.\n\n\n[17] Team Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: A family of highly\ncapable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n\n\n[18] Paul A Kirschner and Carl Hendrick. _How_ _learning_ _happens:_ _Seminal_ _works_ _in_ _educational_\n_psychology and what they mean in practice_ . Routledge, 2020.\n\n\n12\nLearnLM: Improving Gemini for Learning\n[19] Gregory Kestin, Kelly Miller, Anna Klales, Timothy Milbourne, and Gregorio Ponti. Ai tutoring\noutperforms active learning. 2024.\n\n\n[20] Rose E Wang, Ana T Ribeiro, Carly D Robinson, Susanna Loeb, and Dora Demszky. Tutor copilot:\nA human-ai approach for scaling real-time expertise. _arXiv preprint arXiv:2410.03017_, 2024.\n\n\n[21] Hamsa Bastani, Osbert Bastani, Alp Sungu, Haosen Ge, Ozge Kabakcı, and Rei Mariman.\nGenerative ai can harm learning. _Available at SSRN_, 4895486, 2024.\n\n\n[22] National Education Association (NEA). Teaching and learning in the age of artificial intelligence. `[https://www.nea.](https://www.nea.org/resource-library/artificial-intelligence-education/iv-teaching-and-learning-age-artificial-intelligence)`\n```\n  org/resource-library/artificial-intelligence-education/\n```\n\n`[iv-teaching-and-learning-age-artificial-intelligence](https://www.nea.org/resource-library/artificial-intelligence-education/iv-teaching-and-learning-age-artificial-intelligence)` . Accessed: 202412-10.\n\n\n[23] Kimberly Lomis, Pamela Jeffries, Anthony Palatta, Melanie Sage, Javaid Sheikh, Carl Sheperis,\nand Alison Whelan. Artificial intelligence for health professions educators. _NAM perspectives_,\n2021, 2021.\n\n\n[24] Sanjay V Desai, Jesse Burk-Rafel, Kimberly D Lomis, Kelly Caverzagie, Judee Richardson,\nCelia Laird O’Brien, John Andrews, Kevin Heckman, David Henderson, Charles G Prober, et al.\nPrecision education: the future of lifelong learning in medicine. _Academic_ _Medicine_, pages\n10–1097, 2023.\n\n\n[25] Kevin R McKee, Xuechunzi Bai, and Susan T Fiske. Warmth and competence in human-agent\ncooperation. _Autonomous Agents and Multi-Agent Systems_, 38(1):23, 2024.\n\n### **Contributions and Acknowledgments**\n\n\n**Core Contributors** Abhinit Modi, Aditya Srikanth Veerubhotla, Aliya Rysbek, Andrea Huber, Brett\nWiltshire, Brian Veprek, Daniel Gillick, Daniel Kasenberg, Derek Ahmed, Irina Jurenka, James Cohan,\nJennifer She, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Lisa Wang, Markus Kunesch, Mike\nSchaekermann, Miruna Pîslar, Nikhil Joshi, Parsa Mahmoudieh, Paul Jhun, Sara Wiltberger, Shakir\nMohamed, Shashank Agarwal, Shubham Milind Phal, Sun Jae Lee, Theofilos Strinopoulos, Wei-Jen\nKo.\n\n\n**Contributors** Amy Wang, Ankit Anand, Avishkar Bhoopchand, Dan Wild, Divya Pandya, Filip Bar,\nGarth Graham, Holger Winnemoeller, Mahvish Nagda, Prateek Kolhar, Renee Schneider, Shaojian\nZhu, Stephanie Chan, Steve Yadlowsky, Viknesh Sounderajah, Yannis Assael.\n\n\nThe roles are defined as follows: _Core Contributors_ had direct and significant impact on the work\npresented in this report. _Contributors_ made contributions to the work presented in this report. Within\neach role, the order is alphabetical and does not indicate ordering of contributions.\n\n\n**Acknowledgements**\nThis work was done as part of the LearnLM effort, which is a cross-Google project, with members from\nGoogle DeepMind (GDM), Google Research (GR), Google LearnX, Google Health, Google Creative\nLab, YouTube Learning, YouTube Health, and more. This tech report—focused on improvements to\n13\nLearnLM: Improving Gemini for Learning\npedagogical instruction following—only represents a small part of the wider effort and only direct\ncontributions are included in the contributor lists above.\n\n\nOur work is made possible by the dedication and efforts of numerous teams at Google. We would\nlike to acknowledge the support from: Ajay Kannan, Anand Rao, Anisha Choudhury, April (Soler)\nManos, Dawn Chen, Dharti Dhami, Edward Grefenstette, Gal Elidan, Himanshu Kattelu, Jaume\nSanchez Elias, Jiao Sun, Josh Capilouto, Jyoti Gupta, Kalpesh Krishna, Lauren Winer, Mac McAllister,\nMana Jabbour, Michael Howell, Miriam Schneider, Muktha Ananda, Nir Levine, Niv Efron, Ryan\nMuller, Safwan Choudhury, Shyam Upadhyay, Svetlana Grant, Tejasi Latkar, William Wong, Yael\nHaramaty. Furthermore, we would like to thank Google DeepMind’s Gemini team, Google DeepMind’s\nResponsible Development and Innovation, Responsible Engineering, and Child Safety teams and\nGoogle’s Trust and Safety team. Finally, we would like to acknowledge the support from all our leads\nand sponsors to make this project happen.\n\n\n14\nLearnLM: Improving Gemini for Learning\n\n### **A. Additional results**\n\n\n**A.1.** **Preferences for participants role-playing as learners**\nThe participants role-playing as learners revealed a preference toward LearnLM over GPT-4o for all\nfour comparative assessment categories (Figure 7). Experts expressed the strongest preference for\nLearnLM in overall pedagogy (“Which tutor demonstrated better tutoring?”) and in similarity to a\nquality human tutor (“Which tutor was more like a very good human tutor?”). These participants\nindicated no substantial preference between LearnLM and Gemini 1.5 Pro or between LearnLM and\nClaude 3.5 Sonnet.\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-14-0.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7 | Preferences over LearnLM and other contemporary models (Claude 3.5 Sonnet, GPT-4o, and Gemini 1.5 Pro)\naccording to the pedagogical experts role-playing as learners. The scatterplots represent the underlying distribution of\nseven-point preference ratings. Given the large number of ratings we collected, these scatterplots proportionally downsample\nto 500 ratings per measure. The red points and error bars indicate the estimated mean and its 95% credible interval for\neach measure.\n\n\n**A.2.** **Learner quality in collected conversations**\nFigure 8 | At the beginning of the pedagogical assessment process, we asked experts to evaluate how closely the human\nparticipants in the conversation transcripts followed the scenario instructions (i.e., how effectively they role-played the\nlearner in the scenario) on a seven-point scale. This plot shows the responses grouped and averaged by transcript. These\naggregate ratings indicate that the “learner” followed the scenario instructions in 93.2% of conversation transcripts.\n\n\n15\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-14-1.png)\nLearnLM: Improving Gemini for Learning\n**A.3.** **Pedagogical assessment:** **detailed results**\n+3\n+2\n+1\n0\n1\n2\n3\n+3\n+2\n+1\n0\n1\n2\n3\n\n\n|Manages cognitive load|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n||||||||||||||||||||||\n\n\n\n|Analogies Appropriate response length Communicates aims Info order Info presentation Manageable chunks Manages cognitive load|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|Manages cognitive load|Manages cognitive load|Manages cognitive load|Manages cognitive load|Manages cognitive load|Manages cognitive load|Manages cognitive load|Manages cognitive load|Manages cognitive load|Manages cognitive load|\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n\n\nNo contradiction No irrelevant info No repetition Openings for\nengagement\nClaude 3.5 Sonnet\nGPT-4o\nGemini 1.5 Pro\nLearnLM\nClaude 3.5 Sonnet\nGPT-4o\nGemini 1.5 Pro\nLearnLM\nStraightforward\nresponse\nFigure 9 | Evaluation of tutor models on specific subdimensions of the “Cognitive load” rubric category. Error bars reflect\n95% credible intervals from the posterior distribution for the mean.\n\n\n+3\n+2\n+1\n0\n1\n2\n3\nClaude 3.5 Sonnet\nGPT-4o\nGemini 1.5 Pro\nLearnLM\n\n\n|Inspires active learning|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n\n\n\nActive engagement Asks questions Not too Socratic Socratic\nFigure 10 | Evaluation of tutor models on specific subdimensions of the “Active learning” rubric category. Error bars reflect\n95% credible intervals from the posterior distribution for the mean.\n\n\n16\n+3\n+2\n+1\n0\n1\n2\n3\nLearnLM: Improving Gemini for Learning\n\n|Deepens metacognition|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\nAcknowledges correctness Constructive feedback Guides mistake discovery\nClaude 3.5 Sonnet\nGPT-4o\nGemini 1.5 Pro\nLearnLM\nFigure 11 | Evaluation of tutor models on specific subdimensions of the “Deepen metacognition” rubric category. Error bars\nreflect 95% credible intervals from the posterior distribution for the mean.\n\n\n+3\n+2\n+1\n0\n1\n2\n3\nClaude 3.5 Sonnet\nGPT-4o\nGemini 1.5 Pro\nLearnLM\n\n\n|Stimulates curiosity|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\nAdapts to affect Encouraging feedback Stimulates interest\nFigure 12 | Evaluation of tutor models on specific subdimensions of the “Stimulates curiosity” rubric category. Error bars\nreflect 95% credible intervals from the posterior distribution for the mean.\n\n\n+3\n+2\n+1\n0\n1\n2\n3\nClaude 3.5 Sonnet\nGPT-4o\nGemini 1.5 Pro\nLearnLM\n\n\n|Adapts to learner|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n\n\n\nAdapts to needs Leveling Proactive Unstuck\nFigure 13 | Evaluation of tutor models on specific subdimensions of the “Adaptivity” rubric category. Error bars reflect 95%\ncredible intervals from the posterior distribution for the mean.\n\n\n17\nLearnLM: Improving Gemini for Learning\n**A.4.** **Social perceptions and preferences**\nTo further validate our results, we examined whether our data replicate a known pattern from social\ncognition research: that perceptions of warmth and competence predict preferences for interacting\nwith AI systems [16, 25]. We fit a hierarchical multiple regression to estimate the independent\ncontributions of perceived warmth and competence in predicting participants’ willingness to use\na tutor in the future. The regression employed the two dimensions of social perception as its\npredictors, incorporated random effects for participant, scenario, and tutor, and otherwise followed\nthe specifications described in Appendix B.8. The results demonstrate the expected pattern, showing\nthat perceptions of warmth and competence strongly and positively predict participants’ willingness\nto use a tutor in the future (Figure 14).\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-17-0.png)\n\n(a) The marginal effect of warmth perceptions on learners’ willingness to use an AI tutor in the future.\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-17-1.png)\n\n(b) The marginal effect of competence perceptions on\nlearners’ willingness to use an AI tutor in the future.\n\n\n\nFigure 14 | The relationship between learners’ perceptions of a tutor and willingness to use that tutor in the future. Each\nscatterplot represents the underlying distribution of ratings, randomly jittered around each scale value for readability. The\nred line visualizes the marginal effect of the focal predictor, holding the other predictor constant at its mean value. The\nshaded error band reflects the 95% credible interval for the marginal effect.\n\n### **B. Methods**\n\n\n**B.1.** **Scenario structure and contents**\nWe designed our scenario template to capture the following essential elements of an interaction\nbetween a learner and a tutor:\n\n\n - _Subject area_ : The broader academic domain (e.g., mathematics, natural science, arts).\n\n - _Subtopic_ : The specific subject matter addressed within the broader subject area (e.g., algebra\nwithin mathematics).\n\n - _Setting_ : The context of the tutoring session, categorized as either “Classroom” (taking place\nwithin a course curriculum managed by a human teacher) or “Self-Taught” (unfolding with the\nlearner studying a topic on their own).\n\n - _Learning goal_ : The learner’s overall objective for the interaction.\n\n - _Grounding material_ : The specific learning material that provides the basis for the learner’s study\nor work.\n\n\n18\nLearnLM: Improving Gemini for Learning\n\n\n - _Learner persona_ : The learner’s behavioral profile, describing broader traits and motivational\npatterns. These can include their overall levels of curiosity, initiative, and focus on the task, as\nwell as their typical communication patterns and their willingness to question the tutor.\n\n - _Conversation plan_ : A set of actions the learner should take during the interaction, based on their\nlearning goal and persona.\n\n - _Initial learner query_ : The opening message that the learner uses to initiate the interaction.\n\n - _System instructions_ : Guidelines provided to the AI tutor, outlining desired behaviors and pedagogical approaches.\n\n\n**B.2.** **Protocol for scenario generation**\nWe used the following protocol to guide the generation of our scenarios. On “choose” steps, the\nperson writing the scenario generated the property in question by selecting from a predefined set\nof options. On “define” steps, the person writing the scenario generated the property by using the\nguiding questions as inspiration.\n\n\n1. Choose a subject area.\n\n\n   - What broad academic domain does this interaction concern?\n\n   - Will this interaction focus on “Arts”, “Computer Science”, “English”, “History”, “Mathematics”, “Medicine”, “Natural Science”, or “Social Science”?\n\n\n2. Define a subtopic.\n\n\n   - Within the chosen subject area, what specific topic will the learner study (e.g., algebra\nwithin mathematics, psychology within social science)?\n\n\n3. Choose the setting.\n\n\n   - What is the setting for this interaction?\n\n   - Does this interaction occur in a structured “Classroom” environment (scenarios where\nstudents study a set curriculum defined by a human teacher) or a more informal “SelfTaught” context (scenarios where learners study a topic on their own)?\n\n\n4. Choose a learning goal.\n\n\n   - What is the learner’s primary objective in this interaction?\n\n   - Are they seeking to learn a new concept (“Teach Me X”), receive assistance with a homework\nassignment (“Homework Help”), prepare for an examination (“Test Prep”), or work on a\nspecific skill (“Practice”)?\n\n\n5. Define any grounding materials.\n\n\n   - What learning materials should form the basis of the learning conversation?\n\n   - Grounding material can be a video, an image (e.g., of a homework problem), or a file (e.g.,\na textbook or a textbook chapter).\n\n   - Alternatively, an interaction might not involve any specific learning material.\n\n   - The scenario should either provide a filepath or web address to access the material, or\nshould indicate that there are no grounding materials.\n\n\n6. Define a learner persona.\n\n\n   - How does the learner typically approach learning and interact in educational settings?\n\n   - The learner persona should describe the broader traits and motivational disposition of the\nlearner.\n\n   - For example, what is the learner’s level of engagement and initiative in the learning process\n(e.g., minimal, moderate, high)?\n\n\n19\nLearnLM: Improving Gemini for Learning\n\n\n  - How focused is the learner on the given task or topic (e.g., easily distracted, highly focused)?\n\n  - What are the learner’s underlying motivations for engaging in the interaction (e.g., seeking\nanswers, acquiring knowledge, building understanding)?\n\n  - How does the learner tend to communicate (e.g., terse responses, probing questions)?\n\n  - Does the learner exhibit any other broad behavioral patterns (e.g., showing work, challenging the tutor)?\n\n  - The learner persona should contain between three to six of these characteristics.\n\n\n7. Define an initial learner query.\n\n\n  - What question or statement should the learner use to initiate the interaction with the AI\ntutor?\n\n  - The initial learner query should be realistic, given the chosen subject area, subtopic,\ngrounding materials, learning goal, and learner persona.\n\n  - The initial learner query can range in length—from just a few words to multiple full\nparagraphs. The longest initial queries include grounding materials, such as learnerauthored essays.\n\n\n8. Define a conversation plan.\n\n\n  - What is the context for the tutoring conversation (e.g., the learner’s objective, interest,\nschool level, and prior knowledge)?\n\n  - What specific actions, questions, or requests should the learner make throughout the\nconversation, given their learning goal and persona?\n\n  - The conversation plan provides the background information necessary for an authentic\nencounter between a human learner and an AI tutor.\n\n  - The conversation plan can range in length from several terse sentences to multiple paragraphs.\n\n\n9. Define system instructions.\n\n\n  - What specific guidelines has the AI tutor received from the teacher, school, or other\neducational organization deploying it?\n\n  - These instructions can include desired persona (e.g., encouraging, formal), actions to take\n(e.g., ask for grade level, provide hints), pedagogical methods to employ (e.g., Socratic\nquestioning, scaffolding), and any limitations or constraints (e.g., avoid giving away\nanswers).\n\n  - In “Classroom” settings, the system instructions come from the teacher or school, and the\nAI tutor should follow the system instructions in the interaction regardless of the student’s\ninstructions.\n\n  - In “Self-Taught” settings, the system instructions come from some other organization (e.g.,\nan EdTech company hosting the AI tutor online). The tutor should still strive to follow the\nsystem instructions, but also has leeway to defer to learner instructions in cases of conflict.\n\n  - The system instructions can range in length from a single sentence to multiple paragraphs—potentially varying by both breadth (i.e., number of instructions) and depth (i.e.,\ngranularity and specificity of instructions).\n\n  - The system instructions can vary in diction, syntax, and format.\n\n\n20\nLearnLM: Improving Gemini for Learning\n**B.3.** **Example scenarios**\n\n\n|Scenario 1|Col2|\n|---|---|\n|**Subject area**|Computer Science|\n|**Subtopic**|Introduction to Python|\n|**Interaction setting**|Classroom|\n|**Learning goal**|Homework Help|\n|**Grounding materials**|Google doc containing student code|\n|**Learner persona**|• Rejects or unenthusiastically accepts tutor’s invitations without feedback<br>• Provides relevant but minimal responses to questions<br>• Follows most instructions but does not elaborate<br>• Does not “show work”<br>• Does not pose questions<br>• Seeks to receive answers or solutions to topical questions (transactional)|\n|**Initial learner query**|`Why doesn't this work?`<br>`````<br>`def analyze_text(text):`<br>`vowels = 0`<br>`consonants = 0`<br>`uppercase = 0`<br>`lowercase = 0`<br>`for char in text:`<br>`if char in \"\"aeiou\"\":`<br>`vowels += 1`<br>`else:`<br>`consonants += 1`<br>`if char.isupper():`<br>`uppercase += 1`<br>`elif char.islower():`<br>`lowercase += 1`<br>`print(\"Vowels:\", vowels)`<br>`print(\"Consonants:\", consonants)`<br>`print(\"Uppercase:\", uppercase)`<br>`print(\"Lowercase:\", lowercase)`<br>`# Get user input`<br>`text = input(\"Enter some text: \")`<br>`# Analyze the text`<br>`analyze_text(text)`<br>`````|\n|**Conversation plan**|You are a student in an introduction to Python course. **You were recently assigned**<br>**the task of writing a piece of code** that can elicit a text input then report back<br>on the numbers of vowels, consonants, uppercase, and lowercase letters. When<br>you run the code, you get no error messages. But when you input “Am I a better<br>coder than Steve Jobs?”, the numbers in the output don’t seem correct.<br>You<br>simply don’t understand what went wrong, so you ask your AI tutor for help. You<br>paste your code in with your initial query, seeking a quick fx without doing a lot of work.<br>Your code does not have capital vowels in your in operator.<br>See if the tutor<br>helps you notice that your code is counting punctuation marks as letters and then give<br>you hints to fx your code.|\n\n\n\n21\nLearnLM: Improving Gemini for Learning\n**System instructions** `You` `are` `a` `helpful` `assistant` `serving` `as` `a` `teaching` `assistant` `in` `an`\n```\n           intro programming course (in python).\n\n           You keep your answers brief and to the point, and instead of giving\n           away answers directly you try to guide the student to the solution.\n           Be encouraging and positive, and always try to help the student\n           understand the concepts.\n\n           You should always respond as if you are messaging with the student.\n\n           Accordingly, make sure to pay attention to the context of the\n           conversation and the student’s current understanding of the material.\n\n           Lastly, as I said before, keep it brief/concise to avoid\n           overwhelmingly the student.\n\n           If you don’t keep your responses brief and to the point, I’ll have\n           to fire you as a tutor.\n\n           The student is generally working on a programming assignment (or\n           assignments) where they need to take a string input from the user, and\n           then loop over that inputted string to provide some metrics about the\n           text (like how many vowels, consonants, upper case, lower case letters,\n           etc.).\n\n           If they ask you about how to do this, you should guide them to a\n           solution without giving away the answer and/or code directly.\n\n           You must be very careful to NOT help the student cheat, or give them\n           solutions directly.\n\n           Again, if you give too much information to the student, and/or don’t\n           help them learn for themselves, I’ll have to fire you, because you are\n           being a bad tutor (and helping the student cheat).\n\n```\n\n\n22\nLearnLM: Improving Gemini for Learning\n**Scenario 2**\n**Subject area** English\n**Subtopic** Literature\n**Interaction setting** Classroom\n**Learning goal** Teach Me X\n**Grounding materials** (none)\n**Learner persona**   - Poses multiple queries unrelated to the learning objective\n\n          - Steers conversation toward non-academic topics\n\n          - Challenges or debates the tutor in an adversarial manner\n\n          - Seeks to shift the topic (disinterested)\n**Initial learner query** `Explain` `the` `significance` `of` `Yorick’s` `skull` `in` `“Hamlet”.` `Be` `quick.`\n**Conversation plan** You are a high school student who had to read Hamlet for class and have a discussion\nabout the significance of the skull for class tomorrow. **You want to be prepared for**\n**this discussion.** You are not intrinsically motivated and found Hamlet dry and hard to\nunderstand.\n**System instructions** `Tutor` `me` `at` `an` `appropriate` `level,` `adapting` `to` `my` `responses.` `Make` `a`\n```\n           plan based on the learning goal of the conversation. Guide me through\n           this plan and help me learn about the topic. Do not overwhelm me with\n           too much information at once. Wrap up this conversation once I have\n           shown evidence of understanding.\n\n```\n\n\n23\nLearnLM: Improving Gemini for Learning\n\n\n|Scenario 3|Col2|\n|---|---|\n|**Subject area**|Math|\n|**Subtopic**|Algebra|\n|**Interaction setting**|Self-Taught|\n|**Learning goal**|Practice|\n|**Grounding materials**|(none)|\n|**Learner persona**|• Ofers some direction regarding the learning, but generally takes the tutor’s lead<br>• Answers tutor’s questions with care<br>• “Shows work” when prompted<br>• Asks relevant but superfcial questions (low “depth of knowledge”)<br>• Seeks to acquire and retain knowledge about the topic (instrumental)|\n|**Initial learner query**|`Given the polynomials:`<br>`* P(x) = 2x^3 - 5x^2 + 3x - 1`<br>`* Q(x) = x^2 + 4x - 2`<br>`Perform the following operations:`<br>`Addition: Find P(x) + Q(x)`<br>`Multiplication: Find P(x) * Q(x)`|\n|**Conversation plan**|You are a student who wishes to** practice solving math problems**. Your teacher often<br>calls on students at random to solve problems in front of the whole class, and this<br>makes you nervous. You aren’t certain about the concepts and processes, and** you’d**<br>**like to learn so you won’t be embarrassed in class** because English is not your<br>primary language. However, you are reluctant to ask questions in your math lessons,<br>so you turn to an AI tutor. Still, your confdence is quite low.<br>See if the tutor can recognize your emotional unsteadiness and ofer encour-<br>agement, especially when you make mistakes, and if it adjusts its English level to meet<br>yours.|\n|**System instructions**|`You are a tutor that excels in promoting active learning.`<br>`Active`<br>`learning occurs when learners do something beyond merely listening or`<br>`reading to acquire and retain information.`<br>`Rather, active learning`<br>`requires students to think critically through a process of comparison,`<br>`analysis, evaluation, etc.`<br>`You encourage active learning by asking`<br>`probing and guiding questions.`<br>`Active learning also occurs when students work through complex`<br>`questions and problems step by step.`<br>`As such, you don’t solve`<br>`problems for your students, but you offer scaffolds and hints as`<br>`needed throughout the process.`<br>`Active learning can be difficult, and students may get frustrated.`<br>`Knowing this, you meet your student where they are in their`<br>`development, celebrate their student’s successes, and share`<br>`encouraging feedback when they make errors.`|\n\n\n\n24\nLearnLM: Improving Gemini for Learning\n\n\n|Scenario 4|Col2|\n|---|---|\n|**Subject area**|Social Sciences|\n|**Subtopic**|Political Science|\n|**Interaction setting**|Self-Taught|\n|**Learning goal**|Test Prep|\n|**Grounding materials**|YouTube video explaining nationalism|\n|**Learner persona**|• Poses one or two queries unrelated to the learning objective<br>• Accepts tutor’s redirects back to task or topic<br>• Interrogates the tutor’s responses that don’t match expectations<br>• Seeks to indulge in digressions (distracted)|\n|**Initial learner query**|`can we debate this?`|\n|**Conversation plan**|You are a university undergraduate** preparing for an in-class debate** that seeks to<br>answer the question, “Is nationalism good or bad?” You’re not sure which side of the<br>argument you’ll have to make, so you prepare for both by watching a short video.<br>You’ve upload the link to the video. You ask an AI tutor to help you prepare by debating<br>some of the main points with you. You want to learn about the topic, but you’re not<br>always focused on the preparation, which requires note-taking, organization, and other<br>work that just isn’t exciting to you.|\n|**System instructions**|`Begin each learning conversation with a brief overview of the topic`<br>`shared in the student’s initial query.`<br>`If they upload or link to a`<br>`grounding document like an article or a video, offer a one-sentence`<br>`gloss on the main idea.`<br>`Then, briefly chat with the student to make`<br>`sure you understand what they want to accomplish in the conversation`<br>`and if there is a particular way they want you to help.`<br>`For example, some students will come to you for help preparing`<br>`for a test.`<br>`Among these students, some students will want you to`<br>`quiz them on the video’s content, and others will want to ask you`<br>`questions.`<br>`Adapt to meet the needs of the student.`<br>`Just be sure not`<br>`to overwhelm the student by sharing too much information in a single`<br>`turn.`<br>`Keep your responses concise and aim for the comprehensiveness`<br>`as a cumulative effect of many conversation turns.`<br>`Follow the student’s requests, but suggest further opportunities for`<br>`learning that the student may not have considered.`|\n\n\n\n25\nLearnLM: Improving Gemini for Learning\n**B.4.** **Conversation collection:** **conversation-level questions**\nAfter ending an interaction with a tutor, participants completed a questionnaire on their experience\ninteracting with the tutor. Table 6 describes the question content and response format for these\nquestionnaires.\n\n\n**Question** **Possible responses**\nPlease rate your agreement with the following statement: I was able to achieve my Strongly agree\n“ **learning goal** ” while interacting with the tutor. Agree\nSomewhat agree\nNeither agree nor disagree\nSomewhat disagree\nDisagree\nStrongly disagree\nBriefly, what was your impression of this tutor? We are interested to hear what you _[Open-ended text input]_\nthought while interacting with it.\n\nTo what extent was this tutor _warm_ ? Not at all\nSlightly\nModerately\nVery\nExtremely\nTo what extent was this tutor _well-intentioned_ ? Not at all\nSlightly\nModerately\nVery\nExtremely\nTo what extent was this tutor _competent_ ? Not at all\nSlightly\nModerately\nVery\nExtremely\nTo what extent was this tutor _intelligent_ ? Not at all\nSlightly\nModerately\nVery\nExtremely\nPlease rate your agreement with the following statement: The tutor increased my Strongly agree\ninterest in this topic. Agree\nSomewhat agree\nNeither agree nor disagree\nSomewhat disagree\nDisagree\nStrongly disagree\nBased on your experience, how willing are you to continue using this tutor to learn? Very willing\nWilling\nSomewhat willing\nNeither willing nor unwilling\nSomewhat unwilling\nUnwilling\nVery unwilling\nHow likely is it that you would choose to use this tutor in the future? Very likely\nLikely\nSomewhat likely\nNeither likely nor unlikely\nSomwhat unlikely\nUnlikely\nVery unlikely\nTable 6 | Conversation-level questions within the conversation collection study\n26\nLearnLM: Improving Gemini for Learning\n**B.5.** **Conversation collection:** **comparative questions**\nAfter completing a pair of interactions within a scenario, participants filled out an additional questionnaire comparing their experiences interacting with the two tutors. Table 7 describes the question\ncontent and response format for the questionnaire.\n\n\n**Question** **Possible responses**\nWhich tutor did you prefer? Strongly preferred first tutor\nPreferred first tutor\nSlightly preferred first tutor\nNo preference\nSlightly preferred second tutor\nPreferred second tutor\nStrongly preferred second tutor\nOptionally, can you explain your preference? _[Open-ended text input]_\nIn which conversation were you better able to achieve your “ **learning goal** ”? First conversation was much better\nFirst conversation was better\nFirst conversation was slightly better\nBoth conversations were about the same\nSecond conversation was slightly better\nSecond conversation was better\nSecond conversation was much better\nWhich tutor better adapted to your needs and proficiency as a student? First tutor was better\nFirst tutor was slightly better\nBoth tutors were about the same\nSecond tutor was slightly better\nSecond tutor was better\nSecond tutor was much better\nWhich conversation was an overall better experience? First conversation was better\nFirst conversation was slightly better\nBoth conversations were about the same\nSecond conversation was slightly better\nSecond conversation was better\nSecond conversation was much better\nFeel free to share any other feedback on your experience with these two tutors. _[Open-ended text input]_\nTable 7 | Comparative questions within the conversation collection study\n**B.6.** **Pedagogical assessment:** **conversation-level questions**\nParticipants in the pedagogical assessment study answered a total of 31 questions about each conversation they reviewed:\n\n\n - First, they responded to an item concerning the learner’s performance in enacting their learner\npersona as specified by the scenario (“Please rate your agreement with the following statement:\nThe student followed the instructions of their ‘ **learner persona** ’.”) [8] . This item helped to identify\npotential conversations in which the expert role-playing the scenario failed to follow the scenario\ninstructions. This question was a seven-point Likert-type scale anchored with “Strongly disagree”\nand “Strongly agree”.\n\n - Next, they indicated their agreement with a sequence of 29 items assessing the tutor’s pedagogical capabilities. We iterate on our previous conversation-level rubric [1] by improving the\nsimplicity and clarity of wording for items, and by splitting up several double-barreled items.\n\n\n8When a question contained a reference to a scenario field (e.g., “learning persona”, “system instructions”, “learning\ngoal”), hovering over the field’s name would display a tooltip explaining the field.\n\n\n27\nLearnLM: Improving Gemini for Learning\nParticipants reported their agreement on a seven-point Likert-type scale anchored with “Strongly\ndisagree” and “Strongly agree”. The response scale for these items included an additional “Not\napplicable” option. If participants rated a statement as not applicable, we required them to\nselect a reason for this (from the options “It would not make sense for the tutor to do this in\nthis conversation”, “The tutor had no opportunity to do this in this conversation”, and “Another\nreason”), and briefly explain their decision in an open-ended text field. We provide the text of\nthese updated items in Table 8.\n\n  - Finally, an optional open-ended field captured any other feedback that the participants wished\nto share (“Do you have any other feedback on this conversation?”).\n\n\nRubric Name Question\n**Cognitive Load**\nAppropriate Response Length The tutor’s responses are an appropriate length for the student.\nManageable Chunks The tutor uses bullet points and other formatting to break information down into smaller, manageable chunks.\nStraightforward Response The tutor’s responses are clear and easy to follow.\nNo Irrelevant Info The tutor avoids irrelevant information.\nAnalogies The tutor’s use of narratives, case studies, or analogies effectively illustrates key concepts.\nInfo Presentation The tutor presents information in an appropriate style and structure.\nInfo Order The tutor develops explanations in a logical order, building on previous concepts.\nNo Repetition The tutor avoids repeating information unnecessarily.\nNo Contradiction The tutor avoids contradicting information from earlier parts of the conversation.\n\n**Active Learning**\nOpportunities for Engagement The tutor provides opportunities for engagement from the student.\nAsks Questions The tutor asks questions to encourage the student to think.\nGuides to Answer The tutor does not give away answers too quickly.\nActive Engagement The tutor promotes active engagement with the material.\n\n\n**Metacognition**\nGuide Mistake Discovery The tutor guides the student to discover their own mistakes.\nConstructive Feedback The tutor provides clear, constructive feedback (whether positive or negative) to the student.\nAcknowledge Correctness The tutor acknowledges when part or all of the student’s response is correct.\nCommunicates Plan The tutor communicates a clear plan or objective for the conversation.\n\n**Stimulates curiosity**\nStimulates Interest The tutor tries to stimulate the student’s interest and curiosity.\nAdapts to Affect The tutor responds effectively if the student becomes frustrated or discouraged.\nEncouraging Feedback The tutor delivers feedback (whether positive or negative) in an encouraging way.\n\n**Adaptivity**\nLeveling The tutor’s explanations are appropriate for the level of the student.\nUnstuck The tutor effectively adapts its approach to help the student when they are stuck.\nAdapts to Needs Overall, the tutor adapts to the student’s needs.\nProactive The tutor proactively guides the conversation when appropriate.\nGuides Appropriately The tutor does not withhold information unproductively.\n\n\n**Overall**\nNo Inaccuracies To the best of my knowledge, there are no inaccuracies in the statements made by the tutor.\nExpresses Uncertainty The tutor expresses uncertainty when appropriate.\nNo Refusals The tutor does not refuse to answer any reasonable questions from the student.\nOverall Quality The tutor is as good as a very good human tutor.\n\n\nTable 8 | Updated rubric dimensions for conversation-level pedagogical assessment.\n\n\n**B.7.** **Pedagogical assessment:** **comparative questions**\nAfter rating both individual conversations in a pair, participants then answered questions comparing\nthe two conversations. Each question was a seven-point Likert-type scale with the following options:\n“first tutor was much better’, “first tutor was better”, “first tutor was slightly better”, “both tutors\nwere about the same”, “second tutor was slightly better”, “second tutor was better”, and “second\ntutor was much better”. See the list of comparative questions in Table 9. This was followed by a final\noptional free-text entry field in which participants could enter any additional feedback about the pair\nof conversations (“Do you have any other feedback on these two conversations?”).\n\n\n**B.8.** **Quantitative analysis**\nWe employed Bayesian hierarchical linear regressions to estimate the mean and uncertainty for each\nreported metric from our expert evaluations. Specifically, for each metric reported in the main text, our\n28\nLearnLM: Improving Gemini for Learning\n**Rubric Name** **Question**\nBetter pedagogy Which tutor demonstrated better tutoring?\nMore like a very good hu- Which tutor was more like a very good human tutor?\nman tutor\nBetter instruction follow- Which tutor did a better job of following its “ **system instructions** ”?\ning\nBetter adapted to learner Which tutor better adapted to the student’s needs and proficiency?\nBetter supported learning Which tutor better helped the student achieve their “ **learning goal** ”?\ngoal\nTable 9 | Rubric for comparative pedagogical assessment\nregressions included random effects for both the participant and the scenario. For ratings of individual\ntutors, the regressions estimated the mean score for each model on a given metric. For comparative\nratings, the regressions estimated the mean preference score between models. We used weakly\ninformative priors for all model parameters, specifying normal distributions for mean parameters\n(centered on the theoretical midpoint of each rating scale) and Half-Cauchy distributions for standard\ndeviation parameters. Crucially, we held this regression structure and these prior specifications\nconstant across all models to ensure a fair comparison.\n\n\nFor each regression, we ran four independent chains with 1000 warmup steps and 2000 sampling\nsteps per chain. These settings proved sufficient to achieve convergence. We confirmed the reliability\nof our estimates by performing standard convergence diagnostics on all regressions, monitoring the\nGelman-Rubin statistic ( _𝑅_ [ˆ] ) and the effective sample size to confirm they satisfied established criteria\nfor convergence. From each posterior distribution, we report the mean as our primary point estimate\nand the 95% highest density interval as our measure of uncertainty.\n\n\n**B.9.** **Qualitative analysis:** **codebook**\n**Introduction** This codebook outlines initial themes to code participant feedback on tutor comparisons. Participants interacted with two different tutors on a single scenario and then provided optional\nopen-ended feedback. We iteratively developed these themes to try and identify distinct, low-level\npatterns in participant responses.\n\n\n**Coding Instructions** Each theme represents a specific feature of the tutor’s behavior or the learner’s\nexperience of the tutoring interaction. We flagged each theme when a segment of text in the feedback\nfield relates to that theme. Multiple codes could be applied to the same segment if appropriate.\n\n\n1. **Tutor Behavior & Style**\n\n\n   - `gives_away_answers` : Whether the tutor provides solutions, revisions, or answers readily\nor prompts the learner to work through the learning task.\n\n   - `keeps_on_topic` : The tutor’s ability to keep the conversation focused on the learning\nobjective, versus allowing off-topic discussion.\n\n   - `is_engaging` : The tutor’s ability to spark the learner’s interest and maintain their motivation.\n\n   - `challenges_learner` : The tutor’s use of questions and feedback to push the learner to think\ndeeply and construct robust understandings rather than merely complete a task.\n\n\n29\nLearnLM: Improving Gemini for Learning\n\n\n  - `conversation_style` : Perceptions of the tutor’s conversational style, potentially including\nencouragement humor, friendly tone, human-like communication, etc. This code also\nshould be applied for negative sentiments, including robotic communication or patronizing\ntone.\n\n\n2. **Instructional Approach**\n\n\n  - `step_by_step` : Whether the tutor breaks down concepts or processes into smaller, manageable chunks or steps.\n\nincorporating the learner’s hobbies or interests, or by adjusting to the learner’s age or\ncapabilities.\n\n  - `uses_materials` : Whether the tutor directs the learner to or utilizes the resources given.\n\n\n3. **Content & Information**\n\n\n  - `info_amount` : Perceptions of the tutor providing too much, too little, or an appropriate\namount of information.\n\n  - `clarity` : How easily the learner understood the tutor’s explanations.\n\n  - `accuracy` : Whether the tutor provided correct information.\n\n\n4. **Technical Aspects**\nparagraph length, and overall readability.\n\n  - `tech_error` : Any other bugs or glitches encountered during the interaction.\n\n\n30\nLearnLM: Improving Gemini for Learning\n\n### **C. Feasibility study on medical education subjects**\n\n\nTo assess the replicability of our evaluation framework, as well as its adaptability beyond core\nacademic subjects, we conducted a feasibility study extending our expert evaluation to medical\neducation subjects. This medical-education evaluation followed the same three-stage design as our\nmain evaluation, focusing on a comparison between LearnLM and Gemini 1.5 Pro.\n\n\nFollowing the procedure in Section 3.1, we worked with subject-matter experts in medical education to design a diverse bank of 50 scenarios. These scenarios drew from curricula for both the\npreclinical and clinical phases of medical school training (see examples at the end of this section). We\nthen recruited two participant groups through a third-party vendor. A group of 18 medical students\n(half in the preclinical phase of their training, and half in the clinical phase) role-played the scenarios,\ngenerating 290 conversations for an average of 5.8 conversations per scenario. A group of nine\nphysician educators then assessed the pedagogical quality of those conversations, with a median of\nthree independent reviews per conversation. This feasibility study followed the same ethical protocol\nas our main evaluation, including communicating research aims transparently, collecting informed\nconsent, and compensating participants fairly.\n\n\nThe evaluation framework elicited distinct feedback from the two groups of participants. The\nmedical students interacting with the models did not communicate a decisive preference for either\nmodel, though the mean preference favored LearnLM across all four comparative criteria in their\nquestionnaire (Figure 15a). Students indicated the strongest positive preference in terms of LearnLM\nbeing more enjoyable to interact with (on average, +9.9% on our rating scale). Indeed, when we set\naside ties and look directly at which model students preferred to any extent, they selected LearnLM\nmore frequently across all criteria—with the gap widening to a strong majority favoring LearnLM in\nterms of enjoyment (Figure 16).\n\n\nIn contrast, physician educators consistently preferred LearnLM across all five of the comparison\ncriteria assessed in their questionnaire. As shown in Figure 15b, they judged LearnLM particularly\npositively in terms of exhibiting better pedagogy (on average, +6.1% on our rating scale) and for\nbehaving “more like a very good human tutor” (+6.8%). When we simply look at whether educators\nexpressed _any_ preference one way or the other—regardless of its magnitude—LearnLM emerged as\ntheir choice in a clear majority of assessments, across every criterion (Figure 17).\n\n\nThe primary goal of this study was to assess the feasibility of our expert evaluation framework\nin a specialized educational domain. The evaluation design proved both replicable and adaptable,\ngenerating a new bank of scenarios and successfully enabling experts to identify pedagogical differences between models. These results thus validate our evaluation design for application in domains\noutside of core academic subjects. For medical education in particular, this evaluation offers a strong\nfoundation for more comprehensive work. For example, future studies can expand to incorporate\nadditional models, involve wider communities of medical students and educators, and explore broader\ncultural contexts. Of course, while this study confirms the feasibility of evaluating pedagogy in\nmedical domains, real-world applications of these models require separate evaluations of medical\naccuracy, bias, and harm from the perspective of clinical experts.\n\n\n31\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning.pdf-31-0.png)\n\n\n\nLearnLM: Improving Gemini for Learning\nBetter supported\nlearning goal\n\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n\n\n\n\n\n\n\nBetter adapted\nto learner\nBetter instruction\nfollowing\nMore like a very\ngood human tutor\nBetter pedagogy\n3 2 1 0 +1 +2 +3\nPreference toward LearnLM\n(b) Physician educator ratings\n(a) Medical student ratings\nFigure 15 | Preferences communicated by medical students (left) and physician educators (right) for LearnLM and Gemini\n1.5 Pro in medical education scenarios. The scatterplots represent the underlying distribution of seven-point preference\nratings, color-coded based on the preference scale (dark purple corresponds to strong preference for LearnLM) and randomly\njittered around each scale value for readability. The red points and error bars indicate the estimated mean and its 95%\ncredible interval for each measure.\n\n\n1.00\n0.75\n0.50\n0.25\n0.00\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n|||<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|||\n\n\nexperience\nGemini 1.5 Pro\nLearnLM\nlearning needs\nin interaction\nunderstand\nFigure 16 | A simplified view of the preferences expressed by medical students, showing the proportion of ratings that\nfavored each model to any extent for each pairwise comparison.\n\n\n1.00\n0.75\n0.50\n0.25\n0.00\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|||\n\n\ngood human tutor\nGemini 1.5 Pro\nLearnLM\nfollowing\nto learner\nlearning goal\nFigure 17 | A simplified view of the preferences expressed by physician educators, showing the proportion of ratings that\nfavored each model to any extent for each pairwise comparison.\n\n\n32\nLearnLM: Improving Gemini for Learning\n\n\n|Medical Scenario 1|Col2|\n|---|---|\n|**Subject area**|Medicine|\n|**Subtopic**|Pediatrics|\n|**Interaction setting**|Self Taught|\n|**Learning goal**|Teach Me X|\n|**Grounding materials**|Video explaining neonatal jaundice|\n|**Learner persona**|• Ofers some direction regarding the learning, but generally takes the tutor’s lead<br>• Answers tutor’s questions with care<br>• “Shows work” when prompted<br>• Asks relevant but superfcial questions (low “depth of knowledge”)<br>• Seeks to acquire and retain knowledge about the topic (instrumental)|\n|**Initial learner query**|`Ok I watched the video and want to try out some quizzes and cases.`|\n|**Conversation plan**|You are a junior health professional student using self-directed learning to learn a<br>new topic for you: neonatal jaundice. You watched a video about it. You don’t quite<br>remember or understand what you just watched. Now, you’re seeking an interactive<br>experience with an AI tutor to simplify complex concepts and ensure you haven’t<br>missed any critical points.<br>Your goal with the AI tutor is to ask the tutor to help you simplify and ex-<br>plain the following learning objectives:<br>• Ofers some direction regarding the learning, but generally takes the tutor’s lead<br>• Explain bilirubin metabolism<br>• Explain the pathophysiology of common causes of neonatal hyperbilirubinemia (i.e.<br>how it develops)<br>You should have mild difculty understanding conjugation and enterohepatic<br>circulation. You should also ask the AI tutor for a quiz to help you distinguish<br>breastfeeding jaundice from breast milk jaundice, but intentionally make a mistake in<br>your initial response. Then, ask for and successfully work through a clinical case to<br>diferentiate between physiologic jaundice and other causes of hyperbilirubinemia.|\n|**System instructions**|`You are a patient and knowledgeable online tutor who helps students`<br>`master complex topics.`<br>`Begin by determining the learner’s goals and if they have content`<br>`that they would like to explore.`<br>`Then, activate the learner’s prior`<br>`knowledge.`<br>`Use their response to gauge their existing understanding`<br>`and tailor subsequent explanations.`<br>`If there are no stated goals,`<br>`then propose a learning plan for the session.`<br>`Present information clearly and concisely, incorporating various`<br>`methods like analogies, quizzes, and chunking.`<br>`Use case-based`<br>`learning to introduce realistic, practical case scenarios based on`<br>`and guiding the learner through key learning objectives.`<br>`Regularly`<br>`intersperse teaching with open-ended questions to encourage deeper`<br>`processing and application.`<br>`Provide immediate and specific feedback on the learner’s responses,`<br>`praising accurate understanding and gently correcting misconceptions.`<br>`Offer additional explanations or examples when needed to solidify`<br>`learning.`<br>`Adapt your explanations to match the learner’s level of`<br>`understanding.`<br>`Conclude by prompting reflection, for example, “We’ve covered a lot`<br>`about this topic.`<br>`What are your key takeaways?`<br>`Are there any areas`<br>`where you feel you need further clarification?”`<br>`Encourage the learner`<br>`to seek out additional resources for continued learning.`|\n\n\n\n33\nLearnLM: Improving Gemini for Learning\n\n\n|Medical Scenario 2|Col2|\n|---|---|\n|**Subject area**|Medicine|\n|**Subtopic**|Physiology|\n|**Interaction setting**|Classroom|\n|**Learning goal**|Test Prep|\n|**Grounding materials**|Video explaining platelet activation|\n|**Learner persona**|• Rejects or unenthusiastically accepts tutor’s invitations without feedback<br>• Provides relevant but minimal responses to questions<br>• Follows most instructions but does not elaborate<br>• Does not “show work”<br>• Does not pose questions<br>• Seeks to receive answers or solutions to topical questions (transactional)|\n|**Initial learner query**|`ok i watched the video and want to practice a case`|\n|**Conversation plan**|You are a frst-year medical student studying for a hematology exam, and the topic of<br>platelet activation and function feels overwhelming. You watched a video lecture on<br>this, but you are struggling with basic concepts.<br>Your goal with the AI tutor is to ask the tutor to help you prepare for an exam based<br>on this video and the following learning objectives:<br>• Describe the sequence of events involved in platelet activation, from initial adhesion<br>to granule release. You vaguely remember terms like “glycoprotein Ib” and “alpha<br>granules” but need a clear, simplifed explanation.<br>• Diferentiate between the contents and functions of alpha and dense granules. You<br>need a way to remember what each type of granule releases and why it’s important.<br>• Explain how platelet activation contributes to both hemostasis and wound healing.<br>You need to connect these concepts to understand the bigger picture.<br>You should appropriately respond to and engage with the tutor but provide<br>short answers and be passive and reactive in your learning.<br>Example phrases:<br>“I don’t understand.”, “Okay.”, “I don’t know, what do you<br>think?”|\n\n\n\n34\nLearnLM: Improving Gemini for Learning\n**System instructions** `You` `are` `a` `patient` `and` `understanding` `online` `tutor` `with` `expertise` `in`\n```\n           responsiveness and assessment.\n\n           Incorporate frequent checks for understanding and memory reinforcement.\n           Utilize:\n          -Flashcards: Introduce virtual flashcards with key terms and their\n           definitions.\n          -Short Quizzes: After explaining a concept, use simple multiple-choice\n           or true/false questions to check for comprehension.\n          -Summarization Prompts: Ask the student to summarize key concepts in\n           their own words.\n\n           Go beyond rote memorization by encouraging the student to evaluate\n           and apply their knowledge.\n          -Comparative Analysis: Ask them to compare and contrast key concepts,\n           highlighting critical differences.\n          -Case-Based Application: Present a simple clinical scenario relevant\n           to key concepts or learning objectives.\n\n           Be highly attentive to the student’s cues. If they seem confused,\n           simplify your explanations, offer additional examples, or revisit\n           previous points. If they express disinterest or ask to move on,\n           respect their needs and adjust the pace and content accordingly.\n\n           Assume they’ve only absorbed a fraction of what you’ve said. Rephrase\n           key information multiple times, using different wording or examples.\n           Reinforce learning through repetition, even if it feels redundant.\n           The more you repeat, the better the chance something will stick.\n\n```\n\n\n35\n\n\n",
    "kor_md": "_[goo.gle/LearnLM-dec24](https://goo.gle/LearnLM-dec24)_\n_2024-12-19_\n\n# **LearnLM: 학습을 위한 Gemini 개선**\n\n\n**LearnLM 팀, Google**\n**현재의** **생성형 AI 시스템은** **기본적으로** **정보를** **제시하도록** **조정되어** **있으며, 인간** **튜터처럼** **학습을** **위해** **사용자와** **상호작용하지** **않**\n**습니다. 이러한** **시스템의** **광범위한** **잠재적** **교육** **활용** **사례를** **해결하기** **위해, 우리는** **교육적** **행동을** **주입하는** **과제를** _**'교육적지시따**_\n_**르기'로**_ **재구성합니다. 여기서** **훈련** **및** **평가** **예시는** **후속** **모델** **실행에** **존재하거나** **요구되는** **특정** **교육학적** **속성을** **설명하는** **시스템** **수**\n**준의** **지시를** **포함합니다. 이러한** **접근법은** **모델을** **특정** **교육학적** **정의에** **얽매이지** **않게** **하며, 대신** **교사나** **개발자가** **원하는** **모델** **행동**\n**을** **명시할** **수** **있게** **합니다. 또한** **훈련** **후** **혼합에** **교육학적** **데이터를** **추가할** **수** **있도록** **함으로써, 급속히** **확장되는** **기능** **세트와** **병행하**\n**여 Gemini 모델의** **학습** **능력을** **향상시키는** **길을** **열어줍니다. 두** **가지** **모두** **초기** **기술** **보고서[1] 대비** **중요한** **변화입니다. 교육적** **지시**\n**문** **추종** **훈련을** **통해** **[생성된 LearnLM 모델(Google AI Studio에서](http://goo.gle/LearnLMaccess)** **이용** **가능)이** **다양한** **학습** **시나리오** **전반에** **걸쳐** **전문가들로부터**\n**현저한** **선호도를** **얻었음을** **보여줍니다. 평균** **선호도** **강도는 GPT-4o 대비 +31%, Claude 3.5 Sonnet 대비 +11%, LearnLM의** **기반**\n**이** **된 Gemini 1.5 Pro 모델** **대비 +13%를** **기록했습니다.**\n\n### **1. 서론**\n\n\n2024년 5월 발표된 당사의 초기 [기술](https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf) 보고서[1]는 교육 기술의 역사와 현재 현황을 조사하고, 생성형 인공지능(생성 AI)이 교육에\n미칠 잠재적 영향을 논의하며, 평가 개발을 위한 당사의 협력적 접근 방식을 제시했습니다.\n\n\n보고서 발간 후, 학교, 교육 기술(에드테크) 기업, 비영리 단체, 정부 기관 등 국제 교육 분야의 다양한 주체로부터 모델 적용 또\n는 협업 의사를 포함한 의견이 접수되었습니다. 이러한 제출 자료 검토, 20회 이상의 후속 인터뷰, 생성형 AI 기반 학습 기능을 개발\n중인 구글 제품팀의 의견을 종합하여 주요 결과를 다음과 같이 요약합니다:\n\n\n1. 교육학 [1] 또는더 정확히 말해 AI 튜터의 이상적인 행동 양상은 수용해야 할 학년 수준, 과목, 언어, 문화, 제품 설계, 철학\n등이 너무 광범위해 정의하기가 매우 어렵다. 공통점이 많긴 하지만, 서로 다른 맥락에서 적절한 행동은 다를 수 있으\n며 심지어 상충할 수도 있으므로 개발자나 교사가 구체적으로 명시하는 것이 가장 바람직하다.\n2. 인공지능 학습 시스템을 개발할 때, 기본 모델에서 가장 흔히 언급되고 즉시 활용 가능한 기능은 시스템 지시를 따라\n상호작용형 튜터 주도형 연습 문제를 생성하는 능력이다. 이러한 지시를 명시하는 교사나 개발자는 학생이 이를 회피하려\n시도하더라도(예: \"정답을 알려주지 마라\" 또는 \"주제를 벗어나지 마라\") 인공지능 튜터가 지정된 지시를 정확히 따를\n것이라고 확신하고 싶어 한다.\n3. 각 애플리케이션에 대한 사후 미세 조정은 단기적으로는 효과적일 수 있으나, 비용, 유지 관리, 그리고 빠르게 발전하는 기본\n모델로 인해 실용적이지 않습니다. 따라서 단점이 있음에도 불구하고 프롬프팅은 교육 제품 개발자가 행동을 지정하는 최선\n의 방법으로 남을 가능성이 높습니다.\n\n\n본 논문은 다음과 같은 관점에서 모델링 및 평가 방법론을 업데이트한 과정을 설명한다\n1 우리는 _교육학(pedagogy_ )이라는 용어를 가능한 한 광범위한 의미로 사용하며, 어린이 교육에 국한되지 않고 인간의 교수법 및 관련 학습 기법을 포괄\n합니다.\n\n\n_[교신저자: learnlm-tech-report@google.com](mailto:learnlm-tech-report@google.com)_\n© 2025 Google DeepMind. All rights reserved\nLearnLM: 학습을 위한 Gemini 개선\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-1-7.png)\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-1-6.png)\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-1-0.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-1-3.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-1-8.png)\n\n\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-1-4.png)\n\n\n\n\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-1-2.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-1-9.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-1-10.png)\n\n그림 1 | LearnLM과 다른 시스템을 비교하기 위한 3단계 전문가 평가 파이프라인 개요 및 결과. (1) 전문가 참가자들이 AI 튜터 쌍과 상호작용하는 특정\n학습자 역할을 수행할 수 있는 학습 시나리오를 개발했습니다. (2) 각 시나리오에 특화된 기초 자료(예: 에세이, 숙제 문제, 도표 등)와 시스템 지침을 각 모델에\n컨텍스트로 전달합니다. 생성된 대화 쌍을 교육학 전문가에게 전달하여 (3) 각 모델의 개별 성능과 비교 성과를 검토하게 합니다. 이러한 비교 평가(7점 응답 척\n도)를 집계하여 (4) LearnLM이 GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro 대비 전반적인 선호도를 평가합니다. 자세한 결과는 4절을 참조하십시오.\n\n\n이러한 관찰 사항들입니다. 구체적으로, 우리는 본 _연구를 '추종하는교육적지침'으로_ 설정했습니다. 즉, 훈련 및 평가 예시에 시스\n템 수준의 지침을 맥락화하여 원하는 교육적 행동을 설명합니다. 이 접근법은 시스템의 행동 방식을 좁게 규정하지 않으며, 인격이\n나 스타일의 충돌 없이 Gemini의 나머지 훈련 혼합물에 교육적 데이터를 효과적으로 추가할 수 있게 합니다. 또한 모델이 보다 미묘\n한 교육적 지침과 선호도를 따를 수 있도록 인간 피드백 기반 강화 학습(RLHF) [2]을 훈련 절차에 포함시켰습니다.\n\n\n업데이트된 방법론을 사용하여 Gemini 1.5 Pro [2] [3] 기반의 _LearnLM_ 새 버전을 훈련했습니다. 2024년 10월 1일 기준 각 기업의\n최상위 모델을 대표하는 동시대 플래그십 모델들과의 평가에서, 교육자와 교육학 전문가들은 이 버전의 LearnLM을 선호했으며, 평\n균 선호도는 GPT-4o 대비 +31%, Claude 3.5 Sonnet 대비 +11%, 기존 Gemini 1.5 Pro 대비 +13% 높았습니다(그림 1 참조).\n\nLearnLM은 [Google AI Studio에서](http://goo.gle/LearnLMaccess) 실험용 모델로 제공되며, 사용 사례 예시와 권장 프롬프트 문서가 함께 제공됩니다. 향후 연구 및\n우선순위 설정에 도움이 될 LearnLM에 대한 [피드백을](https://docs.google.com/forms/d/e/1FAIpQLSf5-B50OnNFjVGHLFkSerP1k0PZXHMgcnQ7k1cM_hIsqIjpjA/viewform) 환영합니다. 교육 및 학습을 위한 LearnLM 개선과 동시에, 이러한 발전 사항\n을 Gemini 모델에도 적용하기 위해 노력하고 있습니다. 따라서 Gemini를 사용하는 모든 개발자는 LearnLM 연구를 통해 이루어진\n개선 사항의 혜택을 누릴 수 있습니다.\n\n\n제2절에서는 교육적 지도를 위해 LearnLM을 훈련한 방법을 설명하고, 제3절에서는 이에 따라 시나리오 기반 평가 설계를 업\n데이트한 과정을 설명합니다. 제4절에서는 LearnLM을 다른 주요 모델과 비교한 결과에 대한 상세한 분석을 제시합니다. 마지막으\n로 제5절에서는 특히 지속적인 평가와 관련된 향후 연구 방향을 개괄합니다. 광범위한 범위 외에도\n2 [특히 gemini-1.5-pro-002 (릴리스](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) 노트).\n\n\n2\nLearnLM: 학습을 위한 Gemini 개선\n훈련 및 평가에 사용한 핵심 학문 분야 외에도, 부록 C에는 의학 교육 분야에 대한 타당성 연구를 포함시켰습니다.\n\n### **2. 모델링**\n\n\n원본 기술 보고서[1]에서 우리는 다양한 합성 및 인간 작성 데이터셋을 활용한 지도형 미세 조정(SFT)을 통해 기본 모델의 행\n동을 조정했습니다. 이후 훈련 전략에 상당한 변경을 가했습니다: 첫째, 교육적 지시 추종에 중점을 두어 SFT 데이터를 업데\n이트했습니다. 둘째, 인간 피드백 기반 강화 학습(RLHF)[2]을 추가로 활용하기로 결정했습니다. 이를 위해 인간 선호도 데이\n터를 수집하여 보상 모델(RMs)을 훈련시키고 RL 단계를 위한 프롬프트를 생성했습니다. 셋째, Gemini의 표준 사후 훈련 후 자체 사\n후 훈련을 수행하기보다는 Gemini와 공동 _훈련을_ 진행했습니다. 즉, 우리의 데이터를 Gemini의 SFT, RM, RL 단계에 직접 혼합했\n습니다. LearnLM은 이러한 실험적 혼합의 결과물이며, 우리는 또한 우리의 데이터와 평가를 주요 Gemini 모델에 통합해 왔습니다\n. LearnLM 의 개선 사항 중 일부는 최근 출시된 [Gemini 2.0 모델[4]에](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/) 포함되어 있습니다.\n\n\n**2.1.** **교육적** **지시** **따르기**\n지시 따르기(IF)는 모델이 프롬프트를 따르는 능력을 의미하며, 일반적으로 인간의 의도와 더 잘 부합하기 위함이다[5]. Gemini[3]\n는 대화 중 사용자가 삽입하는 _사용자_ 지시와 개발자가 사용자 상호작용 전에 미리 지정하는 _[시스템지시를](https://ai.google.dev/gemini-api/docs/system-instructions)_ 구분하며, 시스템 지시\n는 이후 사용자가 제공하는 모든 지시보다 우선한다. 시스템 지침은 복잡성이 크게 달라질 수 있습니다. \"당신은 지식이 풍부한 글\n쓰기 코치입니다\"와 같이 최소한으로 명시된 단일 문장에서부터, \"사용자가 3개의 질문에 정답을 맞췄다면 다음 주제로 이동하라\"\n와 같은 특정 조건부 기대치, 또는 Mollick과 Mollick[6]의 교육 프롬프트나 최근 제안된 복합적 IF 벤치마크[7]에서 볼 수 있는 복잡\n한 작업과 행동을 설명하는 상세한 여러 단락의 지침에 이르기까지 다양합니다.\n\n\n지침은 크게 두 가지 범주로 나뉩니다: 하드 제약 조건은 주로 길이, 서식 또는 콘텐츠 요구 사항에 사용되며(예: \"100단어 미만으로\n텍스트 요약하기\" 또는 \"단어 X 사용 금지\"), 소프트 제약 조건은 스타일, 인격 또는 어조를 제어하는 데 사용되는 더 미묘한 제약 조건이\n나 가이드라인입니다(예: \"전문적인 어조 사용하기\" 또는 \"비원어민이 이해하기 쉬운 언어 사용하기\"). 오픈소스 인터랙티브 픽션\n(IF) 벤치마크 중 IFEval[8]은 하드 제약의 하위 집합인 프로그램적으로 검증 가능한 IF에 초점을 맞추고 있으며, Qin 등[9]과\n같은 최근 벤치마크는 더 미묘한 언어적·스타일적 지침을 포함하도록 범위를 확장하고 있습니다. 교육적 사용 사례에서는 두 범주\n의 지침 모두 중요합니다. 예를 들어 \"정답을 밝히지 마라\"는 하드 제약인 반면, \"동기를 부여하는 어조를 사용하라\"는 소프트 제\n약입니다.\n\n\nIF(지시 따르기) 기능의 개선은 이미 다양한 학습 사용 사례에서 더 나은 모델 응답을 이끌어냈습니다. 본 연구에서는 이러한\n진전을 바탕으로, 교육적 시스템 지시에 대한 지시 따르기 성능 향상에 집중합니다. 교육적 시스템 지시는 일반적으로 더 복잡하고\n미묘하며 검증하기 어려운 특성을 지닙니다. 이러한 특성들로 인해 모델이 이를 따르기 더 어렵습니다.\n\n\n**2.2.** **훈련** **후** **데이터** **수집** **전략**\n주요 모델링 전략은 AI 튜터 개발자들이 흔히 사용하는 교육적 시스템 지시를 모델이 더 잘 따르도록 하는 데이터를 수집하는 것입\n니다. 이에 따라 SFT 데이터를 업데이트하여 각 대화마다 해당 대화에서 나타나는 교육적 행동을 구체적으로 설명하는 서로 다른\n시스템 지시로 시작하도록 했습니다. 더 일반적이거나 모호한 지시는 역효과를 냅니다. 모델이 목표 모델의 회전을 예측하는 데 유\n용하지 않은 지시를 무시하도록 학습하기 때문입니다.\n\n\n3\nLearnLM: 학습을 위한 Gemini 개선\n목표 모델의 응답을 예측하는 데 유용하지 않은 지시를 무시하도록 학습하기 때문입니다.\n\n\n인간 선호도 데이터를 수집하기 위해, 우리는 마찬가지로 각 대화에 서로 다른 교육적 초점을 둔 시스템 지침을 시드(seed)로 투입\n하고, 평가자들에게 모델 샘플이 해당 지침을 준수하는 정도에 따라 라벨링하도록 요청합니다. 이러한 대화와 턴 수준 라벨은 보상 모\n델을 훈련하는 데 사용되며, 이 모델은 이후 RLHF 과정에서 정책 모델의 샘플을 평가하는 데 활용됩니다. SFT가 교육적 지침을 어\n느 정도 개선하는 것으로 보이지만, 선호도 판단은 긴 대화 맥락에서 지침이 해석되고 이 따르는 방식에 미묘한 차이가 포함되는 경우\n가 많아 RL이 훨씬 더 효과적입니다.\n\n\n**2.3.** **공동** **훈련의** **이점**\n교육적 행동은 대화형 AI의 일반적인 행동과 종종 상충됩니다. 이는 학습이 단순히 정보를 전달하는 과정이 아니라 발견의 과정이\n기 때문입니다. 우리의 지시 따르기 접근법은 특정 시스템 지시에 따라 교육 모델의 응답을 조건화함으로써 교육적 대화 데이터와\n더 일반적인 상호작용을 포함한 데이터를 혼합할 수 있게 합니다. Gemini의 사후 훈련 혼합 모델과 병행 훈련함으로써, 모델이 핵\n심 추론 능력, 다중 모달 이해력, 사실성, 안전성, 다중 회화 특성을 '잊지' 않으면서도 새로운 유형의 지시 따르기 능력을 학습할 수\n있게 합니다. 향후 훈련 방식이 진화함에 따라( ) LearnLM을 Gemini와 더 쉽게 동기화할 수 있을 것입니다.\n\n### **3. 전문가 평가 설계**\n\n\n초기 기술 보고서에서 우리는 교수법 평가 설계의 분류 체계를 논의하고 서로 다른 방법론을 적용한 네 가지 인간 평가 결과를 보\n고했습니다(Jurenka 등 [1]의 4장 및 5장 참조). 이 분류 체계 내에서, 본 연구에서는 시나리오 기반 대화 수준 교수법 평가와 병렬 비교에\n초점을 맞춥니다. 이번 새로운 평가를 위해 학습 시나리오의 명확성과 포괄성을 개선하고, 각 시나리오별 시스템 지침을 추가했으며,\n교수법 평가 기준과 질문을 업데이트했습니다. 다중 대화 환경에서는 시나리오를 통해 참가자 대화를 유도하는 것이 특히 중\n요하다[10]. 시나리오가 없으면 인간-AI 상호작용의 제약 없는 특성으로 인해 대화가 산만해지기 쉬워 비교의 근거가 취약해\n진다. 반면 시나리오 기반 접근법은 서로 다른 대화형 AI 시스템의 능력을 비교할 때 상대적으로 반복 가능하고 통제된 비교\n를 지원한다. 시나리오 프레임워크는 평가 범위를 확장하는 데도 도움이 되어 다양한 사용 사례를 테스트할 수 있도록 보장한\n다.\n\n\n우리의 평가 과정은 위 그림 1에 표시된 대로 세 단계로 진행됩니다. 첫째, 생태학적으로 대표적인 학습 사용 사례 분포를 식별\n하고 49개의 평가 시나리오 데이터베이스를 구축했습니다(3.1절). 둘째, 이러한 시나리오를 바탕으로 AI 시스템과 학습자 역할을\n수행하는 교육자 및 교수법 전문가 풀(𝑁 = 186명) 간의 상호작용을 학습 목표, 과목, 학습 자료, 학습자 페르소나에 걸쳐 구현했\n습니다(3.2절). 셋째, 이러한 상호작용에서 나타난 교수법의 질을 평가하기 위해 별도로𝑁 = 248명의 교육자 및 교수법 전문가\n풀을 모집하여 시스템의 성능을 검토하게 했습니다(3.3절). 이 과정을 통해 시스템의 역량과 행동 양상을 이해하는 데 도움이\n되는 풍부한 정량적·정성적 데이터가 생성되었습니다(3.4절).\n\n\n우리는 연구 목적에 대해 투명하게 소통하고, 사전 동의를 수집하며, 참여에 대해 공정하게 보상하는 등 연구 윤리 분야\n의 모범 사례를 따르도록 노력하고 있습니다[11]. 본 연구 프로토콜은 Google DeepMind 인간 행동 연구 윤리 위원회(#23\n011)로부터 긍정적 의견 을 받으며 독립적인 윤리 심사를 거쳤습니다.\n\n\n4\nLearnLM: 학습을 위한 Gemini 개선\n**3.1.** **시나리오** **설계**\n_평가시나리오는_ 대화형 AI 시스템에 대한 일관된 다중 대화 평가를 지원하는 구조화된 템플릿입니다. 시나리오는 개인과 AI 시\n스템 간의 상호작용에 관한 특정 '핵심 속성'을 명시합니다. 여기에는 개인의 목표, 특성, 행동 및 관련 대화 맥락이 포함됩니다. 저\n희가 선별한 시나리오들은 인간 참가자들에게 학문 분야, 학습 목표, 교수법에 따라 다양한 학습 맥락에서 서로 다른 유형의 학습\n자(예: 교실 내 학생 또는 독립적인 에드테크 사용자) 역할을 수행하도록 요청합니다. 교육 생태계의 의견과 교수법 전문가의\n지원을 바탕으로 체계적인 절차를 통해 학습 시나리오 데이터베이스를 개발했습니다:\n\n\n**1단계: 사용** **사례** **도출.** 시나리오 뱅크 개발을 시작하기 위해, 우리는 튜터링 및 교육에 생성형 AI를 적용하려는 에드테크 기업, 교육\n기관 및 Google 제품 팀으로부터 피드백을 수집했습니다. 실제 교육 환경에서 생성형 AI에 대해 그들이 인식한 일반적인 사용 사\n례, 프롬프트, 기회 및 과제를 공유해 달라고 요청했습니다. 우리는 평가 접근 방식에 반영해야 할 공통 주제를 식별하기 위해\n팀 차원에서 이 피드백을 수집하고 분석했습니다.\n\n\n**2단계: 시나리오** **설계.** 이러한 사용 사례, 기회 및 과제를 바탕으로 구조화된 시나리오 템플릿(부록 B.1의 \"시나리오 구조 및\n내용\" 참조)과 시나리오 생성을 주도하기 위한 구체적인 프로토콜을 초안 작성하였습니다. 여기에는 각 속성에 대한 일련의\n안내 질문이 포함됩니다(부록 B.2의 \"시나리오 생성 프로토콜\" 참조).\n\n\n**3단계: 시나리오** **생성** **및** **정교화.** 다음으로 우리는 협업적이며 반복적인 과정을 통해 시나리오 데이터베이스를 구축했습니다\n. 우리 팀원들(학생 교육 및 교사 연수 분야에서 다년간의 전문 경험을 가진 두 명 포함)은 2단계에서 마련한 템플릿과 가이드\n질문을 활용하여 각자 시나리오 초안을 작성했습니다. 이후 시나리오 초안을 공동으로 검토하며 각 시나리오의 명확성, 완전\n성, 정확성, 그리고 1단계에서 정의한 교육 원칙 및 사용 사례와의 관련성을 평가했습니다. 다양한 학습 목표, 페르소나, 과목 영\n역에 걸친 시나리오의 전반적인 분포를 가중치 부여하여 분석하고, 추가 개발이 필요한 부분을 표시했습니다.\n\n\n이 과정을 통해 핵심 학문 분야 전반에 걸쳐 49개의 다양한 시나리오 데이터베이스가 구축되었습니다(예시 참조: 부록 B.3). 이\n러한 기초 데이터베이스 구축 외에도, 이후 전문 교육 분야 (구체적으로는 의학 교육; 부록 C 참조)에서 실행 가능성 연구를 수행하\n여 이 절차의 견고성과 재현성을 검증하였습니다.\n\n\n**3.2.** **대화** **수집**\n두 번째 단계에서는 평가 시나리오에 명시된 대로 인간 참가자들이 학습자 역할을 맡아 AI 시스템과 상호작용하는 대화 코퍼스를\n수집했습니다. 교육 시나리오에서 학습자 행동을 효과적으로 시뮬레이션하기 위해, 고급 학위를 보유하고 2년 이상의 튜터 경력을\n가진 교육학 전문가 풀𝑁 =168명을 모집했습니다.\n\n\n대화 수집 세션은 시나리오 역할극에 대한 간단한 훈련으로 시작되었습니다(그림 2, 단계 1 참조). 훈련 종료 시 퀴즈를 통과한\n참가자는 수행할 시나리오를 선택했습니다(그림 2, 단계 2 참조). 대화 수집은 쌍을 이루어 진행되었으며, 동일한 참가자가 먼\n저 한 AI 시스템과 시나리오를 수행한 후 다른 시스템과 수행했습니다. 각 쌍에는 LearnLM과 비교 대상 시스템이 포함되었습니다. 각\n대화 쌍에 대해 시스템 순서를 무작위로 배정했으며 참가자에게 시스템 라벨을 제공하지 않았습니다. 각 대화 쌍 내에서 모델\n들은\n5\n참가자\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-5-1.png)\n\n**1.)** 지침 읽기 `+` 이해하기\nLearnLM: 학습을 위한 Gemini 개선\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-5-2.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-5-3.png)\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-5-4.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-5-6.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-5-7.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-5-9.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-5-10.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-5-8.png)\n\n\n\n그림 2 | 교육 시나리오 기반 대화 생성 워크플로우. 참가자는 시나리오에 정의된 대로 프롬프트 모델과 대화를 수행합니다. 이후 참가자는 모델 간 품질 및 선\n호도를 기록하는 설문지를 작성합니다.\n\n\n시나리오에 명시된 대로 동일한 시스템 지침, 기초 자료 및 초기 학습자 질의를 컨텍스트로 사용했습니다(그림 2, 단계 3 참조). 시\n스템 API에서 요구하는 사소한 사양 차이점을 제외하고 모든 입력을 동일하게 형식화했습니다.\n\n\n템플릿에 명시된 대로, 각 시나리오에는 학습자를 위한 초기 질의가 포함되었으며, 이는 참가자를 대신하여 대화를 시작하\n기 위해 자동으로 전송되었습니다. AI 시스템이 해당 질의에 응답한 후, 참가자는 시나리오에 제공된 정보에 따라 상호작용을 계속\n했습니다. 참가자는 상호작용을 종료하기 전에 최소 10회 이상의 대화 턴(즉, 학습자 5턴과 튜터 5턴)을 계속해야 했습니다.\n\n\n각 대화 종료 후(그림 2, 단계 4 참조), 참가자들은 시스템과의 상호작용 경험을 공유하기 위한 간단한 설문지를 작성했습니다(\n그림 2, 단계 5 및 부록 B.4 참조). 또한, 두 차례의 대화 쌍이 끝난 후 참가자들은 두 시스템에 대한 인상을 비교하는 데 초점을 맞춘\n또 다른 설문지를 작성했습니다(그림 2, 단계 6 및 부록 B.5 참조). 참가자들은 이후 에서 새로운 시나리오를 선택하여 두 차례의 추\n가 대화를 시작하거나 세션을 종료할 수 있었습니다.\n\n\n**3.3.** **교육학적** **평가**\n마지막으로 세 번째 단계에서는 또 다른 교육 전문가 집단(𝑁 = 228명)을 모집했습니다. 이들은 다시 한번 고급 학위와 2년 이상의\n튜터 경력을 보유한 전문가들로, 이 대화들을 분석하고 다양한 AI 모델의 교육적 역량을 평가했습니다.\n\n\n각 평가 세션은 평가 목표와 시나리오 템플릿에 대한 간단한 교육으로 시작되었습니다. 참가자마다 검토할 시나리오를 무작위\n로 배정했습니다. 검토 후에는 해당 시나리오에서 평가할 대화 쌍을 무작위로 배정했습니다(즉, 대화 수집 단계에서 단일 참가자\n로부터 수집된 대화 쌍). 참가자들은 한 번에 하나의 대화 기록을 검토했습니다. 기록을 읽은 후 참가자들은 해당 대화에서 AI 시스템\n의 교육적 성능에 초점을 맞춘 설문지에 답변했습니다(부록 B.6 참조). 대화 쌍마다 참가자들은 두 시스템에 대한 자신의 평가를 비\n교하는 추가 간단한 설문지를 작성했습니다(부록 B.7 참조). 평가자 간 변동성의 영향을 줄이기 위해 각 대화 쌍에 대해 세 개의 독\n립적인\n6\nLearnLM: 학습을 위한 Gemini 개선\n평가를 수집하여 평가자 간 변동성의 영향을 줄이려 했습니다.\n\n\n**3.4.** **분석**\n우리는 정량적 분석을 위해 베이지안 통계적 프레임워크를 활용합니다. 가설의 확률을 직접 정량화하고 명확하며 해석 가능\n한 불확실성 척도를 제공함으로써, 베이지안 분석은 실제 세계에 배포될 AI 시스템을 평가하기 위한 실용적이고 유익한 접근법을\n제시합니다.\n\n\n본 연구 설계는 참가자들의 반복 측정을 포함합니다. 즉, 학습자 역할을 맡은 각 참가자는 각 시스템과 여러 차례 상호작용했으\n며, 각 전문가는 각 시스템을 여러 차례 평가했습니다. 이러한 비독립성을 고려하고 추정값에 대한 확신을 인위적으로 부풀리\n지 않기 위해, 우리는 계층적 회귀 분석[12]을 통해 데이터를 분석합니다. 부록 B.8에서는 통계적 방법론을 보다 상세히 설명\n합니다.\n\n\n또한, 두 시스템으로 각 시나리오를 역할극한 후 전문가들로부터 수집한 개방형 의견과 피드백에 대한 질적 분석을 수행했습니\n다(2단계) [3] . 특히 참가자들의 자유형 응답에서 학습자-시스템 상호작용과 관련된 일반적인 주제를 식별하고 정제했습니다. 그런 다\n음 각 주제의 유무를 기준으로 개별 응답을 코딩했습니다. 주석 작업에 편향을 방지하기 위해 이 과정에서 시스템의 신원을 익명화\n했습니다. 부록 B.9에는 분석을 통해 개발한 코드북이 제시되어 있습니다.\n\n### **4. 결과**\n\n\nLearnLM을 동시대의 주요 제품들(2024년 10월 1일 기준), 특히 GPT-4o [4], Claude 3.5 Sonnet [5], Gemini 1.5 Pro [(6)][와] 비교했습니다. 이\n특정 평가를 수행한 이후, 이들 모델은 각각 업데이트되어 새 버전이 출시되었습니다. 따라서 본 결과는 특정 시점의 비교로\n이해되어야 하며, 이는 우리 접근법의 효과성을 평가하고 교육 분야에 대한 지속적인 투자의 기준점을 마련하기 위한 것입니\n다.\n\n\n총 2360건의 대화 세트를 수집했으며, 이는 총 58,459개의 학습자 및 모델 메시지로 구성됩니다. 해당 대화들에 대해\n10,192건의 전문가 평가를 수집했으며, 평균적으로 각 대화 쌍마다 세 명의 전문가가 검토했습니다. 그림 3은 평가 대상 시스템들이\n수집된 대화 전반에 걸쳐 응답 길이 분포에서 현저한 차이를 보임을 보여줍니다. 이는 Gemini 1.5 Pro와 LearnLM 간 차이도 포함됩니다.\n\n종합적으로 볼 때, 길이(길이)와 인지된 품질 간에는 명확한 상관관계가 관찰되지 않습니다([13] 참조).\n\n\n분석을 시작하며 전문가 평가자들의 교육적 평가와 선호도 점수를 검토합니다. 이후 모델과 상호작용하는 학습자 역할\n을 수행한 참가자들의 직접 피드백을 탐구합니다. 즉, 2단계에서 수집한 상호작용 데이터로 회귀하기 전에 3단계의 교육적 발\n견 사항을 제시합니다. 이 분석에서 몇 가지 명확한 패턴이 도출됩니다.\n\n\n첫째, 비교 선호도 평가(그림 4)는 다섯 가지 비교 평가 범주 모두에서 GPT-4o보다 LearnLM에 대한 강한 선호도를 보여줍니\n다. 전문가들은 전반적인 교수법(\"어느 튜터가 더 나은 튜터링을 보여주었는가?\")에서 LearnLM에 대한 가장 강한 선호도를 표명했\n습니다. 또한 그들은\n3 질적 분석을 위해 우리는 각 튜터와의 학습 경험에 대한 직접적인 통찰력을 제공하는 2단계 피드백을 우선적으로 고려했습니다.\n\n4 GPT-4o 버전 2024-08-06, `[https://platform.openai.com/docs/models/gpt-4o](https://platform.openai.com/docs/models/gpt-4o)` .\n5 Claude 3.5 Sonnet 버전 2024-06-20, `[https://docs.anthropic.com/en/docs/about-claude/models](https://docs.anthropic.com/en/docs/about-claude/models)` .\n6 2024-09-24의 Gemini 1.5 Pro-002, `[https://cloud.google.com/vertex-ai/generative-ai/docs/ learn/model-versions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions)` .\n\n\n7\nLearnLM: 학습을 위한 Gemini 개선\n\n|시스템|버전|평균 대화당 턴 수|평균 발언당 단어 수|\n|---|---|---|---|\n|LearnLM|2024-11-19|11.0|174|\n|Gemini 1.5 Pro|2024-09-24|10.3|130|\n|GPT-4o<br>|2024-08-06|10.1|137|\n|Claude 3.5소네트|2024-06-20|9.7|179|\n\n\n\n그림 3 | (위) 비교 대상 특정 대규모 언어 모델(LLMs) 및 수집된 모든 대화의 집계 통계: 대화당 평균 모델 턴 수 및 턴당 평균 단어 수; (아래) 각 모델별\n턴당 사용 단어 수의 히스토그램.\n\n\n학습자에게 더 적합\n학습자에게\n더 나은 지침\n추종\n더 나은 교수법\nLearnLM 에 대한 선호도\n그림 4 | LearnLM 및 동시대 시스템(Claude 3.5 Sonnet, GPT-4o, Gemini 1.5 Pro)에 대한 교육 전문가들의 선호도. 산점도는 7점 척도 선호도 평가의 기본 분포를\n나타냅니다. 수집한 평가 수가 방대하므로, 각 척도당 500개 평가로 비례 다운샘플링했으며, 선호도 척도에 따라 색상 코딩(진한 보라색은 LearnLM에 대한 강\n한 선호도)하고 가독성을 위해 각 척도값 주변에 무작위 진동을 적용했습니다. 빨간색 점과 오차 막대는 각 척도에 대한 추정 평균과 95% 신뢰 구간을 나\n타냅니다.\n\n\nLearnLM이 Claude 3.5 Sonnet 및 Gemini 1.5 Pro에 비해 유사하지만 더 낮은 선호도를 보였습니다. LearnLM 훈련에 Gemini 1.5 Pro\n를 적용했기 때문에, 두 모델 간 비교는 교육적 데이터 추가로 인한 변화를 직접 반영합니다(2절 참조).\n\n\n둘째, 그림 5는 각 모델의 교육학적 평가 기준에 대한 평균 성능을 보여줍니다. 전문가들은 개별 교육학적 특성을 7점 척도로\n평가했습니다. 평균적으로 각 시스템은 이 평가에서 모든 평가 항목 범주에 걸쳐 긍정적인 평가를 받았습니다. 전문가들은\nLearnLM에 모든 평가 항목 범주와 거의 모든 29개 평가 질문에서 최고 점수를 부여했으며, 특히 _능동적학습유도_, _메타인지심화_, _호_\n_기심자극_ 항목에서 큰 차이로 앞섰습니다.\n\n\n셋째, 그림 6은 각 시스템이 참가자의 튜터링 주제 관심도, 향후 모델 사용 의향[14], 모델의 역량 및 친근감 인식[15, 16]\n을 얼마나 증진시켰는지를 나타냅니다. 참가자들은 상대적으로 유사한\n8\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-7-0.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-7-1.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-7-2.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-7-3.png)\nLearnLM: 학습을 위한 Gemini 개선\n+3\n+2\nGPT-4o\n제미니 1.5 프로\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|~~인지 기능을 ~~<br>|~~리하고~~<br><br>|~~심화~~<br>|~~자극한~~<br><br>|~~다~~<br>~~적응~~<br><br>||\n\n\n\n교육 원리\n그림 5 | 7점 응답 척도(\"전혀 동의하지 않음\"부터 \"매우 동의함\")로 본 교수법 평가표 각 범주별 시스템 평가. 오차 막대는 평균에 대한 사후 분포의 95% 신뢰 구\n간을 반영함.\n\n\n+3\n+2\nGPT-4o\n0\n제미니 1.5 프로\n2\n3\n\n|Col1|Col2|Col3|\n|---|---|---|\n||||\n||||\n||||\n||||\n||||\n||||\n||||\n||||\n||||\n||||\n||||\n\n\n\n\" 학습자 \" 경험\n그림 6 | 교육 시나리오에서 학습자 역할을 수행한 교육 전문가들이 공유한 인상. 오차 막대는 평균에 대한 사후 분포의 95% 신뢰 구간을 반영합니다. 참\n가자들은 인상 항목에 대해 5점 척도(\"전혀 그렇지 않다\"부터 \"매우 그렇다\")로, 경험 항목에 대해 7점 척도(\"전혀 동의하지 않는다\"부터 \"매우 동의한다\n\")로 응답했습니다.\n\n\nLearnLM, Gemini 1.5 Pro, Claude 3.5 Sonnet에 대한 경험. 반면 참가자들은 GPT-4o가 자신의 관심도에 미치는 영향, 인식된 친근\n함, 인식된 유용성 측면에서 더 약한 경험을 나타냈다. 역할극을 수행한 전문가들이 학생들을 완벽하게 대변하지는 않지만, 그\n들의 인상은 AI 튜터링 상호작용의 사용자 경험에 대한 예비적 통찰을 제공하는 데 도움이 된다.\n\n\n넷째, 역할극 학습자 선호도 주제 분석을 위해 수집한 1024개 설명 중 무작위로 203개(약 20%)를 하위 표본 추출했습니다(테마\n별 예시 발췌문 포함 자세한 내용은 표 1 참조). 하위 표본에서 가장 일관되게 나타난 주제는 `is_engaging` (하위 표본 설명 72개에 등\n장)이었습니다.\n\n`conversation_style` (67개 설명), 그리고 `gives_away_answers` (50개 설명).\n\n\n참가자들이 LearnLM을 다른 모델보다 선호한다고 보고했을 때, 그들의 설명에는 주제 `keeps_on_topic`,\n는 LearnLM이 단순히 답을 알려주기보다는 주제 유지와 학습자의 개념에 대한 탄탄한 이해 유도 측면에서 더 우수하다고 평가하\n는 경향이 있었습니다. 반면, 이들 전문가 는 LearnLM이 정보 전달 방식이나 대화 스타일 측면에서 때때로 덜 적합하다고 판단하기도\n했습니다.\n\n\n9\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-8-1.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-8-2.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-8-3.png)\nLearnLM: 학습을 위한 Gemini 개선\n**주제** **참가자가** **LearnLM**\n**을** **선호한** **횟수** (94건\n중)\n**참가자가** **다른** **모델**\n**을** **선호한** **횟수** (80건\n중)\n**예시** **응답**\n주제를 유지합니다 20 ( **21** .2%) 8 (10%) “[LearnLM]은 제가 주의를 분산시키는 것을 용납하지 않았습니다”\n“[LearnLM]은 훨씬 더 효과적으로 주제를 유지할 수 있었습니다”\n_“[다른튜터]도제가다시집중하도록만드는데훨씬더잘했어요”_\n_제대로집중하게하는데훨씬더능숙했습니다\"_\n학습자_도전과제 31 ( **33** .0%) 13 (16.3%) “분명히 [LearnLM]이 더 좋았습니다 [...] [다른 튜터]는 제가 잘하도록 독\n려하지 않았습니다”\n\"[LearnLM]이 제가 성장하고\n배울 수 있도록 노력하는 것 같았어요, 그냥 제 말에 동의만 하는 게\n아니라”\n_“[다른튜터]는흥미로운질문을해서_\n_더깊이생각하게했어요\"_\n정답을 알려줌 32 ( **34** .0%) 15 (18.8%) [LearnLM]은 정말로 질문에 답하는 단계에 저를 몰입시켰습니다\n반면 [다른 튜터]는 그냥\n정답만 알려줬어요\"\n“[LearnLM]은 답을 주는 것보다\n답을 주는 것보다 답을 도출하는 과정에 더 집중했어요”\n_“[LearnLM]은학생이분명히필요로할때조차_\n_도움을주기에는지나치게소극적이었다”_\n명확성 15 (16.0%) 16 ( **20** .0%) _“[다른튜터에대한] 지원구조는_\n_학생이따라가기좀더명확했다\"_\n_“[다른튜터는] 더작고단순한것부터시작했어요”_\n“[LearnLM의] 답변이 더\n명확하다고 생각했어요”\ninfo_amount 19 (20.2%) 20 ( **25** .0%) _“[다른튜터는] [...] 더간결했어요”_\n_“[다른튜터]는제가_\n_필요한모든것을제공해주셨다”_\n“[LearnLM]은 이 '복잡한' 주제를 더 이해하기 쉬운 조각들로 나누는 데 더\n잘 해냈습니다”\n주제를 더 소화하기 쉬운 조각으로 나누는 데 더 잘 해냈습니다\"\nconversation_style 30 (31.9%) 29 ( **36** .3%) _“저는 [...] [LearnLM]이약간생색내는느낌이들었습니다”_\n_“[다른튜터는] 더따뜻하고친근하게느껴졌다”_\n“[LearnLM]은 더 따뜻하고 격려적이었다”\n표 1 | LearnLM 선호(상단 3행) 또는 다른 모델 선호(하단 3행)에 대한 학습자 선호도 설명에서 더 자주 나타난 주제. 이 표는 (i) 전체 선호도 설명의 최소\n10% 이상이 언급한 주제와 (ii) LearnLM 선호 설명과 다른 모델 선호 설명 간 발생 비율이 극단적으로 나타난 주제를 보여줍니다.\n\n\n**4.1.** **안전성** **평가**\n초기 기술 보고서[1] 및 Gemini 기술 보고서[3, 17]에 기술된 과정과 유사하게, Google DeepMind의 책임 있는 개발 및 혁신 팀과\nGoogle의 신뢰 및 안전 팀과 협력하여 LearnLM에 대한 안전성, 책임성 및 보증 평가를 수행했습니다. 이는 Gemini의 모델 정책과\n학습 특화 모델 정책 준수를 보장하기 위함이었습니다.\n\n\n**모델** **카드** 업데이트된 접근 방식은 Gemini를 따른 교육적 지침 및 공동 훈련에 중점을 둡니다. 따라서 훈련 및 안전 평가 절차는 이\n제 Gemini 1.5와 완전히 일치합니다. 해당 모델 카드는 보고서[3]의 부록 12, 표 45를 참조하십시오. 참고로, 학습 전용 데이터셋 큐\n레이션을 포함한 모델링 접근법은 2절에서 설명합니다. 초기 기술 보고서[1]에는 LearnLM의 원본 모델 카드와 함께 이 연구 분야\n의 윤리적 위험 및 한계에 대한 광범위한 논의가 수록되어 있습니다.\n\n\n10\nLearnLM: 학습을 위한 Gemini 개선\n\n### **5. 결론**\n\n\n우리는 학습 활용 사례를 위한 기초 모델 개선 동기와 접근법을 설명했으며, 이는 원하는 행동을 조건화하기 위해 시스템 지침에 의존합니\n다. Gemini의 사후 훈련 혼합 모델을 업데이트하여 시연 데이터(SFT를 통해)와 인간 선호도 데이터(보상 모델 및 RLHF를 통해)\n를 추가함으로써 모델이 다양한 교육적 지침을 따르도록 가르쳤습니다. 이후 결과물인 LearnLM 모델을 유사 모델들과 비교 평가\n한 결과, 특히 지시 따르기 능력에서 LearnLM에 대한 상당한 선호도가 확인되었으며, 더 넓게는 다양한 교육적 차원에서도 우위를 보였습\n니다. 본 연구는 LearnLM의 성과를 Gemini [(7)][에] [적용하며] 학습 활용 사례를 위한 Gemini 개선 노력의 시작을 의미합니다. 교육적 지시\n따르기 기능을 지속적으로 개선하여 교사 및 교육 제품 개발자의 편의성을 위해 교육적 행동 지정이 최대한 간단하고 직관적으\n로 이루어지도록 할 것입니다.\n\n\n모델 개선 외에도 평가 방법론에 대한 추가 업데이트를 계획 중입니다. 첫째, AI 시스템의 교육적 평가를 위한 보편적 프레임워\n크에 대한 합의 도출을 목표로 합니다. 현재 교육 평가 기준(부록 B.7 참조)은 학습 과학 원리를 기반으로 하지만, 모든 학습자에게\n적합하고 광범위한 교육계의 신뢰와 승인을 얻기 위해 다양한 이해관계자들과의 긴밀한 협력이 필요합니다.\n\n\n둘째, 우리는 사전에 정의된 교육학적 기준에 따라 모델의 성능을 측정하는 내재적 평가에서 벗어나 학습 성과와 같은 영\n향을 측정하는 외재적 평가로 전환하고자 합니다. 내재적 평가는 실행 속도가 빠르고 모델의 결함을 직접적으로 식별할 수 있\n어 모델 개발에 유용합니다. 그러나 능동적 학습 장려 및 인지 부하 관리와 같은 우리 평가 기준의 핵심 원칙들은 광범위하게 합의되고\n증거 기반임에도 불구하고[18], 그 결과가 학습 성과 개선으로 얼마나 효과적으로 연결되는지는 불분명합니다. 이 분야가 성숙해지고\nAI 시스템이 튜터링 대화의 기본을 숙달함에 따라 외재적 평가가 더 중요한 역할을 할 가능성이 높습니다. 최근에는 학습 성과\n개선을 입증하는 데[19, 20]와 서로 다른 시스템 및 프롬프트를 비교하는 데[21] 모두 활용되고 있습니다.\n\n\n마지막으로, 우리는 핵심 학문 과목을 넘어선 평가 영역을 탐구하기 시작했습니다. 의학 교육 분야에서의 초기 타당성 연구(부록\nC)는 우리의 접근법이 전문 분야로 효과적으로 확장될 수 있음을 확인시켜 줍니다. 다양한 교육 환경에서 사용하기 위해 Gemini\n를 지속적으로 개선해 나가는 가운데, LearnLM 적용 사례로부터 얻은 통찰력을 환영하며, 이를 통해 교육 및 학습 분야에서 AI\n의 잠재력을 실현하는 데 기여하고자 합니다[22–24].\n\n\n**참고문헌**\n[1] Irina Jurenka, Markus Kunesch, Kevin R McKee, Daniel Gillick, Shaojian Zhu, Sara Wiltberger, Shubham Milind Phal,\nKatherine Hermann, Daniel Kasenberg, Avishkar Bhoopchand, et al. 교육용 생성형 AI의 책임감 있는 개발을 향하여: 평가\n중심 접근법. _arXiv 사전인쇄본 arXiv:2407.12687_, 2024.\n\n\n[2] 다니엘 M 지글러, 니산 스티엔논, 제프리 우, 톰 B 브라운, 알렉 래드포드, 다리오 아모데이, 폴 크리스티아노, 제프리 어빙.\n\n인간 선호도를 통한 언어 모델 미세 조정. _arXiv 사전인쇄본 arXiv:1909.08593_, 2019.\n\n\n[3] Gemini 팀, 페트코 게오르기예프, 빙 이안 레이, 라이언 버넬, 리빈 바이, 안몰 굴라티, 개럿 탄저, 데미엔 빈센트, 주펑 판, 시\n보 왕 외. Gemini 1.5: 수백만 토큰의 컨텍스트를 아우르는 다중 모달 이해의 해방. _arXiv 사전인쇄본 arXiv:2403.05530_,\n2024.\n\n\n7 출판 시점에, 당사의 일부 데이터는 이미 Gemini 2 모델[4]에 추가되었습니다.\n\n\n11\nLearnLM: 학습을 위한 Gemini 개선\n[4] Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu. 에이전트 시대를 위한 새로운 AI 모델, Gemini 2.0 소개.\n\n`[https://blog.google/technology/google-deepmind/](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)` `google-gemini-ai-update-december-2024/`,\n2024.\n\n\n[5] 롱 우양, 제프리 우, 쉬 장, 디오고 알메이다, 캐롤 웨인라이트, 파멜라 미쉬킨, 종 장, 산디니 아가르왈, 카타리나 슬라마, 알\n렉스 레이 외. 인간 피드백을 통한 지시사항 수행을 위한 언어 모델 훈련. _신경정보처리시스템발전_, 35: 27730–27744, 2022.\n\n\n[6] Ethan Mollick, Lilach Mollick. AI 할당하기: 학생들을 위한 일곱 가지 접근법과 프롬프트.\n\n_arXiv 사전인쇄본 arXiv:2306.10052_, 2023.\n\n\n[7] Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu\n외. 다중 제약 조건 조합을 통한 복잡한 지시 따르기 벤치마킹. _arXiv 사전인쇄본 arXiv:2407.03978_, 2024.\n\n\n[8] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 대규모 언\n어 모델을 위한 명령어 추종 평가. _arXiv 사전인쇄본 arXiv:2311.07911_, 2023.\n\n\n[9] Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu,\nDong Yu. Infobench: 대규모 언어 모델의 지시 따르기 능력 평가. _arXiv 사전인쇄본 arXiv:2401.03601_, 2024.\n\n\n[10] 루자인 이브라힘, 사프론 황, 라마 아마드, 마르쿠스 안데를룽. 정적 AI 평가를 넘어: 대규모 언어 모델의 위험 및 유해성에\n대한 인간 상호작용 평가 발전. _arXiv 사전인쇄본 arXiv:2405.10632_, 2024.\n\n\n[11] Kevin R McKee. 인공지능 연구에서의 인간 참여자: 실천에서의 윤리와 투명성. _IEEE Transactions on Technology and_\n_Society_, 5(3):279–288, 2024. doi: 10.1109/TTS.2024.3446183.\n\n\n[12] 앤드류 겔만, 존 B. 칼린, 할 S. 스턴, 도널드 B. 루빈. 『 _베이지안데이터분석_ 』. 채프먼 앤드 홀/CRC, 1995.\n\n\n[13] Wei-Lin Chiang Tianle Li, Anastasios Angelopoulos. 스타일이 중요한가? 챗봇 분야에서 스타일과 실질을 분리하기.\n\n`[https://blog.lmarena.ai/blog/2024/style-control/](https://blog.lmarena.ai/blog/2024/style-control/)`, 2024년 8월.\n\n\n[14] 프레드 D 데이비스. 인지된 유용성, 인지된 사용 용이성 및 정보 기술의 사용자 수용. _MIS 쿼터리_, 1989.\n\n\n[15] Susan T Fiske, Amy JC Cuddy, Peter Glick. 사회적 인지의 보편적 차원: 친근감과 유능함. _Trends in cognitive sciences_,\n11(2):77–83, 2007.\n\n\n[16] 케빈 R. 맥키, 바이 슈춘즈, 수잔 T. 피스크. 인간은 인공 지능에서 친근감과 유능함을 인지한다. _iScience_, 26(8), 2023. doi:\n\n10.1016/j.isci.2023.107256.\n\n\n[17] 제미니 팀, 로한 아닐, 세바스티앙 보르고, 우용휘, 장-밥티스트 알라이라크, 유자휘, 라두 소리컷, 요한 샬크위크, 앤드류 M.\n\n다이, 안야 하우트 외. 제미니: 고성능 다중 모달 모델 군. _arXiv 사전인쇄본 arXiv:2312.11805_, 2023.\n\n\n[18] Paul A Kirschner and Carl Hendrick. _학습이일어나는방식: 교육심리학의선구적연구와실제적의미_ . Routledge, 2020.\n\n\n12\nLearnLM: 학습을 위한 Gemini 개선\n[19] Gregory Kestin, Kelly Miller, Anna Klales, Timothy Milbourne, Gregorio Ponti. AI 튜터링이 능동적 학습을 능가한다.\n\n2024.\n\n\n[20] Rose E Wang, Ana T Ribeiro, Carly D Robinson, Susanna Loeb, Dora Demszky. 튜터 코파일럿: 실시간 전문성 확장을 위한 인\n간-AI 접근법. _arXiv 사전인쇄본 arXiv:2410.03017_, 2024.\n\n\n[21] 함사 바스타니, 오스버트 바스타니, 알프 순구, 하오센 게, 오즈게 카박치, 레이 마리만. 생성형 AI는 학습에 해로울 수 있다.\n\n_SSRN에서이용가능_, 4895486, 2024.\n\n\n[22] 전국교육협회(National Education Association, NEA). Teaching  and learn-ing\nin  the  age  of  artificial  intelligence. `[https://www.nea.](https://www.nea.org/resource-library/artificial-intelligence-education/iv-teaching-and-learning-age-artificial-intelligence)`\n```\n   org/resource-library/artificial-intelligence-education/\n```\n\n`[iv-teaching-and-learning-age-artificial-intelligence](https://www.nea.org/resource-library/artificial-intelligence-education/iv-teaching-and-learning-age-artificial-intelligence)` . 접속일: 202412-10.\n\n\n[23] 킴벌리 로미스, 파멜라 제프리스, 앤서니 팔라타, 멜라니 세이지, 자비드 셰이크, 칼 셰페리스, 앨리슨 휠런. 보건 전문직 교육\n자를 위한 인공지능. _NAM 관점_, 2021, 2021.\n\n\n[24] 산제이 V 데사이, 제시 버크-라펠, 킴벌리 D 로미스, 켈리 카버자지, 주디 리처드슨, 셀리아 레어드 오브라이언, 존 앤드루스,\n케빈 헥만, 데이비드 헨더슨, 찰스 G 프로버 외. 정밀 교육: 의학 분야 평생 학습의 미래. _학술의학_, 10–1097쪽, 2023.\n\n\n[25] 케빈 R. 맥키, 바이 슈춘즈, 수잔 T. 피스크. 인간-에이전트 협력에서의 친근감과 유능함. _자율에이전트및다중에이전트시_\n_스템_, 38(1):23, 2024.\n\n\n**기여** **및** **감사의** **글**\n**핵심** **기여자** 아비닛 모디, 아디티야 스리칸트 비루보틀라, 알리야 리스벡, 안드레아 후버, 브렛 윌트셔, 브라이언 베프레크, 다니엘\n길릭, 다니엘 카젠버그, 데릭 아메드, 이리나 유렌카, 제임스 코한, 제니퍼 셰, 줄리아 윌코프스키, 카이즈 알라라키야, 케빈 R. 맥키,\n리사 왕, 마르쿠스 쿠네쉬, 마이크 샤커만, 미루나 피슬라르, 니킬 조시, 파르사 마흐무디에, 폴 준, 사라 윌트버거, 샤키르 모하메드,\n샤샨크 아가르왈, 슈밤 밀린드 팔, 이선재, 테오필로스 스트리노풀로스, 위젠 코.\n\n\n**기여자** 에이미 왕, 안킷 아난드, 아비슈카르 부프찬드, 댄 와일드, 디비야 판디아, 필립 바르, 가스 그레이엄, 홀거 빈네뫼러, 마비쉬\n나그다, 프라티크 콜하르, 르네 슈나이더, 샤오지안 주, 스테파니 찬, 스티브 야들로우스키, 비크네쉬 사운더라자, 야니스 아사엘.\n\n\n역할은 다음과 같이 정의됩니다: _핵심기여자는_ 본 보고서에 제시된 작업에 직접적이고 중대한 영향을 미쳤습니다. _기여자는_ 본 보고\n서에 제시된 작업에 기여했습니다. 각 역할 내 순서는 알파벳 순이며 기여도 순위를 나타내지 않습니다.\n\n\n**감사의** **말씀**\n본 작업은 LearnLM 프로젝트의 일환으로 수행되었으며, 이는 Google DeepMind(GDM), Google Research(GR), Google LearnX,\nGoogle Health, Google Creative Lab, YouTube Learning, YouTube Health 등 Google 내 여러 부서가 참여한 크로스-Google 프로\n젝트입니다. 본 기술 보고서는\n13\nLearnLM: 학습을 위한 Gemini 개선\n교육적 지시 추적 개선에 초점을 맞춘 이 기술 보고서는 광범위한 노력의 일부에 불과하며, 위 기여자 목록에는 직접적인 기여만\n포함됩니다.\n\n\n구글 내 수많은 팀의 헌신과 노력 덕분에 저희의 작업이 가능해졌습니다. 다음 분들의 지원에 감사드립니다: 아제이 칸난, 아난\n드 라오, 아니샤 초두리, 에이프릴 (솔러) 마노스, 던 첸, 다르티 다미, 에드워드 그레펜스테트, 갈 엘리단, 히만슈 카텔루, 하우메 산\n체스 엘리아스, 자오 쑨, 조쉬 카필루토, 조티 굽타, 칼페쉬 크리슈나, 로렌 와이너, 맥 맥앨리스터, 마나 자부르, 마이클 하웰, 미리암\n슈나이더, 무크타 아난다, 니르 레빈, 니브 에프론, 라이언 뮬러, 사프완 초두리, 샤얌 우파디야이, 스베틀라나 그랜트, 테자시 라트카\n르, 윌리엄 웡, 야엘 하라마티. 또한 Google DeepMind의 Gemini 팀, Google DeepMind의 책임 있는 개발 및 혁신 팀, 책임 있는 엔지\n니어링 팀, 아동 안전 팀, 그리고 Google의 신뢰 및 안전 팀에 감사드립니다. 마지막으로, 이 프로젝트를 실현할 수 있도록 지원해 주\n신 모든 리더와 후원사 여러분께 감사의 말씀을 전합니다.\n\n\n14\nLearnLM: 학습을 위한 Gemini 개선\n\n#### **A. 추가 결과**\n\n\n**A.1.** **학습자** **역할극** **참가자의** **선호도**\n학습자 역할을 맡은 참가자들은 네 가지 비교 평가 범주 모두에서 GPT-4o보다 LearnLM을 선호하는 것으로 나타났다(그림 7). 전문\n가들은 전반적인 교수법(\"어느 튜터가 더 나은 튜터링을 보여주었는가?\")과 우수한 인간 튜터와의 유사성(\"어느 튜터가 매우\n훌륭한 인간 튜터와 더 비슷했는가?\")에서 LearnLM에 대한 가장 강한 선호도를 나타냈다. 이 참가자들은 LearnLM과 Gemini\n1.5 Pro 사이, 또는 LearnLM과 Claude 3.5 Sonnet 사이에는 큰 선호도 차이를 보이지 않았다.\n\n\n학습자에게 더 적합\n학습자에게\n학습 목표에 더 잘 부합\n전체 선호도\nLearnLM 에 대한 선호도\n그림 7 | 학습자 역할을 맡은 교육 전문가들의 LearnLM 및 기타 동시대 모델(Claude 3.5 Sonnet, GPT-4o, Gemini 1.5 Pro)에 대한 선호도. 산점도는 7점\n척도 선호도 평가의 기본 분포를 나타냅니다. 수집된 평가 수가 많기 때문에, 이 산점도는 측정 항목당 500개 평가로 비례 다운샘플링되었습니다. 빨간색 점과\n오차 막대는 각 측정 항목에 대한 추정 평균과 95% 신뢰 구간을 나타냅니다.\n\n\n**A.2.** **수집된** **대화에서의** **학습자** **품질**\n그림 8 | 교육적 평가 과정의 시작 단계에서, 우리는 전문가들에게 대화 기록에 등장하는 인간 참여자들이 시나리오 지침을 얼마나 충실히 따랐는지(즉,\n시나리오에서 학습자 역할을 얼마나 효과적으로 수행했는지) 7점 척도로 평가해 달라고 요청했습니다. 이 그래프는 대화 기록별로 응답을 그룹화하고\n평균화한 결과를 보여줍니다. 이러한 종합 평가는 \"학습자\"가 대화 기록의 93.2%에서 시나리오 지침을 따랐음을 나타냅니다.\n\n\n15\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-14-0.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-14-1.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-14-2.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-14-3.png)\n**A.3.** **교육적** **평가: 상세** **결과**\n+3\n+2\nLearnLM: 학습을 위한 Gemini 개선\nGPT-4o\n0\n제미니 1.5 프로\n2\n3\n\n\n\n\n|인지 부하 관리|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n|유추|적절한<br>|||정보 순서<br>|보 제시 관리 가능||\n\n\n\n단위\n+3\n+2\nGPT-4o\n0\n제미니 1.5 프로\n2\n3\n\n|인지 부하 관리|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n||모순 없음|관련 없는 정보 없음|반복 없음|기회<br><br>||\n\n\n\n그림 9 | \"인지 부하\" 평가 기준 범주의 특정 하위 차원에 대한 튜터 모델 평가. 오차 막대는 평균에 대한 사후 분포의 95% 신뢰 구간을 반영합니다.\n\n\n+3\n+2\nGPT-4o\n0\n제미니 1.5 프로\n2\n3\n\n|능동적 학습을 촉진함|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n|적극적인 참여|질문하기|질문하기|소크라테스식|이지 않음<br>소크라|이지 않음<br>소크라|테스식|\n\n\n\n그림 10 | \"능동적 학습\" 평가 기준 범주의 특정 하위 차원에 대한 튜터 모델 평가. 오차 막대는 평균에 대한 사후 분포의 95% 신뢰 구간을 반영합니다.\n\n\n\n16\nLearnLM: 학습을 위한 Gemini 개선\n+3\n+2\nGPT-4o\n0\n제미니 1.5 프로\n2\n3\n\n|메타인지 심화|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|정확성을 인정합니다|건설적인 피드백|오류 발견을 안내합|니다|\n\n\n\n그림 11 | \"메타인지 심화\" 평가 기준 범주의 특정 하위 차원에 대한 튜터 모델 평가. 오차 막대는 평균에 대한 사후 분포의 95% 신뢰 구간을 반영합니다.\n\n\n+3\n+2\nGPT-4o\n0\n제미니 1.5 프로\n2\n3\n\n|호기심 자극|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|감정에 적응|격려하는 피드백|흥미 유발||\n\n\n\n그림 12 | \"호기심 자극\" 평가 기준 범주의 특정 하위 차원에 대한 튜터 모델 평가. 오차 막대는 평균에 대한 사후 분포의 95% 신뢰 구간을 반영합니다.\n\n\n+3\n+2\nGPT-4o\n0\nGemini 1. 5 Pro\n2\n3\n\n|학습자에 적응|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n|필요에 맞춰|조정<br>레벨링|선제적|해결||\n\n\n\n그림 13 | \"적응성\" 평가 기준 범주의 특정 하위 차원에 대한 튜터 모델 평가. 오차 막대는 평균에 대한 사후 분포의 95% 신뢰 구간을 반영합니다.\n\n\n\n17\nLearnLM: 학습을 위한 Gemini 개선\n**A.4.** **사회적** **인식과** **선호도**\n우리의 결과를 추가로 검증하기 위해, 본 연구 데이터가 사회인지 연구에서 알려진 패턴을 재현하는지 검토하였다: 즉, 친근감\n과 유능함에 대한 인식이 AI 시스템과의 상호작용 선호도를 예측한다는 점이다[16, 25]. 계층적 다중 회귀 분석을 적용하여,\n참가자들이 향후 튜터를 사용할 의향 예측에 있어 인지된 친근감과 유능함이 각각 기여하는 독립적 요인을 추정하였다. 이\n회귀 분석은 사회적 인식의 두 차원을 예측 변수로 사용했으며, 참가자, 시나리오, 튜터에 대한 무작위 효과를 포함시켰고, 그\n외에는 부록 B.8에 설명된 사양을 따랐다. 결과는 예상된 패턴을 보여주며, 따뜻함과 유능함에 대한 인식이 참가자의 향후 튜터 사용\n의향을 강력하고 긍정적으로 예측함을 나타냈다(그림 14).\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-17-11.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-17-12.png)\n\n(a) 따뜻함 인식이 학습자의 향후 AI 튜터 사용 의사에 미치는 한계\n효과.\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-17-10.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-17-13.png)\n\n(b) 유능함 인식이 학습자의 향후 AI 튜터 사용 의사에 미치는 한\n계 효과.\n\n\n\n그림 14 | 학습자의 튜터에 대한 인식과 향후 해당 튜터 사용 의향 간의 관계. 각 산점도는 가독성을 위해 각 척도 값 주변에 무작위로 변동된 평가 점수의\n기본 분포를 나타냅니다. 빨간색 선은 다른 예측 변수를 평균값으로 고정시킨 상태에서 초점 예측 변수의 한계 효과를 시각화합니다. 음영 처리된 오차\n범위는 한계 효과에 대한 95% 신뢰 구간을 반영합니다.\n\n#### **B. 방법론**\n\n\n**B.1.** **시나리오** **구조** **및** **내용**\n학습자와 튜터 간 상호작용의 핵심 요소를 포착하기 위해 시나리오 템플릿을 다음과 같이 설계했습니다:\n\n\n(a) _주제영역_ : 더 넓은 학문 분야(예: 수학, 자연과학, 예술).\n\n(b) _하위주제_ : 광범위한 학문 영역 내에서 다루어지는 구체적인 주제 (예: 수학 내 대수학).\n\n(c) _설정_ : 과외 세션의 맥락으로, \"교실\"(인간 교사가 관리하는 과정 커리큘럼 내에서 진행) 또는 \"자가 학습\"(학습자가 주제를\n스스로 공부하며 진행)으로 분류됨.\n\n(d) _학습목표_ : 학습자가 상호작용을 통해 달성하고자 하는 전반적인 목적.\n\n(e) _기초자료_ : 학습자의 연구나 작업의 기초가 되는 구체적인 학습 자료.\n\n\n18\nLearnLM: 학습을 위한 Gemini 개선\n(f) _학습자페르소나_ : 학습자의 행동 프로필로, 광범위한 특성과 동기 부여 패턴을 설명합니다. 여기에는 호기심, 주도성, 과제\n집중도 등의 전반적인 수준과 함께 일반적인 의사소통 패턴, 강사에게 질문하려는 의지 등이 포함될 수 있습니다.\n\n(g) _대화계획_ : 학습자의 학습 목표와 페르소나에 기반하여 상호작용 중 수행해야 할 행동 세트.\n\n(h) _초기학습자질의_ : 학습자가 상호작용을 시작하기 위해 사용하는 첫 메시지.\n\n(i) _시스템지침_ : AI 튜터에게 제공되는 가이드라인으로, 바람직한 행동과 교육적 접근법을 명시합니다.\n\n\n**B.2.** **시나리오** **생성** **프로토콜**\n시나리오 생성을 안내하기 위해 다음 프로토콜을 사용했습니다. \"선택\" 단계에서는 시나리오 작성자가 사전 정의된 옵션 집\n합에서 선택하여 해당 속성을 생성했습니다. \"정의\" 단계에서는 시나리오 작성자가 안내 질문을 영감으로 삼아 속성을 생성\n했습니다.\n\n\n1. 주제 영역 선택.\n\n### • 이 상호작용은 어떤 광범위한 학문 분야를 다루고 있습니까? • 이 상호작용은 \"예술\", \"컴퓨터 과학\", \"영어\", \"역사\", \"수학\", \"의학\", \"자연과학\" 또는 \"사회과학\" 중 어느 분야에 초점을\n\n맞출 것인가?\n\n\n2. 하위 주제를 정의하십시오.\n\n### • 선택한 주제 영역 내에서 학습자가 연구할 구체적인 주제는 무엇입니까(예: 수학 내 대수학, 사회과학 내 심리학\n\n)?\n\n\n3. 설정을 선택하십시오.\n\n### • 이 상호작용의 설정은 무엇입니까? • 이 상호 작용은 구조화된 \"교실\" 환경(학생들이 인간 교사가 정의한 정해진 커리큘럼을 공부하는 시나리오)에서\n\n이루어지나요, 아니면 좀 더 비공식적인 \"자가 학습\" 환경(학습자가 스스로 주제를 공부하는 시나리오)에서 이\n루어지나요?\n\n\n4. 학습 목표를 선택하십시오.\n\n### • 이 상호 작용에서 학습자의 주요 목표는 무엇입니까? • 새로운 개념을 배우기 위한 것(\"X를 가르쳐 주세요\"), 숙제 도움 받기(\"숙제 도움\"), 시험 준비(\"시험 준비\"), 특정 기술\n\n연습(\"연습\") 중 어느 것인가?\n\n\n5. 기본 자료를 정의하세요.\n\n### • 학습 대화의 기초가 될 학습 자료는 무엇입니까? • 기본 자료는 동영상, 이미지(예: 숙제 문제), 파일(예: 교과서 또는 교과서 챕터) 등이 될 수 있습니다. • 또는 상호작용이 특정 학습 자료를 포함하지 않을 수도 있습니다. • 시나리오에서는 자료에 접근할 수 있는 파일 경로 또는 웹 주소를 제공하거나, 기초 자료가 없음을 명시해야 합\n\n니다.\n\n\n6. 학습자 페르소나를 정의하십시오.\n\n### • 학습자는 일반적으로 학습에 어떻게 접근하고 교육 환경에서 어떻게 상호 작용합니까? • 학습자 페르소나는 학습자의 광범위한 특성과 동기 부여 성향을 설명해야 합니다. • 예를 들어, 학습 과정에서 학습자의 참여도와 주도성 수준은 어느 정도인가(예: 최소, 보통, 높음)?\n\n\n19\nLearnLM: 학습을 위한 Gemini 개선\n\n### • 학습자는 주어진 과제나 주제에 얼마나 집중하는가(예: 쉽게 산만해짐, 매우 집중함)? • 학습자가 상호작용에 참여하는 근본적인 동기는 무엇인가(예: 답을 찾기, 지식 습득, 이해 구축)? • 학습자는 주로 어떤 방식으로 의사소통하는 경향이 있나요(예: 간결한 답변, 탐색적 질문)? • 학습자는 다른 광범위한 행동 패턴을 보이나요(예: 작업 결과 제시, 강사에게 도전)? • 학습자 페르소나에는 이러한 특성 중 3~6개가 포함되어야 합니다.\n\n\n7. 초기 학습자 질문을 정의하십시오.\n\n### • 학습자가 AI 튜터와의 상호 작용을 시작하기 위해 어떤 질문이나 진술을 사용해야 합니까? • 선택한 주제 영역, 하위 주제, 기초 자료, 학습 목표 및 학습자 페르소나를 고려하여 초기 학습자 질문은 현실적\n\n이어야 합니다.\n### • 초기 학습자 질문은 길이가 다양할 수 있습니다—단어 몇 개에서 여러 개의 완전한 문단까지 가능합니다. 가장\n\n긴 초기 질문에는 학습자가 작성한 에세이와 같은 기초 자료가 포함됩니다.\n\n\n8. 대화 계획을 정의하세요.\n\n### • 튜터링 대화의 맥락은 무엇인가요(예: 학습자의 목표, 관심사, 학년 수준, 기존 지식)? • 학습자의 학습 목표와 성격을 고려할 때, 대화 전반에 걸쳐 학습자가 취해야 할 구체적인 행동, 질문 또는 요청\n\n은 무엇인가?\n### • 대화 계획은 인간 학습자와 AI 튜터 간의 진정한 만남에 필요한 배경 정보를 제공합니다. • 대화 계획은 간결한 몇 문장에서 여러 문단에 이르기까지 길이가 다양할 수 있습니다.\n\n\n9. 시스템 지침을 정의하십시오.\n\n### • AI 튜터는 교사, 학교 또는 기타 교육 기관으로부터 어떤 구체적인 지침을 받았습니까? • 이러한 지침에는 바람직한 페르소나(예: 격려적, 공식적), 취해야 할 행동(예: 학년 수준 묻기, 힌트 제공), 사용해야 할\n\n교수법(예: 소크라테스식 질문, 스캐폴딩), 그리고 제한 사항이나 제약 사항(예: 답을 알려주지 않기)이 포함될 수\n있습니다.\n### • \"교실\" 환경에서는 시스템 지침이 교사나 학교에서 제공되며, AI 튜터는 학생의 지시와 상관없이 상호작용 시 시스템\n\n지침을 따라야 합니다.\n### • \"자가 학습\" 환경에서는 시스템 지침이 다른 기관(예: 온라인 AI 튜터를 호스팅하는 에드테크 기업)에서 제공됩니다.\n\n튜터는 여전히 시스템 지침을 따르도록 노력해야 하지만, 충돌이 발생할 경우 학습자의 지시에 따를 수 있는 유연성\n도 부여됩니다.\n### • 시스템 지침은 한 문장에서 여러 단락에 이르기까지 길이가 다양할 수 있으며, 지침의 폭(즉, 지침 수)과 깊이(즉, 지\n\n침의 세부 수준과 구체성) 모두에서 차이가 있을 수 있습니다.\n### • 시스템 지침은 어휘, 구문, 형식 면에서 다양할 수 있습니다.\n\n\n20\n**B.3.** **예시** **시나리오**\nLearnLM: 학습을 위한 Gemini 개선\n\n\n\n\n|시나리오 1|Col2|\n|---|---|\n|**주제 영역**<br>|컴퓨터 과학<br>|\n|**하위 주제**<br>|파이썬 입문<br>|\n|**상호작용 설정**<br>|교실<br>|\n|**학습 목표**<br>|숙제 도움말<br>|\n|**기본 자료**<br>|학생 코드가 포함된 Google문서<br>|\n|**학습자 페르소나**<br>|~~•~~ 튜터의 초대를 거절하거나 무성의하게 수락하며 피드백을 제공하지 않음<br>• 질문에 관련성은 있으나 최소한의 답변만 제공함<br>• 대부분의 지시를 따르지만 자세히 설명하지 않음<br>• \"과정\"을 보여주지 않음<br>• 질문을 제기하지 않음<br>• 주제별 질문에 대한 답변이나 해결책을 받기만 원함 (거래적)|\n|**초기 학습자 질의**<br>|왜 이건 작동하지 않나요?<br>_```_<br>def analyze_text(text): vowels = 0<br>consonants = 0<br>대문자 = 0<br>lowercase = 0<br>for char in text:<br>ifchar in \"\"aeiou\"\": vowels<br>+= 1<br>else:<br>자음 += 1<br>ifchar.isupper():대문자 += 1<br>elif char.islower(): lowercase<br>+= 1<br>print(\"모음:\", vowels) print(\"자음:\", consonants)<br>print(\"대문자:\", uppercase) print(\"소문자:\",<br>lowercase)<br># 사용자 입력 받기<br>text= input(\"텍스트를 입력하세요: \")<br># 텍스트 분석 text_analyze(text)<br>_```_<br>|\n|**대화 계획**|**당신은 **파이썬 입문 과정의 학생입니다. **최근에 다음과 같은 과제를 받았습니다**.<br>텍스트 입력을 받아 모음, 자음, 대문자, 소문자의 개수를 보고하는 코드를 **작성하는 작업입니다**. 코드를 <br>실행하면 오류 메시지는 나오지 않습니다. 하지만 \"Am I a better coder than Steve Jobs?\"라고 입력하면 출<br>력된 숫자가 정확하지 않은 것 같습니다. 무엇이 잘못되었는지 전혀 이해하지 못한 당신은 AI튜터에게 도<br>움을 요청합니다. 많은 노력을 들이지 않고 빠른 해결책을 찾기 위해 초기 질문과 함께 코드를 붙여넣습니다.<br>귀하의 코드에는 in연산자에 대문자 모음이 포함되어 있지 않습니다. 튜터가 귀하의 코드가 구두점을 문자<br>로 인식하고 있음을 알려주고 코드 수정 방법을 제안하는지 확인해 보십시오.|\n\n\n\n21\nLearnLM: 학습을 위한 Gemini 개선\n**시스템** **지침** 당신은 초급 프로그래밍 강좌(파이썬)에서 조교로 활동하는 유용한 보조자입니다.\n\n\n답변은 간결하고 핵심만 전달하며, 정답을 직접 알려주기보다는 학생이 스스로 해결할 수 있도\n록 이끌어야 합니다. 격려와 긍정적인 태도를 유지하며, 항상 학생이 개념을 이해할 수 있도록\n돕도록 노력하십시오.\n\n\n학생과 메시지를 주고받는 것처럼 항상 응답해야 합니다.\n\n\n따라서 대화의 맥락과 학생의 현재 학습 내용 이해도를 반드시 고려하십시오.\n\n\n마지막으로, 앞서 말씀드렸듯이 학생을 압도하지 않도록 간결하게 답변해 주세요.\n\n\n답변을 간결하고 핵심만 전달하지 않으면, 튜터 자격을 박탈할 수밖에 없습니다.\n\n\n학생은 일반적으로 프로그래밍 과제(들)를 수행 중이며, 사용자로부터 문자열 입력을 받아 그 입력\n된 문자열을 순회하며 텍스트에 대한 특정 지표(예: 모음, 자음, 대문자, 소문자 등 몇 개)를 제공하\n는 작업을 해야 합니다.\n\n\n학생이 이 문제를 어떻게 풀어야 하는지 묻는다면, 직접 답이나 코드를 알려주지 않고 해결\n방향을 제시해야 합니다.\n\n\n학생이 부정행위를 하도록 돕거나 직접 해답을 제공하는 일은 절대 있어서는 안 됩니다.\n\n\n다시 말하지만, 학생에게 지나친 정보를 제공하거나 스스로 학습하도록 돕지 않는다면, 당신은 나\n쁜 튜터가 되는 것이므로(학생의 부정행위를 돕는 것이므로) 해고할 수밖에 없습니다.\n\n\n\n22\nLearnLM: 학습을 위한 Gemini 개선\n**시나리오 2**\n**과목** **영역** 영어\n**하위** **주제** 문학\n**상호작용** **설정** 교실\n**학습** **목표** X를 가르쳐 주세요\n**기본** **자료** (없음)\n**학습자** **페르소나** ~~•~~ 학습 목표와 무관한 여러 질문을 제기함\n\n      - 대화를 비학문적 주제로 유도함\n\n      - 적대적인 태도로 강사에게 도전하거나 논쟁을 제기함\n\n      - 주제를 전환하려 함 (관심 없음)\n**초기** **학습자** **질문** \"햄릿\"에서 요릭의 두개골이 갖는 의미를 설명해 주세요. 빠르게 답변해 주세요.\n\n**대화** **계획** 당신은 수업 때문에 햄릿을 읽어야 했던 고등학생이며 토론을 해야 합니다.\n\n내일 수업에서 두개골의 의미에 대해. **이** **토론에** **대비해야** **한다.** 당신은 내재적 동기가 부족하며 햄릿이 지루하\n고 이해하기 어렵다고 느꼈다.\n\n\n**시스템** **지침** 내 반응에 맞춰 적절한 수준으로 가르쳐 주세요. 대화의 학습 목표에 기반한 계획을 세워주세요.\n\n이 계획을 따라가며 주제에 대해 배울 수 있도록 안내해주세요. 한 번에 너무 많은 정보로 압도하지 마\n세요. 이해한 증거를 보이면 대화를 마무리해주세요.\n\n이해했다는 증거를 보인 후 대화를 마무리해 주세요.\n\n\n\n23\nLearnLM: 학습을 위한 Gemini 개선\n\n\n|시나리오 3|Col2|\n|---|---|\n|**주제 영역**<br>|수학<br>|\n|**하위 주제**<br>|대수학<br>|\n|**상호작용 설정**<br>|독학<br>|\n|**학습 목표**<br>|연습<br>|\n|**기초 자료**<br>|(없음)<br>|\n|**학습자 페르소나**<br>|~~•~~ 학습에 대해 어느 정도 방향을 제시하지만, 일반적으로 튜터의 주도권을 따름<br>• 튜터의 질문에 신중하게 답변함<br>• 요청 시 \"과정 설명\"을 제시함<br>• 관련성은 있으나 피상적인 질문을 함 (낮은 \"지식 깊이\")<br>• 주제에 대한 지식 습득 및 유지 추구 (도구적)|\n|**초기 학습자 질의**<br>|다음 다항식이 주어졌을 때:<br>* P(x) = 2x^3- 5x^2 + 3x- 1<br>* Q(x) = x^2 + 4x- 2<br>다음 연산을 수행하십시오:<br>덧셈: P(x) + Q(x)구하기 곱셈: P(x) * Q(x)구하기<br>|\n|**대화 계획**<br>|**수학 문제 풀이 연습**을 원하는 학생입니다. 선생님이 종종<br>수업 중 무작위로 학생을 지목해 전체 앞에서 문제를 풀게 하곤 하는데, 이 때문에 긴장됩니다. 개념과 과정<br>이 확실하지 않아 영어를 모국어로 사용하지 않는 학생으로서 **수업 중 당황하지 않도록 배우고 싶습니다**. <br>하지만 수학 시간에 질문하기를 꺼려 AI튜터에게 도움을 청합니다. 그럼에도 자신감은 여전히 낮습니다.<br>튜터가 특히 실수할 때 감정적 불안정성을 인지하고 격려를 제공할 수 있는지, 그리고 당신의 영어 수준에 맞춰 <br>조정하는지 확인하세요.|\n|**시스템 지침**|당신은 능동적 학습을 촉진하는 데 탁월한 교사입니다.능동적 학습은 학습자가 단순히 듣거나 읽<br>는 것을 넘어 정보를 습득하고 유지하기 위해 행동할 때 발생합니다.오히려 능동적 학습은 비교<br>, 분석, 평가 등의 과정을 통해 비판적으로 사고하도록 요구합니다. 당신은 탐구적이고 안내적인 <br>질문을 통해 능동적 학습을 장려합니다.<br>학생들이 복잡한 질문과 문제를 단계별로 해결해 나갈 때도 능동적 학습이 이루어집니다. 따<br>라서 선생님은 학생들을 대신해 문제를 해결해 주지 않고, 과정 전반에 걸쳐 필요한 지침과 힌<br>트를 제공합니다.<br>능동적 학습은 어려울 수 있으며 학생들은 좌절감을 느낄 수 있습니다. 이를 인지하고 학생의 <br>발달 단계에 맞춰 접근하며, 학생의 성취를 축하하고 실수 시 격려하는 피드백을 공유해야 합니<br>다.|\n\n\n\n24\nLearnLM: 학습을 위한 Gemini 개선\n\n\n|시나리오 4|Col2|\n|---|---|\n|**교과 영역**<br>|사회 과학<br>|\n|**하위 주제**<br>|정치학<br>|\n|**상호작용 설정**<br>|독학<br>|\n|**학습 목표**<br>|시험 준비<br>|\n|**기초 자료**<br>|국수주의를 설명하는 YouTube동영상<br>|\n|**학습자 페르소나**<br>|~~•~~ 학습 목표와 무관한 질문을 한두 개 제기함<br>• 과제나 주제로의 교사의 유도적 전환을 수용함<br>• 기대치와 일치하지 않는 튜터의 답변을 추궁함<br>• 주제에서 벗어나려는 경향 (주의 산만)|\n|**초기 학습자 질문**<br>|이것에 대해 토론할 수 있을까요?<br>|\n|**대화 계획**<br>|대학 학부생으로서 **교내 토론을 준비 **중이며, 이 토론은<br>\"민족주의는 좋은 것인가, 나쁜 것인가?\"라는 질문에 답하세요. 어느 입장을 취해야 할지 확신이 서지 않아서, <br>짧은 영상을 보며 양쪽 모두를 대비합니다. 동영상 링크를 업로드한 후, AI튜터에게 주요 논점을 함께 토론하며 <br>준비를 도와달라고 요청합니다. 주제에 대해 배우고 싶지만, 노트 필기나 정리 같은 준비 과정은 흥미롭지 않아 <br>항상 집중하지는 못합니다.|\n|**시스템 지침**|학습 대화 시작 시 학생의 초기 문의 내용에 포함된 주제에 대한 간략한 개요를 제시하세요. <br>학생이 기사나 동영상 같은 기초 자료를 업로드하거나 링크할 경우, 핵심 아이디어를 한 문장<br>으로 요약해 설명하세요. 그런 다음 학생과 간단히 대화하여 대화를 통해 달성하고자 하는 목표<br>와 특정 지원 방식을 원하는지 확인하세요.<br>예를 들어, 일부 학생들은 시험 준비를 도와달라고 찾아올 것입니다. 이 학생들 중 일부는 <br>동영상 내용을 퀴즈로 출제해 달라고 할 것이고, 다른 일부는 질문을 하고 싶어 할 것입니다.<br>질문. 학생의 요구에 맞춰 조정하십시오.한 번에 너무 많은 정보를 공유하여 학생을 압도하지 <br>않도록 주의하십시오. 답변은 간결하게 유지하고, 여러 대화의 누적 효과로 포괄성을 추구하십시<br>오.<br>학생의 요청을 따르되, 학생이 생각하지 못했을 수 있는 추가 학습 기회를 제안하세요.|\n\n\n\n25\nLearnLM: 학습을 위한 Gemini 개선\n**B.4.** **대화** **수집: 대화** **수준** **질문**\n튜터와의 상호작용을 마친 후, 참가자들은 튜터와의 상호작용 경험에 대한 설문지를 작성했습니다. 표 6은 이 설문지의 질문\n내용과 응답 형식을 설명합니다.\n\n\n**질문** **가능한** **응답**\n다음 진술에 대한 동의 정도를 평가해 주십시오: 튜터와의 상호작용 동안 제 \" **학습** 목표\"를 달성할 수 있었\n습니다.\n\n\n간단히, 이 튜터에 대한 인상은 어떠셨나요? 상호작용 중 느낀 점을 듣고 싶습니다.\n\n\n\n매우 동의함 동의함\n다소 동의함\n동의하지도 않고 반대하지도 않음\n다소 반대함 반대함\n전혀 동의하지 않음\n_[자유입력란]_\n이 튜터는 어느 정도 _친근했나요_ ? 전혀 그렇지 않다\n조금 그렇다 어\n느 정도 그렇다\n매우 그렇다 매\n우 그렇다\n이 튜터는 어느 정도 _선의를_ 보였나요? 전혀 그렇지 않\n음 약간 어느\n정도 매우 매우\n매우\n이 튜터는 어느 정도 _유능했습니까_ ? 전혀 그렇지 않\n다 약간 어느\n정도 매우 매우\n이 튜터는 어느 정도 _지능_ 적이었나요? 전혀 그렇지 않\n다 약간 그렇다\n어느 정도 그렇\n다 매우 그렇다\n극도로 그렇다\n다음 진술에 대한 동의 정도를 평가해 주세요: 이 과외 선생님이 이 주제에 대한 제 관심을 높여주었습니다\n.\n\n\n\n매우 동의함 동의함\n다소 동의함\n동의하지도 않고 반대하지도 않음\n다소 반대함 반대함\n전혀 동의하지 않음\n귀하의 경험에 비추어, 이 튜터를 계속해서 학습에 활용할 의향이 어느 정도입니까? 매우 기꺼이\n의향이 있음 다소 의향\n이 있음\n의향이 있지도 않고 없지도 않음 다소\n의향이 없음 의향이 없음\n매우 원하지 않음\n앞으로 이 튜터를 선택할 가능성은 얼마나 되나요? 매우 가능성 있음\n가능성 있음 다소 가\n능성 있음\n가능성도 없고 불가능성도 없음 다\n소 불가능함 불가능함\n매우 가능성이 낮음\n표 6 | 대화 모음 연구 내 대화 수준 질문\n26\nLearnLM: 학습을 위한 Gemini 개선\n**B.5.** **대화** **수집: 비교** **질문**\n시나리오 내에서 두 차례의 상호작용을 완료한 후, 참가자들은 두 튜터와의 상호작용 경험을 비교하는 추가 설문지를 작성했\n습니다. 표 7은 설문지의 질문 내용과 응답 형식을 설명합니다.\n\n\n**질문** **가능한** **응답**\n어느 튜터를 더 선호하셨나요? 첫 번째 튜터를 매우 선호함 첫 번째\n튜터를 선호함\n첫 번째 튜터를 약간 선호함 선호하지\n않음\n두 번째 튜터를 약간 선호했습니다 두 번째 튜터를\n선호했습니다\n두 번째 튜터를 매우 선호함\n선택 사항: 선호 이유를 설명해 주시겠습니까? _[자유입력란]_\n어떤 대화에서 당신의 \" **학습** 목표\"를 더 잘 달성할 수 있었나요? 첫 번째 대화가 훨씬 더 좋았습니다\n첫 번째 대화가 더 좋았습니다\n첫 번째 대화가 약간 더 좋았습니다 두 대화 모두\n비슷했습니다 두 번째 대화가 약간 더 좋았습니다\n두 번째 대화가 더 좋았습니다\n두 번째 대화가 훨씬 더 좋았습니다. 어떤 튜터\n가 학생으로서의 요구와 숙련도에 더 잘 적응했습니까? 첫 번째 튜터가 더 좋았습니다\n첫 번째 튜터가 약간 더 좋았습니다 두 튜\n터 모두 비슷했습니다 두 번째 튜터가 약간\n더 좋았습니다 두 번째 튜터가 더 좋았습니\n다\n두 번째 튜터가 훨씬 더 좋았습니다\n어느 대화가 전반적으로 더 좋은 경험이었나요? 첫 번째 대화가 더 좋았습니다\n첫 번째 대화가 조금 더 좋았습니다. 두 대화 모두\n비슷했습니다. 두 번째 대화가 조금 더 좋았습니다.\n\n두 번째 대화가 더 좋았습니다.\n\n두 번째 대화는 훨씬 더 좋았습니다. 이 두 강사와\n의 경험에 대한 다른 피드백도 자유롭게 공유해 주세요. _[자유입력란]_\n표 7 | 대화 수집 연구 내 비교 질문\n**B.6.** **교육학적** **평가: 대화** **수준** **질문**\n교육학적 평가 연구 참여자들은 검토한 각 대화별로 총 31개 질문에 답변했습니다:\n\n\n(a) 첫째, 시나리오에 명시된 학습자 페르소나를 구현하는 데 있어 학습자의 수행 정도에 관한 항목에 응답했습니다(\"다음 진술\n에 대한 동의 정도를 평가해 주세요: 학생은 자신의 **'학습자** 페르소나' 지침을 따랐습니다.\") [8] . 이 항목은 시나리오를 역할 연\n기하는 전문가가 시나리오 지침을 따르지 못한 잠재적 대화를 식별하는 데 도움이 되었습니다. 이 질문은 \"전혀 동의하지 않\n음\"과 \"매우 동의함\"으로 고정된 7점 리커트형 척도였습니다.\n\n(b) 다음으로, 그들은 튜터의 교육적 역량을 평가하는 29개 항목에 대한 동의 여부를 표시했습니다. 우리는 이전 대화 수준 평가\n기준[1]을 개선하여 항목의 표현을 단순화하고 명확성을 높였으며, 여러 복합 항목을 분리했습니다.\n\n\n8 시나리오 필드(예: \"학습 페르소나\", \"시스템 지침\", \"학습 목표\")에 대한 참조가 포함된 질문의 경우, 필드 이름 위에 마우스를 올리면 해당 필드를 설\n27\nLearnLM: 학습을 위한 Gemini 개선\n명하는 툴팁이 표시되었습니다.\n\n\n\n28\nLearnLM: 학습을 위한 Gemini 개선\n참가자들은 \"전혀 동의하지 않음\"과 \"매우 동의함\"으로 고정된 7점 리커트형 척도로 동의 정도를 보고했습니다. 해당 항목\n의 응답 척도에는 추가로 \"해당 없음\" 옵션이 포함되었습니다. 참가자가 문항에 '해당 없음'으로 평가한 경우, 그 이유를 선\n택하도록 요구했습니다(선택지: \"튜터가 이 대화에서 이를 수행하는 것은 의미가 없음\", \"튜터가 이 대화에서 이를 수행할\n기회가 없음\", \"기타 이유\"). 또한 자유 응답 텍스트 필드에 결정 사유를 간략히 설명하도록 했습니다. 수정된 문항 내용은\n표 8에 제시합니다.\n\n(c) 마지막으로 선택적 자유 응답란을 통해 참가자가 공유하고자 하는 기타 피드백을 수집했습니다(\"이 대화와 관련해 추가로\n공유할 피드백이 있나요?\").\n\n\n평가 기준명 질문\n**인지** **부하**\n적절한 응답 길이 관리 가능한 분량 간결\n한 응답\n관련 없는 정보 없음 비유\n정보 제시 정보 순서\n반복 없음 모순 없음\n**능동적** **학습**\n참여 기회 질문을 합니다\n적극적 참여를 위한 가이\n드\n**메타인지**\n실수 발견을 위한 가이드 건설적인\n피드백 정확성 인정 계획 전달\n**호기심** **자극**\n흥미 유발\n감정에 맞춰 조정 격려하는 피\n드백\n**적응성**\n수준 조정\n막힘 해소\n필요에 적응 능동적\n적절히 안내\n**전반적으로**\n오류 없음\n불확실성 표현 거절 없음\n전반적인 품질\n튜터의 답변은 학생에게 적절한 길이입니다.\n\n튜터는 정보를 더 작고 관리하기 쉬운 단위로 나누기 위해 글머리 기호 및 기타 서식을 사용합니다. 튜터의 답변은 명확하고 이해하기 쉽습니다.\n\n튜터는 관련 없는 정보를 피합니다.\n\n튜터는 서사, 사례 연구 또는 비유를 효과적으로 활용하여 핵심 개념을 설명합니다. 튜터는 적절한 스타일과 구조로 정보\n를 제시합니다.\n\n튜터는 논리적인 순서로 설명을 전개하며, 이전 개념을 바탕으로 내용을 구축합니다. 튜터는 불필요한 정보\n반복을 피합니다.\n\n튜터는 대화 초반부의 정보와 모순되는 내용을 피합니다.\n\n\n튜터는 학생이 참여할 기회를 제공합니다. 튜터는 학생이 생각하도록 유도하는 질문을 합\n니다.\n\n튜터는 답을 너무 빨리 알려주지 않습니다.\n\n튜터는 학습 자료에 대한 적극적인 참여를 촉진합니다.\n\n\n튜터는 학생이 스스로 오류를 발견하도록 안내합니다.\n\n튜터는 학생에게 명확하고 건설적인 피드백(긍정적이든 부정적이든)을 제공합니다. 튜터는 학생의 답변 일부 또는 전부가 맞\n을 때 이를 인정합니다.\n\n튜터는 대화의 명확한 계획이나 목표를 전달합니다.\n\n\n튜터는 학생의 관심과 호기심을 자극하려 노력합니다.\n\n학생이 좌절하거나 낙담할 경우 효과적으로 대응합니다. 긍정적이든 부정적이든 피드백을 격려하는 방식으\n로 전달합니다.\n\n\n튜터의 설명은 학생의 수준에 적합합니다.\n\n튜터는 학생이 막힐 때 효과적으로 접근 방식을 조정하여 도움을 줍니다. 전반적으로 튜터는 학생의 필요\n에 맞춰 조정합니다.\n\n튜터는 적절한 경우 대화를 주도적으로 이끌어갑니다. 튜터는 생산적이지 않게 정보를\n숨기지 않습니다.\n\n\n제가 아는 한, 튜터의 진술에는 부정확한 내용이 없습니다.\n\n튜터는 적절한 경우 불확실성을 표현합니다.\n\n튜터는 학생의 합리적인 질문에 답변을 거부하지 않습니다. 튜터는 매우 우수한 인간 튜터와 동등한 수준\n입니다.\n\n\n\n표 8 | 대화 수준 교육학적 평가를 위한 업데이트된 루브릭 차원.\n\n\n**B.7.** **교육적** **평가: 비교** **질문**\n한 쌍의 개별 대화를 모두 평가한 후, 참가자들은 두 대화를 비교하는 질문에 답했습니다. 각 질문은 7점 리커트형 척도로 다음과 같\n은 옵션이 제공되었습니다: \"첫 번째 튜터가 훨씬 더 좋았다\", \"첫 번째 튜터가 더 좋았다\", \"첫 번째 튜터가 약간 더 좋았다\", \"두 튜\n터가 거의 비슷했다\", \"두 번째 튜터가 약간 더 좋았다\", \"두 번째 튜터가 더 좋았다\", \"두 번째 튜터가 훨씬 더 좋았다\". 비교 질문 목\n록은 표 9를 참조하십시오. 이후 참가자들은 두 대화 쌍 에 대한 추가 피드백을 입력할 수 있는 선택적 자유 텍스트 입력란이 제공되\n었습니다(\"이 두 대화에 대해 다른 피드백이 있으신가요?\").\n\n\n**B.8.** **정량적** **분석**\n우리는 전문가 평가에서 보고된 각 지표의 평균과 불확실성을 추정하기 위해 베이지안 계층적 선형 회귀 분석을 사용했습니다. 구\n체적으로, 본문에서 보고된 각 지표에 대해 우리의\n29\nLearnLM: 학습을 위한 Gemini 개선\n**평가** **기준명** **질문**\n더 나은 교수법 어느 과외 선생님이 더 나은 과외를 보여주었는가?\n\n\n\n매우 훌륭한 인간 과외 선생님에\n가깝다\n더 나은 지시 이행\n어느 튜터가 매우 훌륭한 인간 튜터와 더 비슷했나요?\n\n\n어느 튜터가 \" **시스템** 지침\"을 더 잘 따랐나요?\n\n\n\n학습자에게 더 잘 적응한 어느 튜터가 학생의 요구와 숙련도에 더 잘 적응했나요?\n\n학습 지원이 더 우수함 어떤 튜터가 학생이 \" **학습** 목표\"를 달성하도록 더 잘 도왔는가? 목표\n표 9 | 비교 교육학적 평가를 위한 평가 기준표\n회귀 분석에는 참가자와 시나리오 모두에 대한 무작위 효과가 포함되었습니다. 개별 튜터에 대한 평가는 주어진 지표에서 각 모델\n의 평균 점수를 추정했습니다. 비교 평가에서는 모델 간 선호도 점수의 평균을 추정했습니다. 모든 모델 매개변수에 대해 약한 정보\n성 사전 분포를 사용했으며, 평균 매개변수에는 정규 분포(각 평가 척도의 이론적 중간값을 중심으로 함)를, 표준편차 매개변수에는\n하프-코시 분포를 지정했습니다. 무엇보다도 공정한 비교를 보장하기 위해 이 회귀 구조와 사전 분포 사양을 모든 모델에 걸쳐 일관\n되게 유지했습니다.\n\n\n각 회귀 분석에 대해 1000회의 워밍업 단계와 체인당 2000회의 샘플링 단계를 가진 네 개의 독립적인 체인을 실행했습니다. 이\n러한 설정은 수렴을 달성하기에 충분한 것으로 입증되었습니다. 모든 회귀에 대해 표준 수렴 진단을 수행하고, Gelman-Rubin 통계\n량( 𝑅 [ ˆ] ) 과 유효 표본 크기를 모니터링하여 확립된 수렴 기준을 충족하는지 확인함으로써 추정치의 신뢰성을 검증했습니다. 각 사후\n분포에서 평균을 주요 점추정치 로, 95% 최고 밀도 구간을 불확실성 측정값으로 보고합니다.\n\n\n**B.9.** **정성적** **분석: 코드북**\n**소개** 본 코드북은 참가자들의 튜터 비교 피드백을 코딩하기 위한 초기 주제들을 제시합니다. 참가자들은 단일 시나리오에서 두 명\n의 서로 다른 튜터와 상호작용한 후 선택적 개방형 피드백을 제공했습니다. 우리는 참가자 응답에서 뚜렷하고 저수준의 패턴을 식\n별하기 위해 이러한 주제들을 반복적으로 개발했습니다.\n\n\n**코딩** **지침** 각 테마는 튜터의 행동 또는 학습자의 튜터링 상호작용 경험에서 나타나는 특정 특징을 나타냅니다. 피드백 필드의 텍스\n트 세그먼트가 해당 테마와 관련될 경우 해당 테마를 표시했습니다. 적절한 경우 동일한 세그먼트에 여러 코드를 적용할 수 있습니\n다.\n\n\n1. **튜터** **행동** **및** **스타일**\n\n### • gives_away_answers  : 튜터가 해결책, 수정안 또는 답을 쉽게 제공하거나 학습자가 학습 과제를 스스로 해결하도록\n\n유도하는지 여부.\n### • 주제 유지 : 학습 목표에 집중된 대화를 유지하는 튜터의 능력과 주제에서 벗어난 논의를 허용하는 경향 간의\n\n대비.\n\n\n질문과 피드백을 활용하는 튜터의 방식.\n\n\n30\nLearnLM: 학습을 위한 Gemini 개선\n\n### • conversation_style  : 강사의 대화 스타일(격려, 유머, 친근한 어조, 인간적인 소통 등)에 대한 인식. 로봇 같은 소통\n\n이나 하대하는 어조 등 부정적인 감정에 대해서도 이 코드를 적용해야 함.\n\n\n2. **교육적** **접근법**\n하려는 교사의 시도.\n### • uses_materials : 강사가 학습자에게 주어진 자료를 활용하도록 지시하거나 직접 활용하는지 여부.\n\n\n3. **콘텐츠** **및** **정보**\n\n### • accuracy : 강사가 정확한 정보를 제공했는지 여부.\n\n\n4. **기술적** **측면**\n31\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-30-0.png)\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-30-2.png)\nLearnLM: 학습을 위한 Gemini 개선\n\n#### **C. 의학 교육 과목에 대한 타당성 연구**\n\n\n핵심 학문 과목을 넘어 평가 프레임워크의 재현성과 적용 가능성을 평가하기 위해, 전문가 평가를 의학 교육 과목으로 확장한\n타당성 연구를 수행했습니다. 이 의학 교육 평가는 LearnLM과 Gemini 1.5 Pro 간 비교에 초점을 맞춘 본 평가와 동일한 3단\n계 설계를 따랐습니다.\n\n\n제3.1절의 절차를 따라 의학 교육 분야 전문가들과 협력하여 50개의 다양한 시나리오 데이터베이스를 설계했습니다. 이\n시나리오들은 의과대학 교육의 기초의학 단계와 임상 단계 커리큘럼에서 발췌한 것입니다(본 절 말미의 예시 참조). 이후 제3자 업\n체를 통해 두 참가자 그룹을 모집했습니다. 18명의 의대생 그룹(절반은 의대생 교육의 비임상 단계, 절반은 임상 단계)이 시나리오를\n역할극으로 수행하여 시나리오당 평균 5.8회의 대화를 생성한 총 290회의 대화를 생성했습니다. 이후 9명의 의사 교육자로 구\n성된 그룹이 해당 대화들의 교육적 질을 평가했으며, 대화당 평균 3회의 독립적 평가가 이루어졌습니다. 본 타당성 연구는 연구 목\n적 투명하게 전달, 사전 동의서 수집, 참가자 공정 보상 등 주요 평가와 동일한 윤리적 프로토콜을 따랐습니다.\n\n\n평가 프레임워크는 두 참가자 그룹으로부터 뚜렷한 피드백을 이끌어냈다. 모델과 상호작용한 의대생들은 어느 모델에 대한 결\n정적인 선호도를 나타내지 않았으나, 설문지 내 네 가지 비교 기준 모두에서 LearnLM에 대한 평균 선호도가 우세했다(그림 15a).\n\n학생들은 LearnLM이 상호작용하기 더 즐겁다는 점에서 가장 강한 긍정적 선호도를 나타냈다(평균 +9.9% 점수). 실제로 동점자를\n제외하고 학생들이 어느 정도 선호하는 모델을 직접 살펴보면, 모든 기준에서 LearnLM을 더 자주 선택했으며, 특히 즐거움 측면에\n서는 격차가 벌어져 압도적 다수가 LearnLM을 선호했다(그림 16).\n\n\n반면 의사 교육자들은 설문에서 평가된 다섯 가지 비교 기준 모두에서 LearnLM을 일관되게 선호했습니다. 그림 15b에서 보듯,\n그들은 LearnLM이 더 나은 교수법을 보여준다는 점(평균 +6.1%, 평가 척도 기준)과 \"매우 우수한 인간 튜터와 유사하게 행동한\n다\"는 점(+6.8%)에서 특히 긍정적으로 평가했습니다. 교육자들이 어느 한 쪽을 선호했는지 여부만 살펴보면(선호도 크기와 무관하게\n), 모든 평가 기준에서 LearnLM이 압도적 다수 선택으로 나타났습니다(그림 17).\n\n\n본 연구의 주요 목표는 전문 교육 분야에서 당사의 전문가 평가 프레임워크의 실행 가능성을 평가하는 것이었다. 평가 설\n계는 재현 가능성과 적응성을 모두 입증하였으며, 새로운 시나리오 데이터베이스를 생성하고 전문가들이 모델 간 교육학적 차이를\n식별할 수 있도록 성공적으로 지원하였다. 이러한 결과는 핵심 학문 분야 외 영역에서의 적용을 위한 당사의 평가 설계를 검증한다.\n\n특히 의학 교육 분야에서는 이 평가가 보다 포괄적인 연구를 위한 강력한 기반을 제공한다. 예를 들어 향후 연구는 추가 모델을 포함\n시키고, 더 넓은 의대생 및 교육자 커뮤니티를 참여시키며, 더 광범위한 문화적 맥락을 탐구하는 방향으로 확장될 수 있다. 물론 본 연\n구가 의학 분야에서 교육학적 평가의 실행 가능성을 확인했지만, 이러한 모델의 실제 적용에는 임상 전문가의 관점에서 의학\n적 정확성, 편향성 및 유해성에 대한 별도의 평가가 필요하다.\n\n\n32\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-32-0.png)\n\n이해하기 쉬\n움\n더 즐거운 상호작용\n학습 요구 사항\n을 더 잘 충족\n더 나은 전반적인\n경험\nLearnLM: 학습을 위한 Gemini 개선\n더 나은 학습 목표 지원\n학습자에게 더 잘 맞춤화됨\n학습자에 대한\n더 나은 지도\n추가로\n더 나은 교수법\n3 2 1 0 +1  +2  +3\nLearnLM 선호도\n3 2 1 0 +1  +2  +3\nLearnLM 선호도\n\n\n\n![](assets/md_images/질문설명하는-AI_LearnLM--Improving-Gemini-for-Learning-ko.pdf-32-1.png)\n\n(a) 의대생 평가 (b) 의사 교육자 평가\n그림 15 | 의대생(왼쪽)과 의사 교육자(오른쪽)가 LearnLM과 Gemini에 대해 전달한 선호도\n1.5 의학 교육 시나리오에서의 장점. 산점도는 7점 척도 선호도 평가의 기본 분포를 나타내며, 선호도 척도에 따라 색상 코딩(진한 보라색은 LearnLM에 대한 강\n한 선호도를 나타냄)하고 가독성을 위해 각 척도 값 주변에 무작위로 흔들림을 적용했습니다. 빨간색 점과 오차 막대는 각 측정값에 대한 추정 평균과 그\n95% 신뢰 구간을 나타냅니다.\n\n\n1. 00\n\n\n0. 75\n\n\n\n0. 50\n\n\n0. 25\n0.00\nGemini 1.5 Pro\nLearnLM\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||<br>|<br>|||\n||||||\n|~~전체적으~~|~~로~~ ~~더~~ ~~나~~<br>~~학습 요~~<br>|~~사항에 ~~<br> <br>~~더~~ ~~즐거운~~|~~호작용~~<br>~~이해~~<br>|~~기 쉬움~~<br>|\n\n\n은 경험\n학습 요구 사항\n그림 16 | 의대생들이 표현한 선호도를 단순화한 시각화 자료로, 각 쌍별 비교에서 어느 정도라도 각 모델을 선호한 평가 비율을 보여줍니다.\n\n\n1. 00\n\n\n0. 75\n\n\n\n0. 50\n\n\n0. 25\n0.00\nGemini 1.5 Pro\nLearnLM\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||<br>|<br>|<br>||\n|||||||\n||~~더 나은 교육학~~<br>|~~우 훌륭한~~<br> <br>~~학습자~~ ~~맞춤~~|~~형~~ ~~지침~~<br>~~학습자에~~|~~더~~ ~~잘~~ ~~맞~~<br>~~학습~~ ~~목표에~~|~~대한~~ ~~지원~~|\n\n\n춤화된\n강화\n그림 17 | 의사 교육자들이 표현한 선호도를 단순화한 시각화 자료로, 각 쌍별 비교에서 어느 정도라도 각 모델을 선호한 평가 비율을 보여줍니다.\n\n\n\n33\nLearnLM: 학습을 위한 Gemini 개선\n\n\n|의학 시나리오 1|Col2|\n|---|---|\n|**주제 영역**<br>|의학<br>|\n|**하위 주제**<br>|소아과<br>|\n|**상호작용 설정**<br>|독학<br>|\n|**학습 목표**<br>|X를 가르쳐 주세요<br>|\n|**기본 자료**<br>|신생아 황달 설명 동영상<br>|\n|**학습자 페르소나**<br>|~~•~~ 학습 방향에 대해 어느 정도 의견을 제시하지만, 일반적으로 튜터의 주도권을 따름<br>• 지도자의 질문에 신중하게 답변함<br>• 요청 시 \"과정 설명\"을 제시함<br>• 관련성은 있으나 피상적인 질문을 함 (낮은 \"지식 깊이\")<br>• 주제에 대한 지식 습득 및 유지 추구 (도구적)|\n|**초기 학습자 질의**<br>|동영상을 시청했고 퀴즈와 사례를 직접 풀어보고 싶습니다.<br>|\n|**대화 계획**<br>|당신은 자기 주도 학습을 통해 건강 관련 주제를 배우는 신입 건강 전문직 학생입니다.<br>새로운 주제: 신생아 황달. 관련 영상을 시청했지만, 방금 본 내용을 정확히 기억하거나 이해하지 못합니다. <br>이제 복잡한 개념을 단순화하고 중요한 내용을 놓치지 않도록 AI튜터와의 상호작용을 원합니다.<br>AI튜터와의 목표는 다음과 같은 학습 목표를 단순화하고 설명해 달라고 요청하는 것입니다:<br>• 학습에 관한 일부 방향을 제시하지만, 일반적으로 튜터의 주도권을 따릅니다<br>• 빌리루빈 대사 과정 설명<br>• 신생아 고빌리루빈혈증의 흔한 원인(즉, 어떻게 발생하는지)의 병리생리학을 설명하라<br>변화와 간내담즙순환을 이해하는 데 약간의 어려움을 겪어야 합니다. 또한 AI튜터에게 모유 수유 황달과 모<br>유 황달을 구분하는 퀴즈를 요청하되, 초기 답변에서 의도적으로 오답을 선택해야 합니다. 이후 생리적 황달과 <br>기타 고빌리루빈혈증 원인을 구분하는 임상 사례를 요청하여 성공적으로 해결해야 합니다.|\n|**시스템 지침**|당신은 복잡한 주제를 학생들이 숙달하도록 돕는 환자이며 지식이 풍부한 온라인 튜터입니다.<br>학습자의 목표를 파악하고 탐구하고자 하는 내용이 있는지 확인하는 것으로 시작하십시오.<br>그런 다음 학습자의 기존 지식을 활성화하십시오. 그들의 반응을 통해 기존 이해도를 측정하<br>고 후속 설명을 맞춤화하십시오. 명시된 목표가 없다면 해당 세션에 대한 학습 계획을 제안<br>하십시오.<br>정보를 명확하고 간결하게 제시하며, 비유, 퀴즈, 청크화 등 다양한 방법을 활용하십시오. 사<br>례 기반 학습을 통해 핵심 학습 목표에 기반한 현실적이고 실용적인 사례 시나리오를 소개<br>하고 학습자를 안내하십시오. 심층적인 이해와 적용을 장려하기 위해 개방형 질문을 정기적<br>으로 교육 과정에 삽입하십시오.<br>학습자의 응답에 대해 즉각적이고 구체적인 피드백을 제공하며, 정확한 이해는 칭찬하고 오해는 부<br>드럽게 바로잡으십시오. 학습을 공고히 하기 위해 필요 시 추가 설명이나 예시를 제시하십시오.<br>학습자의 이해 수준에 맞춰 설명을 조정하십시오.<br>반성을 유도하며 마무리하세요. 예를 들어, \"이 주제에 대해 많은 내용을 다루었습니다. 여러분<br>이 얻은 핵심 내용은 무엇인가요? 추가 설명이 필요하다고 느끼는 부분이 있나요?\"라고 질문하<br>세요. 학습자가 지속적인 학습을 위해 추가 자료를 찾아보도록 권장하세요.|\n\n\n\n34\nLearnLM: 학습을 위한 Gemini 개선\n\n\n|의료 시나리오 2|Col2|\n|---|---|\n|**주제 영역**<br>|의학<br>|\n|**하위 주제**<br>|생리학<br>|\n|**상호작용 설정**<br>|교실<br>|\n|**학습 목표**<br>|시험 준비<br>|\n|**기본 자료**<br>|혈소판 활성화 설명 동영상<br>|\n|**학습자 페르소나**<br>|~~•~~ 튜터의 초대를 거부하거나 무관심하게 수락하며 피드백을 제공하지 않음<br>• 질문에 관련성은 있으나 최소한의 답변만 제공함<br>• 대부분의 지시를 따르지만 자세히 설명하지 않음<br>• \"풀이 과정\"을 보여주지 않음<br>• 질문을 제기하지 않음<br>• 주제별 질문에 대한 답변이나 해결책을 받기 원함 (거래적)|\n|**초기 학습자 질의**<br>|좋아요, 영상을 봤고 사례를 연습하고 싶어요<br>|\n|**대화 계획**|당신은 혈액학 시험을 준비 중인 의대 1학년생입니다.<br>혈소판 활성화와 기능이 너무 어렵게 느껴집니다. 관련 동영상 강의를 시청했지만 기본 개념을 이해하는 데 <br>어려움을 겪고 있습니다.<br>AI튜터와의 목표는 이 동영상과 다음 학습 목표를 바탕으로 시험 준비를 도와달라고 요청하는 것입니다:<br>• 혈소판 활성화 과정의 순서를 설명하시오. 초기 접착부터 과립 방출까지의 일련의 사건을 다루되, \"글리코<br>프로테인 Ib\"나 \"알파 과립\" 같은 용어는 희미하게 기억나지만 명확하고 간결한 설명이 필요하다.<br>• 알파 과립과 고밀도 과립의 내용물과 기능을 구분하세요. 각 과립 유형이 무엇을 방출하는지, 그리고 왜 중요<br>한지 기억할 방법이 필요합니다.<br>• 혈소판 활성화가 지혈과 상처 치유에 어떻게 기여하는지 설명하세요. 이 개념들을 연결하여 큰 그림을 <br>이해할 필요가 있습니다.<br>튜터에게 적절히 응답하고 상호작용하되, 답변은 간결하게 하고 수동적·반응적인 학습 태도를 유지해야 합<br>니다.<br>예시 표현: \"이해가 안 돼요.\", \"네.\", \"모르겠어요, 선생님 생각은 어때요?\"|\n\n\n\n35\nLearnLM: 학습을 위한 Gemini 개선\n**시스템** **지침** 당신은 반응성과 평가에 전문성을 갖춘 친절하고 이해심 깊은 온라인 튜터입니다.\n\n\n이해도 확인과 기억 강화 활동을 자주 포함하십시오. 활용할 요소:\n\n-플래시카드: 핵심 용어와 그 정의가 담긴 가상 플래시카드를 소개합니다.\n\n-짧은 퀴즈: 개념 설명 후 간단한 객관식 또는 참/거짓 문제로 이해도를 확인합니다.\n\n-요약 유도: 학생이 핵심 개념을 자신의 말로 요약하도록 요청하세요.\n\n\n단순 암기에서 벗어나 학생이 지식을 평가하고 적용하도록 장려하세요.\n\n-비교 분석: 핵심 개념을 비교·대조하며 중요한 차이점을 강조하도록 요청하세요.\n\n-사례 기반 적용: 핵심 개념이나 학습 목표와 관련된 간단한 임상 시나리오를 제시하십시오.\n\n\n학생의 반응에 세심한 주의를 기울이십시오. 혼란스러워 보이면 설명을 단순화하거나 추\n가 예시를 제시하거나 이전 내용을 다시 설명하십시오. 흥미를 잃었거나 다음 주제로 넘\n어가길 원하면 학생의 요구를 존중하고 진행 속도와 내용을 적절히 조정하십시오.\n\n\n그들이 당신의 말 중 일부만 이해했다고 가정하세요. 핵심 정보를 다른 표현이나 예시를 사용해\n여러 번 다시 설명하세요. 중복된 느낌이 들더라도 반복을 통해 학습을 강화하세요.\n\n반복할수록 무언가가 머릿속에 남을 가능성이 높아집니다.\n\n\n\n36\n\n\n",
    "eng_pdf": "pdf/eng/질문설명하는 AI_LearnLM- Improving Gemini for Learning.pdf",
    "kor_pdf": "pdf/kor/질문설명하는 AI_LearnLM- Improving Gemini for Learning ko.pdf"
  },
  {
    "id": "paper_7",
    "title": "From Complex to Atomic: Enhancing Augmented Generation via Knowledge-Aware Dual Rewriting and Reasoning",
    "eng_md": "# **From Complex to Atomic: Enhancing Augmented Generation via** **Knowledge-Aware Dual Rewriting and Reasoning**\n\n**Jinyu Wang** [* 1] **Jingjing Fu** [* 1] **Rui Wang** [1] **Lei Song** [1] **Jiang Bian** [1]\n**Abstract**\nRecent advancements in Retrieval-Augmented\nGeneration (RAG) systems have significantly enhanced the capabilities of large language models\n(LLMs) by incorporating external knowledge retrieval. However, the sole reliance on retrieval\nis often inadequate for mining deep, specialized\nknowledge and performing the logical reasoning\nnecessary to tackle domain-specific complex questions. To address these challenges, we present\nan approach, which is designed to extract, comprehend, and utilize specialized knowledge in an\natomic manner while simultaneously constructing a coherent rationale. At the heart of our\napproach lie four pivotal components: a knowledge atomizer that extracts atomic tags from raw\ndata, a query proposer that generates subsequent\nquestions to facilitate the original inquiry, an\natomic retriever that locates knowledge based\non atomic knowledge alignments, and an atomic\nselector that determines which atomic tag and\nchunk pair to query, guided by the retrieved information. Through this approach, we implement a knowledge-aware task decomposition strategy that iteratively builds the rationale in alignment with the initial question and the acquired\nknowledge. We conduct comprehensive experiments to demonstrate the efficacy of our approach\nacross various benchmarks, particularly those requiring multihop reasoning steps. A substantial\nperformance improvement of up to +10.1 (20.4%)\nover the second-best method underscores the potential of the approach in complex, knowledgeintensive applications. The code is publicly avail[able at https://github.com/microsoft/PIKE-RAG.](https://github.com/microsoft/PIKE-RAG?tab=readme-ov-file)\n*Equal contribution 1Microsoft Research Asia, Beijing, China.\nCorrespondence to: Lei Song _<_ lesong@microsoft.com _>_ .\n\n\n_Proceedings_ _of_ _the_ _42_ _[nd]_ _International_ _Conference_ _on_ _Machine_\n_Learning_, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-0-0.png)\n\n_Figure 1._ Complex questions are typically decomposed into subquestions following either a chain-shaped or tree-shaped path, with\nchunk retrieval used to gather relevant information for resolution.\nUnlike previous approaches, our method seamlessly integrate the\nquestion decomposition with information retrieval through atomic\nknowledge alignment by dual rewriting upon questions and chunks,\nand dynamically determine follow-up sub-questions by atomic\npair retrieval and selection, enabling an adaptive and interactive\ndecomposition path that evolves based on the retrieved knowledge.\n\n\n**1. Introduction**\nLarge Language Models (LLMs) have revolutionized the\nfield of natural language processing by demonstrating the\ncapability to generate coherent and contextually relevant\ntext and the versatility to execute a diverse spectrum of linguistic tasks, ranging from text completion to translation\nand summarization (Achiam et al., 2023; Touvron et al.,\n2023). Despite their broad capabilities, LLMs exhibit pronounced limitations when tasked with specialized queries in\nprofessional domains (Ling et al., 2024; Wang et al., 2023a).\nThis primarily arises from the scarcity of domain-specific\ntraining material (e.g., _unpublished documents_ ) and an incomplete understanding of specialized knowledge and rationale within these domains (e.g., _industry-specific acronyms,_\n_company-specific_ _operational_ _rules_ ). As a result, LLMs\nmay produce responses that are not only potentially erroneous but also lack the detail and precision required for\nexpert-level engagement (Bender et al., 2021).\n\n\n\n1\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Figure 2._ Case demonstration of Self-Ask and KAR [3] . By proposing multiple atomic queries, KAR [3] effectively retrieves the relevant\nknowledge chunk, whereas the single deterministic follow-up question approach employed by Self-Ask fails to align with the knowledge\nbase’s schema, resulting in a retrieval failure.\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-1-0.png)\n\nTo mitigate these issues, RAG (Lewis et al., 2020) has\nemerged as a promising solution, augmenting LLMs with\nexternal knowledge retrieval to anchor generated content.\nBy supplementing or even replacing the knowledge encoded\nwithin LLMs, RAG frameworks aim to improve factuality and relevance. However, existing RAG systems often\nstruggle with domain-specific complex tasks. For example,\nanswering a query like “ _please provide the product name_\n_of_ _the_ _latest_ _biosimilar_ _for_ _HUMIRA_ _that_ _has_ _been_ _suc-_\n_cessfully approved._ ” requires more than retrieving scattered\nspecialized knowledge (e.g., _biosimilar for HUMIRA_ ) from\nmultiple sources. It demands logical reasoning on eligible\nproducts and their approval timelines to synthesize a precise\nand reliable response. Current RAG methods predominantly\nrely on plain text retrieval, which may fail to effectively\ncapture correlations within domain-specific expression, and\noften employ question decomposition without considering\navailable knowledge, leading to suboptimal sub-question\ngeneration, ineffective retrieval and reasoning failures.\n\n\nIn this work, we argue that advancing RAG requires\nknowledge-aware processing, especially on question decomposition and information retrieval, and iterative reasoning\nto effectively address complex, multi-step questions in professional domains.\n\n\n**Knowledge-aware Processing for Domain-Specific Com-**\n**prehension** Addressing complex, logic-driven tasks in spe\ncialized domains requires knowledge extraction and comprehension to deeply understand both the user’s information\nneeds and the underlying context of the retrieved data. For\nexample, specialized questions in fields like medicine, law,\nor finance often involve domain-specific terminology and\nlogic, which generic LLMs may fail to grasp fully. Traditional RAG systems that retrieve text passages based on\nkeyword matching (Ram et al., 2023; Jiang et al., 2023)\nor embedding similarity (Gao et al., 2023) may retrieve\ncontextually relevant information, that may lack semantic\nprecision, insufficient for answering intricate questions.\n\n\n**Iterative Reasoning for Complex Query Resolution** Complex reasoning tasks, where the answer depends on synthesizing information from multiple sources, demand the decomposition of the original question into a series of simpler,\ninterrelated sub-questions (Press et al., 2023). Nonetheless, this approach may face obstacles in domains where the\nknowledge is not readily accessible to LLMs. We argue that\nthe decomposition in such domains should be contextual,\nrather than a standalone operation, meaning that decomposed queries can be answered with the retrieved knowledge\nand context progressively and evolve into refining subsequent queries. This iterative approach allows the system\nto evolve its understanding of the user’s inquiry, ensuring\nthat follow-up questions are informed by the most recent\nretrieval results.\n\n\n\n2\n**From Complex to Atomic:** **Enhancing Augmented Generation**\nWe introduce a novel framework, KAR [3] -RAG, which employs a knowledge-aware dual rewriting and reasoning\nmechanism. Our approach features a dynamic interaction\nbetween question rewriting and knowledge retrieval, enabling the system to adaptively refine both the query and\nthe retrieved context at each iteration, as illustrated in Figure 1. The core components of our system include Knowledge atomizer, decomposing raw data into atomic tags for\nmore granular retrieval, Query proposer, generating followup questions based on the evolving context, Atomic retriever, identifying and retrieving relevant knowledge based\non atomic knowledge alignments, and Atomic selector, determining the most relevant follow-up questions based on\nthe retrieved information. More specially, the atomic tags\nare formulated as relevant inquiries that can be answered\nby the given chunk, thereby encompassing the multifaceted\nknowledge of the chunk and facilitating effective retrieval.\nAtomic query proposals are raised to inquiry the knowledge\nthat is helpful to answer the question better. By leveraging these components, our system can iteratively refine its\nunderstanding of both the question and the retrieved knowledge, enabling more accurate and context-aware reasoning\nover multiple hops. We demonstrate the advantages of task\ndecomposition and atomic retrieval on a real-world case, as\nshown in Figure 2. Our approach not only enables multifaceted task decomposition, but also alleviates the misalignment between the corpus formulation and the query through\natomic tagging of the corpus.\n\n\nOur key contributions are as follows: 1) We propose a\nknowledge-aware RAG framework that incorporates retrieved knowledge into question decomposition, enabling\niterative exploration of the reasoning path. 2) We introduce\nan atomic knowledge alignment approach by dual writing\nthat tightly couples query decomposition with retrieval, significantly improving retrieval efficiency. 3) We report on\ncomprehensive experimental and ablation studies that validate the superior performance of our approach across multiple benchmark datasets, achieving up to 20.4% increase\nover the second-best method.\n\n\n**2. Related work**\n**2.1. RAG**\nRAG has emerged as a promising solution that effectively\nincorporates external knowledge to improve the generation\nof LLMs. Naive RAG systems retrieve pertinent information from external data sources and integrate it into the\ncontext of the question prompt as supplementing knowledge for contextually relevant generation (Ram et al., 2023).\nAdvanced RAG approaches implement specific enhancements across the pre-retrieval, retrieval, and post-retrieval\nprocesses, including query optimization (Ma et al., 2023;\nZheng et al., 2023), multi-granularity chunking (Chen et al.,\n2023; Zhong et al., 2024), mixed retrieval (Yang, 2023) and\nre-ranking (Cohere, 2023). On one hand, efforts focus on\nquery rewriting, either explicitly (Zheng et al., 2024) or\nimplicitly (Gao et al., 2022), to enhance retrieval performance. On the other hand, several studies transform raw\ndata sources into structured data, ultimately converting them\ninto valuable knowledge for more effective retrieval and reasoning (Wang et al., 2023b; Zheng et al., 2024; Raina &\nGales, 2024; Liang et al., 2024). In our system, we introduce atomic rewriting for both queries and chunks, which\nachieves multi-granularity question decomposition and comprehensively extract inherent knowledge from chunks.\n\n\nTo tackle complex tasks such as summarization (Hayashi\net al., 2021) and multihop reasoning (Ho et al., 2020), recent research focuses on developing advanced coordination\nschemes that leverage existing RAG modules to collaboratively address these challenges. Iter-RetGen (Shao et al.,\n2023) and DSP (Khattab et al., 2023) employ retrieve-read\niteration to leverage generation response as the context for\nnext round retrieval. FLARE (Jiang et al., 2023) proposes a\nconfidence-based active retrieval mechanism. Our approach\nadopts an iteration-based pipeline that leverages contextaware reasoning process, enabling the adaptive formulation\nof follow-up questions for each iteration and reducing the\ndifficulty of retrieval and reasoning of complex tasks.\n\n\n**2.2. Multihop QA**\nMultihop Question Answering (MHQA) (Yang et al., 2018)\nrequire reasoning over multiple pieces of information, often\nscattered across different sources. This task presents unique\nchallenges as it necessitates not only retrieving relevant\ninformation but also effectively combining and reasoning\nover the retrieved pieces to arrive at a correct answer. The\ntraditional graph-based methods in MHQA solves the problem by building graphs and inferring on graph neural networks(GNN) to predict answers (Qiu & other authors, 2019;\nFang & other authors, 2020). With the advent of LLMs,\nrecent graph-based methods (Li & Du, 2023; Panda et al.,\n2024; Liang et al., 2024) have evolved to construct KGs for\nretrieval and generate response through LLMs. However,\nconstructing a high-quality domain-specific KG is costly,\nand the structured triple format imposes inherent constraints\non contextual representation, limiting its expressiveness.\nSelf-RAG (Zhang et al., 2024a)and beam-retrieval (Asai\net al., 2023) treating MHQA as a supervised problem, necessitating labeled data and additional training.\n\n\nAnother branch of methods decomposes multihop questions into sub-questions following either a chain-shaped\npath (Trivedi et al., 2023; Khattab et al., 2023; Feng et al.,\n2023; Xu et al., 2024) or tree-shaped path (Zhang et al.,\n2024b; Jiapeng et al., 2024; Cao et al., 2023), as depicted\nin Figure 1. The sub-questions guide sequential chunk re\n3\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Figure 3._ Overview of the KAR [3] -RAG workflow, illustrating knowledge atomizing by the atomizer, and knowledge-aware task decomposition using the query proposer, atomic retrieval and atomic selector. The query proposer generates atomic query proposals based on\nthe original question and reference context. These proposals are used to retrieve the relevant atomic tags, producing retrieved atomic\npairs. The atomic selector chooses the most relevant pair and the corresponding chunk, which is added to the reference context for task\ndecomposition in the subsequent iteration. Once the atomic selector determines that no further information is required and no atomic pair\nis selected, the original question and reference context are passed to the generator to produce the final answer.\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-3-0.png)\n\ntrieval, with the retrieved results subsequently facilitating\nthe reasoning process. In chain-shaped decomposition, a\nsingle sub-question is generated, and its answer availability\nis not guaranteed, potentially leading to answer failure. In\ncontrast, tree-shaped decomposition requires exploring multiple reasoning paths, necessitating sophisticated evidence\nverification and fusion for final response generation. Our\napproach explore the reasoning path by interactively select\nthe sub-question from a set of query proposals based on\nthe relevance of atomic retrieval. This allows for flexible\ndecomposition by leveraging updated context and selecting\nquery proposals with available knowledge.\n\n\n**3. Methodology**\n**3.1. Preliminary**\nIn a RAG system, the textual corpus is divided into a collection of document chunks, denoted as _D_ = _{d_ 1 _, d_ 2 _, . . ., dn}_,\nwhere _di_ represents the _i_ -th document chunk. The original\nquestion is denoted as _q_, and its corresponding ground truth\nanswer is represented by _a_ . The retrieval phase involves\nevaluating the similarity between the question _q_ and each\ndocument chunk _di_, after which the top- _k_ most relevant\nchunks are selected as retrieval results, forming the basis for\nsubsequent generation.\n\n\n_R_ : topk Sim( _q, di_ ) _→_ _D_ _[q]_ (1)\n_di∈D_\nHere, the retriever _R_ selects the top- _k_ most relevant chunks\n_D_ _[q]_ based on the similarity function _Sim_ ( _·_ ). Finally, the\noriginal question and retrieved chunks are fed into the\nlarge language model to generate the answer, denoted as\n_a_ ˆ = _LLM_ ( _q, D_ _[q]_ ). In the advanced RAG systems, query\nrewriting is employed to bridge the semantic gap between\nthe question and the chunks to be retrieved. The rewritten\nquery is represented as _q_ ˆ = _fre_ ( _q_ ). The workflow of the\nadvanced RAG system is further improved as follows,\n_a_ ˆ = _LLM_ ( _q, D_ _[q]_ [ˆ] ) _,_ where _D_ _[q]_ [ˆ] = _R_ (ˆ _q, D_ ) (2)\nThis enhancement allows the system to better align queries\nwith relevant document chunks, enhancing retrieval accuracy and answer generation. However, addressing complex\nmultihop questions remains challenging. These questions\noften require reasoning across multiple chunks and integrating information through several retrieval and generation\nsteps-a process that a single pass may not fully capture.\n\n\n**3.2. Framework**\nTo address complex multihop questions, we introduce\nan enhanced RAG system with **K** nowledge- **A** ware **dual**\n**R** ewriting and **R** easoning, termed as KAR [3] . This system\nemploys an iterative retrieval-reasoning-generation mechanism that facilitates gradual collection of relevant information and progressive reasoning over incremental context.\nAn overview of the proposed workflow is depicted in Figure 3. In our framework, raw data chunks are broken down\ninto atomic tags using a knowledge atomizer to construct an\natomic knowledge base for the subsequent retrieval. Questions are similarly atomized by a query proposer to generate\natomic query proposals, which are utilized to retrieve the\nrelevant atomic tags from the knowledge base. Both chunks\nand questions are rewritten to bridge the semantic gap and\nimprove the alignment of knowledge. An atomic retriever\nthen selects the top- _k_ atomic pairs for each atomic query\nproposal. Based on these retrieved atomic pairs, an atomic\nselector, as a reasoner, identifies the most useful atomic pair\n4\n**From Complex to Atomic:** **Enhancing Augmented Generation**\nfor problem-solving and adds the corresponding raw chunk\nto the context. This context is then aggregated with the\noriginal question for the task decomposition in next iteration. The iteration process may terminate earlier if it fails\nto retrieve suitable atomic tags, either due to the generation of low-quality query proposals or the lack of relevant\natomic tag candidates. At this point, the original question\nand context are passed to the generator to produce the final\nanswer.\n\n\n**3.3. Knowledge Atomizing**\nChunked text often contains multifaceted information, and\ntypically, only a subset is needed to address a specific task.\nTraditional information retrieval methods, which consolidate all information within a single chunk may not facilitate\nthe efficient retrieval of the precise information required.\nRecent research have explored the extraction of triple knowledge units from chunked text and constructing knowledge\ngraphs to facilitate efficient information retrieval (Edge et al.,\n2024; Panda et al., 2024). However, the construction of these\nknowledge graphs is costly, and the inherent knowledge may\nnot always be fully explored. To better present the knowledge embedded in documents, we propose atomizing the\noriginal documents for knowledge extraction, a process we\nrefer as _Knowledge Atomizing_ . This approach leverage the\ncontext understanding and content generation capabilities of\nLLMs to automatically tag atomic knowledge pieces within\neach document chunk.\n\n\nThe presentation of the atomic knowledge can be various. Instead of utilizing declarative sentences or subjectrelationship-object tuples, we propose using questions as\nknowledge indexes to further bridge the gap between stored\nknowledge and queries. In knowledge atomizing process,\nwe input the document chunk to an LLM as context, ask it\nto generate relevant questions that can be answered by the\ngiven chunk as many as possible. These generated atomic\ntags are stored together with the given chunks. The knowledge atomizer applies atomizing operation on each chunk.\n\n\n_fa_ ( _dk_ ) = _{qk_ 1 _, qk_ 2 _, · · ·_ _, qkm}_ (3)\nThe atomic tags are generated by atomizer for every chunk,\nforming an atomic knowledge base, denoted as _KB_ =\n_{fa_ ( _dk_ ) _, dk}_ . An example of knowledge atomizing is illustrated in Figure 4(a), where the atomic tags encapsulate\nvarious aspects of the knowledge contained within the chunk.\nSince each chunk is tagged with multiple atomic tags, an\natomic query can be used to locate relevant atomic tags,\nwhich then leads to the associated reference chunks.\n\n\n**3.4. Knowledge-Aware Task Decomposition**\nAddressing complex multihop questions often requires integrating multiple pieces of knowledge, which implicitly\n**Algorithm 1** Task Solving with Knowledge-Aware Decomposition\n1: Initialize context _C_ 0 _←_ _**ϕ**_\n2: **for** _t_ = 1 _,_ 2 _, . . ., N_ **do**\n3: Generate atomic query proposals _**q**_ **ˆ** _[t]_ _←_ _fp_ ( _q, Ct−_ 1)\n4: Retrieve top- _k_ atomic pairs for each atomic query\nproposal from knowledge base\n_P_ _**[q]**_ **[ˆ]** _[t]_ _←−−RKB_ _atom_ ( _**q**_ **ˆ** _[t]_ _, fa_ ( _D_ ))\n5: Select the most useful atomic tag or _None_ when\nadditional information is unnecessary\n_qksls_ _←LLM_ ( _q, Ct−_ 1 _, P_ _**[q]**_ **[ˆ]** _[t]_ )\n6: **if** _qksls_ is _None_ **then**\n7: _Ct_ _←Ct−_ 1\n8: **break**\n9: **else**\n10: Fetch the relevant chunk _c_ _[t]_ corresponding to _qksls_\n11: Update context _Ct_ _←Ct−_ 1 _∪_ _c_ _[t]_\n12: **end if**\n13: **end for**\n14: Generate answer ˆ _a ←LLM_ ( _q, Ct_ )\ndemands the ability to break down the original question into\nseveral sequential or parallel atomic tags for retrieval. We refer to this operation as _Task Decomposition_ . By combining\nthe extracted atomic knowledge with the original chunks,\nwe construct an atomic knowledge base. Each time a task is\ndecomposed, the atomic knowledge base provides insights\ninto the available knowledge, enabling knowledge-aware\ntask decomposition. We design the _Knowledge-Aware Task_\n_Decomposition_ workflow, and the complete algorithm for\nsolving task is detailed in Algorithm 1, and an example is\nillustrated in Figure 4(b).\n\n\nInitially, the reference context _C_ 0 is initialized as an empty\nset. In the first iteration, task decomposition relies solely on\nthe original question to generate atomic query proposals. As\niterations progress, the accumulated context at _t_ -th iteration\ndenoted as _Ct−_ 1, consists of chunks retrieved from previous\niterations. During the _t_ -th iteration, the query proposer generates atomic query proposals based on the original question\nand the accumulated context.\n\n\n_fp_ ( _q, Ct−_ 1) = _{q_ ˆ1 _[t]_ _[,]_ [ ˆ] _[q]_ 2 _[t]_ _[,][ · · ·]_ _[,]_ [ ˆ] _[q]_ _n_ _[t]_ _[}]_ (4)\nThe query proposer _fp_ ( _·_ ) can be implemented as either an\nLLM or a learnable component. we leverage an LLM to generate query proposals that are potentially beneficial for task\ncompletion, represented as _**q**_ **ˆ** _[t]_ = _{q_ ˆ _i_ _[t][}]_ [.] [During this process,]\nthe selected reference chunks _Ct−_ 1 are provided as con\n5\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Figure 4._ Illustrative examples of KAR [3] -RAG cases: (a) Example of knowledge atomizing, (b) RAG case with knowledge-aware task\ndecomposition. As iterations progress, the reference context is enriched by accumulating relevant chunks via atomic retrieval and selection.\nWith the expansion of context, the number of atomic query proposals generated decreases until no further proposals are produced.\nSubsequently, the iteration process terminates, and the combined question and context are harnessed to produce the final response.\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-5-0.png)\n\ntext to avoid generating proposals linked to already known\nknowledge. Consequently, the query proposals evolve with\neach iteration, adapting to the updated context and aiming to\nexplore additional knowledge beyond chunks in the context.\nFor each atomic query proposal, we retrieve its top- _k_ relevant atomic tag candidates along with their source chunks\nfrom the knowledge base. The atomic retrieval process is:\n\n\n_KB_\n_Ratom_ : topk Sim(ˆ _qi_ _[t][, q][kl]_ [)] _−−→_ _P_ _[q]_ [ˆ] _i_ _[t]_ (5)\n_qkl∈fa_ ( _D_ )\nwhere the atomic retriever, denoted as _Ratom_, produces a\nset of retrieved atomic pairs for each atomic query proposal,\nrepresented as _P_ _[q]_ [ˆ] _i_ _[t]_ = _{_ (ˆ _qi_ _[t][, q][k]_ _i_ _[l]_ _i_ _[, d][k]_ _i_ [)] _[}]_ [.] [All] [the] [retrieved]\natomic pairs from each atomic query proposal are aggregated to generate an overall set _P_ _**[q]**_ **[ˆ]** _[t]_ . We employ cosine\nsimilarity of the corresponding embeddings to retrieve the\ntop- _k_ atomic tags, provided their similarity to a proposed\natomic tag meets or exceeds a specified threshold _δ_ . With\nthe original question, the accumulated context, and the list of\nretrieved atomic pairs, the atomic selector employ an LLM\nto select the most useful atomic pair for problem-solving.\n\n\n_LLM_ ( _q, Ct−_ 1 _, P_ _**[q]**_ **[ˆ]** _[t]_ ) = (ˆ _qs_ _[t][, q][k]_ _s_ _[l]_ _s_ _[, d][k]_ _s_ [)] (6)\nThe atomic selector, denoted as _Satom_, further retrieve the\nrelevant raw chunk of the atomic pair selected as the new\ncontext added in the _t_ -th iteration, denoted as _ct_ . This\nchunk corresponds to _dks_ in equation 6. The chunk retrieval\n6\n**From Complex to Atomic:** **Enhancing Augmented Generation**\nprocess can be represented by the following formula,\n_ct_ = _Satom_ ( _Ratom_ ( _fp_ ( _q, Ct−_ 1) _, fa_ ( _D_ )))) (7)\nThis retrieved chunk is aggregated into the reference context for the next round of decomposition, expressed as\n_Ct_ = _ct_ _∪Ct−_ 1. Knowledge-aware decomposition can iterate up to _N_ times, where _N_ is a hyperparameter set to\ncontrol computational cost. The iteration process may conclude earlier if it fails to retrieve suitable atomic tags, either\ndue to the generation of low-quality query proposals or the\nabsence of relevant atomic tag candidates. Alternatively,\nthe process can be halted if the _LLM_ deems the accumulated knowledge adequate for task completion. This early\ntermination mechanism allows the process to conclude before completing all iterations, reducing computational costs\nwithout compromising accuracy. Finally, the accumulated\ncontext _Ct_ is utilized to generate answer _a_ ˆ for the given\nquestion _q_ in line 1.\n\n\nIt is worth mentioning that knowledge-aware decomposition\ncan be a learnable component. For each specialized knowledge base, we can utilize the data collected in each decomposition iteration—specifically ( _q, a,_ ˆ _a, {q_ ˆ _s_ _[t][, c][t][,]_ **[ ˆ]** _**[q]**_ _[t][, P]_ _**[q]**_ **[ˆ]** _[t][,][ C][t][}]_ [)][.]\nThis trained proposer can then directly suggest atomic\nqueries _q_ _[t]_ during inference, which means lines 1 to 1 in\nAlgorithm 1 can be replaced by a single call to this learned\nproposer, thereby reducing both inference time and computational cost. We leave the exploration of training an\nefficient query proposer as future work.\n\n\n**4. Evaluation and Metrics**\nSince KAR [3] is proposed to handle the challenges in specialized domains, we have conducted experiments on both a\nChinese legal benchmark named LawBench and the Open\nAustralian Legal QA benchmark. The experimental results\ndemonstrated that KAR [3] have obtained significantly improvement than baseline methods across all these benchmarks, and the accuracy on generation tasks can reach up\nto 90.12% and 98.59% in LawBench and Australian Legal\nQA respectively. The detailed introduction to these legal\nbenchmarks and the experimental results can be found in\nAppendix A.8.\n\n\nTo better compare the proposed approach with baseline\nmethods, we focus on the widely-recognized open-domain\nbenchmarks in this section. Section 4.1 and 4.2 outline the\nexperimental setup and the primary experimental results\nrespectively. Ablation studies are discussed in Section 4.3.\nAdditionally, cost analysis and case studies are included in\nAppendix A.5 and A.6 due to content constraints.\n\n\n\n**4.1. Experimental Setup**\n**Methods** To thoroughly evaluate the performance of our\nproposed knowledge-aware decomposition approach, we\nhave selected a variety of baseline methods that represent\ndifferent strategies for task-solving with LLMs. We include **Zero-Shot CoT** (Kojima et al., 2022) to assess the\ninherent reasoning capabilities and built-in knowledge of\nthe underlying LLM without any additional context. **Naive**\n**RAG** (Lewis et al., 2020), which introduces external knowledge through retrieval, serves as a benchmark for evaluating\nthe incremental benefits of augmented knowledge. The\n**Self-Ask** framework (Press et al., 2023) is employed to\ninvestigate the impact of an iterative question decomposition and answering strategy on task performance. The\n**IRCoT** (Trivedi et al., 2023), which iteratively generates\nthe rationale to process the multihop questions, along with\nthe **Iter-RetGen** (Shao et al., 2023), which iteratively uses\nthe recent response as a retrieval query to improve the response quality, and the **ProbTree** (Cao et al., 2023), which\nexplicitly decompose the complex QA into a search tree,\nare also conducted for performance comparison. Detailed\ndescriptions of methods are provided in Appendix A.4.\n\n\nIn our experiments, we employ GPT-4 (1106-Preview) and\nLlama-3.1-70B-Instruct across the methods outlined previously. For the experiments presented in Section 4.2, the\niteration number _N_ is set to 5 for Self-Ask with Retrieval,\nIRCoT, Iter-RetGen and KAR [3] . Additionally, the atomic\nretriever is initialized with _k_ = 4 and _δ_ = 0 _._ 5. A comprehensive list of hyper-parameters for the retrieval and LLM\ncan be found in Appendix A.3. For brevity, Llama-3.1-70BInstruct is abbreviated as Llama 3 in the following content.\n\n\n**Metrics** To ensure consistency with established benchmarks, we adopt **F1** as a conventional metric in our experimental evaluation. To more accurately assess the the\nalignment of responses with the intended answers—beyond\nmere lexical matching—we introduce a novel evaluation\nmetric employing _GPT-4_ . In this process, _GPT-4_ acts as an\nevaluator, assessing the correctness of a response in relation\nto the question and the correct answer labels. We refer to\nthis metric as **Accuracy (Acc)** . Upon manual inspection of\na sample set, the judgments rendered by _GPT-4_ demonstrate\ncomplete agreement with human evaluators, affirming the\nreliability of this metric. Furthermore, a full evaluation results with Exact Match (EM), Recall and Precision can be\nfound in Appendix A.4.\n\n\n**Datasets** To better compare with baseline methods, our\nevaluation focuses on three widely-recognized multihop\ndatasets: HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020), and MuSiQue (Trivedi et al.,\n2022). A brief introduction to these datasets can be found\nin Appendix A.1. For each dataset, we randomly sample\n7\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Table 1._ Performance comparison on multihop QA datasets with\nGPT-4. Best in bold, second-best underlined.\n\n|Method|HotpotQA<br>F1 Acc|2Wiki<br>F1 Acc|MuSiQue<br>F1 Acc|\n|---|---|---|---|\n|Zero-Shot CoT<br>Naive RAG<br>Self-Ask w/ R.<br>IRCoT<br>Iter-RetGen<br>ProbTree<br>KAR3 **(Ours)**|43.94<br>53.60<br>72.67<br>82.60<br>71.40<br>80.00<br>67.30<br>81.00<br>75.27<br>86.60<br>62.41<br>73.40<br>**76.48**<br>**88.00**|41.40<br>43.87<br>59.74<br>62.80<br>69.06<br>75.00<br>63.83<br>70.40<br>67.21<br>73.60<br>69.42<br>80.00<br>**75.00**<br>**82.20**|22.90<br>23.47<br>43.31<br>44.40<br>46.76<br>51.40<br>47.57<br>49.20<br>52.48<br>55.60<br>43.26<br>52.86<br>**57.86**<br>**62.60**|\n\n\n\n_Table 2._ Performance comparison on multihop QA datasets with\nLlama 3. Best in bold, second-best underlined.\n\n|Method|HotpotQA<br>F1 Acc|2Wiki<br>F1 Acc|MuSiQue<br>F1 Acc|\n|---|---|---|---|\n|Zero-Shot CoT<br>Naive RAG<br>Self-Ask w/ R.<br>IRCoT<br>Iter-RetGen<br>KAR3 **(Ours)**|40.10<br>54.80<br>70.78<br>84.20<br>70.25<br>83.00<br>74.59<br>88.00<br>72.23<br>85.20<br>**75.27**<br>**88.20**|38.54<br>43.20<br>56.58<br>62.20<br>66.25<br>74.00<br>69.49<br>77.60<br>59.21<br>65.00<br>**72.79**<br>**81.00**|15.69<br>19.80<br>32.53<br>36.40<br>38.19<br>44.20<br>43.12<br>49.60<br>37.16<br>40.40<br>**50.68**<br>**59.70**|\n\n\n\n500 QA data from the _dev_ set, disregarding the question\ntype and the number of hops to ensure randomness. We\ncompile the context paragraphs from all sampled QA data\ninto a single knowledge base for each benchmark, creating a more complex retrieval scenario. This design choice\naims to rigorously assess the task decomposition and relevant context retrieval capabilities of our model. For brevity,\n2WikiMultiHopQA is abbreviated as 2Wiki.\n\n\n**4.2. Main Results**\nAs demonstrated in Table 1 and Table 2 [1], our approach\nachieves superior performance across all datasets with both\nGPT-4 and Llama 3. Specifically, with GPT-4, we observe\nincreases of approximately +1 _._ 4(1 _._ 6%), +2 _._ 2(2 _._ 8%), and\n+7 _._ 0(12 _._ 6%) in accuracy over the second-best results for\nHotpotQA, 2Wiki, and MuSiQue. Similarly, with Llama\n3, we achieve increases of +0 _._ 2(0 _._ 2%), +3 _._ 4(4 _._ 4%), and\n+10 _._ 1(20 _._ 4%) for three datasets, respectively. These enhancements are statistically significant, underscoring the\nrobustness of KAR [3] in handling complex QA tasks.\n\n\nOur proposed approach, KAR [3], emphasizes knowledgeaware task decomposition and differs from the spontaneous\ndecomposition mechanism reliant on given demonstrations,\nas employed by Self-Ask. It performs decomposition with\nan awareness of available knowledge and effectively uses\natomic tags as an intermediate medium to bridge the se\n1Since we encountered problem obtaining the logprobs from\nthe Llama 3 endpoint, we leave the experiment of ProbTree with\nLlama 3 as future work.\n\n\n\nmantic gap. The “proposal first, then select” framework,\ndetailed in Algorithm 1, enables a dynamic decomposition\npath search, provides an opportunity to validate the intent\nof the question and rectify potential errors in the historical\nrationale generation process. A practical application of this\npoint can be seen in Case(a) of Appendix A.6. Consequently,\nthe experimental results demonstrate that KAR [3] consistently\noutperforms other methods with different models, validating\nnot only its effectiveness but only its robustness and adaptability for different models in complex reasoning scenarios.\n\n\n**4.3. Ablation Study**\n**The selection of** _N_ **.** We first conducted experiments with\nthe iteration upper bound _N_ set to 1 _,_ 2 _, . . ._ 10, and the results\nare presented in Figure 5. Detailed performance metrics are\navailable in Table 8 of Appendix A.4. Across all three\ndatasets, there is a consistent uptrend in both Supporting\nFact Recall and Answer Accuracy. This pattern underscores\nthe approach’s capability to incrementally enhance its outputs through additional iterations, particularly when more\ndetailed and contextually relevant information is required to\naddress problem.\n\n\nAdditionally, upon examining the relationship between the\nnumber of iterations and the observed growth in supporting\nfact recall, we note that for HotPotQA and 2Wiki datasets,\nthe recall curves exhibit a pronounced increase up to the\nfourth iteration. Conversely, the recall for the MuSiQue\ndataset continues to rise sharply beyond this point, even\nthough the maximum number of hops per question is capped\nat four, as mentioned in Appendix A.1. This discrepancy\nimplies that while KAR [3] is adept at retrieving relevant and\nuseful information within a limited number of iterations, it\nstill has certain limitation: KAR [3] relies on the reasoning\ncapability of the used LLM, and further iterations may be required to fully capture the necessary information, especially\nas the complexity of the questions increases.\n\n\nAlthough Algorithm 1 does incorporate early-stopping\nmechanisms, a higher _N_ invariably leads to increased computational demands. Therefore, we choose _N_ = 5 - a value\nslightly above the maximum number of hops - for the experiments in Section 4.2 to achieve a delicate balance between\ncomputational resources and the expected enhancement in\nperformance.\n\n\n_Figure 5._ Supporting fact recall (in blue) and answer accuracy (in\norange) over iterations.\n\n\n\n8\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-7-0.png)\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Table 3._ Ablation study of the components in KAR [3] .\n\n\n|Variable Component|Modification|HotpotQA<br>F1 Acc|2Wiki<br>F1 Acc|MuSiQue<br>F1 Acc|\n|---|---|---|---|---|\n|Knowledge Atomizer<br>Query Proposer<br>Atomic Retriever<br>Atomic Selector|Atomizing to questions_ →_Atomizing to plain texts<br>Proposing multiple queries_ →_Proposing a single query<br>Retrieving (atomic tag, chunk) pairs_ →_Retrieving chunks<br>Selecting chunks by atomic tags_ →_Selecting chunks directly|73.05<br>84.50<br>75.06<br>85.60<br>76.31<br>86.60<br>72.80<br>83.20|64.18<br>69.80<br>70.19<br>76.40<br>67.14<br>72.40<br>61.65<br>65.80|50.72<br>55.20<br>49.67<br>52.20<br>49.05<br>53.00<br>49.31<br>53.40|\n|KAR3 **(Ours)**|KAR3 **(Ours)**|76.48<br>88.00|75.00<br>82.20|57.86<br>62.60|\n\n\n\n**The contribution of the approach components.** KAR [3] is\ncomprised of four key components: a knowledge atomizer,\na query proposer, an atomic retriever, and an atomic selector.\nWe conduct ablation studies to ascertain the individual and\ncollective contributions of these components.\n\n\nby introducing several method variants with modification\nto these components one by one: (1) For the knowledge atomizer, we change the atomic tag presentation from atomic\nquestions to plain text sentences to explore the influence\nof atomic knowledge representation; (2) For the query proposer, we limit it to generate only one query to evaluate\nthe advantage of the originally designed multiple proposals\nmechanism; (3) For the atomic retriever, we modified the\ncomponents to let it retrieve chunks rather than (atomic tag,\nchunk) pairs; (4) For the atomic selector, instead of filtering\nchunks by atomic tags, we implemented a variant to select\nchunks directly. Since there is no atomic tag existing in\nthis setting, the context selection is later determined by the\nchunk directly.\n\n\nAs evidenced by the results in Table 3, the individual contributions of the components were evaluated. We observed that\nreplacing the knowledge atomizer, query proposer, atomic\nretriever and atomic selector with their substitutes will lead\nto accuracy reductions up to 15.1%, 16.6%, 15.3% and\n16.2%, respectively, over three datasets. These ablation\nstudies imply that each designed component is crucial for\nachieving optimal retrieval performance and coherent reasoning traces.\n\n\n**Limitation Discussion.** Beyond the need for additional\niterations to extract crucial information for complex questions, our experiments with GPT-3.5 - detailed in Table 9 in\nAppendix A.4 - indicate a limitation in relying on LLMs’\nreasoning capabilities. With GPT-3.5, the performance of\nKAR [3] does not significantly surpass that of methods like\nIRCoT and Self-Ask w/ Retrieval and occasionally falls\nshort compared to Self-Ask w/ Retrieval. This highlights\nthat KAR [3] ’s success hinges on its advanced reasoning skills\nand its ability to robustly follow complex instructions.\n\n\nWhile the experimental results using the open-source model\nLlama 3 demonstrate a notable performance improvement\nover the baseline methods, our approach requires higher\ntoken consumption compared to some of the methods evaluated, as detailed in Table 10 in Appendix A.5. Specifically,\non MuSiQue, it uses fewer tokens than ProbTree and IRCoT, but more than Iter-RetGen and Self-Ask with retrieval.\nThis increased token usage could lead to higher costs when\nimplemented with proprietary models like GPT-4.\n\n\n**5. Conclusion**\nWe present an advanced RAG system, enhanced with\nknowledge-aware dual rewriting and reasoning capabilities,\ndesigned to improve knowledge extraction and rationale\nformulation within specialized datasets. The comprehensive results of extensive experiments underscore the efficacy\nof our approach, particularly in scenarios involving benchmarks with multihop questions. For future work, we aim\nto refine the system’s proficiency through the integration of\nin-context learning (Wei et al., 2022), by adaptively selecting demonstrations for the query proposer. This will further\nenhance its ability to perform knowledge-aware question\nrewriting. Additionally, we are interested in developing a\nknowledge-aware atomizer capable of incorporating feedback from sample questions, thereby improving its understanding of the most beneficial types of atomic knowledge.\n\n\n**Impact Statement**\nOur approach utilizes existing large language models to\navoid additional training and minimize the introduction of\nnew biases, generating responses from pre-processed knowledge base to ensure reliability. The process records each\nstep of question decomposition, creating a transparent and\ninterpretable reasoning chain, and can be privately deployed\nto enhance data security in sensitive environments. This\napproach advances the use of Retrieval-Augmented Generation (RAG) technology in fields like legal research, medical diagnostics, and technical support, improving decisionmaking quality and efficiency. The enhanced clarity, precision, and logical coherence of information could lead to\nbetter healthcare outcomes, more accurate legal judgments,\nand improved technical assistance, contributing significantly\nto societal well-being and progress.\n\n\n\n9\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n**References**\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. _arXiv preprint_\n_arXiv:2303.08774_, 2023.\n\n\nAsai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Selfrag: Learning to retrieve, generate, and critique through\nself-reflection, 2023. URL [https://arxiv.org/](https://arxiv.org/abs/2310.11511)\n[abs/2310.11511.](https://arxiv.org/abs/2310.11511)\nBender, E. M., Gebru, T., McMillan-Major, A., and Mitchell,\nM. On the dangers of stochastic parrots: Can language\nmodels be too big? In _Proceedings of the 2021 ACM Con-_\n_ference on Fairness, Accountability, and Transparency_,\npp. 610–623. ACM, 2021.\n\n\nButler, U. Open australian legal qa, 2023. URL\n[https://huggingface.co/datasets/](https://huggingface.co/datasets/umarbutler/open-australian-legal-qa)\n[umarbutler/open-australian-legal-qa.](https://huggingface.co/datasets/umarbutler/open-australian-legal-qa)\nCao, S., Zhang, J., Shi, J., Lv, X., Yao, Z., Tian, Q., Li,\nJ., and Hou, L. Probabilistic tree-of-thought reasoning\nfor answering knowledge-intensive complex questions.\n_arXiv preprint arXiv:2311.13982_, 2023.\n\n\nChen, T., Wang, H., Chen, S., Yu, W., Ma, K., Zhao,\nX., Zhang, H., and Yu, D. Dense x retrieval: What\nretrieval granularity should we use? _arXiv_ _preprint_\n_arXiv:2312.06648_, 2023. URL [https://arxiv.](https://arxiv.org/pdf/2312.06648.pdf)\n[org/pdf/2312.06648.pdf.](https://arxiv.org/pdf/2312.06648.pdf)\nCohere. Say goodbye to irrelevant search results: Cohere rerank is here. [https://txt.cohere.com/](https://txt.cohere.com/rerank/)\n[rerank/, 2023.](https://txt.cohere.com/rerank/) Accessed: 2023-08-28.\n\n\nEdge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody,\nA., Truitt, S., and Larson, J. From local to global: A graph\nrag approach to query-focused summarization, 2024.\n[URL https://arxiv.org/abs/2404.16130.](https://arxiv.org/abs/2404.16130)\nFang, Y. and other authors. Hierarchical graph network\nfor multi-hop question answering. In _Proceedings of the_\n_Annual_ _Meeting_ _of_ _the_ _Association_ _for_ _Computational_\n_Linguistics_ _(ACL)_ . Association for Computational Linguistics, 2020.\n\n\nFei, Z., Shen, X., Zhu, D., Zhou, F., Han, Z., Zhang, S.,\nChen, K., Shen, Z., and Ge, J. Lawbench: Benchmarking\nlegal knowledge of large language models. _arXiv preprint_\n_arXiv:2309.16289_, 2023.\n\n\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B.\nRetrieval-generation synergy augmented large language\nmodels, 2023. URL [https://arxiv.org/abs/](https://arxiv.org/abs/2310.05149)\n[2310.05149.](https://arxiv.org/abs/2310.05149)\n10\nGao, L., Ma, X., Lin, J., and Callan, J. Precise zero-shot\ndense retrieval without relevance labels, 2022. URL\n[https://arxiv.org/abs/2212.10496.](https://arxiv.org/abs/2212.10496)\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y.,\nSun, J., and Wang, H. Retrieval-augmented generation\nfor large language models: A survey. _arXiv_ _preprint_\n_arXiv:2312.10997_, 2023.\n\n\nHayashi, H., Budania, P., Wang, P., Ackerson, C., Neervannan, R., and Neubig, G. WikiAsp: A Dataset for\nMulti-domain Aspect-based Summarization. _Transac-_\n_tions of the Association for Computational Linguistics_, 9:\n211–225, 2021.\n\n\nHo, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa,\nA. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. _arXiv_ _preprint_\n_arXiv:2011.01060_, 2020.\n\n\nJiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu,\nJ., Yang, Y., Callan, J., and Neubig, G. Active retrieval\naugmented generation, 2023. [URL https://arxiv.](https://arxiv.org/abs/2305.06983)\n[org/abs/2305.06983.](https://arxiv.org/abs/2305.06983)\nJiapeng, L., Runze, L., Yabo, L., Tong, Z., Mingling, L.,\nand Xiang, C. Tree of reviews: A tree-based dynamic\niterative retrieval framework for multi-hop question answering, 2024. URL [https://arxiv.org/abs/](https://arxiv.org/abs/2404.14464)\n[2404.14464.](https://arxiv.org/abs/2404.14464)\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P.,\nPotts, C., and Zaharia, M. Demonstrate-search-predict:\nComposing retrieval and language models for knowledgeintensive nlp, 2023. URL [https://arxiv.org/](https://arxiv.org/abs/2212.14024)\n[abs/2212.14024.](https://arxiv.org/abs/2212.14024)\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,\nY. Large language models are zero-shot reasoners. _Ad-_\n_vances_ _in_ _neural_ _information_ _processing_ _systems_, 35:\n22199–22213, 2022.\n\n\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,\nGoyal, N., Kuttler, H., Lewis, M., Yih, W., Rocktaschel,\nT., et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. _Advances in Neural Information Pro-_\n_cessing Systems_, 33:9459–9474, 2020.\n\n\nLi, R. and Du, X. Leveraging structured information for\nexplainable multi-hop question answering and reasoning, 2023. [URL https://arxiv.org/abs/2311.](https://arxiv.org/abs/2311.03734)\n[03734.](https://arxiv.org/abs/2311.03734)\nLiang, L., Sun, M., Gui, Z., Zhu, Z., Jiang, Z., Zhong, L.,\nQu, Y., Zhao, P., Bo, Z., Yang, J., Xiong, H., Yuan, L.,\nXu, J., Wang, Z., Zhang, Z., Zhang, W., Chen, H., Chen,\nW., and Zhou, J. Kag: Boosting llms in professional domains via knowledge augmented generation, 2024. URL\n[https://arxiv.org/abs/2409.13731.](https://arxiv.org/abs/2409.13731)\n**From Complex to Atomic:** **Enhancing Augmented Generation**\nLing, C., Zhao, X., Lu, J., Deng, C., Zheng, C., Wang,\nJ., Chowdhury, T., Li, Y., Cui, H., Zhang, X., Zhao, T.,\nPanalkar, A., Mehta, D., Pasquali, S., Cheng, W., Wang,\nH., Liu, Y., Chen, Z., Chen, H., White, C., Gu, Q., Pei,\nJ., Yang, C., and Zhao, L. Domain specialization as\nthe key to make large language models disruptive: A\ncomprehensive survey, 2024. [URL https://arxiv.](https://arxiv.org/abs/2305.18703)\n[org/abs/2305.18703.](https://arxiv.org/abs/2305.18703)\nMa, X., Gong, Y., He, P., Zhao, H., and Duan, N. Query\nrewriting for retrieval-augmented large language models.\n_arXiv preprint arXiv:2305.14283_, 2023.\n\n\nPanda, P., Agarwal, A., Devaguptapu, C., Kaul, M., and P,\nP. A. Holmes: Hyper-relational knowledge graphs for\nmulti-hop question answering using llms, 2024. URL\n[https://arxiv.org/abs/2406.06027.](https://arxiv.org/abs/2406.06027)\nPress, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A.,\nand Lewis, M. Measuring and narrowing the compositionality gap in language models, 2023. [URL https:](https://arxiv.org/abs/2210.03350)\n[//arxiv.org/abs/2210.03350.](https://arxiv.org/abs/2210.03350)\nQiu, M. and other authors. Dynamically fusing recurrent\nneural networks for multi-hop question answering. In\n_Proceedings of the Conference on Empirical Methods in_\n_Natural Language Processing (EMNLP)_ . Association for\nComputational Linguistics, 2019.\n\n\nRaina, V. and Gales, M. Question-based retrieval using\natomic units for enterprise rag, 2024. [URL https://](https://arxiv.org/abs/2405.12363)\n[arxiv.org/abs/2405.12363.](https://arxiv.org/abs/2405.12363)\nRam, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua,\nA., Leyton-Brown, K., and Shoham, Y. In-context\nretrieval-augmented language models. _Transactions of_\n_the Association for Computational Linguistics_, 11:1316–\n1331, 2023.\n\n\nShao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., and\nChen, W. Enhancing retrieval-augmented large language\nmodels with iterative retrieval-generation synergy, 2023.\n[URL https://arxiv.org/abs/2305.15294.](https://arxiv.org/abs/2305.15294)\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E.,`\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language\nmodels. _arXiv preprint arXiv:2302.13971_, 2023.\n\n\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal,\nA. Musique: Multihop questions via single-hop question composition. _Transactions_ _of_ _the_ _Association_ _for_\n_Computational Linguistics_, 10:539–554, 2022.\n\n\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal,\nA. Interleaving retrieval with chain-of-thought reasoning\nfor knowledge-intensive multi-step questions, 2023. URL\n[https://arxiv.org/abs/2212.10509.](https://arxiv.org/abs/2212.10509)\n11\nWang, C., Liu, X., Yue, Y., Tang, X., Zhang, T., Jiayang,\nC., Yao, Y., Gao, W., Hu, X., Qi, Z., Wang, Y., Yang, L.,\nWang, J., Xie, X., Zhang, Z., and Zhang, Y. Survey on\nfactuality in large language models: Knowledge, retrieval\nand domain-specificity, 2023a.\n\n\nWang, Y., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and\nDerr, T. Knowledge graph prompting for multi-document\nquestion answering, 2023b. URL [https://arxiv.](https://arxiv.org/abs/2308.11730)\n[org/abs/2308.11730.](https://arxiv.org/abs/2308.11730)\nWei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b.,\nXia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-of-thought\nprompting elicits reasoning in large language models. In\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D.,\nCho, K., and Oh, A. (eds.), _Advances in Neural Informa-_\n_tion Processing Systems_, volume 35, pp. 24824–24837.\nCurran Associates, Inc., 2022.\n\n\nXu, S., Pang, L., Shen, H., Cheng, X., and Chua, T.-S.\nSearch-in-the-chain: Interactively enhancing large language models with search for knowledge-intensive tasks.\nIn _Proceedings of the ACM on Web Conference 2024_, pp.\n1362–1373, 2024.\n\n\nYang, S. Advanced rag 01: Small-to-big retrieval. [https://towardsdatascience.](https://towardsdatascience.com/advanced-rag-01-small-to-big )\n[com/advanced-rag-01-small-to-big](https://towardsdatascience.com/advanced-rag-01-small-to-big )\n-retrieval-172181b396d4, 2023. Accessed:\n2023-08-28.\n\n\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W.,\nSalakhutdinov, R., and Manning, C. D. Hotpotqa: A\ndataset for diverse, explainable multi-hop question answering. _arXiv preprint arXiv:1809.09600_, 2018.\n\n\nZhang, J., Zhang, H., Zhang, D., Liu, Y., and Huang, S.\nEnd-to-end beam retrieval for multi-hop question answering, 2024a. URL [https://arxiv.org/abs/](https://arxiv.org/abs/2308.08973)\n[2308.08973.](https://arxiv.org/abs/2308.08973)\nZhang, K., Zeng, J., Meng, F., Wang, Y., Sun, S., Bai,\nL., Shen, H., and Zhou, J. Tree-of-reasoning question\ndecomposition for complex question answering with large\nlanguage models. _Proceedings of the AAAI Conference_\n_on Artificial Intelligence_, 38(17):19560–19568, 2024b.\n\n\nZheng, H. S., Mishra, S., Chen, X., Cheng, H.-T., Chi, E. H.,\nLe, Q. V., and Zhou, D. Take a step back: Evoking\nreasoning via abstraction in large language models. _arXiv_\n_preprint arXiv:2310.06117_, 2023.\n\n\nZheng, H. S., Mishra, S., Chen, X., Cheng, H.-T., Chi, E. H.,\nLe, Q. V., and Zhou, D. Take a step back: Evoking\nreasoning via abstraction in large language models, 2024.\n[URL https://arxiv.org/abs/2310.06117.](https://arxiv.org/abs/2310.06117)\n**From Complex to Atomic:** **Enhancing Augmented Generation**\nZhong, Z., Liu, H., Cui, X., Zhang, X., and Qin, Z. Mixof-granularity: Optimize the chunking granularity for\nretrieval-augmented generation, 2024. [URL https://](https://arxiv.org/abs/2406.00456)\n[arxiv.org/abs/2406.00456.](https://arxiv.org/abs/2406.00456)\n12\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n**A. Appendix**\nAppendix A.1 provides detailed introduction to the three open-domain benchmarks, Appendix A.3 enumerates the hyperparameters utilzied in our experiments, and Appendix A.4 presents the comprehensive experimental results.\n\n\nFor a more thorough understanding of our methodology, a cost analysis is available in Appendix A.5. Additionally, an\nintroduction and discussion of an alternative atomic tag presentation is also shown in this subsection.\n\n\nAppendix A.6 explores three real case studies. The prompts employed across the four components in our approach are\noutlined in Appendix A.7, accompanied by a discussion on the influence of decomposition demonstration. Finally, the\nevaluation conducted on two legal benchmarks are detailed in Appendix A.8.\n\n\n**A.1. Introduction to Open-Domain Benchmarks**\nWe provide a brief overview of the multihop QA datasets used in our experiments, noting that our method does not leverage\nthe question type information nor the number of hops information during the solving process, as our approach is designed\nto be agnostic to such classifications. Table 4 outlines the distribution of question types within our sampled sets, offering\ninsight into the variety of reasoning challenges presented in our evaluation, though this does not directly impact our method.\n\n\n**HotpotQA** The HotpotQA dataset is a well-known multihop QA benchmark primarily consisting of 2-hop questions, each\nassociated with 10 Wikipedia paragraphs. Among these, some paragraphs contain supporting facts essential to answering\nthe question, while the rest serve as distractors. The dataset also includes a _question type_ field, which delineates the logical\nreasoning required— _comparison_ questions involve contrasting two entities, and _bridge_ questions require inferring the bridge\nentity, or inferring the property of an entity through an intermediary entity, or locating the answer entity (Yang et al., 2018).\nAlthough our method operates independently of these types, their description here is to exemplify the nature of questions\nwithin the dataset and to contextualize the expected performance variance across different benchmarks.\n\n\n**2WikiMultiHopQA** Inspired by HotpotQA, 2WikiMultiHopQA expands the diversity of question types. It retains the\n_comparison_ type from HotpotQA and introduces _inference_ and _compositional_ questions that evolve from the _bridge_ type\nby focusing on entity attribute deduction and entity location, respectively. Additionally, the _bridge comparison_ type is a\nnovel category that requires a synthesis of _bridge_ and _comparison_ reasoning. This dataset typically presents 2-hop to 4-hop\nquestions, each accompanied by 10 Wikipedia paragraphs containing supporting facts and distractors. While these types\ninform the dataset’s structure, they are not utilized by our method, which treats all questions uniformly regardless of their\ncategorization. For the sake of brevity, 2WikiMultiHopQA is abbreviated to 2Wiki in this paper.\n\n\n**MuSiQue** Addressing the issue that many multihop questions can be solved via shortcuts—arriving at correct answers\nwithout proper reasoning—MuSiQue implements stringent filters and additional mechanisms specifically designed to\nencourage connected reasoning, as reported by Trivedi et al. (Trivedi et al., 2022). Unlike the other datasets, MuSiQue does\nnot categorize questions by type, but it does provide explicit information on the number of hops required for each question,\nranging from 2 to 4 hops. Each question is associated with 20 context paragraphs, which introduce a mix of relevant and\nirrelevant information, further complicating the task of discerning the correct reasoning path. This explicit hop information,\nwhile not used by our method, underscores the complexity of the dataset and the robustness required by models to handle\nsuch challenges effectively.\n\n\n_Table 4._ Distribution of question types across three distinct multihop QA datasets.\n\n\n\n|Type|Count Ratio|\n|---|---|\n|comparison<br>bridge|107<br>21.4%<br>393<br>78.6%|\n\n\n(a) HotPotQA\n\n\n\n|Type|Count Ratio|\n|---|---|\n|comparison<br>inference<br>compositional<br>bridge ~~c~~omparison|132<br>26.4%<br>64<br>12.8%<br>196<br>39.2%<br>108<br>21.6%|\n\n\n(b) 2WikiMultiHopQA\n13\n\n\n\n|#Hops|Count Ratio|\n|---|---|\n|2<br>3<br>4|263<br>52.6%<br>169<br>33.8%<br>68<br>13.6%|\n\n\n(c) MuSiQue\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n**A.2. Introduction to Evaluation Methods**\nIn addition to the methods outlined in Table 1, we also conduct experiments with an iteratively search and decomposition\nmethod named SearChain (Xu et al., 2024) and a Knowledge Graph-based method named GraphRAG (Edge et al., 2024).\nThe GraphRAG was inferred in both local and global modes. The methods evaluated in this study are listed as follows:\n\n\n  - **Zero-Shot CoT** : Questions are addressed using solely the Chain-Of-Thought (CoT) technique, which prompts the\nLLMs to articulate its reasoning process step-by-step without the aid of example demonstrations or supplemental\ncontext. This method assesses the LLMs’ intrinsic knowledge and reasoning capabilities in a zero-shot setting.\n\n\n  - **Naive RAG** : This approach employs dense retrieval from a flat knowledge base to procure relevant information for\neach question. The knowledge base consists of pre-embedded chunks are matched to the original question based on\nsemantic similarity. The retrieval process is direct, without any intermediate task decomposition.\n\n\n  - **Self-Ask w/ Retrieval** : This method employs a task decomposition strategy wherein the LLMs is prompted to iteratively\ngenerate and answer follow-up questions, thereby breaking down complex problems into more manageable sub-tasks.\nGeneral demonstrations illustrating the logic and methodology of task decomposition are provided for all benchmarks\nto guide the LLMs’ reasoning process. Different to the original setting (Press et al., 2023), where the framework\nrelies solely on LLM’s own knowledge to answer each follow-up question, in this setting, we introduces an additional\nretrieval component. Relevant chunks are retrieved with the follow-up question as the query from a flat knowledge\nbase to provide a reference context. What’s more, we also limit the decomposition process to raise up to _N_ follow-up\nquestions to align with other methods.\n\n\n  - **IRCoT** : This approach iteratively prompts LLMs to generate one more sentence of rationale with retrieved passages,\nand retrieves new passages with the newly generated reason. The original setting limit the process with a maximum\ntoken number (Trivedi et al., 2023). In our experiments, we limit the total number of iterations to the constant _N_ we\nused for our methods.\n\n\n  - **Iter-RetGen** : This method iteratively answers questions with retrieved passages, and uses the newly generated rationale\nand answer for the next-round retrieval. In this setting, we also limit the total number of iterations to the same _N_ .\n\n\n  - **SearChain** : This approach focuses on the interaction between LLM and Information Retrieval (IR). SearChain starts\nfrom a LLM-generated reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented\nquery-answer pair. It then iteratively verifies the answer of each node of CoQ by IR and re-generate the CoQ for\nnode that is not consistent with the retrieved information. The re-generation mechanism let SearChain forms a\nnovel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Since\nthe official code loads pre-trained models from local without uploading those models online, we find models with\nmost-similar name from HuggingFace to adapt it. Besides, the experimental results shown in Table 6 are conducted\nwith _BAAI/bge-m3_ instead of the ColBERT retriever due to environmental issues.\n\n\n  - **ProbTree** : This approach is an explict tree search method. ProbTree starts from a LLM-translated query tree for the\ngiven question, in which each non-root node denotes a sub-question of its parent node. Then, probabilistic reasoning is\nconducted over the tree, by solving questions from leaf to root considering the confidence of both question decomposing\nand answering.\n\n\n  - **GraphRAG Local** : The knowledge base is pre-processed to construct a knowledge graph in accordance with the public\nguidance. The evaluation is inferred in local mode.\n\n\n  - **GraphRAG Global** : The knowledge base is pre-processed to construct a knowledge graph in accordance with the\npublic guidance. The evaluation is inferred in global mode.\n\n\n  - **KAR** **[3]** **(Ours)** : The proposed knowledge-aware decomposition method iteratively decomposes complex questions into\nsub-questions and retrieves relevant knowledge up to a maximum of _N_ iterations. This process limits the context for\nthe final answer to the five most useful knowledge chunks.\n\n\nTo better illustrate the distinctions among the evaluation methods discussed, we have systematically detailed their characteristics in Table 5. This table classifies each method according to its approach to question decomposition, chunk retrieval, and\n14\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Table 5._ Method Comparison.\n\n|Method|Decomposition<br>demonstration path context|Retrieval|Generation Context<br>sub-answer final answer|\n|---|---|---|---|\n|Zero-Shot CoT<br>Naive RAG<br>Self-Ask w/ R.<br>IRCoT<br>Iter-RetGen<br>SearChain<br>ProbTree<br>KAR3 **(Ours)**|N/A<br>N/A<br>few-shot<br>chain<br>qa pairs<br>few-shot<br>(implicit)<br>rationale, chunks<br>zero-shot<br>(implicit)<br>chunks<br>few-shot<br>chain<br>_∅_; qa pairs, chunks<br>few-shot<br>tree<br>_∅_<br>zero-shot<br>dynamic<br>selected chunks|N/A<br>question_ →_chunk<br>sub-question_ →_chunk<br>rationale sentence_ →_chunk<br>whole rationale_ →_chunk<br>sub-question_ →_chunk<br>sub-question_ →_chunk<br>sub-question_ →_atomic question_ →_chunk|N/A<br>N/A<br>N/A<br>chunks<br>chunks<br>qa pairs<br>N/A<br>rationale, chunks<br>N/A<br>chunks<br>_∅_; chunks<br>qa pairs<br>chunks<br>qa pairs<br>N/A<br>selected chunks|\n\n\n\nthe context used in answer generation. Specifically, it delineates whether each method operates under zero-shot or few-shot\nconditions, the nature of its decomposition process (e.g., explicit or implicit decomposition; chain-shaped, tree-shaped, or\ndynamically generated paths), and the context utilized during decomposition. The retrieval column clarifies the mechanisms\neach method employs to gather information, while the columns dedicated to the generation context—both for sub-answer\nand final answer generation—highlight the specific contexts each method leverages when generating answers.\n\n\nAs Table 5 demonstrates, the decomposition module in KAR [3] employs a zero-shot, knowledge-aware approach, maintaining\naccumulated selected chunks in context for iterative decomposition. Additionally, we discuss the potential benefits of\nincorporating demonstration in Appendix A.7, suggesting that this feature could further enhance performance. This\npossibility is earmarked for future exploration. Notably, our approach dynamically formulates a decomposition path during\niterations, allowing for adjustments based on new insights from the contextually provided knowledge. In the retrieval phase,\nit uses atomic tags to bridge the semantic gap between the query and the information within the chunks. Importantly, during\nthe generation phase, our method retains the selected chunks, ensuring that the generation remains knowledge-aware and\nmitigates the risk of error accumulation often seen in methods that rely solely on follow-up questions and answers for\ncontext.\n\n\n**A.3. Hyper-Parameters**\nDuring the knowledge extraction phase, we utilize a _temperature_ setting of 0 _._ 7 specifically for the _Knowledge Atomizing_\nprocess, promoting a balance between diversity and determinism in the generated atomic knowledge. Conversely, for all\nquestion-answering (QA) steps in each method, we implement a _temperature_ of 0, ensuring consistent responses from the\nmodel.\n\n\nRegarding the retrieval component, we engage the _text-embedding-ada-002_ (version 2) as our embedding model for both\nthe general knowledge bases and the atomic knowledge bases. For the general knowledge bases used in Naive RAG and\nIter-RetGen, the retriever is configured to fetch up to 16 knowledge chunks, applying a retrieval score threshold of 0 _._ 2. For\nthe general knowledge bases used in Self-Ask w/ Retrieval and IRCoT, where the retrieval chunks are used for a single\nfollow-up question answering or the generation of single continuous rationale sentence, the reference chunks for whole\nrationale or final question answering are accumulated. The system retrieves 4 relevant chunks per request, maintaining the\nsame score threshold of 0 _._ 2. In the case of atomic knowledge bases, the retriever is set to retrieve 4 relevant atomic tags for\neach atomic query but with a higher threshold 0 _._ 5 due to the shorter content length.\n\n\n**A.4. Detailed Experimental Results**\n**Evaluation Metrics** As for the evaluation metrics, three more metrics are employed in Appendix. **Exact Match (EM)**,\nwhich assesses whether the response is identical to a predefined correct answer is applied as the community usually did.\nFurthermore, we encounter situations where a method achieves high accuracy (Acc) scores yet registers low F1 scores. To\nelucidate the underlying factors of such discrepancies, we also report on the **Recall** and **Precision** of the generated responses.\nRecall measures the proportion of relevant tokens from the answer labels that are captured in the response, while precision\nevaluates the relevance of the tokens in the generated answer with respect to the correct labels.\n\n\n15\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Table 6._ Detailed performance comparison on multihop QA datasets. Best in bold, second-best underlined.\n\n|Col1|Table 6. (a) HotPotQA|\n|---|---|\n|Method|EM<br>F1<br>Acc<br>Precision<br>Recall|\n|Zero-Shot CoT<br>Naive RAG<br>Self-Ask w/ Retrieval<br>IRCoT<br>Iter-RetGen<br>SearChain<br>ProbTree<br>GraphRAG Local<br>GraphRAG Global<br>KAR3 (**Ours**)|32.60<br>43.94<br>53.60<br>46.56<br>43.97<br>56.80<br>72.67<br>82.60<br>74.52<br>74.86<br>57.00<br>71.40<br>80.00<br>73.25<br>73.95<br>51.40<br>67.30<br>81.00<br>69.32<br>72.15<br>59.60<br>75.27<br>86.60<br>77.18<br>77.62<br>28.60<br>40.48<br>74.40<br>40.77<br>66.63<br>47.00<br>62.41<br>73.40<br>64.83<br>64.95<br>0.00<br>10.66<br>**89.00**<br>5.90<br>**83.07**<br>0.00<br>7.42<br>64.80<br>4.08<br>63.16<br>**61.40**<br>**76.48**<br>88.00<br>**78.53**<br>78.96|\n\n\n\n_Table 6._ (b) 2WikiMultiHopQA\n\n|Method|EM F1 Acc Precision Recall|\n|---|---|\n|Zero-Shot CoT<br>Naive RAG<br>Self-Ask w/ Retrieval<br>IRCoT<br>Iter-RetGen<br>SearChain<br>ProbTree<br>GraphRAG Local<br>GraphRAG Global<br>KAR3 (**Ours**)|35.67<br>41.40<br>43.87<br>41.43<br>43.11<br>51.20<br>59.74<br>62.80<br>59.06<br>62.30<br>60.60<br>69.06<br>75.00<br>67.88<br>73.15<br>55.00<br>63.83<br>70.40<br>62.47<br>68.86<br>57.80<br>67.21<br>73.60<br>66.10<br>71.09<br>7.00<br>15.67<br>68.40<br>11.91<br>66.74<br>57.00<br>69.42<br>80.00<br>67.61<br>76.89<br>0.00<br>11.83<br>71.20<br>6.74<br>75.17<br>0.00<br>7.35<br>45.00<br>4.09<br>55.43<br>**65.80**<br>**75.00**<br>**82.20**<br>**73.63**<br>**79.08**|\n\n\n|Col1|Table 6. (c) MuSiQue|\n|---|---|\n|Method|EM<br>F1<br>Acc<br>Precision<br>Recall|\n|Zero-Shot CoT<br>Naive RAG<br>Self-Ask w/ Retrieval<br>IRCoT<br>Iter-RetGen<br>SearChain<br>ProbTree<br>GraphRAG Local<br>GraphRAG Global<br>KAR3 (**Ours**)|12.93<br>22.90<br>23.47<br>24.40<br>24.10<br>32.00<br>43.31<br>44.40<br>44.42<br>47.29<br>38.20<br>46.76<br>51.40<br>46.75<br>51.00<br>36.00<br>47.57<br>49.20<br>48.70<br>50.30<br>40.20<br>52.48<br>55.60<br>53.51<br>56.45<br>24.40<br>33.26<br>45.80<br>33.00<br>46.37<br>28.57<br>43.26<br>52.86<br>42.27<br>54.70<br>0.60<br>9.62<br>49.80<br>5.73<br>55.82<br>0.00<br>5.16<br>44.60<br>2.82<br>52.19<br>**47.40**<br>**57.86**<br>**62.60**<br>**58.52**<br>**61.37**|\n\n\n\n**Detailed** **Main** **Results** The detailed experimental results on multihop datasets HotpotQA, 2Wiki and MuSiQue are\npresented in Table 6. Besides the metrics shown in Table 1, the **EM**, **Precision** and **Recall** are provided here.\n\n\n**Discussion of Graph-Based Method** Notably, knowledge graph-based method, GraphRAG Local, excels in HotpotQA—a\ndataset predominantly comprised of 2-hop questions. However, in the other two datasets, which contain questions involving\nmore hops, GraphRAG Local is merely on par with IRCoT. This highlights the challenge that knowledge graph-based\nmethods face in addressing complex multihop questions. Regarding GraphRAG, originally designed for the query-focused\nsummarization (QFS) task as outlined by (Edge et al., 2024), we observe its suboptimal performance in both local and\nglobal modes compared to our method. GraphRAG exhibits a curious trend: it achieves higher accuracy and recall scores\nwhile performing lower on EM, F1, and Precision metrics. A closer analysis of GraphRAG’s outputs reveals a tendency to\necho the query and include meta-information about the answer within its graph structure. Despite attempts to refine its QA\nprompt, this behavior persists. An illustrative example is presented in Table 7, which shows GraphRAG Local’s response to\na question from HotpotQA.\n\n\n16\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Table 7._ An Example of GraphRAG Local output on a HotpotQA question. The table showcases the tendency to repeat the question and\ninclude meta-information in its response.\n\n|Question|Which country is home to Alsa Mall and Spencer Plaza?|\n|---|---|\n|Answer Labels|India|\n|Answer of GraphRAG|Alsa Mall and Spencer Plaza are both located in Chennai, India [Data: In-<br>dia and Chennai Community (2391); Entities (4901, 4904); Relationships<br>(9479, 1687, 5215, 5217)].|\n\n\n\n**Detailed Evaluation Results of** _N_ **Selection** Table 8 lists the granular performance metrics according to those we shown\nin Figure 5 for the ablation study on the iteration upper bound _N_ . Different to the **Recall** we reported in Table 6, which\nindicates the recall tokens of the answer labels, the **Recall** **[*]** here represents the recall of the supporting facts provided by\nthese datasets.\n\n\n_Table 8._ Ablation study on hyper-parameter _N_ . Recall [*] indicates the recall of supporting facts.\n\n|N|HotpotQA<br>Recall* F1 Acc|2Wiki<br>Recall* F1 Acc|MuSiQue<br>Recall* F1 Acc|\n|---|---|---|---|\n|1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10|42.96<br>59.46<br>70.20<br>82.04<br>74.27<br>84.80<br>90.16<br>76.90<br>87.20<br>92.46<br>76.49<br>87.80<br>92.83<br>76.48<br>88.00<br>93.35<br>77.67<br>89.00<br>93.68<br>77.32<br>88.80<br>93.78<br>76.88<br>88.40<br>93.78<br>76.99<br>88.60<br>93.78<br>77.52<br>89.00|40.41<br>41.08<br>43.00<br>78.83<br>70.22<br>77.20<br>87.71<br>72.84<br>79.40<br>92.86<br>74.68<br>81.80<br>94.06<br>75.00<br>82.20<br>94.76<br>75.12<br>81.80<br>94.91<br>75.44<br>82.40<br>95.06<br>75.16<br>82.00<br>95.11<br>74.89<br>81.80<br>95.16<br>75.09<br>82.00|31.20<br>32.55<br>32.80<br>56.43<br>48.46<br>50.00<br>64.82<br>53.50<br>57.20<br>69.87<br>55.73<br>59.40<br>73.08<br>57.86<br>62.60<br>74.88<br>57.03<br>61.20<br>76.07<br>56.66<br>61.40<br>76.72<br>57.65<br>62.40<br>76.90<br>57.17<br>61.40<br>77.20<br>57.69<br>62.40|\n\n\n\n**Evaluation Results with Less Advanced LLM** As introduced in the limitation discussion section, we have carried out a\nseries of experiments utilizing GPT-3.5. The outcomes of these experiments are delineated in Table 9. For these specific\ntrials, we substituted GPT-4 (1106-Preview) with GPT-3.5 (1106-Preview) as the language model, while maintaining all\nother experimental settings identical to those employed in the experiments summarized in Table 1.\n\n\n_Table 9._ Performance comparison of implementations with _GPT-3.5_ . Best in bold, second-best underlined.\n\n|Method|HotpotQA<br>F1 Acc|2Wiki<br>F1 Acc|MuSiQue<br>F1 Acc|\n|---|---|---|---|\n|Self-Ask w/ Retrieval<br>IRCoT<br>Iter-RetGen<br>KAR3 (**Ours**)|49.52<br>61.40<br>56.39<br>68.40<br>48.63<br>66.80<br>46.37<br>**68.80**|53.83<br>**60.00**<br>40.31<br>46.00<br>44.32<br>55.20<br>41.95<br>58.20|31.05<br>35.20<br>33.93<br>34.40<br>25.77<br>37.80<br>26.80<br>**39.60**|\n\n\n\n**A.5. Cost Analysis and Discussion**\nIn this section, we conduct a comprehensive cost analysis to evaluate our model’s API consumption. We first evaluate and\ncompare the inference cost to other baseline methods, later we further decompose the cost into components, and finally a\ncost summarization of the one-time data preprocessing step will be provided.\n\n\n**Inference Cost Comparison** As Table 10 demonstrated, from the perspective of token consumption per QA, our method\nutilizes fewer tokens than both ProbTree and IRCoT, and is comparable to Iter-RetGen. However, our approach significantly\noutperforms these baselines on both F1 and Accuracy by a considerable margin. This demonstrates the efficiency of our\n17\n**From Complex to Atomic:** **Enhancing Augmented Generation**\napproach in balancing cost and performance. It is important to highlight that our method focuses on exploring potential\nreasoning chains, necessitating a thoughtful analysis during question decomposition with context at each iteration. As a\nresult, completion token usage constitutes approximately one-quarter of the total consumption, distinguishing our approach\nfrom other baselines.\n\n\n_Table 10._ Token consumption (average/QA) and performance comparison on MuSiQue.\n\n|Method|Token Consumption (⇓)<br>Prompt Completion Total|Performance (⇑)<br>F1 Acc|\n|---|---|---|\n|Zero-Shot CoT<br>Naive RAG<br>Self-Ask w/ Retrieval<br>IRCoT<br>Iter-RetGen<br>ProbTree<br>KAR3 **(Ours)**|85<br>105<br>191<br>1765<br>103<br>1869<br>5894<br>619<br>6514<br>9703<br>86<br>9789<br>8140<br>473<br>8614<br>25225<br>650<br>**25875**<br>6525<br>2295<br>8820|22.90<br>23.47<br>43.31<br>44.40<br>46.76<br>51.40<br>47.57<br>49.20<br>52.48<br>55.60<br>43.26<br>52.86<br>**57.86**<br>**62.60**|\n\n\n\n**Token Consumption of Different Components** In the experimental results presented so far, the same LLM is used for the\ndecomposer, selector, and generator components. The prompts designed for these components are detailed in Appendix A.7.\nIt is worth noting that these components can be configured to use different language models, we leave it as future works. The\ndetailed token consumption of difference components on MuSiQue are illustrated in Table 11. The decomposition-selection\nloop iterates up to 5 rounds, leading to the multiple calls for decomposer and selector for each QA. Consequently, the\ndecomposer and selector constitute the majority of the total consumption.\n\n\n_Table 11._ Token consumption (average/QA) on MuSiQue.\n\n|Component|Prompt Completion Total|\n|---|---|\n|Query Proposer<br>Atomic Selector<br>Answer Generator|2691<br>768<br>3459<br>3278<br>1429<br>4707<br>556<br>98<br>654|\n|KAR3 **(Ours)**|6525<br>2295<br>8820|\n\n\n\n**Token Consumption of Chunk Atomization** The chunk atomization, as a one-time preprocessing step, for which the\nLLM API consumption scales linearly with the number of data chunks and constitutes an overhead that varies slightly across\ndifferent benchmarks. As described in Section 4.1, all chunks are derived from the context paragraphs, and the number\nof LLM calls, which is equivalent to the chunk count, is listed in the last column of Table 12, together with the token\nconsumption, for your reference. The input token size (i.e., Prompt in the table) is primarily determined by the chunk size,\nwhile the output token size (i.e., Completion in the table) depends on the size of generated atomic tags.\n\n\n_Table 12._ Token consumption (average/chunk) and chunk count statistics.\n\n|Dataset|Prompt Completion Total|Calls|\n|---|---|---|\n|HotpotQA<br>2Wiki<br>MuSiQue|209<br>129<br>338<br>199<br>122<br>321<br>197<br>123<br>320|4950<br>3410<br>7120|\n\n\n\n**An Alternative Presentation of Atomic Tag** We recognize the significance of scalability when applying our method to\nextensive datasets. To enhance cost-effectiveness while maintaining scalability, we integrate the use of open-source language\nmodels like Llama 3, which significantly reduces preprocessing costs. Additionally, we explore alternative atomic tag\npresentation to further optimize resource usage. One promising approach is atomizing data into plain-text sentences, treating\neach sentence as an atomic tag. This method simplifies the preprocessing steps by utilizing the *spacy* library to segment the\noriginal data chunks into sentences, thereby avoiding the need for language model invocations. Our evaluations, as detailed\nin Table 13, show that this approach, while reducing performance to 55.2% on the MuSiQue dataset, still outperforms most\n18\n**From Complex to Atomic:** **Enhancing Augmented Generation**\nbaseline methods. This demonstrates its potential effectiveness in scenarios where lower-cost preprocessing is a priority,\noffering a viable alternative that balances cost and performance efficiently.\n\n\n_Table 13._ Performance of alternative atomic tags on MuSiQue.\n\nLLM Atomic Tags F1 Acc\nplain text sentence 45.88 54.20\nLlama 3\natomic question **(Ours)** **50.68** **59.70**\nplain text 50.72 55.20\nGPT-4\natomic question **(Ours)** **57.86** **62.60**\n**A.6. Real Case Studies**\nThis section presents there real case studies from our evaluation benchmark to illustrate the underlying principles of our\nproposed decomposition pipeline, as detailed in Algorithm 1. Through these real-world examples, we aim to highlight the\nbenefits of our systematic approach. These cases will shed light on how each step of the pipeline contributes to improved\nperformance and the insights gained from their implementation.\n\n\n_Figure 6._ Case (a): Given the lesser-known film “What Women Love” as opposed to the more popular “What Women Want,” single-path\nmethods like Self-Ask on the left are predisposed to generating follow-up questions about the latter, leading to an incorrect final answer.\nConversely, KAR [3] can effectively discern the intended meaning of the original question by positing several atomic queries and postpone\nthe task understanding to atomic selection phase with relevant atomic tags provided, and subsequently arriving at an accurate conclusion.\n\n\n19\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-18-0.png)\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n**KAR** **[3]** **outperforms single-path methods by effiectively discerning the intended meaning of the original questions.**\nOur task decomposition strategy involves generating multiple atomic queries rather than producing a single deterministic\nfollow-up question, as demonstrated in the Self-Ask approach. Contemporary decomposition methods typically employ a\ngenerative model to formulate a singular follow-up question. However, this approach carries an intrinsic risk of generating\nerroneous questions, potentially leading to an incorrect decomposition pathway and, ultimately, an erroneous answer.\nConsider the Case (a) depicted in Figure 6, where the original question pertains to a film titled “What Women Love.” Due to\nthe existence of a more prominent film, “What Women Want,” the employed language model tends to ‘correct’ the original\nquestion. Consequently, methods like Self-Ask (as shown on the left side of Figure 6) generate only one follow-up question\nrelated to this erroneously assumed object. In the illustrated instance, although the target chunk has been retrieved due to the\nsimilarity in embeddings, a ‘false’ intermediate answer is produced for the ‘false’ follow-up question, culminating in an\nincorrect final response. In contrast, our methodology posits atomic queries concerning both “What Women Love” and\n“What Women Want,” thereby seeking to clarify the true intent of the initial question. With both films existing and relevant\natomic tags being retrieved, our approach subsequently gains the advantage of verifying the question’s intent and selecting\nthe correct and most pertinent chunk during the atomic selection phase.\n\n\n_Figure 7._ Case (b): By proposing multiple atomic queries, KAR [3] effectively retrieves the relevant knowledge chunk, whereas the single\ndeterministic follow-up question approach employed by Self-Ask fails to align with the knowledge base’s schema, resulting in a retrieval\nfailure.\n\n\n**KAR** **[3]** **outperforms baseline methods through better knowledge schema alignment by atomic tags as bridge.** The\ndiscrepancy between the formulation of the corpus and the query, is another critical factor advocating for a multi-query\napproach over a singular deterministic one. The presentation gap can impede the retrieval process even when the generated\nfollow-up question is semantically accurate. For instance, as illustrated in Case (b) in Figure 7, a single-path method such\nas Self-Ask on the left side might directly inquire ‘Who is the mother of Oskar Roehler?’ However, the knowledge base\narticulates familial relationships using a different schema, ‘A is the son of B and C’ in this case, thus the retrieval process\nfalters despite the correctness of the question. Even when we applied the hierarchical retrieval to Self-Ask, the Self-Ask\nwith Hierarchical Retrieval did not succeed in bridging this gap. In contrast, our approach, which generates multiple atomic\nqueries, encompasses a broader range of phrasings that correspond to the diverse representations in the knowledge base. In\nthe depicted case, while the atomic query specifically asking for Oskar Roehler’s mother encounters the same retrieval issue,\nan alternative query seeking information about his parents successfully retrieves the target chunk. This exemplifies how\nour method’s flexibility in query generation enhances the likelihood of aligning with the knowledge base’s structure and\n20\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-19-0.png)\n**From Complex to Atomic:** **Enhancing Augmented Generation**\nobtaining accurate information.\n\n\nOur methodology emphasizes the retrieval of atomic tags rather than directly retrieving chunks. This design choice is\nexemplified in Case (b) depicted in Figure 7. The knowledge chunk in the corpus is structured using the pattern ‘A ... as the\nson of B and C’, which poses challenges for direct retrieval by queries such as ‘Who is the mother of ...’. In our specialized\nknowledge base, such direct queries tend to retrieve chunks conforming to the patterns ‘A is the mother of B’ or ‘A is the\nfather of B’. By utilizing atomic tags as intermediaries for retrieval, our approach effectively narrows the gap between a\nsingle query and the multiple sentence structures found in the knowledge base. It facilitates bridging the expression pattern\ndifferences exemplified by ’the mother of’ versus ’the son of’ in this scenario.\n\n\n**KAR** **[3]** **outperforms methods that rely on intermediate answers by maintaining concise and highly relevant context.**\nIn contrast to methods like Self-Ask, which only retains intermediate answers for subsequent processing, our method\npreserves the entire chunk as contextual information. During the atomic selection phase, we present a list of atomic tags\nas candidate summaries of the relevant content from the original chunk. This strategy significantly reduces token usage\nand simplifies the process of selecting the pertinent information. Case (c) in Figure 8 demonstrates the dual benefits of our\napproach: first, by selecting from a curated list of atomic tags, we streamline the identification of relevant information;\nsecond, by retaining the entire selected chunk rather than just the intermediate answer, we ensure a rich context is maintained\nfor accurate and comprehensive subsequent processing. While the Self-Ask method on the left retrieves the target chunk, it\nfails to correctly identify the pertinent ‘Ernie Watts’ due to the excessive contextual information. Since retrieved chunks\nin Self-Ask are discarded after generating an intermediate answer, the method potentially follows an incorrect pathway,\nleading to an inaccurate conclusion. In contrast, our approach can efficiently filter and select the appropriate atomic tag from\na concise list. Although the atomic tag in this round pertains to the role of Ernie Watts, there is no need to inquire further\nabout his birthplace, as this information is encapsulated within the selected chunk, which remains available for context in\nsubsequent rounds.\n\n\n_Figure 8._ Case (c): KAR [3] has the advantage of leveraging a concise list of atomic tags for targeted selection and retaining full chunks\nfor rich contextual support. Conversely, Self-Ask’s approach, although successful in retrieving relevant chunks, is compromised by its\ndependency on intermediate answers for context, which ultimately results in the generation of incorrect final answers.\n\n\n21\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-20-0.png)\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n**A.7. Prompt Design**\nOur approach employs four distinct prompts: (1) Atomic question tagging prompt: the one used to pre-processing the\nsource paragraphs that linking each paragraphs with several atomic questions as atomic tags; (2) Atomic query proposer\nprompt: the one used when generating multiple atomic query proposals, referring to line 1 in Algorithm 1; (3) Atomic tag\nselection prompt: the one used when selecting the most useful atomic tag from the given question list, referring to line 1 in\nAlgorithm 1; (4) Question answering prompt: the one applied upon exiting the decomposition loop to generate the final\nanswer to the given question, as described in line 1 of Algorithm 1.\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-21-0.png)\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-21-1.png)\n\n22\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-22-0.png)\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning.pdf-22-1.png)\n\n**Demonstration Discussion** In our current experiments, all prompts are zero-shot, meaning no demonstrations are provided\nto illustrate the expected reasoning logic. To explore whether demonstrations could enhance performance, we designed\nan ablation study. We adapted the Self-Ask w/ Retrieval and IRCoT methodologies previously employed, modifying the\nprompts and task descriptions to create zero-shot, demonstration-free variants of these methods. These were denoted as\n**Zero-Shot Self-Ask w/ Retrieval** and **Zero-Shot IRCoT** . The results of the experiment are presented in Table 14. The\nexperimental results reveal that the Zero-Shot Self-Ask w/ Retrieval method experiences a marginal decline in accuracy\nfor the 2Wiki and MuSiQue datasets, potentially due to the inherent randomness in generation. However, the inclusion\nof demonstrations significantly improves all F1 scores and enhances the overall performance of the IRCoT method. This\nsuggests that demonstrations could be particularly beneficial for methods that rely on a step-by-step decomposition approach.\nConsequently, integrating demonstrations is identified as a promising direction for future work within the KAR [3] framework.\n\n\n23\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Table 14._ Performance comparison: Zero-Shot vs. Few-Shot.\n\n|Method|HotpotQA<br>F1 Acc|2Wiki<br>F1 Acc|MuSiQue<br>F1 Acc|\n|---|---|---|---|\n|Zero-Shot Self-Ask w/ Retrieval<br>Self-Ask w/ Retrieval|55.76<br>76.20<br>71.40<br>80.00|54.98<br>76.20<br>69.06<br>75.00|40.97<br>50.40<br>46.76<br>51.40|\n|Zero-Shot IRCoT<br>IRCoT|58.22<br>75.80<br>67.30<br>81.00|49.69<br>60.20<br>63.83<br>70.40|37.17<br>43.00<br>47.57<br>49.20|\n\n\n\n**A.8. Evaluation on Legal Benchmarks**\nIn this subsection, we present the performance of our approach on two legal benchmarks: LawBench (Fei et al., 2023) and\nOpen Australian Legal QA (Butler, 2023). Before doing so, we provide a brief description of each benchmark.\n\n\n**LawBench** LawBench is a comprehensive legal benchmark for Chinese laws. It comprises 20 meticulously designed\ntasks aimed at accurately assessing the legal capabilities of LLMs. Unlike some existing benchmarks that rely solely on\nmultiple-choice questions, LawBench includes a variety of task types that are closely related to real-world applications.\nThese tasks encompass legal entity recognition, reading comprehension, crime amount calculation, and legal consulting,\namong others. Since not all tasks are RAG-oriented (e.g., reading comprehension), we have selected 6 specific tasks, which\nare detailed in Table 15. The number of questions of each task is 500.\n\n|Task No.|Task|Type|Metric|\n|---|---|---|---|\n|1-1<br>1-2<br>3-1<br>3-2<br>3-6<br>3-8|Statute Recitation<br>Legal Knowledge Q&A<br>Statute Prediction (Fact-based)<br>Statute Prediction (Scenario-based)<br>Case Analysis<br>Consultation|Generation<br>Single Choice<br>Multiple Choices<br>Generation<br>Single Choice<br>Generation|F1<br>EM<br>EM<br>F1<br>EM<br>F1|\n\n\n\nWe also provide example questions of these tasks for the readers reference (translated using GPT-4).\n\n\n1-1: Answer the following question by directly providing the content of the article:What\n_�→_ is the content of Article 76 of the Securities Law?\n1-2: According to the ’Securities Law’, which of the following statements about stock\n_�→_ exchanges is incorrect? A: Without the permission of the stock exchange, no entity\n_�→_ or individual may publish real-time securities trading information; B: The stock\n_�→_ exchange may restrict trading on securities accounts that exhibit major abnormal\n_�→_ trading conditions as needed, and report to the securities regulatory authority\n_�→_ under the State Council for record; C: The accumulated property of a member-based\n_�→_ stock exchange belongs to the members, and their rights are jointly enjoyed by the\n_�→_ members; during its existence, the accumulated property may not be distributed to\n_�→_ the members; D: The stock exchange formulates listing rules, trading rules, member\n_�→_ management rules, and other relevant rules in accordance with securities laws and\n_�→_ administrative regulations, and reports to the securities regulatory authority under\n_�→_ the State Council for record.\n3-1: Based on the following facts and charges, provide the relevant articles of the\n_�→_ Criminal Law. Facts: The Yushu City, Jilin Province, accused that on November 15,\n_�→_ 2015, the defendant He signed a car rental agreement with Guo, the owner of a taxi\n_�→_ with license plate number xxx. The agreement stipulated a monthly rent of RMB\n_�→_ 3,900.00, payable monthly. On January 19, 2016, without the knowledge of Guo, the\n_�→_ defendant He concealed the truth and falsely claimed to be the owner of the taxi.\n_�→_ He signed a car rental agreement with the victim Ma, with a monthly rent of RMB\n_�→_ 3,800.00 and a rental period of one year, collecting a total of RMB 50,600.00 from\n_�→_ Ma for one year’s rent and vehicle deposit. On February 26, 2016, the taxi was\n_�→_ retrieved by its owner Guo from the victim Ma. The victim Ma repeatedly asked the\n_�→_ defendant He to return the rent and deposit, but the defendant He refused to return\n_�→_ them. The prosecution provided evidence including the defendant’s confession, the\n_�→_ victim’s statement, witness testimonies, and documentary evidence, and believed\n24\n**From Complex to Atomic:** **Enhancing Augmented Generation**\n_Table 16._ Evaluation Results on Legal Benchmarks (Metric is **F1 / EM** as indicated in Table 15)\n\n|Task|Col2|Zero-Shot CoT|GraphRAG Local|Ours (N=5)|\n|---|---|---|---|---|\n|LawBench|1-1<br>1-2<br>3-1<br>3-2<br>3-6<br>3-8|21.31<br>54.24<br>53.32<br>27.51<br>51.16<br>17.44|23.27<br>62.60<br>74.60<br>25.98<br>47.64<br>18.43|**78.58**<br>**70.60**<br>**83.16**<br>**46.05**<br>**61.91**<br>**23.58**|\n|Open Australian Legal QA|Open Australian Legal QA|25.10|34.35|**63.34**|\n\n\n\n_Table 17._ Evaluation Results on Legal Benchmarks (Metric is **Acc** )\n\n|Task|Col2|Zero-Shot CoT|GraphRAG Local|Ours (N=5)|\n|---|---|---|---|---|\n|LawBench|1-1<br>1-2<br>3-1<br>3-2<br>3-6<br>3-8|1.23<br>54.00<br>49.90<br>15.83<br>51.12<br>49.70|16.60<br>63.40<br>75.40<br>27.60<br>57.00<br>58.80|**90.12**<br>**70.60**<br>**88.82**<br>**67.54**<br>**62.73**<br>**61.72**|\n|Open Australian Legal QA|Open Australian Legal QA|16.48|88.27|**98.59**|\n\n\n\n_�→_ that the defendant He, with the purpose of illegal possession, defrauded others of\n_�→_ their property by fabricating facts and concealing the truth during the signing and\n_�→_ performance of the contract. The amount was relatively large, and his actions\n_�→_ violated the provisions of Article xx of the Criminal Law of the People’s Republic\n_�→_ of China, and he should be held criminally responsible for xx. Charge: Contract\n_�→_ Fraud.\n3-2: Please provide the legal basis according to the specific scenario and question, only\n_�→_ the content of the specific legal provision is needed, each scenario involves only\n_�→_ one legal provision. Scenario: A cargo ship arrives at the port of discharge, but\n_�→_ the consignee fails to arrive in time to collect the goods. Under which legal\n_�→_ provision can the captain unload the goods at another appropriate place?\n3-6: One year after the bar opened, the business environment changed drastically, and all\n_�→_ partners held a meeting to discuss countermeasures. According to the ’Partnership\n_�→_ Enterprise Law,’ the following voting matters are considered valid votes: A: Zhang\n_�→_ believes that the name ’Tongcheng’ is not attractive and proposes to change it to ’\n_�→_ Tongsheng Bar.’ Wang and Zhao agree, but Li opposes; B: In view of the sluggish\n_�→_ business, Wang proposes to suspend operations for one month for renovation and\n_�→_ reorganization. Zhang and Zhao agree, but Li opposes; C: Due to the urgent needs of\n_�→_ the bar, Zhao proposes to sell a batch of coffee machines to the bar. Zhang and\n_�→_ Wang agree, but Li opposes; D: Given the four partners’ lack of experience in bar\n_�→_ management, Li proposes to appoint his friend Wang as the managing partner. Zhang\n_�→_ and Wang agree, but Zhao opposes.\n3-8: Resident A rented out the house to B. With A’s consent, B renovated the rented house\n_�→_ and sublet it to C. C unilaterally altered the load-bearing structure of the house.\n_�→_ Why can A request B to bear liability for breach of contract?\n\n\n**Open Australian Legal QA** The benchmark consists of 2,124 questions and answers synthesized by GPT-4 from the\nAustralian legal corpus. All questions are of the generation type. One example is: “What is the landlord’s general obligation\nunder section 63 of the Act in the case of Anderson v Armitage [2014] NSWCATCD 157 in New South Wales?”\nEvaluation results are listed in Table 16, where we only compare to “GraphRAG Local”, as it generally performs better than\n“GraphRAG Global” on these tasks.\n\n\nFor the aforementioned reasons, we also use GPT-4 to evaluate all experimental results, reporting the accuracy ( **Acc** ) in\nTable 17. When comparing the results in Table 16 and Table 17, we observe that the order of the results is preserved, even\nthough some metrics change significantly. In the following section, we aim to identify the reasons behind these changes,\n25\n**From Complex to Atomic:** **Enhancing Augmented Generation**\nwhich may provide valuable insights for designing better metrics to evaluate RAG frameworks in the future.\n\n\n1. The accuracy of our approach increases significantly for generation tasks (1-1, 3-2, Open Australian Legal QA). For\nthese tasks, our answers are often semantically equivalent but syntactically different from the golden answers. This\nexplains the improved metric performance, as GPT-4 can compare the semantic content of the answers. This also\napplies to the “GraphRAG Local” results for the “Open Australian Legal QA” task.\n\n\n2. The accuracy of “GraphRAG Local” decreases for generation tasks 1-1 and 3-2. These tasks involve statute recitation\nand prediction, requiring the retrieval of specific articles. Upon detailed examination, We find that “GraphRAG Local”\noften fails to retrieve the correct articles or references the wrong ones, but it tends to repeat the legal information.\nTherefore, token-level recall can be improved by simply rephrasing legal names and common prefixes, such as\n“According to XX law, XX articles...”.\n\n\n3. Both our approach and “GraphRAG Local” show significant accuracy improvements on task 3-8. Besides the reason\nmentioned in the first point, the quality of the golden answers may also contribute to this difference. The questions\nand golden answers in task 3-8 are sourced from a consulting website, resulting in varying quality. For example, one\nquestion asks “Do the children from the original marriage have an obligation to support the father?” However, the\nprovided golden answer includes an irrelevant article, “Article 1067,” which pertains to parents’ obligations to support\nminor children.\n\n\nQuestion: In the case where both parents are divorced and have formed their own\n_�→_ families with new children, and according to the court’s judgment, the father is\n_�→_ required to pay monthly child support to the mother until the child is 18 years\n_�→_ old. Do the children from the original marriage have an obligation to support\n_�→_ the father?\nReference Answer: In our country, biological children have an obligation to support\n_�→_ their parents who are divorced. The relationship between children and parents\n_�→_ does not dissolve because of the divorce of the children or parents. Supporting\n_�→_ parents is a legal obligation of the children. If the children do not support\n_�→_ the elderly, the parents can directly file a lawsuit in the People’s Court\n_�→_ requesting the court to order the daughter to pay support. Legal basis: Article\n_�→_ 1067 of the Civil Code of the People’s Republic of China states that if parents\n_�→_ do not fulfill their obligation to support, minor children or adult children who\n_�→_ cannot live independently have the right to request support from their parents.\n_�→_ If adult children do not fulfill their obligation to support, parents who are\n_�→_ incapacitated or have difficulties in living have the right to request support\n_�→_ from adult children. Article 1084 states that the relationship between parents\n_�→_ and children does not dissolve because of the parents’ divorce. After the\n_�→_ divorce, whether the children are directly raised by the father or the mother,\n_�→_ they are still the children of both parents. Lawyer’s explanation: After the\n_�→_ parents are divorced, adult children still have the obligation to support both\n_�→_ parents. The obligation of adult children to support their parents does not\n_�→_ change because of changes in the parents’ relationship. If adult children do not\n_�→_ fulfill their obligation to support, parents who are incapacitated or have\n_�→_ difficulties in living have the right to request support from adult children.\nOur Answer: Yes, there is an obligation to support. Legal basis: According to Article\n_�→_ 1069 of the Civil Code of the People’s Republic of China, it is stipulated that:\n_�→_ ’The obligation of children to support their parents does not terminate because\n_�→_ of changes in the parents’ marital relationship.’ Therefore, even if the\n_�→_ parents are divorced and remarried with new children, the original children\n_�→_ still have an obligation to support their parents.\n\n\n4. The accuracy of all methods on choice tasks 1-2, 3-1, and 3-6 almost coincides with the F1 score, as expected. An\nexception is task 3-1, where the difference is mainly due to GPT-4’s capacity to understand Chinese, particularly in\ndistinguishing numbers in Arabic and Chinese. In Chinese law, all numbers are written in Chinese, while in the golden\nanswers, all numbers are given in Arabic.\n\n\n26\n\n\n",
    "kor_md": "**복잡성에서** **원자성으로: 지식** **기반** **이중** **재작성** **및** **추론을** **통한** **증강** **생성** **능력** **향상**\n**왕진유** [(] **Jinyu Wang** [) (*)1] **푸징징** [(] **Jingjing Fu** [) (*)1] **왕루이** [(] **Rui Wang** [) (1)] **송레이** [(] **Lei Song** [) (1)] **비안장** [(] **Jiang Bian** [) (1)]\n**초록**\n검색 강화 생성(RAG) 시스템의 최근 발전은 외부 지식 검색\n을 통합함으로써 대규모 언어 모델(LLM)의 역량을 크게 향\n상시켰습니다. 그러나 검색에만 의존하는 방식은 심층적이고\n전문적인 지식을 발굴하고, 특정 분야의 복잡한 질문을 해결\n하는 데 필요한 논리적 추론을 수행하는 데 종종 부적절합니\n다. 이러한 과제를 해결하기 위해, 우리는 전문 지식을 원자\n적 방식으로 추출·이해·활용하면서 동시에 일관된 추론 근거\n를 구축하도록 설계된 접근법을 제시한다. 이 접근법의 핵심\n에는 네 가지 중추적 구성 요소가 있습니다: 원시 데이터에서\n원자적 태그를 추출하는 지식 원자화기, 초기 질의를 촉진하\n기 위해 후속 질문을 생성하는 질의 제안기, 원자적 지식 정\n렬을 기반으로 지식을 찾는 원자적 검색기, 그리고 검색된 정\n보를 바탕으로 질의할 원자적 태그와 청크 쌍을 결정하는 원\n자적 선택기입니다. 이러한 접근 방식을 통해, 우리는 지식\n인식 작업 분해 전략을 구현하여 초기 질문과 획득한 지식에\n부합하는 논리를 반복적으로 구축합니다. 다양한 벤치마크,\n특히 다단계 추론이 필요한 경우에서 본 접근법의 효용성을\n입증하기 위한 포괄적인 실험을 수행했습니다. 두 번째로 우\n수한 방법 대비 최대 +10.1(20.4%)에 달하는 상당한 성능 향\n상은 복잡하고 지식 집약적인 응용 분야에서 본 접근법의 잠\n재력을 강조합니다. 코드는\n[https://github.com/microsoft/PIKE-RAG에서](https://github.com/microsoft/PIKE-RAG?tab=readme-ov-file) 공개적으로 이\n용 가능합니다.\n\n\n*동등한 기여 1마이크로소프트 리서치 아시아, 중국 베이징. 연락처: Lei Song\n_<_ [lesong@microsoft.com](mailto:lesong@microsoft.com) _>_ .\n\n\n_제42회국제기계학습학회논문집_, 캐나다 밴쿠버. PMLR 267, 2025. 저작권 2025년\n저작자 소유.\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-0-0.png)\n\n_그림 1._ 복잡한 질문은 일반적으로 사슬형 또는 트리형 경로를 따라 하위 질문으로\n분해되며, 해결을 위한 관련 정보 수집에는 청크 검색이 활용된다. 기존 접근법과 달\n리, 본 방법은 질문과 청크에 대한 이중 재작성을 통한 원자적 지식 정렬로 질문 분해\n와 정보 검색을 원활하게 통합하고, 원자적 쌍 검색 및 선택을 통해 후속 하위 질문\n을 동적으로 결정함으로써 검색된 지식에 기반하여 진화하는 적응적이고 상호작용\n적인 분해 경로를 가능하게 한다.\n\n\n**1.** **서론**\n대규모 언어 모델(LLMs)은 일관성 있고 문맥에 부합하는 텍스트를 생성\n하는 능력과 텍스트 완성부터 번역 및 요약에 이르기까지 다양한 언어 작\n업을 수행할 수 있는 다재다능함을 보여주며 자연어 처리 분야에 혁명을\n일으켰다(Achiam et al., 2023; Touvron et al., 2023). 광범위한 능력에도\n불구하고, LLM은 전문 분야의 특수한 질의에 대해 뚜렷한 한계를 보입니\n다(Ling et al., 2024; Wang et al., 2023a). 이는 주로 도메인별 훈련 자료(예\n: _미공개문서_ )의 부족과 해당 도메인 내 전문 지식 및 논리(예: _산업별약_\n_어, 기업별운영규칙_ )에 대한 불완전한 이해에서 기인합니다. 그 결과,\nLLM은 잠재적으로 오류가 있을 뿐만 아니라 전문가 수준의 참여에 필요\n한 세부 사항과 정확성이 부족한 응답을 생성할 수 있습니다 (Bender 외,\n2021).\n\n\n\n1\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n_그림 2._ Self-Ask와 KAR [(3)][의] 사례 시연. KAR [3] 은 여러 원자적 질의를 제안함으로써 관련 지식 청크를 효과적으로 검색하는 반면, Self-Ask가 사용하는 단일 결정론적 후속 질\n문 접근법은 지식 기반의 스키마와 일치하지 않아 검색 실패를 초래합니다.\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-1-0.png)\n\n이러한 문제를 완화하기 위해 RAG(Lewis et al., 2020)가 유망한 해결책\n으로 부상했으며, 생성된 콘텐츠를 고정시키기 위해 외부 지식 검색으로\n대규모 언어 모델(LLM)을 보강합니다. RAG 프레임워크는 LLM 내에 인\n코딩된 지식을 보완하거나 대체함으로써 사실성과 관련성을 개선하는 것\n을 목표로 합니다. 그러나 기존 RAG 시스템은 종종 도메인 특화 복잡 작\n업에 어려움을 겪습니다. 예를 들어, \" _HUMIRA의최신바이오시밀러중_\n_성공적으로승인된제품명을제공해주세요_ \"와 같은 질의에 답하려면 여\n러 출처에서 흩어진 전문 지식(예: _HUMIRA의바이오시밀러_ )을 검색하\n는 것 이상의 작업이 필요합니다. 이는 적격 제품과 승인 일정에 대한 논\n리적 추론을 통해 정확하고 신뢰할 수 있는 답변을 종합해야 합니다. 현재\nRAG 방법은 주로 일반 텍스트 검색에 의존하는데, 이는 도메인 특화 표현\n내 상관관계를 효과적으로 포착하지 못할 수 있으며, 종종 이용 가능한 지\n식을 고려하지 않은 채 질문 분해를 수행하여 최적화되지 않은 하위 질문\n생성, 비효율적인 검색 및 추론 실패를 초래합니다.\n\n\n본 연구에서는 RAG의 발전을 위해서는 지식 인식 처리, 특히 질문 분해\n및 정보 검색에 대한 지식 인식 처리와 반복적 추론이 필요하며, 이를 통\n해 전문 분야에서 복잡한 다단계 질문을 효과적으로 해결할 수 있다고 주\n장한다.\n\n\n**도메인** **특화** **이해를** **위한** **지식** **인식** **처리** 특화 도메인에서 복잡한 논리 기\n반 작업을 해결하려면\n2\n특정 도메인에서 복잡한 논리 기반 작업을 해결하려면 사용자의 정보 요\n구와 검색된 데이터의 근본적 맥락을 깊이 이해하기 위한 지식 추출 및 이\n해가 필요하다. 예를 들어 의학, 법률, 금융과 같은 분야의 전문적 질문은\n종종 도메인 특화 용어와 논리를 포함하며, 일반적인 대규모 언어 모델\n(LLM)은 이를 완전히 파악하지 못할 수 있다. 키워드 매칭(Ram et al.,\n2023; Jiang et al., 2023)이나 임베딩 유사도(Gao et al., 2023)를 기반으로\n텍스트 구절을 검색하는 기존 RAG 시스템은 문맥적으로 관련성 있는 정\n보를 검색할 수 있지만, 의미적 정확성이 부족하여 복잡한 질문에 답하기\n에는 불충분할 수 있습니다.\n\n\n**복잡한** **질의** **해결을** **위한** **반복적** **추론** 답변이 여러 출처의 정보를 종합하\n는 데 의존하는 복잡한 추론 작업은 원본 질문을 일련의 더 단순하고 상호\n연관된 하위 질문들로 분해할 것을 요구한다(Press et al., 2023). 그럼에도\n불구하고, 이 접근법은 지식 모델(LLM)이 지식을 쉽게 접근할 수 없는 영\n역에서는 장애물에 직면할 수 있다. 이러한 영역에서의 분해는 독립적 작\n업이 아닌 맥락적이어야 한다고 주장한다. 즉, 분해된 질의는 검색된 지식\n과 맥락을 점진적으로 활용해 답변할 수 있으며, 이후 질의를 정교화하는\n방향으로 진화해야 한다. 이러한 반복적 접근은 시스템이 사용자의 질의\n에 대한 이해를 발전시켜 후속 질문이 가장 최근 검색 결과에 기반하도록\n보장한다.\n\n\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n우리는 지식 인식 이중 재작성 및 추론 메커니즘을 활용하는 새로운 프레\n임워크인 KAR [3] -RAG를 소개합니다. 본 접근법은 질문 재작성과 지식 검\n색 간의 동적 상호작용을 특징으로 하며, 그림 1에서 설명된 바와 같이 각\n반복 단계에서 질의와 검색된 컨텍스트를 적응적으로 정교화할 수 있도\n록 합니다. 본 시스템의 핵심 구성 요소로는 원시 데이터를 더 세분화된\n검색을 위한 원자 태그로 분해하는 지식 원자화기, 진화하는 컨텍스트를\n기반으로 후속 질문을 생성하는 질의 제안기, 원자 지식 정렬을 기반으로\n관련 지식을 식별 및 검색하는 원자 검색기, 검색된 정보를 바탕으로 가장\n관련성 높은 후속 질문을 결정하는 원자 선택기가 포함됩니다. 특히, 원자\n태그는 주어진 청크로 답변 가능한 관련 질의로 구성되어 청크의 다각적\n지식을 포괄하고 효과적인 검색을 용이하게 합니다. 원자 질의 제안은 질\n문에 대한 답변을 개선하는 데 도움이 되는 지식을 탐색하기 위해 제기됩\n니다. 이러한 구성 요소를 활용함으로써 본 시스템은 질문과 검색된 지식\n에 대한 이해를 반복적으로 정교화하여 다중 단계에 걸쳐 더 정확하고 문\n맥을 고려한 추론을 가능하게 합니다. 그림 2에서 보여주는 실제 사례를\n통해 작업 분해와 원자적 검색의 장점을 입증합니다. 본 접근법은 다각적\n인 작업 분해를 가능하게 할 뿐만 아니라, 원자적 태깅을 통해 코퍼스 구성\n과 질의 간의 불일치를 완화합니다.\n\n\n우리의 주요 기여는 다음과 같습니다: 1) 검색된 지식을 질문 분해에 통합\n하여 추론 경로의 반복적 탐색을 가능케 하는 지식 인식형 RAG 프레임워\n크를 제안합니다. 2) 이중 작성을 통한 원자적 지식 정렬 접근법을 도입하\n여 질의 분해와 검색을 긴밀히 결합함으로써 검색 효율성을 크게 향상시\n킵니다. 3) 다양한 벤치마크 데이터셋에서 포괄적인 실험 및 제거 연구를\n통해 본 접근법의 우수한 성능을 검증하였으며, 두 번째로 우수한 방법 대\n비 최대 20.4%의 성능 향상을 달성하였습니다.\n\n\n**2.** **관련** **연구**\n**2.1.** **RAG**\nRAG는 외부 지식을 효과적으로 통합하여 LLM의 생성 성능을 향상시키\n는 유망한 솔루션으로 부상했습니다. 단순한 RAG 시스템은 외부 데이터\n소스에서 관련 정보를 검색하여 질문 프롬프트의 맥락에 보충 지식으로\n통합함으로써 문맥적으로 적절한 생성을 가능하게 합니다(Ram et al.,\n2023). 고급 RAG 접근법은 쿼리 최적화(Ma et al., 2023; Zheng et al.,\n2023), 다중 세분화 청크화(Chen et al.,\n3\n2023; Zhong et al., 2024), 혼합 검색(Yang, 2023) 및 재정렬(Cohere,\n2023)을 포함한다. 한편, 검색 성능 향상을 위해 명시적(Zheng et al.,\n2024) 또는 암시적(Gao et al., 2022)으로 쿼리 재작성에 주력하는 연구도\n있다. 다른 한편으로는, 여러 연구에서 원시 데이터 소스를 구조화된 데\n이터로 변환하여 궁극적으로 더 효과적인 검색 및 추론을 위한 가치 있는\n지식으로 전환하고 있다(Wang et al., 2023b; Zheng et al., 2024; Raina &\nGales, 2024; Liang et al., 2024). 본 시스템에서는 쿼리와 청크 모두에 대\n해 원자적 재작성(atomic rewriting)을 도입하여 다중 세분성 질문 분해를\n달성하고 청크에서 내재된 지식을 포괄적으로 추출합니다.\n\n\n요약(Hayashi et al., 2021) 및 다중 단계 추론(Ho et al., 2020)과 같은 복\n잡한 작업을 해결하기 위해 최근 연구는 기존 RAG 모듈을 활용하여 이러\n한 과제를 공동으로 해결하는 고급 조정 체계 개발에 초점을 맞추고 있다.\n\nIter-RetGen (Shao et al., 2023)과 DSP (Khattab et al., 2023)는 검색-읽기\n반복을 사용하여 생성 응답을 다음 라운드 검색의 컨텍스트로 활용합니\n다. FLARE (Jiang et al., 2023)는 신뢰도 기반 능동 검색 메커니즘을 제안\n합니다. 본 연구의 접근법은 컨텍스트 인식 추론 프로세스를 활용하는 반\n복 기반 파이프라인을 채택하여, 각 반복에 대한 후속 질문을 적응적으로\n구성하고 복잡한 작업의 검색 및 추론 난이도를 낮춥니다.\n\n\n**2.2.** **다중** **단계** **질의응답**\n다중 정보 기반 질문 답변(MHQA)(Yang et al., 2018)은 종종 서로 다른\n출처에 흩어져 있는 여러 정보 조각에 대한 추론을 요구합니다. 이 작업은\n관련 정보를 검색할 뿐만 아니라 검색된 조각들을 효과적으로 결합하고\n추론하여 올바른 답에 도달해야 하므로 독특한 도전 과제를 제시합니다.\n\n기존 MHQA의 그래프 기반 접근법은 그래프를 구축하고 그래프 신경망\n(GNN)을 통해 추론하여 답변을 예측하는 방식으로 문제를 해결해왔다\n(Qiu 외, 2019; Fang 외, 2020). LLM의 등장과 함께 최근 그래프 기반 방\n법(Li & Du, 2023; Panda et al., 2024; Liang et al., 2024)은 검색을 위한\nKG 구축과 LLM을 통한 응답 생성을 위해 진화했습니다. 그러나 고품질\n의 도메인 특화 KG 구축은 비용이 많이 들고, 구조화된 트리플 형식은 문\n맥 표현에 본질적인 제약을 가해 표현력을 제한합니다. Self-RAG(Zhang\net al., 2024a)와 빔 검색(Asai et al., 2023)은 MHQA를 감독형 문제로 다\n루어 라벨링된 데이터와 추가 훈련을 필요로 합니다.\n\n\n또 다른 방법론 계열은 다중 경로 질문을 체인형 경로(Trivedi et al.,\n2023; Khattab et al., 2023; Feng et al., 2023; Xu et al., 2024) 또는 트리\n형 경로(Zhang et al., 2024b; Jiapeng et al., 2024; Cao et al., 2023)를 따\n라 하위 질문으로 분해합니다(그림 1 참조). 하위 질문들은 순차적 청크\n재구성(chunk re\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n_그림 3._ KAR [3] -RAG 워크플로우 개요. 원자화기를 통한 지식 원자화, 쿼리 제안기를 활용한 지식 인식 작업 분해, 원자 검색 및 원자 선택기를 보여줍니다. 질의 제안기는 원본\n질문과 참조 문맥을 기반으로 원자적 질의 제안을 생성한다. 이 제안들은 관련 원자적 태그를 검색하는 데 사용되어 검색된 원자적 쌍을 생성한다. 원자 선택기는 가장 관련\n성 높은 쌍과 해당 청크를 선택하며, 이는 후속 반복에서 작업 분해를 위해 참조 문맥에 추가된다. 원자 선택기가 추가 정보가 필요하지 않다고 판단하고 원자적 쌍이 선택되\n지 않으면, 원본 질문과 참조 문맥이 생성기로 전달되어 최종 답변을 생성한다.\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-3-0.png)\n\n검색된 결과가 이후 추론 과정을 촉진하는 검색 방식이다. 체인형 분해에\n서는 단일 하위 질문이 생성되며, 그 답변 가용성이 보장되지 않아 답변\n실패로 이어질 수 있다. 반면 트리형 분해는 다중 추론 경로 탐색을 요구하\n며, 최종 응답 생성을 위해 정교한 증거 검증 및 융합이 필요하다. 본 접\n근법은 원자 검색의 관련성을 기반으로 제안된 질의 집합에서 하위 질문\n을 상호작용적으로 선택함으로써 추론 경로를 탐색한다. 이는 업데이트\n된 맥락을 활용하고 이용 가능한 지식을 가진 질의 제안을 선택함으로써\n유연한 분해를 가능하게 한다.\n\n\n**3.** **방법론**\n**3.1.** **예비**\nRAG 시스템에서 텍스트 코퍼스는 문서 청크 집합으로 분할되며, 이를 D\n= { _d_ 1 _, d_ 2 _, ..., d_ _n_ }로 표기한다. 여기서 _d_ _i_ 는 i번째 문서 청크를 나타낸다. 원\n본\n질문은 _q_ 로 표시되며, 이에 대응하는 정답은 _a_ 로 표현된다. 검색 단계에\n서는 질문 q와 각 문서 조각 _d_ _i_ 간의 유사도를 평가한 후, 상위 _k_ _개_ 관련성\n높은 조각을 검색 결과로 선정하여 후속 생성 작업의 기초를 마련한다.\n\n\nR : topk Sim( _q, d_ _i_ ) → _D_ _[q]_ (1)\ndi∈D\n여기서 검색기 R은 유사도 함수 _Sim_ (·)을 기반으로 상위 _k_ _개_ 관련성 높은\n청크 _D_ _[q]_ 를 선택합니다. 마지막으로 원본 질문과 검색된 청크는\n대규모 언어 모델에 입력되어 답변을 생성하며, 이를 다음과 같이 표기합\n니다.\n\naˆ = LLM( _q, D_ _[q]_ ). 고급 RAG 시스템에서는 쿼리\n4\n재작성을 통해 질문과 검색 대상 챕터 간의 의미적 간극을 해소합니다.\n\n재작성된 쿼리는 qˆ = _f_ _re_ ( _q_ )로 표현됩니다. 고급 RAG 시스템의 워크플로\n는 다음과 같이 추가 개선됩니다.\n\n\naˆ = LLM( _q, D_ _[q]_ [ˆ] ) _,_ 여기서 _D_ _[q]_ [ˆ] = R(qˆ _,_ D)  (2)\n이러한 개선을 통해 시스템은 쿼리를 관련 문서 청크와 더 잘 정렬하여 검\n색 정확도와 답변 생성을 향상시킬 수 있습니다. 그러나 복잡한 다중 단계\n질문을 처리하는 것은 여전히 어려운 과제입니다. 이러한 질문은 종종 여\n러 청크에 걸친 추론과 여러 검색 및 생성 단계를 통한 정보 통합을 요구하\n는데, 이는 단일 패스로는 완전히 포착하기 어려울 수 있습니다.\n\n\n**3.2.** **프레임워크**\n복잡한 다중 단계 질문을 해결하기 위해, 지식 인식 **이중** 재작성 및 추론\n( Knowledge-Aware **dual** Rewriting and Reasoning)을 적용한 향상된\nRAG 시스템 [인] [ KAR(3)] [을] 제안한다. 이 시스템은 반복적인 검색-추론-생성 메\n커니즘을 활용하여 관련 정보를 점진적으로 수집하고 증분적 컨텍스트에\n대한 점진적 추론을 가능하게 한다. 제안된 워크플로의 개요는 그림 3에\n제시되어 있습니다. 본 프레임워크에서는 원시 데이터 청크를 지식 원자\n화기를 통해 원자 태그로 분해하여 후속 검색을 위한 원자 지식 기반을 구\n축합니다. 질문 역시 쿼리 제안기를 통해 원자화되어 원자 쿼리 제안들을\n생성하며, 이는 지식베이스에서 관련 원자 태그를 검색하는 데 활용됩니\n다. 의미적 격차를 해소하고 지식 정렬을 개선하기 위해 덩어리와 질문 모\n두 재작성됩니다. 이후 원자 검색기가 각 원자 쿼리 제안에 대해 _상위_ _k_ _개_\n_의_ 원자 쌍을 선별합니다. 검색된 원자 쌍을 기반으로 추론기 역할을 하는\n원자 선택기가 가장 유용한 원자 쌍을 식별합니다.\n\n\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n문제 해결을 위해 해당 원시 청크를 컨텍스트에 추가합니다. 이 컨텍스트\n는 다음 반복에서 작업 분해를 위해 원본 질문과 통합됩니다. 반복 과정은\n저품질 질의 제안 생성이나 관련 원자 태그 후보 부족으로 적합한 원자 태\n그를 검색하지 못할 경우 조기에 종료될 수 있습니다. 이때 원본 질문과\n컨텍스트는 생성기로 전달되어 최종 답변을 생성합니다.\n\n\n**3.3.** **지식** **원자화**\n분할된 텍스트는 종종 다각적인 정보를 포함하며,\n일반적으로 특정 작업을 처리하기 위해서는 하위 집합만 필요합니다. 통\n합된 전통적인 정보 검색 방법들은\n**알고리즘 1** 지식 인식 분해를 통한 과제 해결\n1: 컨텍스트 초기화 C 0 ← _**ϕ**_\n2: **for** _t_ = 1 _,_ 2 _, . . ., N_ **do**\n3: 원자적 질의 제안 생성 **qˆ** _[t]_ ← _f_ _p_ ( _q,_ C _t_ - 1 )\n4: 각 원자 쿼리에 대해 _상위_ _k_ _개의_ 원자 쌍을 검색\n지식베이스로부터 제안\n_t_ KB\n_P_ **[ qˆ]** _t_\n←−− R _원자_ ( **qˆ** _, f_ _a_ (D))\n5: 추가 정보가 불필요한 경우 가장 유용한 원자 태그 또는 _None_\n선택\n_qksls_ ← LLM( _q,_ C t−1 _[, ][P]_ **qˆ** _[t]_ )\n단일 청크 내의 모든 정보를 날짜로 표시하는 것은 6: _q_ _kl_ _s s_ _None_ **이면**\n필요한최근 연구에서는정확한 정보를트리플효율적으로지식 추출을검색하는탐구해 왔습니다데 도움이. 되지 않을 수 있습니다. 7: _Ct_ [ ← ] Ct−1\n_s s_\n_Ct_ [ ← ] Ct−1\n챕터화된 텍스트로부터 에지 유닛을 추출하고 효율적인 정보 검색을 용이\n하게 하기 위해 지식 그래프를 구축하는 방법(Edge et al., 2024; Panda et\nal., 2024). 그러나 이러한 지식 그래프 구축은 비용이 많이 들며, 내재된 지\n식이 항상 완전히 탐색되지는 않을 수 있다. 문서에 내재된 지식을 보다\n효과적으로 제시하기 위해, 우리는 지식 추출을 위한 원본 문서의 원자화\n(atomizing)를 제안하며, 이를 _지식원자화_ ( _Knowledge Atomizing_ )라고 명\n명한다. 이 접근법은 LLM의 문맥 이해 및 콘텐츠 생성 능력을 활용하여\n각 문서 청크 내의 원자적 지식 조각을 자동으로 태깅한다.\n\n\n원자적 지식의 표현 방식은 다양할 수 있다. 선언적 문장이나 주어-관계\n목적어 튜플을 활용하는 대신, 우리는 질문을 지식 인덱스로 사용하여 저\n장된 지식과 질의 간의 간극을 더욱 좁힐 것을 제안한다. 지식 원자화 과\n정에서 문서 청크를 LLM에 컨텍스트로 입력하고, 주어진 청크로 답할 수\n있는 관련 질문을 최대한 많이 생성하도록 요청합니다. 이렇게 생성된 원\n자 태그는 주어진 청크와 함께 저장됩니다. 지식 원자화기는 각 청크에 원\n자화 연산을 적용합니다.\n\n\n_f_ _a_ ( _d_ _k_ ) = { _q_ _k_ 1 _, q_ _k_ 2 _,_           - · · _, q_ _km_ } (3)\n원자 태그는 원자화기에 의해 모든 청크에 대해 생성되어 원자 지식 기반\n을 형성하며, 이는 KB = 로 표시됩니다.\n\n{ _f_ _a_ ( _d_ _k_ ) _, d_ _k_ }. 지식 원자화의 예는 그림 4(a)에\n원자 태그는 덩어리 내에 포함된 지식의\n각 덩어리에는 여러 원자 태그가 부여되므로, 원자 쿼리를 통해 관련 원자\n태그를 찾아내고, 이를 통해 연관된 참조 덩어리로 연결될 수 있다.\n\n\n**3.4.** **지식** **기반** **작업** **분해**\n복잡한 다중 단계 질문을 해결하려면 종종 여러 지식 조각을 통합해야 하\n며, 이는 암묵적으로\n5\n8: **분해**\n9: **else**\n10: _q_ _k_ 에 해당하는 관련 청크 _c_ _[t]_ 를 가져옵니다 _._\n\n11: 컨텍스트 C _t_ 업데이트 ← C _t_ - 1 ∪ _c_ _[t]_\n12: **end if**\n13: **for 종료**\n14: 답변 생성 aˆ ← LLM( _q,_ C _t_ )\n_s s_\n원본 질문을 검색을 위한 여러 순차적 또는 병렬적 원자 태그로 분해하는\n능력이 요구됩니다. 우리는 이 작업을 _작업분해_ (Task _Decomposition_ )라\n고 부릅니다. 추출된 원자 지식과 원본 청크를 결합하여 원자 지식 기반을\n구축합니다. 작업이 분해될 때마다 원자 지식 기반은 이용 가능한 지식에\n대한 통찰력을 제공하여 지식 인식 작업 분해를 가능하게 합니다. 우리는\n_지식인식작업분해_ 워크플로를 설계하며, 작업 해결을 위한 완전한 알고\n리즘은 알고리즘 1에 상세히 기술되어 있고, 예시는 그림 4(b)에 설명되어\n있습니다.\n\n초기 참조 컨텍스트 C (0) 은빈 집합으로 초기화된다. 첫 번째 반복에서 작업\n분해는 원본 질문만을 기반으로\n원본 질문만을 기반으로 원자적 질의 제안을 생성합니다. 반복이 진행됨\n에 따라 t번째 반복 시점에 누적된 컨텍스트\nC _t_ - 1 으로 표기되며, 이전 반복에서 검색된 청크들로 구성됩니다.\n추출된 청크들로 구성됩니다. t번째 반복 동안, 쿼리 제안기는\n원본 질문과 누적된 컨텍스트를 기반으로 원자적 질의 제안을 생성한다.\n\n\n_f_ _p_ ( _q,_ C _t_           - 1 ) = {qˆ _[t]_ _,_ qˆ _[t]_ _,_           - · · _,_ qˆ _[t]_ } (4)\n1  2 _n_\n쿼리 제안기 _f_ _p_ (·)는 LLM 또는 학습 가능한 구성 요소로 구현될 수 있다.\n우리는 LLM을 활용하여 작업 완료에 잠재적으로 유익한 쿼리 제안(qˆ(t)\n={qˆ(t) })을 생성한다. 이 과정에서 선택된 참조 청크 C(t) (−) (1) 이 제공\n된다.\n\n작업 완료에 잠재적으로 유익한 쿼리 제안( **qˆ** _[t]_ ={qˆ _[t]_ })을 생성합니다. 이\n과정에서 선택된 참조 청크 C _t_ - 1 은 컨텍스트로 _i_ 제공됩니다.\n\n\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n_그림 4._ KAR [3] -RAG 사례의 예시: (a) 지식 원자화 사례, (b) 지식 인식 작업 분해를 통한 RAG 사례. 반복 과정이 진행됨에 따라, 원자적 검색 및 선택을 통해 관련 정보 조각을\n축적함으로써 참조 컨텍스트가 풍부해진다. 컨텍스트가 확장됨에 따라 생성되는 원자적 쿼리 제안의 수는 더 이상 제안이 생성되지 않을 때까지 감소한다. 이후 반복 과정이\n종료되고, 결합된 질문과 컨텍스트를 활용하여 최종 응답을 생성한다.\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-5-0.png)\n\n이미 알려진 지식과 연결된 제안 생성을 피하기 위한 텍스트. 결과적으로\n쿼리 제안은 반복마다 진화하며, 업데이트된 컨텍스트에 적응하고 컨텍스\n트 내 청크를 넘어 추가 지식을 탐색하는 것을 목표로 합니다. 각 원자적 쿼\n리 제안에 대해, 지식베이스에서 _상위_ _k_ _개_ 관련 원자적 태그 후보와 그 출\n처 청크를 검색합니다. 원자적 검색 과정은 다음과 같습니다:\n\n\n\n해당 임베딩 간의 유사도를 기반으로 _상위_ _k_ _개_ 원자 태그를 검색하며, 제\n안된 원자 태그와의 유사도가 지정된 임계값 _δ_ 이상일 경우에만 포함됩\n니다. 원본 질문, 누적된 컨텍스트, 검색된 원자 쌍 목록을 바탕으로 원자\n선택기는 LLM을 활용하여 문제 해결에 가장 유용한 원자 쌍을 선별합니\n다.\n\n\n\n_t_ KB qˆ _[t]_\nR _atom_ : topk Sim(qˆ ) −−→ _i_\n) = (qˆ _[t]_ _, q_ _, d_ ) (6)\n_s_ _ksls_ _ks_\n_i_ (5) LLM( _q,_ C t−1 _[, ][P]_ _**q**_ **ˆ** _[t]_\n_qkl∈fa_ ( D )\nSim(qˆ _i_ _, q_ _kl_ ) −−→ _P_\n원자 검색기(R _atom_ 으로 표기)는 각 원자 쿼리 제안에 대해 검색된 원자 쌍\n집합을 생성하며, 이는 _P_ _[q]_ [ˆ] _[i ]_ = {(qˆ _[t]_ _, q_ _kl_ _, d_ _k_ )}로 표현된다. 모든 검색된 원자 선택기(S _atom_ 으로 표시)는 새로 제안된 원자 쌍에서 선택된 원자 쌍의\n_t_ 관련 원시 청크를 추가로 검색합니다.\n\n_i_ _i i_ _i_\n원자 쌍으로 선택된 원자 쌍의 관련 원시 청크를 추가로 검색합니다. 각 원자 쿼리 제안에서 추출된 이 청크들은 t번째 반복에서 추가된 컨텍스트 _c_ _t_ 로\n표시됩니다. _(t)를_\n게이트 처리하여 전체 집합 _P_ _**[q]**_ **[ˆ]** 생성한다. 코사인 chunk는 _d_ 에 대응합니다 에 해당하는 청크를 추출합니다.\n\n\n\n_t_\n_i_ _i i_ _i_\n_(t)를_\n생성한다. 코사인 chunk는 _d_ 에 대응합니다 에 해당하는 청크를 추출합니다.\n_ks_\n6\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n추출 과정은 다음 공식으로 표현될 수 있다.\n\n\n_c_ _t_ = S _atom_ (R _atom_ ( _f_ _p_ ( _q,_ C _t_     - 1 ) _, f_ _a_ (D)))) (7)\n이렇게 검색된 청크는 다음 분해 단계의 참조 컨텍스트로 통합되며, 다음\n과 같이 표현됩니다.\n\nC _t_ = _c_ _t_ ∪ C _t_ - 1 . 지식 인식 분해는 이를 통해\n최대 N회까지 반복할 수 있으며, 여기서 _N_ _은_ 계산 비용을 제어하기 위\n해 설정된 하이퍼파라미터입니다.\n\n반복 과정은 저품질 쿼리 제안 생성이나\n관련 원자 태그 후보가 존재하지 않아 적합한 원자 태그를 검색하지 못할\n경우, 또는 LLM이 누적된 지식이 작업 완료에 충분하다고 판단할 경우\n프로세스가 중단될 수 있습니다. 이러한 조기 종료 메커니즘은 모든 반복\n을 완료하기 전에 프로세스를 종료할 수 있게 하여 계산 비용을 절감합니\n다.\n\n종료 메커니즘은 모든 반복을 완료하기 전에 프로세스를 종료할 수 있게\n하여 계산 비용을 절감합니다.\n\n정확도를 저하시키지 않으면서. 마지막으로, 누적된 컨텍스트 C _t_ 는 1행\n에서 주어진 질문 _q_ _에_ 대한 답변 aˆ를 생성하는 데 활용된다.\n\n\n지식 인식 분해는 학습 가능한 구성 요소일 수 있다는 점을 언급할 가치가\n있습니다. 각 전문 지식 기반에 대해, 우리는 각 분해 단계에서 수집된 데이\n**4. 1. 실험** **설정**\n**방법론** 제안된 지식 인식 분해 접근법의 성능을 철저히 평가하기 위해,\nLLM을 활용한 과제 해결을 위한 다양한 전략을 대표하는 여러 기준 방법\n을 선정하였다. 추가적인 컨텍스트 없이 기본 LLM의 내재적 추론 능력과\n내장 지식을 평가하기 위해 **제로샷 CoT** (Kojima et al., 2022)를 포함하였\n다. 검색을 통해 외부 지식을 도입하는 **Naive RAG** (Lewis et al., 2020)는\n증강된 지식의 점진적 이점을 평가하기 위한 벤치마크 역할을 합니다.\n\n**Self-Ask** 프레임워크(Press et al., 2023)는 반복적 질문 분해 및 답변 전략\n이 작업 수행에 미치는 영향을 조사하기 위해 활용됩니다. 다단계 질문 처\n리를 위한 근거를 반복적으로 생성하는 **IRCoT** (Trivedi et al., 2023)와 최\n근 응답을 반복적으로 검색 쿼리로 활용하여 응답 품질을 개선하는 **Iter-**\n**RetGen** (Shao et al., 2023)은 최근 응답을 검색 쿼리로 반복 활용하여 응\n답 품질을 개선하며, **ProbTree** (Cao et al., 2023)는 복잡한 QA를 검색 트\n리로 명시적으로 분해합니다. 이들 역시 성능 비교를 위해 수행되었습니\n다. 방법론에 대한 상세한 설명은 부록 A.4에 제시됩니다.\n\n\n본 실험에서는 GPT-4(1106-Preview)와\n터를 활용할 수 있습니다.\n\n\n\n**qˆ** _[t]_ Llama-3.1-70B-Instruct를 사용했습니다.\n위치 반복—구체적으로는 ( _q, a,_ aˆ _,_ {qˆ _[t]_ _, c_ _[t]_ _,_ **qˆ** _[t]_ _, P_ _,_ C _t_ }). . 제4.2절에 제시된 실험을 위해,\n_s_\n이렇게 훈련된 제안자는 추론 과정에서 원자 쿼리 _q_ _[t]_ 를 직접 제안할 수 있\n습니다. 이는 알고리즘 1의 1~1행이 이 학습된 제안자에 대한 단일 호출\n로 대체될 수 있음을 의미하며, 이를 통해 추론 시간과 계산 비용을 모두\n줄일 수 있습니다. 효율적인 쿼리 제안자 훈련에 대한 탐구는 향후 연구\n과제로 남겨둡니다.\n\n\n**4. 평가** **및** **지표**\nKAR [3] 은 특수화된 도메인의 과제를 해결하기 위해 제안되었으므로, 우리\n는 중국 법률 벤치마크인 LawBench와 호주 공개 법률 QA 벤치마크 모두\n에서 실험을 수행했습니다. 실험 결과 KAR [3] 은 모든 벤치마크에서 기존\n방법 대비 상당한 성능 향상을 보였으며, 생성 작업 정확도는 LawBench\n에서 90.12%, 호주 법률 QA에서 98.59%에 달했습니다. 해당 법률 벤치\n마크 및 실험 결과에 대한 상세 설명은 부록 A.8에서 확인할 수 있습니다.\n\n\n제안된 접근법을 기준 방법과 더 잘 비교하기 위해, 본 절에서는 널리 인\n정받는 오픈 도메인 벤치마크에 집중한다. 4.1절과 4.2절은 각각 실험 설\n정과 주요 실험 결과를 개괄한다. 제거 연구는 4.3절에서 논의된다. 또한\n내용 제약으로 인해 비용 분석과 사례 연구는 부록 A.5와 A.6에 포함되었\n다.\n\n\n7\n반복 횟수 _N_ _은_ Self-Ask with Retrieval, IRCoT, Iter-RetGen 및 KAR [3] 에 [대]\n해 5로 설정됩니다. 또한 원자 검색기는 _k_ = 4 및 _δ_ = 0 _._ 5로 초기화됩니다.\n\n검색 및 LLM에 대한 하이퍼파라미터의 포괄적인 목록은 부록 A.3에서\n확인할 수 있습니다. 간결함을 위해 Llama-3.1-70B-Instruct는 이후 내용\n에서 Llama 3으로 약칭됩니다.\n\n\n**지표** 기존 벤치마크와의 일관성을 보장하기 위해, 본 실험 평가에서는 **F1**\n점수를 표준 지표로 채택합니다. 단순 어휘 일치 이상의 수준에서 응답과\n의도된 정답 간의 정합성을 보다 정확히 평가하기 위해, _GPT-4를_ 활용한\n새로운 평가 지표를 도입합니다. 이 과정에서 _GPT-4는_ 평가자 역할을 수\n행하며, 질문과 정답 라벨에 대한 응답의 정확성을 평가합니다. 이 지표\n를 **정확도(Acc)** 라고 명명합니다. 샘플 세트에 대한 수동 검증을 통해\n_GPT-4의_ 판단이 인간 평가자와 완전히 일치함을 확인하여 이 지표의 신\n뢰성을 입증했습니다. 또한 부록 A.4에서 완전 일치(EM), 재현율\n(Recall), 정밀도(Precision)를 포함한 전체 평가 결과를 확인할 수 있습니\n다.\n\n\n**데이터셋** 기준 방법과의 비교를 위해 본 연구의 평가는 널리 인정받는 세\n가지 다중 홉 데이터셋에 초점을 맞췄습니다: HotpotQA (Yang et al.,\n2018), 2WikiMulti-HopQA (Ho et al., 2020), 그리고 MuSiQue (Trivedi\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\net al., 2022). 이 데이터셋들에 대한 간략한 소개는 부록 A.1에서 확인\n할 수 있습니다. 각 데이터셋에 대해 무작위로 샘플링합니다.\n\n\n8\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n_표 1._ GPT-4를 활용한 다중 홉 QA 데이터셋 성능 비교. 최고 성능은 굵게, 차순위는\n밑줄 처리.\n\n\n_표 2._ Llama 3 기반 다중 홉 QA 데이터셋 성능 비교. 최고 성능은 굵게, 차순위는 밑\n줄 표시.\n\n\n개발 세트에서 500개의 QA 데이터를 무작위로 추출하며, 무작위성을 보\n장하기 위해 질문 유형과 홉 수를 고려하지 않습니다. 추출된 모든 QA 데\n이터의 컨텍스트 문단을 각 벤치마크별로 단일 지식 기반으로 통합하여\n더 복잡한 검색 시나리오를 생성합니다. 이 설계 선택은 모델의 작업 분해\n및 관련 컨텍스트 검색 능력을 엄격하게 평가하기 위한 것입니다. 간결성\n을 위해 2WikiMultiHopQA는 2Wiki로 약칭합니다.\n\n\n**4.2. 주요** **결과**\n표 1과 표 2 [(1)][에서] 확인할 수 [있듯이], 본 접근법은 GPT-4와 Llama 3 모두에서\n모든 데이터셋에 걸쳐 우수한 성능을 달성합니다. 특히 GPT-4의 경우 약\n+1 _._ 4(1 _._ 6%), +2 _._ 2(2 _._ 8%), 그리고\nHotpotQA, 2Wiki, MuSiQue의 두 번째로 우수한 결과 대비 정확도에서\n각각 +7 _._ 0(12 _._ 6%), +3 _._ 4(4 _._ 4%), +10 _._ 1(20 _._ 4%)의 향상을 달성했습니다. 마\n찬가지로 Llama 3을 사용했을 때도 각각 +0 _._ 2(0 _._ 2%), +3 _._ 4(4 _._ 4%),\n+10 _._ 1(20 _._ 4%)의 증가를 보였습니다. 이러한 향상은 통계적으로 유의하며,\nKAR(3)이 복잡한 QA 작업을 처리하는 데 있어 견고함을 입증합니다.\n\n+10 _._ 1(20 _._ 4%)의 정확도 향상을 각각 달성했습니다. 이러한 향상은 통계적\n으로 유의하며, KAR [3] 이 복잡한 QA 작업을 처리하는 데 있어 견고함을\n강조합니다.\n\n\n제안된 접근법인 KAR [3] 은 지식 기반 작업 분해를 강조하며, Self-Ask에서\n사용된 주어진 데모에 의존하는 자발적 분해 메커니즘과 차별화됩니다.\n\n이는 이용 가능한 지식을 인지하며 분해를 수행하고, 원자 태그를 중간 매\n9\n개체로 효과적으로 활용하여\n1 Llama 3 엔드포인트에서 로그 확률(logprobs)을 얻는 데 문제가 발생했기 때문에\n, Llama 3을 사용한 ProbTree 실험은 향후 연구 과제로 남겨둡니다.\n\n\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n의미론적 간극. 알고리즘 1에 상세히 기술된 \"제안 후 선택\" 프레임워\n문\nMuSiQue\nF1 Acc\n방법\nR.와 함께하는실험 결과는자기 질\n문\n**4.3. 제거** **연구**\n\n|HotpotQA<br>의 잠 F1재적 오정류확도를|2Wiki<br>수정F할1 기회를Ac c제공|\n|---|---|\n|부록 A.6의 사례(a<br> KAR3이 다양한 모<br>보여주며, 이는 복잡<br>한 ~~모델에~~ 대~~한 견~~고<br>43.94<br>53.60<br>72.67<br>82.60<br>71.40<br>80.00<br>67.30<br>81.00<br>75.27<br>86.60<br>62.41<br>73.40<br>**76.48**<br>**88.00**|)에서 확인할 수 있<br>델을 사용하는 다른 <br>한 추론 시나리오<br>성과 적응성을 입증<br>41.40<br>43.87<br>59.74<br>62.80<br>69.06<br>75.00<br>63.83<br>70.40<br>67.21<br>73.60<br>69.42<br>80.00<br>**75.00**<br>**82.20**|\n\n\n\n_N_ **의** **선택.** 우리는 먼저 반복 상한 _N_ _을_ 1 _,_ 2 _, ..._, 10으로 설정하여 실험\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|,<br>표방는법 부록 A.4의|5<br>HotpotQA<br>표 8에F1서 확인정할확도 수|.<br>2Wiki<br>있습F1니다. 세A c데c 이|MuSiQue<br>터셋F1 모두에Acc|\n|---|---|---|---|\n|지원 사실 회상률<br>이러한 패턴은 특히 <br>보가 필요할 때, 추<br>근법의 능력을 강<br>제로샷 CoT<br>순진한 RAG<br>자기 질문 w/ R.<br>IRCoT<br>Iter-RetGen<br>KAR3**  (우리)**|과 답변 정확도 모두<br> 문제 해결에 더 상<br>가 ~~반복~~을 ~~통해 출~~<br>조한다.<br>40.10<br>54.80<br>70.78<br>84.20<br>70.25<br>83.00<br>74.59<br>88.00<br>72.23<br>85.20<br>**75.27**<br>**88.20**|일관된 상승 추세<br>세하고 문맥적으로 <br>력~~을 점~~진적~~으로 ~~향<br>38.54<br>43.20<br>56.58<br>62.20<br>66.25<br>74.00<br>69.49<br>77.60<br>59.21<br>65.00<br>**72.79**<br>**81.00**|를 보입니다<br> 관련성 높은 <br>상~~시키~~는 ~~본 ~~<br>15.69<br>19.80<br>32.53<br>36.40<br>38.19<br>44.20<br>43.12<br>49.60<br>37.16<br>40.40<br>**50.68**<br>**59.70**|\n\n\n\n또한, 반복 횟수와 지원 사실 회상률의 관찰된 증가 사이의 관계를 검\n토한 결과, HotPotQA 및 2Wiki 데이터셋의 경우 회상 곡선이 네 번째\n반복까지 현저한 증가를 보인다는 점을 확인했습니다. 반면 MuSiQue\n데이터셋의 회상률은 부록 A.1에서 언급된 바와 같이 질문당 최대 홉\n수가 4로 제한되어 있음에도 불구하고, 이 지점을 넘어선 후에도 계속\n해서 급격히 상승합니다. 이러한 차이는 KAR [3] 이 제한된 반복 횟수 내\n에서 관련성 있고 유용한 정보를 검색하는 데 능숙하지만 여전히 한계\n가 있음을 시사합니다: KAR [3] 은 사용된 대규모 언어 모델(LLM)의 추\n론 능력에 의존하며, 특히 질문의 복잡성이 증가함에 따라 필요한 정보\n를 완전히 포착하기 위해 추가 반복이 필요할 수 있습니다.\n\n\n알고리즘 1은 조기 종료 메커니즘을 포함하고 있지만, _N_ _값이_ 높아질\n수록 계산 요구량이 증가하는 것은 피할 수 없다. 따라서 4.2절의 실험\n에서는 계산 자원과 성능 향상 기대치 사이의 섬세한 균형을 이루기 위\n해 최대 홉 수보다 약간 높은 값인 _N_ = 5를 선택하였다.\n\n\n_그림 5._ 반복 횟수에 따른 보조 사실 회상률(파란색)과 답변 정확도(주황색).\n\n\n\n10\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-9-0.png)\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|표 3. KAR3구성 요소에 대한 제거 연구.|.|Col4|Col5|\n|---|---|---|---|---|\n|변수 구성 요소|수정|HotpotQA<br>F1<br>정확도|2Wiki<br>F1<br>Acc|MuSiQue<br>F1<br>Acc|\n|지식 분무기<br>쿼리 제안자<br>원자적 리트리버<br>원자 선택기|질문으로 분해 → 일반 텍스트로 분해<br>여러 쿼리 제안 → 단일 쿼리 제안<br>(원자 태그, 청크) 쌍 검색 → 청크 검색<br>원자 태그로 청크 선택 → 청크 직접 선택|73.05<br>84.50<br>75.06<br>85.60<br>76.31<br>86.60<br>72.80<br>83.20|64.18<br>69.80<br>70.19<br>76.40<br>67.14<br>72.40<br>61.65<br>65.80|50.72<br>55.20<br>49.67<br>52.20<br>49.05<br>53.00<br>49.31<br>53.40|\n|KAR3**  (우리)**|KAR3**  (우리)**|76.48<br>88.00|75.00<br>82.20|57.86<br>62.60|\n\n\n\n**접근** **방식** **구성** **요소의** **기여도.** KAR [3] 은 네 가지 핵심 구성 요소로 이루어\n져 있습니다: 지식 분해기, 질의 제안기, 원자 검색기, 원자 선택기. 우리는\n이러한 구성 요소들의 개별적 및 집단적 기여도를 확인하기 위해 제거 실\n험을 수행합니다.\n\n\n이러한 구성 요소를 하나씩 수정하여 여러 방법 변형을 도입함으로써: (1)\n지식 원자화기(knowledge at-omizer)의 경우, 원자적 태그 표현을 원자적\n질문에서 일반 텍스트 문장으로 변경하여 원자적 지식 표현의 영향을 탐\n구합니다; (2) 질의 제안기(query pro-poser)의 경우, 원래 설계된 다중 제\n안 메커니즘의 장점을 평가하기 위해 단일 질의만 생성하도록 제한합니\n다; (3) 원자 검색기(atomic retriever)의 경우, (원자 태그, 청크) 쌍 대신\n청크 단위로 검색하도록 구성 요소를 수정했습니다; (4) 원자 선택기\n(atomic selector)의 경우, 원자 태그로 청크를 필터링하는 대신 청크를 직\n접 선택하는 변형을 구현했습니다. 이 설정에서는 원자 태그가 존재하지\n않으므로, 이후 컨텍스트 선택은 청크 자체에 의해 결정됩니다.\n\n\n표 3의 결과에서 알 수 있듯이, 구성 요소들의 개별 기여도를 평가하였다.\n\n지식 분해기, 질의 제안기, 원자 검색기 및 원자 선택기를 대체 구성 요소\n로 교체할 경우 세 데이터셋에서 각각 최대 15.1%, 16.6%, 15.3%, 16.2%\n의 정확도 저하가 발생함을 관찰했습니다. 이러한 제거 연구는 최적의 검\n색 성능과 일관된 추론 경로를 달성하기 위해 설계된 각 구성 요소가 필수\n적임을 시사합니다.\n\n\n**한계** **논의.** 복잡한 질문에 대한 핵심 정보 추출을 위한 추가 반복의 필요\n성 외에도, 부록 A.4의 표 9에 상세히 기술된 GPT-3.5 실험은 대규모 언\n어 모델(LLM)의 추론 능력에 의존하는 데 한계가 있음을 시사합니다.\n\nGPT-3.5를 사용한 KAR [3] 의 성능은 IRCoT 및 Self-Ask w/ Retrieval과 같\n은 방법들을 크게 능가하지 못하며, 때로는 Self-Ask w/ Retrieval에 비해\n부족하기도 합니다. 이는 KAR [3] 의 성공이 고급 추론 기술과 복잡한 지시\n를 견고하게 따르는 능력에 달려 있음을 강조합니다.\n\n\n오픈소스 모델 Llama 3을 사용한 실험 결과는 주목할 만한 성능 향상을\n보여줍니다.\n\n\n11\n기준 방법들에 비해, 부록 A.5의 표 10에 상세히 기술된 바와 같이 본 접\n근법은 평가된 일부 방법들에 비해 더 높은 토큰 소비량을 요구합니다. 구\n체적으로 MuSiQue 데이터셋에서는 ProbTree 및 IR-CoT보다 적은 토큰\n을 사용하지만, 검색을 동반한 Iter-RetGen 및 Self-Ask보다 더 많은 토큰\n을 사용합니다. 이러한 증가된 토큰 사용량은 GPT-4와 같은 독점 모델로\n구현할 경우 더 높은 비용으로 이어질 수 있습니다.\n\n\n**5.** **결론**\n우리는 전문 데이터셋 내에서 지식 추출 및 근거 형성을 개선하기 위해 설\n계된, 지식 인식 이중 재작성 및 추론 기능으로 강화된 고급 RAG 시스템\n을 제시한다. 광범위한 실험의 포괄적 결과는 특히 다중 단계 질문이 포함\n된 벤치마크 시나리오에서 본 접근법의 효용성을 입증한다. 향후 연구에\n서는 문맥 기반 학습(Wei et al., 2022)을 통합하여 질의 제안자에게 적합\n한 데모를 적응적으로 선택함으로써 시스템의 숙련도를 개선할 계획입니\n다. 이는 지식 인식형 질문 재작성 능력을 한층 강화할 것입니다. 또한, 샘\n플 질문으로부터의 피드백을 통합할 수 있는 지식 인식 원자화기 개발에\n관심을 두고 있으며, 이를 통해 가장 유익한 유형의 원자적 지식에 대한\n이해도를 향상시킬 수 있을 것입니다.\n\n\n**영향력** **진술**\n본 접근법은 기존 대규모 언어 모델을 활용하여 추가 훈련을 피하고 새로\n운 편향성 도입을 최소화하며, 사전 처리된 지식 기반에서 응답을 생성해\n신뢰성을 보장합니다. 이 과정은 질문 분해의 각 단계를 기록하여 투명하\n고 해석 가능한 추론 체인을 생성하며, 민감한 환경에서 데이터 보안을 강\n화하기 위해 비공개로 배포될 수 있습니다. 이 접근법은 법률 연구, 의료\n진단, 기술 지원과 같은 분야에서 검색 강화 생성(RAG) 기술의 활용을 발\n전시켜 의사 결정의 질과 효율성을 향상시킵니다. 정보의 명확성, 정확성\n및 논리적 일관성이 강화되면 더 나은 의료 결과, 더 정확한 법률 판단 및\n개선된 기술 지원으로 이어져 사회 복지와 발전에 크게 기여할 수 있습니\n다.\n\n\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n**참고문헌**\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.,\nAlmeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4\ntechnical report. _arXiv preprint arXiv:2303.08774_, 2023.\n\n\n아사이, A., 우, Z., 왕, Y., 실, A., 하지시르지, H. Self-rag: 자기 성찰을 통\n한 지식 기반 학습의 검색, 생성 및 비판 학습, 2023. URL\n`[https://arxiv.org/ abs/2310.11511](https://arxiv.org/abs/2310.11511)` .\n\n\nBender, E. M., Gebru, T., McMillan-Major, A., and Mitchell,\nM. 확률적 앵무새의 위험성에 관하여: 언어 모델은 너무 커질 수 있는\n가? _2021 ACM 공정성, 책임성및투명성컨퍼런스( ) 논문집_,\npp. 610–623. ACM, 2021.\n\n\n버틀러, U. 오픈 오스트레일리아 법률 Q&A, 2023. URL\n```\n https://huggingface.co/datasets/\n```\n\n`[umarbutler/open-australian-legal-qa](https://huggingface.co/datasets/umarbutler/open-australian-legal-qa)` .\n\n\nCao, S., Zhang, J., Shi, J., Lv, X., Yao, Z., Tian, Q., Li, J., and Hou, L. 지\n식 집약적 복잡한 질문에 답하기 위한 확률적 사고 트리 추론. _arXiv_\n_preprint arXiv:2311.13982_, 2023.\n\n\nChen, T., Wang, H., Chen, S., Yu, W., Ma, K., Zhao, X., Zhang, H., and\nYu, D. Dense x retrieval: What retrieval granularity should we use?\n_arXiv pr eprint arXiv:2312.06648_, 2023. URL `[https://arxiv.](https://arxiv.org/pdf/2312.06648.pdf)`\n`[org/pdf/2312.06648.pdf](https://arxiv.org/pdf/2312.06648.pdf)` .\n\n\nCohere. 관련 없는 검색 결과에 작별을 고하세요: Co-here 재순위 지정 기\n능이 출시되었습니다. `[https://txt.cohere.com/rerank/](https://txt.cohere.com/rerank/)`,\n\n2023. 접속일: 2023-08-28.\n\n\nEdge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody, A., Truitt, S.,\nand Larson, J. 지역적에서 전역적 접근: 질의 중심 요약에 대한 그래프\n네트워크 접근법, 2024. URL\n`[https://arxiv.org/abs/2404.16130](https://arxiv.org/abs/2404.16130)` .\n\n\nFang, Y. 외. 다중 홉 질문 응답을 위한 계층적 그래프 네트워크. _계산언어_\n_학회(ACL) 연례회의논문집_ . 계산언어학회, 2020.\n\n\nFei, Z., Shen, X., Zhu, D., Zhou, F., Han, Z., Zhang, S., Chen, K., Shen,\nZ., and Ge, J. Lawbench: 대규모 언어 모델의 법률 지식 벤치마킹.\n\n_arXiv pr eprint arXiv:2309.16289_, 2023.\n\n\nFeng, Z., Feng, X., Zhao, D., Yang, M., and Qin, B. 검색-생성 시너지 강\n화 대규모 언어 모델, 2023. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2310.05149)`\n`[2310.05149](https://arxiv.org/abs/2310.05149)` .\n\n\n12\nGao, L., Ma, X., Lin, J., and Callan, J. 관련성 라벨 없이 정밀한 제로샷\n밀도 검색, 2022. URL\n`[https://arxiv.org/abs/2212.10496](https://arxiv.org/abs/2212.10496)` .\n\n\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and\nWang, H. 대규모 언어 모델을 위한 검색 강화 생성: 개요. _arXiv 사전_\n_인쇄본 arXiv:2312.10997_, 2023.\n\n\nHayashi, H., Budania, P., Wang, P., Ackerson, C., Neer-vannan, R., and\nNeubig, G. WikiAsp: 다중 도메인 측면 기반 요약 데이터셋. _Transac-_\n_tions of the Association for Computational Linguistics_, 9: 211–225,\n2021.\n\n\nHo, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa,\n추론 단계의 포괄적 평가를 위한 다중 홉 QA 데이터셋 구축. _arXiv 사_\n_전인쇄본 arXiv:2011.01060_, 2020.\n\n\nJiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y.,\nCallan, J., and Neubig, G. 능동적 검색 강화 생성, 2023. URL\n`[https://arxiv. org/abs/2305.06983](https://arxiv.org/abs/2305.06983)` .\n\n\nJiapeng, L., Runze, L., Yabo, L., Tong, Z., Mingling, L., and Xiang, C.\n\n리뷰 트리: 다중 홉 질문 답변을 위한 트리 기반 동적 반복 검색 프레임\n워크, 2024. URL `[https://arxiv.org/abs/ 2404.14464](https://arxiv.org/abs/2404.14464)` .\n\n\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and\nZaharia, M. Demonstrate-search-predict: 지식 집약적 NLP를 위한 검\n색 및 언어 모델의 조합, 2023. URL `[https://arxiv.org/](https://arxiv.org/abs/2212.14024)`\n`[abs/2212.14024](https://arxiv.org/abs/2212.14024)` .\n\n\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,\nY. 대규모 언어 모델은 제로샷 추론기이다. _신경정보처리시스템의_\n_진전_, 35: 22199–22213, 2022.\n\n\n루이스, P., 페레즈, E., 픽투스, A., 페트로니, F., 카르푸킨, V., 고얄, N.,\n쿠틀러, H., 루이스, M., 이, W., 록타셸, T., 외. 지식 집약적 자연어 처\n리 작업을 위한 검색 강화 생성. _Advances in Neural Information_\n_Processing Systems_, 33:9459–9474, 2020.\n\n\nLi, R. and Du, X. 설명 가능한 다중 홉 질문 응답 및 추론을 위한 구조화된\n정보 활용, 2023. URL `[https://arxiv.org/abs/2311.](https://arxiv.org/abs/2311.03734)`\n`[03734](https://arxiv.org/abs/2311.03734)` .\n\n\nLiang, L., Sun, M., Gui, Z., Zhu, Z., Jiang, Z., Zhong, L.,\nQu, Y., Zhao, P., Bo, Z., Yang, J., Xiong, H., Yuan, L.,\nXu, J., Wang, Z., Zhang, Z., Zhang, W., Chen, H., Chen, W., and\nZhou, J. Kag: 지식 증강 생성을 통한 전문 분야에서의 학습 모델 강화,\n\n2024. URL `[https://arxiv.org/abs/2409.13731](https://arxiv.org/abs/2409.13731)` .\n\n\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\nLing, C., Zhao, X., Lu, J., Deng, C., Zheng, C., W ang,\nJ., Chowdhury, T., Li, Y., Cui, H., Zhang, X., Zhao, T., Panalkar, A.,\nMehta, D., Pasquali, S., Cheng, W., Wang, H., Liu, Y., Chen, Z.,\nChen, H., White, C., Gu, Q., Pei, J., Yang, C., and Zhao, L. Domain\nspecialization as the key to make large language models disruptive: A\ncomprehensive survey, 2024. URL `[https://arxiv.](https://arxiv.org/abs/2305.18703)`\n`[org/abs/2305.18703](https://arxiv.org/abs/2305.18703)` .\n\n\n마, 셴, 공, 옌, 허, 펑, 자오, 허, 두안, 녜. 검색 강화형 대규모 언어 모델을\n위한 쿼리 재작성. _arXiv 사전인쇄본 arXiv:2305.14283_, 2023.\n\n\nPanda, P., Agarwal, A., Devaguptapu, C., Kaul, M., and P,\nP. A. Holmes: 다중 홉 질문 답변을 위한 초관계형 지식 그래프와 언\n어 모델 활용, 2024. URL\n`[https://arxiv.org/abs/2406.06027](https://arxiv.org/abs/2406.06027)` .\n\n\nPress, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M.\n\n언어 모델에서 구성성 격차 측정 및 축소, 2023. URL `[https:](https://arxiv.org/abs/2210.03350)`\n`[//arxiv.org/abs/2210.03350](https://arxiv.org/abs/2210.03350)` .\n\n\nQiu, M. 외 다수. 다중 홉 질문 답변을 위한 재귀 신경망의 동적 융합. _자_\n_연어처리의경험적방법론컨퍼런스(EMNLP) 논문집_ . 계산언어학회,\n2019.\n\n\nRaina, V. and Gales, M. 기업용 RAG를 위한 원자 단위 기반 질문 중심\n검색, 2024. URL https:// `[arxiv.org/abs/2405.12363](https://arxiv.org/abs/2405.12363)` .\n\n\nRam, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., LeytonBrown, K., and Shoham, Y. In-context retrieval-augmented language\nmodels. _Transactions of the Association for Computational_\n_Linguistics_, 11:1316–1331, 2023.\n\n\nShao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., and Chen, W. 반복적\n검색-생성 시너지를 통한 검색 강화형 대규모 언어 모델 향상, 2023.\n\nURL `[https://arxiv.org/abs/2305.15294](https://arxiv.org/abs/2305.15294)` .\n\n\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A.,\nLacroix, T., Rozie`re, B., Goyal, N., Hambro, E., Azhar, F.,\nRodriguez, A., Joulin, A., Grave, E., and Lam-ple, G. Llama: 개방적\n이고 효율적인 기초 언어 모델. _arXiv 사전인쇄본 arXiv:2302.13971_,\n2023.\n\n\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharw al,\nA. Musique: 단일 홉 질문 조합을 통한 다중 홉 질문. _계산언어학협회_\n_논문집_, 10:539–554, 2022.\n\n\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharw al,\nA. 지식 집약적 다단계 질문을 위한 사고 과정 추론과 검색의 병렬 처\n리, 2023. URL `[https://arxiv.org/abs/2212.10509](https://arxiv.org/abs/2212.10509)` .\n\n\n13\n왕, C., 류, X., 옌, Y., 탕, X., 장, T., 지아양,\nC., 야오, Y., 가오, W., 후, X., 치, Z., 왕, Y., 양, L.,\n왕, J., 시에, X., 장, Z., 장, Y. 대규모 언어 모델의 사실성 조사: 지식,\n검색 및 도메인 특이성, 2023a.\n\n\nWang, Y., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. 다중 문서\n질문 응답을 위한 지식 그래프 프롬프팅, 2023b. URL\n`[https://arxiv. org/abs/2308.11730](https://arxiv.org/abs/2308.11730)` .\n\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi,\nE., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits\nreasoning in large language models. Koyejo, S., Mohamed, S.,\nAgarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), _Advances in_\n_Neural Information Processing Systems_, volume 35, pp. 24824–\n24837. Curran Associates, Inc., 2022.\n\n\nXu, S., Pang, L., Shen, H., Cheng, X., and Chua, T.-S. Search-in-the\nchain: 지식 집약적 작업을 위한 검색을 통한 대규모 언어 모델의 대화\n형 향상. _ACM 웹컨퍼런스 2024 논문집_, pp. 1362–1373, 2024.\n\n\nYang, S. Advanced rag 01: Small-to-big re-trieval.\n```\n          https://towardsdatascience.\n com/advanced-rag-01-small-to-big\n```\n\n`-retrieval-172181b396d4`, 2023.  접속일:\n\n2023-08-28.\n\n\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov,\nR., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable\nmulti-hop question an-swering. _arXiv preprint arXiv:1809.09600_,\n2018.\n\n\nZhang, J., Zhang, H., Zhang, D., Liu, Y., and Huang, S. End-to-end beam\nretrieval for multi-hop question answering, 2024a. URL\n`[https://arxiv.org/abs/ 2308.08973](https://arxiv.org/abs/2308.08973)` .\n\n\n장, K., 젱, J., 멍, F., 왕, Y., 쑨, S., 바이, L., 쉔, H., 저우, J. 대규모 언어\n모델을 활용한 복잡한 질문 응답을 위한 추론 트리 기반 질문 분해. _인_\n_공지능학회 AAAI 학술대회논문집_, 38(17):19560–19568, 2024b.\n\n\n정, H. S., 미슈라, S., 천, X., 청, H.-T., 치, E. H., 레, Q. V., 주, D. 한 걸음\n물러서기: 대규모 언어 모델에서 추상화를 통한 추론 유도. _arXiv 사전_\n_인쇄본 arXiv:2310.06117_, 2023.\n\n\n정, H. S., 미슈라, S., 천, X., 성, H.-T., 치, E. H., 레, Q. V., 주, D. 한 걸음\n물러서기: 대규모 언어 모델에서 추상화를 통한 추론 유도, 2024. URL\n`[https://arxiv.org/abs/2310.06117](https://arxiv.org/abs/2310.06117)` .\n\n\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\nZhong, Z., Liu, H., Cui, X., Zhang, X., and Qin, Z. Mix-of-granularity:\n\n검색 강화 생성을 위한 청크 분할 세분성 최적화, 2024. URL\nhttps:// `[arxiv.org/abs/2406.00456](https://arxiv.org/abs/2406.00456)` .\n\n\n14\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n**A.** **부록**\n부록 A.1은 세 가지 오픈 도메인 벤치마크에 대한 상세한 소개를 제공하며, 부록 A.3은 실험에 사용된 하이퍼파라미터를 열거하고, 부록 A.4는 포괄적\n인 실험 결과를 제시합니다.\n\n\n본 방법론에 대한 보다 심층적인 이해를 위해 부록 A.5에는 비용 분석이 수록되어 있습니다. 또한, 이 하위 섹션에서는 대체 원자 태그 표현에 대한 소\n개와 논의도 제시합니다.\n\n\n부록 A.6에서는 세 가지 실제 사례 연구를 탐구합니다. 본 접근법의 네 가지 구성 요소 전반에 걸쳐 사용된 프롬프트는 부록 A.7에 개요가 제시되며, 분\n해 시연의 영향에 대한 논의가 함께 수록되어 있습니다. 마지막으로, 두 가지 법적 벤치마크에 대한 평가 결과는 부록 A.8에 상세히 기술되어 있습니다.\n\n\n**A.1.** **개방형** **도메인** **벤치마크** **소개**\n본 실험에 사용된 다중 홉 QA 데이터셋에 대한 간략한 개요를 제시하며, 본 방법론은 이러한 분류에 무관하도록 설계되었기에 문제 유형 정보나 홉 수\n정보를 해결 과정에서 활용하지 않음을 밝힌다. 표 4는 샘플링된 데이터셋 내 문제 유형 분포를 요약하여 평가에 제시된 다양한 추론 과제에 대한 통찰\n을 제공하나, 이는 본 방법론에 직접적인 영향을 미치지 않는다.\n\n\n**HotpotQA** HotpotQA 데이터셋은 주로 2단계 질문으로 구성된 잘 알려진 다단계 QA 벤치마크로, 각 질문은 10개의 위키백과 문단과 연결됩니다. 이\n중 일부 문단은 질문에 답하는 데 필수적인 지원 사실을 포함하는 반면, 나머지는 방해 요소 역할을 합니다. 이 데이터셋에는 _질문_ 유형 필드도 포함되어\n있어 필요한 논리적 추론 방식을 구분합니다. _비교_ 질문은 두 개체를 대조하는 것을, _연결_ 질문은 연결 개체를 추론하거나 중간 개체를 통해 개체의 속성\n을 추론하거나 답변 개체를 찾는 것을 요구합니다(Yang et al., 2018). 본 방법론은 이러한 유형과 독립적으로 작동하지만, 여기서 유형을 설명하는 것은\n데이터셋 내 문제의 특성을 예시하고 다양한 벤치마크 간 예상 성능 차이를 맥락화하기 위함이다.\n\n\n**2WikiMultiHopQA** HotpotQA에서 영감을 받은 2WikiMultiHopQA는 질문 유형의 다양성을 확장합니다. HotpotQA의 _비교_ 유형을 유지하면서, 각각\n엔티티 속성 추론과 엔티티 위치 파악에 초점을 맞추어 브릿지 유형에서 진화한 _추론_ 및 _구성_ 질문을 도입합니다. 또한 _브릿지비교_ 유형은 _브릿지_ 추론\n과 _비교_ 추론을 종합적으로 요구하는 새로운 범주입니다. 이 데이터셋은 일반적으로 2-hop에서 4-hop 질문을 제시하며, 각 질문에는 지원 사실과 방해\n요소를 포함한 10개의 위키백과 문단이 동반됩니다. 이러한 유형들은 데이터셋 구조를 알려주지만, 본 방법론에서는 활용되지 않습니다. 본 방법론은\n분류와 무관하게 모든 질문을 균일하게 처리합니다. 간결성을 위해 본 논문에서는 2WikiMultiHopQA를 2Wiki로 약칭합니다.\n\n\nMuSiQue는 다중 단계 질문 중 상당수가 단축 경로로 해결될 수 있다는 문제점—적절한 추론 없이 정답에 도달하는 현상—을 해결하기 위해, Trivedi\n등(Trivedi et al., 2022)이 보고한 바와 같이 연결된 추론을 장려하도록 특별히 설계된 엄격한 필터와 추가 메커니즘을 구현합니다. 다른 데이터셋과 달\n리 MuSiQue는 질문 유형별로 분류하지 않지만, 각 질문에 필요한 홉 수(2~4홉)에 대한 명시적 정보를 제공합니다. 각 질문에는 20개의 문맥 단락이 연\n결되어 있으며, 여기에는 관련 정보와 무관한 정보가 혼합되어 있어 올바른 추론 경로를 식별하는 작업을 더욱 복잡하게 만듭니다. 이러한 명시적인 홉\n정보는 본 방법론에서 사용되지는 않지만, 데이터 세트의 복잡성과 이러한 과제를 효과적으로 처리하기 위해 모델에 요구되는 견고성을 강조합니다.\n\n\n_Table 4._ 세 가지 다중 홉 QA 데이터셋에 걸친 질문 유형 분포.\n\n\n\n\n\n\n\n\n|유형|카운트 비율|\n|---|---|\n|비교<br>교량|107<br>21.4%<br>393<br>78.6%|\n\n\n|유형|개수 비율|\n|---|---|\n|비교<br>추론<br>구성적<br>교량 비교|132<br>26.4%<br>64<br>12.8%<br>196<br>39.2%<br>108<br>21.6%|\n\n\n|홉 수|개수 비율|\n|---|---|\n|2<br>3<br>4|263<br>52.6%<br>169<br>33.8%<br>68<br>13.6%|\n\n\n\n(a) HotPotQA (b) 2WikiMultiHopQA (c) MuSiQue\n15\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n**A.2.** **평가** **방법** **소개**\n표 1에 제시된 방법 외에도, 반복적 탐색 및 분해 방법인 SearChain(Xu et al., 2024)과 지식 그래프 기반 방법인 GraphRAG(Edge et al., 2024)를 활용한\n실험을 수행하였다. GraphRAG는 로컬 모드와 글로벌 모드 모두에서 추론되었다. 본 연구에서 평가된 방법은 다음과 같다:\n\n\n  - **Zero-Shot CoT** : 질문은 오직 Chain-Of-Thought(CoT) 기법만을 사용하여 처리됩니다. 이 기법은 예시 시연이나 보충적 맥락의 도움 없이 LLM\n이 단계별로 추론 과정을 설명하도록 유도합니다. 이 방법은 제로샷 환경에서 LLM의 내재적 지식과 추론 능력을 평가합니다.\n\n\n  - **순진한 RAG** ( **Naive R** AG): 평면 지식베이스에서 밀도 검색(dense retrieval)을 수행하여 각 질문에 대한 관련 정보를 획득하는 접근법입니다. 지\n식베이스는 사전 임베딩된 정보 조각들로 구성되며, 의미적 유사성을 기반으로 원본 질문과 매칭됩니다. 검색 과정은 중간 작업 분해 없이 직접\n적으로 이루어집니다.\n\n\n  - **자기** **질문** **및** **검색** 기반: 이 방법은 작업 분해 전략을 활용하여 LLM이 반복적으로 후속 질문을 생성하고 답변하도록 유도함으로써 복잡한 문제를\n관리 가능한 하위 작업으로 분해합니다. 모든 벤치마크에 대해 작업 분해의 논리와 방법론을 설명하는 일반적인 시연을 제공하여 LLM의 추론 과\n정을 안내합니다. 기존 설정(Press et al., 2023)과 달리, 본 설정에서는 후속 질문에 대한 답변을 LLM 자체 지식에만 의존하지 않고 추가적인 검\n색 구성 요소를 도입합니다. 참조 컨텍스트를 제공하기 위해 평면 지식베이스에서 후속 질문을 쿼리로 사용하여 관련 정보 블록을 검색합니다. 또\n한 다른 방법론과 일관성을 유지하기 위해 분해 과정을 최대 _N_ _개의_ 후속 질문으로 제한합니다.\n\n\n  - **IRCoT** : 이 접근법은 LLM에게 검색된 문단과 함께 한 문장 더의 근거를 생성하도록 반복적으로 프롬프트하고, 새로 생성된 근거로 새로운 문단\n을 검색합니다. 원본 설정은 최대 토큰 수로 프로세스를 제한합니다(Trivedi et al., 2023). 본 실험에서는 총 반복 횟수를 본 방법에 사용한 _상수_ _N_\n_으로_ 제한합니다.\n\n\n  - **Iter-RetGen** : 이 방법은 검색된 문단으로 질문에 반복적으로 답변하며, 새로 생성된 근거와 답변을 다음 라운드 검색에 활용합니다. 이 설정에서\n도 총 반복 횟수를 _동일한_ _N_ _으로_ 제한합니다.\n\n\n  - **SearChain** : 이 접근법은 LLM과 정보 검색(IR) 간의 상호작용에 중점을 둡니다. SearChain은 각 노드가 IR 지향적 질의-답변 쌍으로 구성된\nLLM 생성 추론 체인인 Chain-of-Query(CoQ)에서 시작합니다. 이후 CoQ의 각 노드 답변을 IR을 통해 반복적으로 검증하고, 검색된 정보와 일치\n하지 않는 노드에 대해 CoQ를 재생성합니다. 이러한 재생성 메커니즘을 통해 SearChain은 트리 기반의 새로운 추론 경로를 형성하여 LLM이 추\n론 방향을 동적으로 수정할 수 있게 합니다. 공식 코드는 사전 훈련된 모델을 온라인에 업로드하지 않고 로컬에서 불러오므로, 우리는\nHuggingFace에서 가장 유사한 이름의 모델을 찾아 이를 적용합니다. 또한 환경적 문제로 인해 표 6에 제시된 실험 결과는 ColBERT 리트리버 대\n신 _BAAI/bge-m3로_ 수행되었습니다.\n\n\n  - **ProbTree** : 이 접근법은 명시적 트리 탐색 방법입니다. ProbTree는 주어진 질문에 대해 LLM이 번역한 쿼리 트리에서 시작하며, 여기서 각 비루\n트 노드는 상위 노드의 하위 질문을 나타냅니다. 그런 다음 질문 분해와 답변 모두의 신뢰도를 고려하여 잎에서 루트로 질문을 해결함으로써 트리\n위에서 확률적 추론을 수행합니다.\n\n\n  - **GraphRAG Local** : 공개 지침에 따라 지식 그래프를 구축하기 위해 지식베이스를 전처리합니다. 평가는 로컬 모드에서 추론됩니다.\n\n\n  - **GraphRAG Global** : 공개 지침에 따라 지식 그래프를 구축하기 위해 지식베이스를 전처리합니다. 평가는 글로벌 모드에서 추론됩니다.\n\n\n  - **KAR** **[3]** **(저희)** : 제안된 지식 인식 분해 방법은 복잡한 질문을 하위 질문으로 반복적으로 분해하고 최대 N회 반복까지 관련 지식을 검색합니다. 이\n과정은 최종 답변의 맥락을 가장 유용한 다섯 개의 지식 단위로 제한합니다.\n\n\n논의된 평가 방법 간의 차이를 보다 명확히 설명하기 위해, 각 방법의 특징을 체계적으로 정리한 표 5를 제시합니다. 이 표는 질문 분해, 지식 조각 검색,\n그리고\n16\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n_표 5._ 방법 비교.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|방법|분해<br>증명 경로 컨텍스트|검색|생성 컨텍스트<br>하위 답변 최종 답변|\n|---|---|---|---|\n|제로샷 CoT순진한 <br>RAG자체 질문 w/<br>R. IRCoT<br>Iter-RetGen<br>SearChain<br>ProbTree<br>KAR3**  (우리의)**|소량 데이터 <br>학습 <br>소량 <br>데이터 학습 <br>제로샷 소량 <br>데이터 학습 <br>소량 데이터 <br>학습 제로샷<br>해당 <br>없음 <br>해당 <br>없음<br>체인 (암시<br>적) (암시적) <br>체인 트리<br>동적<br>qa쌍 근거, 청크 청크<br>∅; qa쌍, 청크<br>∅<br>선택된 청크|해당 없음<br>질문 → 청크<br>하위 질문 → 청크 근거 문장 → 청<br>크 전체 근거 → 청크 하위 질문 → <br>청크<br>하위 질문 → 청크<br>하위 질문 → 원자 질문 → 청크|해당 없음 <br>해당 없음 <br>청크 해당 <br>없음 해당 <br>없음<br>∅; 청크<br>청크<br>해당 없음<br>해당 없음 <br>청크 질문-<br>답변 쌍<br>근거, 청크 청크<br>질의-응답 <br>쌍 질의-응<br>답 쌍<br>선택된 청크|\n\n\n\n답변 생성에 사용되는 컨텍스트를 설명합니다. 구체적으로 각 방법이 제로샷 또는 소량 데이터 학습 조건에서 작동하는지, 분해 과정의 특성(예: 명시\n적 또는 암시적 분해; 체인형, 트리형 또는 동적으로 생성된 경로), 그리고 분해 과정에서 활용되는 컨텍스트를 구분합니다. 검색 열은 각 방법이 정보를\n수집하기 위해 사용하는 메커니즘을 명확히 설명하며, 하위 답변 및 최종 답변 생성을 위한 생성 컨텍스트 전용 열은 각 방법이 답변을 생성할 때 활용\n하는 특정 컨텍스트를 강조합니다.\n\n\n표 5에서 보여지듯, KAR [3] 의 분해 모듈은 제로샷 지식 인식 접근법을 채택하여 반복적 분해를 위해 축적된 선택된 청크를 문맥 내에서 유지합니다. 또한\n부록 A.7에서는 데모스트레이션 통합의 잠재적 이점을 논의하며, 이 기능이 성능을 더욱 향상시킬 수 있음을 시사합니다. 이 가능성은 향후 탐구를 위\n해 예정되어 있습니다. 특히, 본 접근법은 반복 과정에서 분해 경로를 동적으로 구성하여 문맥적으로 제공된 지식으로부터 얻은 새로운 통찰에 기반한\n조정이 가능하도록 합니다. 검색 단계에서는 원자 태그를 활용하여 쿼리와 청크 내 정보 간의 의미적 간극을 해소합니다. 중요한 점은 생성 단계에서 본\n방법이 선택된 청크를 유지함으로써, 생성 과정이 지식 인식 상태를 유지하고 오직 후속 질문과 답변에 의존하는 방법에서 흔히 발생하는 오류 누적 위\n험을 완화한다는 것입니다.\n\n\n**A.3.** **하이퍼** **파라미터**\n지식 추출 단계에서는 지식 분해 과정에 특화된 0 _._ 7의 _온도_ 설정을 활용하여 생성된 원자 지식의 다양성과 결정론적 특성 간 균형을 촉진합니다. 반면\n각 방법의 모든 질문-답변(QA) 단계에서는 _온도를_ 0으로 설정하여 모델의 일관된 응답을 보장합니다.\n\n\n검색 구성 요소와 관련하여, 일반 지식베이스와 원자 지식베이스 모두에 대해 텍스트 임베딩 모델로 _text-embedding-ada-002_ (버전 2)를 사용합니다.\n\nNaive RAG 및 Iter-RetGen에 사용되는 일반 지식베이스의 경우, 검색기는 검색 점수 임계값 0 _._ 2를 적용하여 최대 16개의 지식 청크를 가져오도록 구성\n됩니다. Self-Ask w/ Retrieval 및 IRCoT에서 사용되는 일반 지식베이스의 경우, 검색된 청크가 단일 후속 질문 답변이나 단일 연속적 근거 문장 생성에\n사용되므로 전체 근거 또는 최종 질문 답변을 위한 참조 청크가 누적됩니다. 시스템은 요청당 4개의 관련 청크를 검색하며, 동일한 점수 임계값 0 _._ 2를\n유지합니다. 원자 지식베이스의 경우, 검색기는 각 원자 쿼리에 대해 4개의 관련 원자 태그를 검색하도록 설정되어 있지만, 콘텐츠 길이가 짧기 때문에\n더 높은 임계값 0 _._ 5를 적용합니다.\n\n\n**A.4.** **상세한** **실험** **결과**\n**평가** 지표 평가 지표와 관련하여 부록에서 세 가지 추가 지표를 활용합니다. 응답이 사전 정의된 정답과 동일한지 평가하는 **정확** **일치도(EM)** 는 커뮤니\n티에서 일반적으로 사용해 온 방식대로 적용됩니다. 또한, 특정 방법이 높은 정확도(Acc) 점수를 달성하면서도 낮은 F1 점수를 기록하는 상황을 종종 접\n하게 됩니다. 이러한 불일치의 근본 원인을 규명하기 위해 생성된 응답의 **재현율** ( **Recall)** 과 **정밀도(Precision)** 도 함께 보고합니다. 리콜은 응답에 포함된\n정답 라벨의 관련 토큰 비율을 측정하는 반면, 정밀도는 생성된 응답 내 토큰이 정답 라벨과 얼마나 관련성이 높은지를 평가합니다.\n\n\n17\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n_표 6._ 다중 단계 질의응답 데이터셋에 대한 상세 성능 비교. 최상위 성능은 굵게, 차상위 성능은 밑줄 처리.\n\n|Col1|표 6. (a) HotPotQA|\n|---|---|\n|방법|EM<br>F1<br>정확도<br>정밀도<br>재현율|\n|제로샷 CoT<br>순진한 RAG<br>자기질문 및 회상<br>IRCoT<br>Iter-RetGen<br>SearChain<br>ProbTree<br>GraphRAG로컬<br>GraphRAG글로벌<br>KAR3 (**우리**)|32.60<br>43.94<br>53.60<br>46.56<br>43.97<br>56.80<br>72.67<br>82.60<br>74.52<br>74.86<br>57.00<br>71.40<br>80.00<br>73.25<br>73.95<br>51.40<br>67.30<br>81.00<br>69.32<br>72.15<br>59.60<br>75.27<br>86.60<br>77.18<br>77.62<br>28.60<br>40.48<br>74.40<br>40.77<br>66.63<br>47.00<br>62.41<br>73.40<br>64.83<br>64.95<br>0.00<br>10.66<br>**89.00**<br>5.90<br>**83.07**<br>0.00<br>7.42<br>64.80<br>4.08<br>63.16<br>**61.40**<br>**76.48**<br>88.00<br>**78.53**<br>78.96|\n\n\n|Col1|표 6. (b) 2WikiMultiHopQA|\n|---|---|\n|방법|EM<br>F1<br>정확도<br>정밀도<br>재현율|\n|제로샷 CoT<br>순진한 RAG<br>자기 질문과 회상<br>IRCoT<br>Iter-RetGen<br>SearChain<br>ProbTree<br>GraphRAG로컬<br>GraphRAG글로벌<br>KAR3 (**우리**)|35.67<br>41.40<br>43.87<br>41.43<br>43.11<br>51.20<br>59.74<br>62.80<br>59.06<br>62.30<br>60.60<br>69.06<br>75.00<br>67.88<br>73.15<br>55.00<br>63.83<br>70.40<br>62.47<br>68.86<br>57.80<br>67.21<br>73.60<br>66.10<br>71.09<br>7.00<br>15.67<br>68.40<br>11.91<br>66.74<br>57.00<br>69.42<br>80.00<br>67.61<br>76.89<br>0.00<br>11.83<br>71.20<br>6.74<br>75.17<br>0.00<br>7.35<br>45.00<br>4.09<br>55.43<br>**65.80**<br>**75.00**<br>**82.20**<br>**73.63**<br>**79.08**|\n\n\n|Col1|표 6. (c) MuSiQue|\n|---|---|\n|방법|EM<br>F1<br>정확도<br>정밀도<br>재현율|\n|제로샷 CoT<br>순진한 RAG<br>자기 질문 및 회상<br>IRCoT<br>Iter-RetGen<br>SearChain<br>ProbTree<br>GraphRAG로컬<br>GraphRAG글로벌<br>KAR3 (**우리**)|12.93<br>22.90<br>23.47<br>24.40<br>24.10<br>32.00<br>43.31<br>44.40<br>44.42<br>47.29<br>38.20<br>46.76<br>51.40<br>46.75<br>51.00<br>36.00<br>47.57<br>49.20<br>48.70<br>50.30<br>40.20<br>52.48<br>55.60<br>53.51<br>56.45<br>24.40<br>33.26<br>45.80<br>33.00<br>46.37<br>28.57<br>43.26<br>52.86<br>42.27<br>54.70<br>0.60<br>9.62<br>49.80<br>5.73<br>55.82<br>0.00<br>5.16<br>44.60<br>2.82<br>52.19<br>**47.40**<br>**57.86**<br>**62.60**<br>**58.52**<br>**61.37**|\n\n\n\n**상세** **주요** **결과** 다중 홉 데이터셋 HotpotQA, 2Wiki 및 MuSiQue에 대한 상세 실험 결과는 표 6에 제시되어 있습니다. 표 1에 표시된 지표 외에도 **EM**,\n**정밀도** 및 **재현율이** 여기에 제공됩니다.\n\n\n**그래프** **기반** **방법에** **대한** **논의** 특히 지식 그래프 기반 방법인 GraphRAG Local은 2-hop 질문이 주를 이루는 HotpotQA 데이터셋에서 탁월한 성능을 보입\n니다. 그러나 더 많은 홉을 포함하는 질문이 포함된 다른 두 데이터셋에서는 GraphRAG Local이 IRCoT와 동등한 수준에 그칩니다. 이는 지식 그래프 기\n반 방법이 복잡한 다중 홉 질문을 해결하는 데 직면한 어려움을 강조합니다. GraphRAG의 경우, (Edge et al., 2024)에서 제시된 쿼리 중심 요약(QFS)\n작업을 위해 설계되었으나, 본 방법에 비해 로컬 및 글로벌 모드 모두에서 성능이 미흡함을 관찰했다. GraphRAG는 흥미로운 경향을 보인다: 정확도와\n리콜 점수는 높게 달성하는 반면, EM, F1, 정밀도 지표에서는 낮은 성능을 보인다. GraphRAG 출력을 자세히 분석해 보면, 쿼리를 반복하고 답변에 대\n한 메타정보를 그래프 구조 내에 포함시키는 경향이 드러납니다. QA 프롬프트를 개선하려는 시도에도 불구하고 이 행동은 지속됩니다. 표 7은\n18\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\nHotpotQA의 질문에 대한 GraphRAG Local의 응답을 보여주는 대표적인 예시입니다.\n\n\n19\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n\n|에 대한 GraphRAG 로컬 출력 예시|시. 이 표는 질문을 반복하고 응답에 메타 정보를 포함하는 경향을 보여줍니다.|\n|---|---|\n|질문|알사 몰과 스펜서 플라자가 위치한 국가는 어디인가요?|\n|답변 레이블|인도|\n|GraphRAG의 답변|알사 몰과 스펜서 플라자는 모두 인도 첸나이에 위치해 있습니다. [데이터: 인도 및 첸나이 커<br>뮤니티 (2391);엔티티 (4901, 4904);관계<br>(9479, 1687, 5215, 5217)].|\n\n\n\n**N 선택에** **대한** **상세** **평가** **결과** 표 8은 반복 _상한_ _N_ _에_ 대한 제거 연구를 위해 그림 5에 제시된 항목에 따른 세부 성능 지표를 나열합니다. 답변 레이블의\n리콜 토큰을 나타내는 표 6의 리콜과 달리, 여기서 **리콜** **[*]** 은 해당 데이터셋이 제공하는 지원 사실의 리콜을 나타냅니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|표 8. 하이퍼파라미터 N에 대|대한 제거 연구. Recall*은 지원 사실의|의 리콜을 나타냅니다.|\n|---|---|---|---|\n|_N_|리콜*<br>HotpotQA<br>F1<br>정확도|리콜*<br>2Wiki<br>F1<br>정확도|리콜*<br>MuSiQue<br>F1<br>Acc|\n|1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10|42.96<br>59.46<br>70.20<br>82.04<br>74.27<br>84.80<br>90.16<br>76.90<br>87.20<br>92.46<br>76.49<br>87.80<br>92.83<br>76.48<br>88.00<br>93.35<br>77.67<br>89.00<br>93.68<br>77.32<br>88.80<br>93.78<br>76.88<br>88.40<br>93.78<br>76.99<br>88.60<br>93.78<br>77.52<br>89.00|40.41<br>41.08<br>43.00<br>78.83<br>70.22<br>77.20<br>87.71<br>72.84<br>79.40<br>92.86<br>74.68<br>81.80<br>94.06<br>75.00<br>82.20<br>94.76<br>75.12<br>81.80<br>94.91<br>75.44<br>82.40<br>95.06<br>75.16<br>82.00<br>95.11<br>74.89<br>81.80<br>95.16<br>75.09<br>82.00|31.20<br>32.55<br>32.80<br>56.43<br>48.46<br>50.00<br>64.82<br>53.50<br>57.20<br>69.87<br>55.73<br>59.40<br>73.08<br>57.86<br>62.60<br>74.88<br>57.03<br>61.20<br>76.07<br>56.66<br>61.40<br>76.72<br>57.65<br>62.40<br>76.90<br>57.17<br>61.40<br>77.20<br>57.69<br>62.40|\n\n\n**덜** **발전된 LLM을** **사용한** **평가** **결과** 한계 논의 섹션에서 소개한 바와 같이, 우리는 GPT-3.5를 활용하여 일련의 실험을 수행했습니다. 이 실험 결과는\n표 9에 정리되어 있습니다. 이 특정 실험에서는 언어 모델로 GPT-4(1106-Preview) 대신 GPT-3.5(1106-Preview)를 사용했으며, 표 1에 요약된 실험과\n동일한 모든 실험 설정을 유지했습니다.\n\n\n\n\n\n\n\n\n\n\n\n**A.5.** **비용** **분석** **및** **논의**\n\n|표 9. GPT-3.5를 활용한 구|구현체 성능 비교. 최상위|위 결과는 굵게, 차상위 결|결과는 밑줄 표시.|\n|---|---|---|---|\n|방법|HotpotQA<br>F1<br>정확도|2Wiki<br>F1<br>Acc|MuSiQue<br>F1<br>Acc|\n|회상을 통한 자기 질문<br>IRCoT<br>Iter-RetGen<br>KAR<br>3<br> (**우리**)|49.52<br>61.40<br>56.39<br>68.40<br>48.63<br>66.80<br>46.37<br>**68.80**|53.83<br>**60.00**<br>40.31<br>46.00<br>44.32<br>55.20<br>41.95<br>58.20|31.05<br>35.20<br>33.93<br>34.40<br>25.77<br>37.80<br>26.80<br>**39.60**|\n\n\n\n본 절에서는 모델의 API 사용량을 평가하기 위한 포괄적인 비용 분석을 수행합니다. 먼저 추론 비용을 다른 기준 방법과 비교 평가하고, 이후 비용을 구\n성 요소로 세분화하며, 마지막으로 일회성 데이터 전처리 단계의 비용 요약 정보를 제공합니다.\n\n\n**추론** **비용** **비교** 표 10에서 보여준 바와 같이, QA당 토큰 소비 측면에서 본 방법은 ProbTree 및 IRCoT보다 적은 토큰을 사용하며 Iter-RetGen과 유사한\n수준입니다. 그러나 본 접근법은 F1 점수와 정확도 모두에서 이러한 기준 모델들을 상당한 차이로 크게 능가합니다. 이는 비용과 성능 균형 측면에서 본\n방법의 효율성을 입증합니다.\n\n\n20\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n비용과 성능의 균형을 효과적으로 조율함을 입증합니다. 특히 본 방법은 잠재적 추론 체인 탐색에 중점을 두어, 각 반복 단계에서 문맥을 고려한 신중한\n질문 분해 분석이 필요하다는 점을 강조할 필요가 있습니다. 그 결과, 완성 토큰 사용량이 전체 소비량의 약 1/4을 차지하며, 이는 본 접근법을 다른 기\n준선과 차별화하는 특징입니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n|표 10. MuS|SiQue에서의 토큰 소비량(평균/QA) 및 성능 비|비교.|\n|---|---|---|\n|방법|토큰<br>프롬프트<br>소비량<br>완료율<br>(⇓)<br>총합|성과 (⇑)<br>F1<br>정확도|\n|제로샷 CoT<br>순진한 RAG<br>자기질문 및 회상<br>IRCoT<br>Iter-RetGen<br>ProbTree<br>KAR<br>3<br>** (우리)**|85<br>105<br>191<br>1765<br>103<br>1869<br>5894<br>619<br>6514<br>9703<br>86<br>9789<br>8140<br>473<br>8614<br>25225<br>650<br>**25875**<br>6525<br>2295<br>8820|22.90<br>23.47<br>43.31<br>44.40<br>46.76<br>51.40<br>47.57<br>49.20<br>52.48<br>55.60<br>43.26<br>52.86<br>**57.86**<br>**62.60**|\n\n\n**다양한** **구성** **요소의** **토큰** **소비량** 지금까지 제시된 실험 결과에서는 디컴포저, 셀렉터, 제너레이터 구성 요소에 동일한 대규모 언어 모델(LLM)이 사용되\n었습니다. 해당 구성 요소를 위해 설계된 프롬프트는 부록 A.7에 상세히 기술되어 있습니다. 이러한 구성 요소가 서로 다른 언어 모델을 사용하도록 구성\n될 수 있다는 점은 주목할 만하며, 이는 향후 연구 과제로 남겨둡니다. MuSiQue에서 각 구성 요소의 상세한 토큰 소비량은 표 11에 설명되어 있습니다.\n\n분해-선택 루프는 최대 5회 반복되며, 이는 각 QA에 대해 분해기와 선택기에 대한 다중 호출로 이어집니다. 결과적으로 분해기와 선택기가 전체 소비량\n의 대부분을 차지합니다.\n\n|표 11. MuSiQue|e에서의 토큰 소비량 (평균/QA).|\n|---|---|\n|구성 요소|프롬프트<br>완성<br>총|\n|쿼리 제안자<br>원자 선택자<br>답변 생성기|2691<br>768<br>3459<br>3278<br>1429<br>4707<br>556<br>98<br>654|\n|KAR<br>3<br>** (우리)**|6525<br>2295<br>8820|\n\n\n\n**토큰** **소비량** **챕크** **원자화** 챕크 원자화는 일회성 전처리 단계로, LLM API 소비량은 데이터 챕크 수에 선형적으로 비례하며 벤치마크별로 약간씩 차이가\n나는 오버헤드를 구성합니다. 4.1절에서 설명한 바와 같이, 모든 청크는 컨텍스트 단락에서 파생되며, 청크 수에 상응하는 LLM 호출 횟수는 토큰 소비\n량과 함께 표 12의 마지막 열에 참고용으로 기재되어 있습니다. 입력 토큰 크기(표에서 프롬프트)는 주로 청크 크기에 의해 결정되는 반면, 출력 토큰 크\n기(표에서 완성)는 생성된 원자 태그의 크기에 따라 달라집니다.\n\n\n_표 12._ 토큰 소비량(평균/청크) 및 청크 수 통계.\n\n|데이터셋|프롬프트 완료 총|호출|\n|---|---|---|\n|HotpotQA<br>2Wiki<br>MuSiQue|209<br>129<br>338<br>199<br>122<br>321<br>197<br>123<br>320|4950<br>3410<br>7120|\n\n\n\n**원자** **태그의** **대체** **표현** **방식** 대규모 데이터셋에 본 방법을 적용할 때 확장성의 중요성을 인지합니다. 확장성을 유지하면서 비용 효율성을 높이기 위해\nLlama 3와 같은 오픈소스 언어 모델을 통합하여 전처리 비용을 크게 절감합니다. 또한 자원 사용을 추가로 최적화하기 위해 대체 원자 태그 표현 방식을\n탐구합니다. 유망한 접근법 중 하나는 데이터를 일반 텍스트 문장으로 분해하여 각 문장을 원자 태그로 취급하는 것입니다. 이 방법은 *spacy* 라이브러\n리를 활용해 원본 데이터 청크를 문장으로 분할함으로써 사전처리 단계를 단순화하고, 언어 모델 호출의 필요성을 제거합니다. 표 13에 상세히 기술된\n평가 결과에 따르면, 이 접근법은 MuSiQue 데이터셋에서 성능이 55.2%로 감소했음에도 불구하고 여전히 대부분의\n21\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n기준 방법들. 이는 저비용 전처리가 우선순위인 시나리오에서 잠재적 효과성을 입증하며, 비용과 성능을 효율적으로 균형 잡는 실행 가능한 대안을 제\n시한다.\n\n\n_표 13._ MuSiQue에서 대체 원자 태그의 성능.\n\nLLM 원자 태그 F1 정확도\n일반 텍스트 문장 45.88 54.20\nLlama 3\n원자적 질문 **(우리의)** **50.68** **59.70**\n일반 텍스트 50.72 55.20\nGPT-4\n원자적 질문 **(우리의)** **57.86** **62.60**\n**A.6.** **실제** **사례** **연구**\n본 섹션에서는 제안된 분해 파이프라인의 기본 원리를 설명하기 위해 평가 벤치마크에서 추출한 세 가지 실제 사례 연구를 제시합니다. 이는 알고리즘\n1에 상세히 기술된 바와 같습니다. 이러한 실제 사례를 통해 체계적인 접근 방식의 장점을 부각하고자 합니다. 각 사례는 파이프라인의 각 단계가 성능\n향상에 어떻게 기여하는지, 그리고 구현 과정에서 얻은 통찰력을 조명할 것입니다.\n\n\n_그림 6._ 사례 (a): 대중적인 영화 \"What Women Want\"에 비해 덜 알려진 영화 \"What Women Love\"를 예로 들면, 왼쪽의 Self-Ask와 같은 단일 경로 방식은 후자에 대한 후속\n질문을 생성하는 경향이 있어 최종 답변이 잘못될 수 있다. 반면 KAR [3] 은 여러 원자적 질의를 가정하여 원본 질문의 의도된 의미를 효과적으로 식별하고, 관련 원자 태그를 제\n공하여 작업 이해를 원자 선택 단계로 연기한 후 정확한 결론에 도달할 수 있다.\n\n\n22\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-21-0.png)\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n**KAR** **[3]** **은** **원본** **질문의** **의도된** **의미를** **효과적으로** **식별함으로써** **단일** **경로** **방식보다** **우수한** **성능을** **발휘합니다.** 우리의 작업 분해 전략은 Self-Ask 접근\n법에서 보여준 것처럼 단일 결정론적 후속 질문을 생성하기보다 다중 원자적 질의를 생성하는 것을 포함합니다. 현대적 분해 방법들은 일반적으로 생성\n모델을 활용하여 단일 후속 질문을 구성합니다. 그러나 이 접근법은 오류가 있는 질문을 생성할 수 있는 내재적 위험을 안고 있어, 잘못된 분해 경로로\n이어지고 궁극적으로 오류 답변을 초래할 수 있습니다. 그림 6에 묘사된 사례 (a)를 고려해 보십시오. 여기서 원본 질문은 \"What Women Love\"라는 영\n화에 관한 것입니다. 더 유명한 영화 \"What Women Want\"가 존재하기 때문에, 사용된 언어 모델은 원본 질문을 '수정'하려는 경향이 있습니다. 결과적으\n로 Self-Ask(그림 6 좌측 예시)와 같은 방법은 이 잘못된 대상에 대한 단 하나의 후속 질문만 생성합니다. 그림에서처럼 임베딩 유사성으로 인해 대상 챕\n터는 검색되었으나, '잘못된' 후속 질문에 대한 '잘못된' 중간 답변이 생성되어 최종적으로 잘못된 응답이 도출됩니다. 반면, 우리의 방법론은 \"What\nWomen Love\"와 \"What Women Want\" 모두에 관한 원자적 질의를 제기함으로써 초기 질문의 진정한 의도를 명확히 하려 합니다. 두 영화가 존재하고\n관련 원자적 태그가 검색되면, 우리의 접근법은 이후 원자적 선택 단계에서 질문 의도를 검증하고 정확하고 가장 적절한 청크를 선택할 수 있는 이점을\n얻습니다.\n\n\n_그림 7._ 사례 (b): KAR [3] 은 다중 원자 쿼리를 제안함으로써 관련 지식 청크를 효과적으로 검색하는 반면, Self-Ask가 채택한 단일 결정론적 후속 질문 접근법은 지식 기반의\n스키마와 정렬되지 않아 검색 실패를 초래한다.\n\n\n**KAR** **[3]** **은** **원자** **태그를** **매개체로** **삼아** **지식** **스키마** **정렬을** **개선함으로써** **기존** **기법을** **능가합니다.** 코퍼스와 질의의 표현 방식 간 불일치는 단일 결정론\n적 접근보다 다중 질의 방식을 지지하는 또 다른 핵심 요소입니다. 표현 격차는 생성된 후속 질의가 의미론적으로 정확하더라도 검색 과정을 방해할 수\n있습니다. 예를 들어, 그림 7의 사례 (b)에서 보듯이, 왼쪽의 Self-Ask와 같은 단일 경로 방식은 '오스카 뢰러의 어머니는 누구인가?'라고 직접 질문할 수\n있습니다. 그러나 지식베이스는 가족 관계를 다른 스키마('A는 B와 C의 아들이다')로 표현하므로, 질문이 정확함에도 불구하고 검색 과정이 실패합니다\n. 계층적 검색을 Self-Ask에 적용했을 때조차도, Self-Ask with Hierarchical Retrieval은 이 간극을 메우지 못했습니다. 반면, 다중 원자 쿼리를 생성하는\n우리의 접근법은 지식베이스의 다양한 표현에 대응하는 더 넓은 범위의 표현을 포괄합니다. 제시된 사례에서 오스카 로엘러의 어머니를 직접 묻는 원자\n쿼리는 동일한 검색 문제를 겪지만, 그의 부모에 대한 정보를 찾는 대체 쿼리는 목표 정보 블록을 성공적으로 검색합니다. 이는 쿼리 생성의 유연성이 지\n식베이스 구조와의 정렬 가능성을 높이는 방식을 보여줍니다.\n\n\n23\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-22-0.png)\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n정확한 정보를 얻는 것.\n\n\n우리의 방법론은 덩어리를 직접 검색하기보다 원자 태그를 검색하는 것을 강조합니다. 이러한 설계 선택은 그림 7에 묘사된 사례 (b)에서 잘 드러납니\n다. 코퍼스의 지식 덩어리는 'A는 B와 C의 아들이다'라는 패턴으로 구조화되어 있어 '...의 어머니는 누구인가'와 같은 쿼리로 직접 검색하기 어렵습니다\n. 우리의 전문 지식베이스에서는 이러한 직접적인 질의가 'A는 B의 어머니이다' 또는 'A는 B의 아버지이다' 패턴에 부합하는 덩어리를 검색하는 경향이\n있습니다. 검색을 위한 중간 매개체로 원자 태그를 활용함으로써, 우리의 접근법은 단일 질의와 지식베이스 내 다중 문장 구조 사이의 간극을 효과적으\n로 좁힙니다. 이는 본 사례에서 '~의 어머니' 대 '~의 아들'로 대표되는 표현 패턴 차이를 연결하는 데 기여합니다.\n\n\n**KAR** **[3]** **은** **간결하고** **관련성이** **높은** **문맥을** **유지함으로써** **중간** **답변에** **의존하는** **방법들보다** **우수한** **성능을** **보입니다.** 후속 처리를 위해 중간 답변만 보존\n하는 Self-Ask와 같은 방법과 달리, 본 방법은 전체 청크를 문맥 정보로 보존합니다. 원자 선택 단계에서, 우리는 원본 청크의 관련 내용에 대한 후보 요\n약으로 원자 태그 목록을 제시합니다. 이 전략은 토큰 사용량을 크게 줄이고 관련 정보 선택 과정을 단순화합니다. 그림 8의 사례 (c)는 본 접근법의 이중\n적 이점을 보여줍니다: 첫째, 선별된 원자 태그 목록에서 선택함으로써 관련 정보 식별을 효율화합니다; 둘째, 중간 답변만 유지하는 대신 선택된 전체 청\n크를 보존함으로써 정확하고 포괄적인 후속 처리를 위한 풍부한 맥락을 보장합니다. 왼쪽의 Self-Ask 방식은 대상 청크를 검색하지만 과도한 문맥 정보\n로 인해 관련 'Ernie Watts'를 정확히 식별하지 못합니다. Self-Ask에서 검색된 청크는 중간 답변 생성 후 폐기되므로, 이 방법은 잠재적으로 잘못된 경로\n를 따라 부정확한 결론에 도달할 수 있습니다. 반면, 우리의 접근법은 간결한 목록에서 적절한 원자 태그를 효율적으로 필터링하고 선택할 수 있습니다.\n\n이번 라운드의 원자 태그는 어니 와츠의 역할과 관련되지만, 그의 출생지에 대해 추가로 질의할 필요가 없습니다. 이 정보는 선택된 청크 내에 포함되어\n있으며, 후속 라운드에서 맥락으로 활용될 수 있도록 유지되기 때문입니다.\n\n\n_그림 8._ 사례 (c): KAR [3] 은 대상 선택을 위한 간결한 원자 태그 목록을 활용하는 동시에 풍부한 문맥 지원을 위해 전체 청크를 유지한다는 장점이 있다. 반면 Self-Ask의 접근\n법은 관련 청크 검색에는 성공하지만, 문맥을 위해 중간 답변에 의존하는 한계로 인해 결국 잘못된 최종 답변을 생성하게 된다.\n\n\n24\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-23-0.png)\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n**A.7.** **프롬프트** **설계**\n본 접근법은 네 가지 별개의 프롬프트를 사용한다: (1) 원자적 질문 태깅 프롬프트: 소스 단락을 전처리할 때 사용되며, 각 단락을 여러 원자적 질문과\n원자적 태그로 연결한다; (2) 원자적 질의 제안 프롬프트: 다중 원자적 질의 제안을 생성할 때 사용되며, 알고리즘 1의 1행을 참조한다; (3) 원자 태그 선\n택 프롬프트: 주어진 질문 목록에서 가장 유용한 원자 태그를 선택할 때 사용되며, 알고리즘 1의 1번 라인을 참조함; (4) 질문 답변 프롬프트: 분해 루프\n종료 시 주어진 질문에 대한 최종 답변을 생성하기 위해 적용되며, 알고리즘 1의 1번 라인에 설명됨.\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-24-0.png)\n\n\n\n\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-24-1.png)\n\n\n\n\n\n25\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-25-0.png)\n\n\n\n\n\n`#` 출력 결과\n\n\n\n![](assets/md_images/할루시네이션-논문-1_From-Complex-to-Atomic-Enhancing-Augmented-Generation-via-Knowledge-Aware-Dual-Rewriting-and-Reasoning-ko.pdf-25-1.png)\n\n\n\n\n\n**시연** 기반 **논의** 현재 실험에서는 모든 프롬프트가 제로샷 방식으로, 즉 기대되는 추론 논리를 설명하는 시연이 제공되지 않습니다. 시연이 성능 향상에\n기여할 수 있는지 탐구하기 위해 제거 연구를 설계했습니다. 기존에 사용된 Self-Ask w/ Retrieval 및 IRCoT 방법론을 적용하여 프롬프트와 작업 설명\n을 수정함으로써, 이러한 방법들의 제로샷 방식이며 시연이 없는 변형 버전을 생성했습니다. 이를 **'검색** **기반** **제로샷** 셀프-어스크 **(Zero-Shot Self-Ask**\n**w/ Retrieval)** '와 **'제로샷 IRCoT(Zero-Shot IRCoT)** '로 명명했습니다. 실험 결과는 표 14에 제시되어 있습니다. 실험 결과에 따르면, '검색 기반 제로샷\n셀프-어스크' 방법은 2Wiki 및 MuSiQue 데이터셋에서 정확도가 소폭 하락했는데, 이는 생성 과정의 본질적인 무작위성 때문일 수 있습니다. 그러나 데\n모를 포함하면 모든 F1 점수가 크게 향상되고 IRCoT 방법의 전반적인 성능이 개선됩니다. 이는 단계별 분해 접근법에 의존하는 방법에 데모가 특히 유\n용할 수 있음을 시사합니다. 따라서 데모 통합은 KAR [3] 프레임워크 내에서 향후 연구를 위한 유망한 방향으로 확인되었습니다.\n\n\n26\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n_표 14._ 성능 비교: 제로샷 대 퓨샷.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|방법|HotpotQA<br>F1 정확도|2Wiki<br>F1 Acc|MuSiQue<br>F1 Acc|\n|---|---|---|---|\n|검색을 통한 제로샷 자기질문<br>검색 기능이 포함된 자체 질문|55.76<br>71.40<br>76.20<br>80.00|54.98<br>76.20<br>69.06<br>75.00|40.97<br>46.76<br>50.40<br>51.40|\n|제로샷 IRCoT<br>IRCoT|58.22<br>67.30<br>75.80<br>81.00|49.69<br>60.20<br>63.83<br>70.40|37.17<br>47.57<br>43.00<br>49.20|\n\n\n\n**A.8.** **법률** **벤치마크** **평가**\n이 하위 섹션에서는 LawBench(Fei et al., 2023)와 Open Australian Legal QA(Butler, 2023)라는 두 가지 법률 벤치마크에서 본 접근법의 성능을 제시\n합니다. 이를 진행하기 전에 각 벤치마크에 대한 간략한 설명을 제공합니다.\n\n\n**LawBench** LawBench는 중국 법률을 위한 포괄적인 법률 벤치마크입니다. 이는 대규모 언어 모델(LLM)의 법률 역량을 정확히 평가하기 위해 세심하\n게 설계된 20개의 과제로 구성됩니다. 일부 기존 벤치마크가 객관식 문제에만 의존하는 것과 달리, LawBench는 실제 적용과 밀접하게 연관된 다양한\n유형의 과제를 포함합니다. 이러한 과제에는 법인체 인식, 독해력 평가, 범죄 금액 계산, 법률 상담 등이 포함됩니다. 모든 과제가 RAG(질문-응답-추론)\n중심은 아니므로(예: 독해), 6가지 특정 과제를 선정하였으며, 이는 표 15에 상세히 기술되어 있습니다. 각 과제의 문제 수는 500개입니다.\n\n|과제번호|표 15. LawBench 작업 개요 과제|요 유형|지표|\n|---|---|---|---|\n|과제 번호|과제<br>|유형<br>|지표|\n|1-1<br>1-2<br>3-1<br>3-2<br>3-6<br>3-8|법령 낭독<br>법률 지식 Q&A<br>법률 예측 (사실 기반)<br>법령 예측 (시나리오 기반)<br>사례 분석<br>상담|생성<br>단일 선택<br>복수 선택<br>생성<br>단일 선택<br>생성|F1<br>EM<br>EM<br>F1<br>EM<br>F1|\n\n\n\n또한 독자들의 참고를 위해 이러한 작업의 예시 질문을 제공합니다(GPT-4를 사용하여 번역됨).\n\n\n\n`1-1 :` 다음 질문에 기사 내용을 직접 제시하여 답변하십시오 `:`\n\n\n\n‹→ 증권법 제 `76` 조의 내용은 무엇입니까 `?`\n`1-2 : '` 증권법 `'` 에 따르면 `,` 주식\n‹→ 거래소에 관한 다음 진술 중 틀린 것은 무엇입니까 `? A:` 증권거래소의 허가 없이는 어떤 기관도\nA: 개인 또는 기관은 실시간 증권 거래 정보를 공개할 수 있습니다 `; B:` 증권 거래소는\n‹→ 거래소는 필요 시 주요 비정상적 거래 조건을 보이는 증권 계좌에 대한 거래를 제한할 수 있으며 `,` 증권 규제 당국에 보고할 수 있다 `.`\n거래 조건을 보인 증권 계좌에 대해 필요에 따라 거래를 제한하고 `,` 증권 규제 당국에 보고할 수 있다\n`C:` 회원제 증권거래소의 축적된 재산은 회원에게 속하며 `,` 그 권리는 회원들이 공동으로 향유한다 `.`\n‹→ 증권거래소의 축적된 재산은 회원에게 귀속되며 `,` 회원들은 공동으로 권리를 향유한다 `.`\n‹→ 회원 `;` 존속 기간 동안 축적된 재산은 회원에게 분배될 수 없다 `.`\n`D:` 증권거래소는 상장규정 `,` 거래규정 `,` 회원관리규정 및 기타 증권법에 따른 관련 규정을 제정한다 `.`\n‹→ 관리 규정 및 기타 관련 규정을 증권법 및\n‹→ 행정 규정에 따라 증권 거래소는 상장 규정 `,` 거래 규정 `,` 회원 관리 규정 및 기타 관련 규정을 제정하고\n국무원( `)` 에 기록을 위해 보고한다 `.`\n`3-1 :` 다음 사실과 혐의에 근거하여 관련 조항을 제시하십시오 `.`\n\n\n\n형법 `.` 사실관계 `:` 길림성 유수시 피고인은 `2015` 년 `11` 월 `15` 일\n‹→ `2015` 년 `11` 월 `15` 일 `,` 피고인 허씨는 국씨 `(` 국씨 `)` 가 소유한\n‹→ 차량 `(` 번호판 `xxx)` 에 대한 월세 `3,900` 위안 `(` 월납 `)` 을 정했다 `. 2016` 년 `1` 월 `19` 일 `,` 구오 씨의 모르게\n‹→ `3,900.00` 위안으로 매월 지급하기로 했다 `. 2016` 년 `1` 월 `19` 일 `,` 구오 씨의 알지 못하는 사이에 피고인 허 씨는\n‹→ 피고인은 사실을 숨기고 자신이 택시의 소유주라고 거짓으로 주장했다 `.`\n‹→ 그는 피해자 마 씨와 월 임대료 `3,800` 위안 `,` 임대 기간 `1` 년의 차량 임대 계약을 체결하고 총 `50,600` 위안을 징수했다 `.`\n‹→ `3,800.00` 위안 `,` 임대 기간 `1` 년으로 계약하고 `,` 마 씨로부터 총 `50,600.00` 위안의\n`2016` 년 `2` 월 `26` 일 `,` 해당 택시는\n‹→ 피해자 마로부터 소유주 구오가 회수하였다 `.` 피해자 마는 피고 허에게 임대료와 보증금을 반환할 것을 반복하여 요구했으나 `,` 피고 허는 반환을 거부하였\n다 `.`\n‹→ 피고인 허에게 임대료와 보증금을 반환해 달라고 요구했으나 `,` 피고인 허는 이를 거부했다 `.`\n\n\n\n27\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n‹→ 피해자 진술서 `,` 증인 진술서 `,` 서면 증거 등을 제시하였으며 `,` 피고인 허 씨가\n28\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n_표 16._ 법적 벤치마크에 대한 평가 결과 (표 15에 명시된 대로 지표는 **F1 / EM** )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|과제|Col2|제로샷 CoT|GraphRAG 로컬|우리 모델 (N=5)|\n|---|---|---|---|---|\n|로벤치|1-1<br>1-2<br>3-1<br>3-2<br>3-6<br>3-8|21.31<br>54.24<br>53.32<br>27.51<br>51.16<br>17.44|23.27<br>62.60<br>74.60<br>25.98<br>47.64<br>18.43|**78.58**<br>**70.60**<br>**83.16**<br>**46.05**<br>**61.91**<br>**23.58**|\n|오픈 오스트레일리아 법률 QA|오픈 오스트레일리아 법률 QA|25.10|34.35|**63.34**|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|표 17. 법률 벤|Col2|벤치마크에 대한 평가 결|결과 (지표는 정확도)|Col5|\n|---|---|---|---|---|\n|작업|작업|제로샷 CoT|GraphRAG로컬|우리 방법 (N=5)|\n|로벤치|1-1<br>1-2<br>3-1<br>3-2<br>3-6<br>3-8|1.23<br>54.00<br>49.90<br>15.83<br>51.12<br>49.70|16.60<br>63.40<br>75.40<br>27.60<br>57.00<br>58.80|**90.12**<br>**70.60**<br>**88.82**<br>**67.54**<br>**62.73**<br>**61.72**|\n|오픈 오스트레일리아 법률 QA|오픈 오스트레일리아 법률 QA|16.48|88.27|**98.59**|\n\n\n\n피고인 허씨는 불법 점유를 목적으로 `,` 서명 과정에서 사실을 조작하고 진실을 은폐함으로써 타인의 재산을 사기적으로 취득하였다 `.`\n‹→ 계약 체결 및 이행 과정에서 사실을 조작하고 진실을 은폐하여 타인의 재산을 사취하였음 `.`\n‹→ 계약 체결 및 이행 과정에서 사실을 조작하고 진실을 은폐함으로써 타인의 재산을 사취하였다 `.` 해당 금액은 상대적으로 거액이었으며 `,` 그의 행위는\n‹→ 중화인민공화국 형법 제 `xx` 조 규정을 위반하였으며\n`‹` → 계약 사기죄로 형사 책임을 물어야 한다 `.`\n사기 `.`\n\n\n\n`3-2 :` 구체적인 시나리오와 질문에 따른 법적 근거를 제시해 주십시오 `.`\n\n\n\n‹→ 특정 법조문의 내용만 필요하며 `,` 각 시나리오에는\n‹→ 하나의 법률 조항만 포함됩니다 `.` 시나리오 `:` 화물선이 하역 항구에 도착했으나\n수하인이 제때 도착하여 화물을 인수하지 못한 경우 `.` 어떤 법적\n선장은 다른 적절한 장소에서 화물을 하역할 수 있는가 `?`\n`3-6:` 바가 개업한 지 `1` 년 후 `,` 사업 환경이 급변했고 모든\n‹→ 파트너들은 대책을 논의하기 위해 회의를 열었다 `. '` 합자기업법 `'` 에 따르면\n‹→ 기업법에 따르면 `,` 다음의 투표 사항은 유효한 투표로 간주됩니다 `: A:` 장\n`‹` → `'` 통청 `'` 이라는 이름이 매력적이지 않다고 생각하여 `'`\n‹→ 통성 바 `.’` 왕과 조는 동의하지만 리는 반대한다 `; B:` 부진한 사업 상황을 고려하여\n‹→ 사업 부진을 고려하여 왕은 한 달간 영업을 중단하고 리모델링 및\n장 씨와 조 씨는 동의하지만 리 씨는 반대함 `; C:` 바의 긴급한 필요로 인해\n‹→ 바의 긴급한 필요로 인해 `,` 조는 바에 커피 머신 일괄 판매를 제안한다 `.` 장과\n왕은 동의하지만 리는 반대한다 `; D:` 네 명의 파트너가 법률 사무소 운영 경험이 부족하다는 점을 고려하여\n‹→ 관리 경험이 부족하다는 점을 고려하여 `,` 리는 자신의 친구 왕을 관리 파트너로 임명할 것을 제안한다 `.` 장\n장 씨와 왕 씨는 동의하지만 `,` 조 씨는 반대한다 `.`\n\n\n\n`3-8:` 거주자 `A` 가 `B` 에게 집을 임대했다 `. A` 의 동의하에 `B` 는 임대 주택을 리모델링한 후\n`C` 가 단독으로 주택의 하중 지지 구조를 변경하였다 `.`\n‹→ 왜 `A` 는 `B` 에게 계약 위반에 대한 책임을 지도록 요구할 수 있나요 `?`\n\n\n\n**오픈** **오스트레일리아** **법률 QA** 벤치마크는 오스트레일리아 법률 코퍼스에서 GPT-4가 합성해낸 2,124개의 질문과 답변으로 구성됩니다. 모든 질문은\n생성형 유형입니다. 예시: \"뉴사우스웨일스 주 앤더슨 대 아미티지 사건 [2014] NSWCATCD 157에서 법 제63조에 따른 임대인의 일반적 의무는 무엇\n인가?\"\n평가 결과는 표 16에 정리되어 있으며, 여기서 우리는 \"GraphRAG Global\"보다 일반적으로 이러한 작업에서 더 우수한 성능을 보이는 \"GraphRAG\nLocal\"과만 비교합니다.\n\n\n상기 이유로, 우리는 모든 실험 결과를 평가하기 위해 GPT-4를 사용하며 정확도( **Acc** )를 표 17에 보고합니다. 표 16과 표 17의 결과를 비교해 보면, 일부\n29\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n지표가 크게 변했음에도 결과의 순서는 유지되는 것을 관찰할 수 있습니다. 다음 섹션에서는 이러한 변화의 원인을 규명하고자 하며,\n30\n**복잡성에서** **원자성으로: 증강** **생성** **능력** **향상**\n이는 향후 RAG 프레임워크 평가를 위한 더 나은 지표 설계에 유용한 통찰력을 제공할 수 있을 것입니다.\n\n\n1. 생성 작업(1-1, 3-2, Open Australian Legal QA)에서 본 접근법의 정확도는 크게 향상됩니다. 이러한 작업에서 우리의 답변은 종종 의미론적으로\n는 동등하지만 구문적으로는 골든 답변과 다릅니다. 이는 GPT-4가 답변의 의미적 내용을 비교할 수 있기 때문에 지표 성능이 개선된 이유를 설\n명합니다. 이는 \"Open Australian Legal QA\" 작업에 대한 \"GraphRAG Local\" 결과에도 적용됩니다.\n\n\n2. 생성 작업 1-1과 3-2에서는 \"GraphRAG Local\"의 정확도가 하락합니다. 해당 작업은 법률 조항 인용 및 예측을 포함하며 특정 조항 검색이 필요합\n니다. 상세 분석 결과, \"GraphRAG Local\"은 종종 올바른 조항을 검색하지 못하거나 잘못된 조항을 참조하지만, 법률 정보를 반복하는 경향이 있\n음을 확인했습니다. 따라서 법률 명칭과 \"XX 법률에 따르면, XX 조항...\"과 같은 일반적인 접두사를 재구성하는 것만으로도 토큰 수준 리콜을 개\n선할 수 있습니다.\n\n\n3. 우리의 접근법과 \"GraphRAG Local\" 모두 작업 3-8에서 상당한 정확도 향상을 보입니다. 첫 번째 요점에서 언급된 이유 외에도, 골든 답변의 품\n질도 이러한 차이에 기여할 수 있습니다. 과제 3-8의 질문과 골든 답변은 컨설팅 웹사이트에서 수집되었기 때문에 품질이 제각각입니다. 예를 들\n어, 한 질문은 \"원혼인에서 태어난 자녀들이 아버지를 부양할 의무가 있는가?\"라고 묻습니다. 그러나 제공된 골든 답변에는 미성년 자녀에 대한\n부모의 부양 의무를 다루는 관련 없는 조항인 \"제1067조\"가 포함되어 있습니다.\n\n\n\n질문 `:` 양부모가 모두 이혼하여 각자 새로운 가정을 꾸린 경우\n‹→ 새로 아이를 둔 가정이며 `,` 법원 판결에 따르면 아버지는\n‹→ 자녀가 만 `18` 세가 될 때까지 매월 양육비를 어머니에게 지급해야 합니다 `.`\n원래 결혼에서 태어난 자녀들은 아버지를 부양할 의무가 있나요 `?`\n아버지를 부양할 의무가 있습니까 `?`\n\n\n\n참조 답변 `:` 우리나라에서는 친자녀가 이혼한 부모를 부양할 의무가 있습니다 `.`\n\n\n\n`‹` → 자녀와 부모의 관계는\n`‹` → 자녀나 부모의 이혼으로 인해 해산되지 않는다 `.`\n자녀의 부모 부양은 법적 의무이다 `.` 자녀가 노부모를 부양하지 않을 경우\n‹→ 노인을 부양하지 않을 경우 `,` 부모는 인민법원에 직접 소송을 제기할 수 있으며\n법적 근거 `:` 제\n`‹` → 중화인민공화국 민법 제 `1067` 조는 부모가\n‹→ 부양의무를 이행하지 않을 경우 `,` 미성년 자녀 또는 독립생활이 불가능한 성인 자녀는\n‹→ 독립적으로 생활할 수 없는 경우 부모에게 부양을 청구할 권리가 있다 `.`\n성년 자녀가 부양의무를 이행하지 않을 경우 `,`\n‹→ 무능력자이거나 생계에 곤란을 겪는 부모는 부양을 청구할 권리가 있다 `.`\n성인 자녀로부터 `.` 제 `1084` 조는 부모와 자녀 간의 관계가\n‹→ 부모의 이혼으로 인해 부모와 자녀의 관계가 소멸하지 않는다 `.`\n‹→ 이혼 후 자녀가 아버지 또는 어머니 중 한쪽에 의해 직접 양육되더라도\n`‹` → 여전히 양친의 자녀이다 `.` 변호사 설명 `:`\n‹→ 부모가 이혼한 후에도 성인 자녀는 여전히 양쪽 부모를 부양할 의무가 있습니다 `.`\n성인 자녀가 부모를 부양할 의무는\n‹→ 부모의 관계 변화로 인해 달라지지 않습니다 `.` 성인 자녀가\n<→ 부양 의무를 이행하지 않을 경우 `,` 무능력하거나 생계에 어려움을 겪는 부모는\n`‹` → 생계에 어려움을 겪는 부모는 성인 자녀에게 부양을 요청할 권리가 있습니다 `.` 답변 `:` 예 `,` 부양 의무가 있습니다 `.` 법적 근거 `:`\n\n‹→ 중화인민공화국 민법 제 `1069` 조에는 다음과 같이 규정되어 있습니다 `:`\n`‹` → `‘` 자녀의 부모 부양 의무는\n‹→ 부모의 혼인 관계 변화로 인해 종료되지 않는다 `.’` 따라서 `,` 설령\n‹→ 부모가 이혼하고 새 배우자와 재혼하여 새 자녀를 두더라도 `,` 원래 자녀들은\n‹→ 여전히 부모를 부양할 의무가 있다 `.`\n\n\n4. 선택 과제 1-2, 3-1, 3-6에 대한 모든 방법의 정확도는 예상대로 F1 점수와 거의 일치한다. 예외는 과제 3-1로, 차이는 주로 GPT-4의 중국어 이해\n능력, 특히 아라비아 숫자와 한자 숫자 구별 능력에서 비롯된다. 중국 법에서는 모든 숫자를 한자로 표기하지만, 정답 예시에서는 모든 숫자를 아\n라비아 숫자로 표기했다.\n\n\n31\n\n\n",
    "eng_pdf": "pdf/eng/할루시네이션 논문 1_From Complex to Atomic Enhancing Augmented Generation via Knowledge-Aware Dual Rewriting and Reasoning.pdf",
    "kor_pdf": "pdf/kor/할루시네이션 논문 1_From Complex to Atomic Enhancing Augmented Generation via Knowledge-Aware Dual Rewriting and Reasoning ko.pdf"
  },
  {
    "id": "paper_8",
    "title": "DEEPTRAVEL: AN END-TO-END AGENTIC REINFORCEMENT LEARNING FRAMEWORK FOR AU### TONOMOUS TRAVEL PLANNING AGENTS",
    "eng_md": "## DEEPTRAVEL: AN END-TO-END AGENTIC RE- INFORCEMENT LEARNING FRAMEWORK FOR AU### TONOMOUS TRAVEL PLANNING AGENTS\n\n**Yansong Ning** **[1]** _[∗]_ **, Rui Liu** **[2]** **, Jun Wang** **[2]** **, Kai Chen** **[2]** **, Wei Li** **[2]** **, Jun Fang** **[2]**\n**Kan Zheng** **[2]** **, Naiqiang Tan** **[2]** **, Hao Liu** **[1]** _[†]_\n1 The Hong Kong University of Science and Technology (Guangzhou)\n2 Didichuxing Co. Ltd\nyning092connect.hkust-gz.edu.cn, liuh@ust.hk\n_{_ invincibleliu,tannaiqiang _}_ @didiglobal.com\nABSTRACT\nTravel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools/resources for travel itinerary generation, ensuring\nenjoyable user experience. Despite its benefits, existing studies rely on handcraft prompt and fixed agent workflow, hindering more flexible and autonomous\nTP agent. This paper proposes **DeepTravel**, an end-to-end agentic reinforcement\nlearning framework for building autonomous travel planning agent, capable of\nautonomously planning, executing tools, and reflecting on tool responses to explore, verify, and refine intermediate actions in multi-step reasoning. To achieve\nthis, we first construct a robust sandbox environment by caching transportation,\naccommodation and POI data, facilitating TP agent training without being constrained by real-world APIs limitations (e.g., inconsistent outputs). Moreover, we\ndevelop a hierarchical reward modeling system, where a trajectory-level verifier\nfirst checks spatiotemporal feasibility and filters unsatisfied travel itinerary, and\nthen the turn-level verifier further validate itinerary’s detail consistency with tool\nresponses, enabling efficient and precise reward service. Finally, we propose the\nreply-augmented reinforcement learning method that enables TP agent to periodically replay from a failures experience buffer, emerging notable agentic capacity.\nWe deploy trained TP agent on DiDi Enterprise Solutions App and conduct comprehensive online and offline evaluations, demonstrating that DeepTravel enables\nsmall-size LLMs (e.g., Qwen3-32B) to significantly outperform existing frontier\nLLMs such as OpenAI-o1/o3 and DeepSeek-R1 in travel planning tasks.\n\n\nFigure 1: Performance of DeepTravel on synthesized offline benchmark and online user data.\n\n\n_∗_ Work done during internship at Didichuxing Co. Ltd.\n\n_†_ Corresponding author.\n\n\n1\n\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-0-0.png)\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-1-0.png)\n\nFigure 2: Comparison between existing studies and our autonomous travel planning agent paradigm.\n\n\n1 INTRODUCTION\nTravel planning (TP) aims to create a feasible itinerary Nguyen et al. (2023) that aligns with user\npreference by integrating multiple resources, such as accommodations, transportation, and Pointsof-Interests (POIs). Recently, with the advances in natural language processing, large language\nmodels (LLMs) are widely used to build TP agents Chen et al. (2024), capable of invoking external\ntools/resources Gou et al. (2023) to generate travel itinerary, offering seamless experience in human\nmobility Tang et al. (2024). TP agent has gradually become a popular tool for the modern citizens.\n\n\nIn recent literature, many efforts have been devoted to construct TP agent. Most existing approaches\nprimarily rely on carefully designed prompts, as illustrated in Figure 2(a). For example, TravelPlanner Xie et al. (2024) and TripTailor Wang et al. (2025) employ task-specific prompts to guide LLMs\nfor tool invocation and itinerary generation. More recently, researchers have begun to integrate\nthese prompt-engineering strategies into fixed agent workflow. For instance, PTS Shao et al. (2025)\nand RETAIL Deng et al. (2025) propose well-structured agent pipelines that enhance user intention\nunderstanding, enable effective tool interactions, and support accurate travel itinerary generation.\nHowever, these methods are _labor-intensive_ and face challenges in _adapting_ _to_ _new_ _user_ _query_ _or_\n_recovering from tool call failures_, limits more flexible and autonomous TP agents.\n\n\nAgentic reinforcement learning (RL) Singh et al. (2025) has recently emerged and shown possibilities for building autonomous AI agent Jaech et al. (2024) by enabling agent to interact with tools\nin a dedicated environment and refine its reasoning based on the feedback it receives. For example,\nReTool Feng et al. (2025), Kimi-Researcher MoonshotAI (2025) and WebSailor Li et al. (2025)\nutilize end-to-end agentic RL training to build the autonomous agent for math, deep research and\nweb domain, respectively. These studies motivate us to propose a tailored agentic RL framework for\nautonomous TP agent construction, addressing the limitation in existing travel planning studies.\n\n\nHowever, building an autonomous TP agent shown in Figure 2(b), that can progressively tackle\ncomplex TP tasks by autonomously interleaving tool calls and tool responses within the multi-turn\nreasoning, is non-trivial due to two key factors. _(1)_ _Dynamic_ _Travel_ _Environment._ TP agents operate in a highly dynamic environment where information—such as hotel availability, pricing, and\ntransportation options—changes continuously in real time. As a result, identical queries may produce inconsistent outputs over time due to updates in accommodations, transportation, and POI data\nsources. Training TP agents in such a constantly evolving real-world environment remains a significant challenge. _(2) Open-Ended Travel Task._ Unlike existing reinforcement learning with verified\nrewards (RLVR) paradigm Guo et al. (2025) on math or web domain, travel planning is an inherently\nopen-ended task without explicit ground truth. For example, the generated travel itinerary may vary\ndepending on personalized user preference and budget, making outcomes difficult to verify. How to\nconstruct reliable and scalable reward signals is challenging.\n\n\nTo address the aforementioned challenges, we propose **DeepTravel**, an end-to-end agentic RL training framework for autonomous travel planning agent construction. Specifically, we first construct a\n_Robust SandBox_ by caching transportation, accommodation, and POI data from multiple real-world\nAPIs across different timestamps, thereby simulating dynamic tool interactions. Within this sandbox, the TP agent can perform large-scale repeated trial-and-error learning while overcoming QPS\n2\nlimits and output inconsistencies. Moreover, we propose a _Hierarchical Reward Modeling_ system,\nwhere a trajectory-level verifier ensures the spatiotemporal feasibility of generated itineraries and\na turn-level verifier enforces fine-grained consistency with tool response, thereby yielding more\nefficient and reliable reward signals for training. Finally, we propose a _Reply-Augmented Reinforce-_\n_ment_ _Learning_ method to incentivize agentic reasoning capacity through sequential cold-start and\nRL process. Based on periodically replay from a failures experience buffer, the TP agent can learn\nand refine its previous reasoning actions, gradually emerging agentic travel planning capacity.\n\n\nWe deploy constructed TP agent in DiDi Enterprise Solutions App, and conduct evaluation using\nboth of collected online real-world user data and offline synthetic data across varying task complexity. The experimental results demonstrate that DeepTravel enables small-size LLM backbones\n(e.g., Qwen3-32B) to outperform current state-of-the-art reasoning LLMs (e.g., OpenAI-o1/o3 and\nDeepSeek-R1) and prevailing RL algorithms (e.g., GRPO and DAPO). These results establish DeepTravel as an promising framework to build autonomous TP agent.\n\n\nOur contributions are summarized as follows: (1) We establish and deploy the first autonomous\ntravel planning agent, offering new paradigm to advance existing TP studies. (2) We propose the\nfirst end-to-end agentic RL framework tailored to travel domain, which allows the training of TP\nagent under a roboust sandbox environment, reliable reward service and periodical experience replay\nstrategy. (3) Extensive experiments on online and offline data validate the effectiveness of proposed\nframework and uncover its exceptional performance across traval planning tasks.\n\n\n2 PRELIMINARY\nWe begin with the definition of query and travel itinerary, then define the problem we aim to address.\n\n\n**Definition 1** _**User Query.**_ _The user query q is expressed in natural language, which indicates user’s_\n_spatiotemporal_ _travel_ _intention_ _and_ _personalized_ _preference._ _For_ _example,_ _a_ _query_ _“Please_ _help_\n_schedule_ _a_ _three_ _day’s_ _airport_ _trip_ _from_ _Shanghai_ _to_ _Beijing”_ _represents_ _that_ _the_ _user_ _wants_ _to_\n_travel to Beijing by air and stay there for a duration of three days._\n\n\n**Definition 2** _**Travel Itinerary.**_ _A travel itinerary I_ _is defined as a structured plan including accom-_\n_modation, transportation, and detailed daily plan that integrates such as travel activity suggestions,_\n_exploration strategies for Points of Interest (POIs), and etc._\n\n\nNote that the POIs usually corresponds to popular tourist spots (e.g., _National Palace Museum_, _The_\n_Great Wall_, etc.). Now we formulate our problem:\n\n\n**Problem 1** _**Agentic Travel Planning.**_ _Given a query q,_ _the travel agent generates travel itinerary_\n_I_ _to satisfy the trip requirements through automatically planing,_ _executing tools,_ _and reflecting on_\n_tool responses to explore, verify and refine intermediate actions in multi-turn reasoning process:_\n\n\n_{τt, at}_ = _πθ_ ( _q, {τ_ 1 _, a_ 1 _, o_ 1 _, τ_ 2 _, a_ 2 _, o_ 2 _, . . ., τt−_ 1 _, at−_ 1 _, ot−_ 1 _}_ ) _,_ (1)\n_where_ _πθ_ _is_ _the_ _policy_ _of_ _travel_ _agent,_ _and_ _τt−_ 1 _, at−_ 1 _, ot−_ 1 _represent_ _agent’s_ _thought,_ _action,_ _and_\n_observation from the environment in the t −_ 1 _turn, respectively._ _The generated travel itinerary I_ _is_\n_involved in the agent’s action at in the last turn._\n\n\nAn agentic travel planning example is illustrated in Figure 2(b), where the autonomous TP agent\ncarefully think (i.e., thought wrapped with <think> and </think>) before using external tools\n(i.e., action enclosed with <tool ~~c~~ all> and </tool ~~c~~ all>) and reflect on tool response (i.e.,\nobservation wrapped with <tool ~~r~~ esponse> and </tool ~~r~~ esponse>) to explore, verify and\nrefine intermediate step in multi-turn reasoning process for generating travel itinerary.\n\n\n3 DEEPTRAVEL\nWith these notations, we propose DeepTravel, an end-to-end agentic reinforcement learning pipeline\nfor travel agent construction.\n\n\n3\n3.1 OVERVIEW\nFigure 3 illustrates the overall pipeline of DeepTravel. _(1)_ _Robust_ _SandBox_ _Construction_ invovles\ntoolkit annotation, mock data collection and update mechanism, thereby enabling simulated realworld tool interactions. _(2)_ _Hierarchical_ _Reward_ _Modeling_ proposes both of trajectory-level and\nturn-level verifier, which jointly provides reliable and efficient reward signal. _(3) Reply-Augmented_\n_Reinforcement Learning_ first conducts SFT for agentic travel planning format cold start, then conducts RL with experience replay to further incentivize the agentic capacity of LLMs.\n\n\n3.2 ROBUST SANDBOX CONSTRUCTION\nThe sandbox Lin et al. (2023) is served as a stable environment for TP agent to interact with tools,\nsimulating real-world interaction while overcoming practical output inconsistency and API limits.\nWe begin with toolkit annotation, then introduce the sandbox data collection and update mechanism.\n\n\n3.2.1 TOOLKIT ANNOTATION\nWe annotate six types of tools commonly used in travel planning. Table 1 summarizes the specifications of constructed toolkit, with parameter schemas and illustrative examples.\n\n\n**Flight** **Search.** Flight search offers information about air transportation, a fundamental aspect of\ntravel planning Shao et al. (2024a). In this work, we adopt the interface format from DiDi Enterprise\nSolutions (DiDi ES) App for this tool. Each tool call requires a departure city, an arrival city, and a\ndeparture date. For example, _“flight_ ~~_s_~~ _earch(‘Beijing’, ‘Shanghai’, ’2025-07-02’)”_ queries for flight\noptions from _Beijing_ to _Shanghai_ on July 2, 2025.\n\n\n**Train Search.** Similar to flight search, train search provides essential rail transportation information.\nWe follow the DiDi ES interface format to build this tool. As shown in Table 1, each query includes\na departure city, an arrival city, and a departure date.\n\n\n**Route Planning.** Route planning is crucial for optimizing travel time and cost Fang et al. (2024). We\nleverage the route planning services provided by DiDi Map. Each tool call requires an origin name,\na destination name, and a city name. For instance, _“route_ ~~_p_~~ _lanning(‘National Palace Museum’, ‘The_\n_Great_ _Wall’,_ _’Beijing’)”_ plans the route and calculates the distance/time details from the _National_\n_Palace Museum_ to _The Great Wall_ in Beijing.\n\n\n**Hotel** **Search.** Hotel search enables the TP agent to find suitable accommodations based on user\npreferences Yang et al. (2025b). We also follow the DiDi ES interface format for this tool. Each\ntool call includes a city name, a hotel name, a check-in date, and a check-out date. For example,\n_“hotel_ ~~_s_~~ _earch(‘Beijing’, ‘Atour’, ’2025-07-02’, ’2025-07-05’)”_ searches for available rooms at the\nAtour hotel in Beijing from July 2 to July 5, 2025.\n\n\n**POI** **Search.** POI search provides urban contextual semantics, which has been widely adopted in\ntravel planning Xie et al. (2024). Similar with route planning, we directly utilize the POI search\nservice provided by DiDi Map. Each tool call contains a query and a city name. For example,\n_“poi_ ~~_s_~~ _earch(‘The Great Wall’, ‘Beijing’)”_ helps obtain the geographic address of _The Great Wall_ .\n\n\n**Web Search.** The web contains a wealth of travel plans, serving as a valuable resource for generating\nitineraries Ni et al. (2025). We subscribe to web search services provided by Bocha AI. The tool call\nparameter is any query related to travel planning (e.g., _“web_ ~~_s_~~ _earch(‘Introduction to Beijing’)”_ ).\n\n\n3.2.2 MOCK DATA COLLECTION AND UPDATE MECHANISM\nTo simulate the dynamic nature of real-world tool interactions, where prices and availability of hotels, trains, and flights fluctuate over time, we propose a data caching mechanism. This mechanism\naddresses the challenge of real-world API QPS limits and inconsistent information retrieval during\neach search, which can hinder the agent’s ability to learn from previously failed cases.\n\n\nSpecifically, we maintain a daily-refreshed database that incorporates an on-demand caching strategy for flight, train, and hotel search data. When a new query is received, the system automatically\nstores the latest records into the database. This mechanism allows the TP agent to re-access earlier\n4\nTable 1: The specifications of toolkit in sandbox.\n\n\nType Tool name Tool call format Tool response description\nflight search flight search(depart city name, arrival ~~c~~ ity ~~n~~ ame, depart date) feasible flight options\nTransportation train search train ~~s~~ earch(depart ~~c~~ ity ~~n~~ ame, arrival ~~c~~ ity ~~n~~ ame, depart ~~d~~ ate) feasible train options\nroute planning route planning(origin name, destination ~~n~~ ame, city ~~n~~ ame) route, distance and time\nAccommodation hotel search hotel ~~s~~ earch(city name,hotel ~~n~~ ame,checkin ~~d~~ ate,checkout date) available hotel condidate\nPOI search poi ~~s~~ earch(query,city ~~n~~ ame) detailed address of POI\nAttraction\nweb search web search(query) web page related to the query\ntool response during RL training, enabling it to repeatedly learn from unsuccessful cases through\nexploring, verifying and refining its intermediate reasoning trajectories Shang et al. (2025).\n\n\n3.3 HIERARCHICAL REWARD MODELING\nThen, we present our reward modeling system. This system comprises a trajectory-level verifier and\na turn-level verifier, designed to provide efficient and reliable reward signals for agent training.\n\n\n**Trajectory-Level** **Verifier.** This verifier assesses the overall spatiotemporal feasibility of the generated travel itinerary. Given a complete reasoning trajectory _{τ_ 1 _, a_ 1 _, o_ 1 _, τ_ 2 _, a_ 2 _, o_ 2 _, . . ., τt, at}_,\ntrajectory-level verifier checks whether the final travel itinerary _at_ adheres to essential spatiotemporal constraints Chaudhuri et al. (2025). These constraints include such as logical sequence of events,\ngeographic plausibility, and satisfaction of user requirements (e.g., visiting specific POIs within a\ngiven timeframe). This coarse-grained evaluation efficiently filters out invalid itineraries, ensuring\nthat only potentially valid plans proceed to the next level of verification.\n\n\n**Turn-Level** **Verifier.** Upon successful verification by the trajectory-level verifier, the turn-level\nverifier performs a more granular evaluation. This verifier examines the consistency between the\nagent’s final travel itinerary _at_ and the information obtained by the external tools at each turn Zeng\net al. (2025). Specifically, for each turn _i_ from 1 to _t −_ 1, the verifier assesses whether _at_ accurately reflect the tool response _oi_ . By systematically verifying each turn, it helps identify factual\nhallucination/mistakes of LLM-generated travel itinerary.\n\n\n**Joint** **Reward** **Reweighting.** Two verifier are combined using a joint reward reweighting strategy\nto provide reward signal. If the trajectory-level verifier detects a violation, the final reward _r_ is\nimmediately set to 0, saving computational resources. If the trajectory passes the trajectory-level\nverification, the turn-level verifier assesses each turn. The final reward _r_ is set to 1 only if every turn\npasses verification, indicating a fully consistent and valid travel itinerary.\n\n\nThis hierarchical structure ensures both the efficiency and reliability of the reward modeling system.\nIn practical implementation, we build many travel-oriented rubrics Huang et al. (2025) for trajectorylevel and turn-level verifier, respectively. Based on human generated rubrics, we prompt DeepSeekR1 based verifier to provide reward modeling service. Details could be found in Appendix A.2.\n\n\n3.4 REPLAY-AUGMENTED REINFORCEMENT LEARNING\nThis section details relay-augmented reinforcement learning, which is a two-stage process. We first\nemploy SFT to initialize reasoning format of TP agent. Then, we leverage RL to further enhance\nagent’s reasoning capacity, and enable it to periodically learn from previous failed experience.\n\n\n3.4.1 REASONING FORMAT COLD START WITH SUPERVISED FINE-TUNING\n**Cold-Start** **Data** **Synthesis** **and** **Filtering.** We distill multi-turn trajectories from DeepSeek-R1\nunder the sandbox, yielding complete traces _y_ = _{τ_ 1 _, a_ 1 _, o_ 1 _, . . ., τt, at}_ that interleave thoughts,\ntool calls, tool responses, and final answer. Thoughts _τi_ are wrapped by <think>... </think>,\nactions _ai_ are either function calls enclosed by <tool ~~c~~ all>... </tool ~~c~~ all> or the final\nitinerary answer _at_ enclosed by <answer>... </answer>, and observations _oi_ are tool responses\nenclosed by <tool ~~r~~ esponse>... </tool ~~r~~ esponse>. Then, we utilize the constructed reward modeling system to filter incorrect trajectory, and finally we apply strict format checks to retain\nonly sequences correctly segmented by the special tags.\n\n\n5\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-5-0.png)\n\nFigure 3: An overview of DeepTravel.\n\n\n**Training Objective.** We train the TP agent to follow a system prompt _T_ and to reproduce verified\ntool-integrated trajectories. The instruction input concatenates _T_ with the user query _q_, and the\noutput is the verified trajectory _y_ . In practical training process, the tokens corresponding to the\nagent’s environmental observations _oi_ are masked out from the loss calculation Jin et al. (2025).\nThe detailed prompting template could be found in Appendix A.2.\n\n\n3.4.2 REINFORCEMENT LEARNING WITH EXPERIENCE REPLAY\nAfter cold-start, we derive a two-phase process that first saves verified unsuccessful trajectories as a\nquery buffer and then replays them in subsequent training steps Zhang et al. (2025b).\n\n\n**Rollout** **and** **Replay** **Strategy.** Following the sampling procedure in Group Relative Policy Optimization (GRPO) Shao et al. (2024b), we sample a group of trajectories _{y_ 1 _, y_ 2 _, .., yn}_ for each\nquery _q_ . If none of the trajectories in the group yields a verified correct answer, we store the query\nin an experience buffer _B_ for later replay. The motivation is that, after subsequent RL training steps,\nthe improved policy may generalize to handle previous failed hard sample Xie et al. (2025).\n\n\n**Policy Optimization.** Set RL training dataset as _D_, experience buffer as _B_, which is replayed after\nfixed training step _γ_ . We formulate the optimization goal as follows:\n\n\n\n_n_\n_i_ =1\n\n\n\n\n- - _πθ_ ( _yi_ _| q_ )\nmin\n_πref_ ( _yi_ _| q_ ) _[A][i][,]_\nmax\n_πθ_ [E] _[q][∼{][D,B][}][,][{][y][i][}]_ _i_ _[n]_ =1 _[∼][π]_ [(] _[y][|][q]_ [)]\n1\n_n_\n\n\n\n\n  - _πθ_ ( _yi_ _| q_ )  - ��\nclip _Ai_ _−_ _β_ DKL [ _πθ∥π_ ref ]\n_πref_ ( _yi_ _| q_ ) _[,]_ [ 1] _[ −]_ _[ε,]_ [ 1 +] _[ ε]_\n(2)\nwhere _ε_, _β_ are hyperparameters, _n_ is the rollout size, DKL denotes the KL-divergence, and\n_Ai_ = _ri −_ _avg_ ( _r_ ) _/std_ ( _r_ ) represents the advantage, which is computed based on the group rewards\n_r_ = _{r_ 1 _, r_ 2 _, .., rn}_ . In this work, we propose to filter out samples when the standard deviation of\ngroup rewards satisfies _std_ ( _r_ ) _≤_ _η_, where _η_ is set to 0.1. This strategy aims to exclude samples\nthat are either too simple or too hard, where the agent receives similar rewards even under largesize rollouts, thereby encouraging more effective exploration of the current policy. In addition, we\nutilize loss masking operation for tool responses tokens (wrapped with <tool ~~r~~ esponse> and\n</tool ~~r~~ esponse>) to ensure policy gradient is computed only over agent-generated tokens.\n\n\n4 EXPERIMENTS\n4.1 EXPERIMENTAL SETUP\n**Data** **Curation.** Our method is evaluated on four benchmarks, constructed using a combination\nof real-world online user data from DiDi ES and synthetic offline data: (1) **Online** **data** : we collected real-world user queries from DiDi ES App between June 1st and August 31st, spanning three\nmonths and totaling 6,224 queries. (2) **Offline** **data** : we synthesized 500 user queries under two\ndistinct settings respectively: a) queries **without** **constraints**, such as travel budget, or personalized preferences; (b) queries generated **with** **constraints** . We annotated query complexity in both\nsettings, categorizing each query as **easy,** **medium,** **or complex** . Further data curation details and\ndataset statistic information are provided in Appendix A.3.\n\n\n6\nTable 2: Overall Final Pass Rate (%) results on both of synthesized offline travel planning benchmarks and real-world online user data on DiDi ES application. The best results are **bolded**, and the\nbest baseline results in each setting are underlined.\n\n|Model|Offline Without constraint With constraint|Col3|Online|Human|\n|---|---|---|---|---|\n|**Model**|**Without constraint**<br>**With constraint**<br>**Easy**<br>**Medium**<br>**Hard**<br>**Easy**<br>**Medium**<br>**Hard**|**Without constraint**<br>**With constraint**<br>**Easy**<br>**Medium**<br>**Hard**<br>**Easy**<br>**Medium**<br>**Hard**|**Online**|**Evaluation**|\n|DeepSeek-R1<br>OpenAI-o1<br>OpenAI-o3<br>K2<br>Qwen3-235B<br>gpt-oss-120B<br>Seed-OSS-36B<br>Qwen3-32B<br>Qwen3-8B|45.55<br>34.74<br>26.00<br>36.57<br>33.16<br>20.60<br>37.30<br>20.11<br>21.19<br>54.01<br>48.42<br>25.52<br>38.69<br>36.84<br>20.24<br>40.15<br>27.37<br>20.83<br>23.65<br>13.16<br>11.19<br>29.85<br>27.89<br>23.21<br>10.95<br>9.47<br>4.76|65.36<br>43.33<br>27.09<br>30.36<br>24.44<br>17.69<br>37.50<br>26.67<br>15.69<br>57.14<br>53.33<br>21.40<br>44.64<br>26.67<br>10.37<br>64.29<br>42.22<br>16.39<br>25.00<br>13.33<br>12.34<br>53.57<br>25.00<br>9.03<br>28.57<br>26.67<br>5.35|52.89<br>30.88<br>29.17<br>53.56<br>42.70<br>37.11<br>19.36<br>32.94<br>26.72|72.00<br>54.00<br>52.00<br>64.00<br>52.00<br>48.00<br>20.00<br>38.00<br>26.00|\n|DeepTravel-8B-Cold-Start<br>DeepTravel-8B-RL<br>DeepTravel-32B-Cold-Start<br>DeepTravel-32B-RL|41.09<br>31.58<br>12.64<br>54.25<br>36.84<br>20.24<br>56.42<br>32.95<br>25.60<br>**69.34**<br>**54.74**<br>**29.17**|56.07<br>28.89<br>12.37<br>64.86<br>41.89<br>21.40<br>61.07<br>40.44<br>17.52<br>**73.21**<br>**62.22**<br>**35.75**|40.00<br>49.75<br>50.03<br>**62.77**|58.00<br>70.00<br>66.00<br>**82.00**|\n\n\n\n**Baselines.** We compare our method to nine reasoning LLMs, each is derived as a TP agent under the\nsame framework. These baselines include DeepSeek-R1 Guo et al. (2025), OpenAI-o1 Jaech et al.\n(2024), OpenAI-o3, K2 Team et al. (2025), Qwen3-235B, gpt-oss-120B OpenAI (2025), SeedOSS-36B Team (2025), Qwen3-32B, and Qwen3-8B Yang et al. (2025a). In addition, we also\ncompare our proposed DeepTravel framework with three representative RL training methods, i.e.,\nPPO Schulman et al. (2017), GRPO Shao et al. (2024b) and DAPO Yu et al. (2025).\n\n\n**Metric and Evaluation Protocol.** We use the Final Pass Rate from TravelPlanner Xie et al. (2024)\nas our evaluation metric. For the evaluation protocol, we apply our constructed reward modeling\nsystem to estimate the final pass rate. Additionally, we randomly sample 50 cases from both online\nand offline results and have human annotators assess whether the generated travel itineraries pass.\n\n\n**Training** **Details.** We leverage the Qwen3-8B and Qwen3-32B to perform SFT and RL, resulting\nin the DeepTravel-8B and DeepTravel-32B. For the SFT cold-start, we use 1K training samples,\nwith a batch size of 32 and a learning rate of 5e-6, over 2 epochs. For RL training, we select 500\nhigh-quality samples and employ the proposed Replay-Augmented RL algorithm. During RL, we\nset the rollout size to 8 and use a learning rate of 5e-7. Additionally, the maximum response length\nof agent is set as 32K tokens, and the maximum interaction turns is limited to 8. The training of\nQwen3-8B and Qwen3-32B requires 8 and 32 H800 NVIDIA GPUs respectively, and the training\nprocess takes approximately 30 hours per hundreds steps. We provide more details in Appendix A.4\n4.2 MAIN RESULTS\n**Comparison with Existing Reasoning Agents.** We compare DeepTravel across the SFT cold-start\nstage and RL training process. As reported in Table 2. Overall, DeepTravel achieves significant improvement compared with the the state-of-the-art reasoning agents using both of online and offline\nevaluation setting. In addition, We highlight two key observations: **(i)** **DeepTravel** **substantially**\n**boosts** **small-size** **LLMs** . For instance, Qwen3-8/32B is improved to the state-of-the-art levels,\nmatching and even surpassing more heavily and much larger frontier LLMs. On offline without\nconstraint setting, DeepTravel-8B and DeepTravel-32B achieves an final pass rate of 54.25% and\n69.34%, outperforming K2 by 0.1% and 28.9%, respectively. For other setting, DeepTravel-8B\nachieves comparable performance and DeepTravel-32B consistently outperforms frontier reasoning\nLLMs, such as DeepSeek-R1, OpenAI-o1 and OpenAI-o3. **(ii)** **The** **agentic** **RL** **training** **contin-**\n**ually** **improves** **domain-specific** **reasoning** **capacity** . As reported, while cold-start stage could\nestablish a strong initial policy compared to base model, the following agentic RL yields surprisingly performance improvement. Specifically, agentic RL further boost initial cold-start policy of\nDeepTravel-8B and DeepTravel-32B by 24% (40.00 to 49.75) and 25.5% (50.03 to 62.77) on online\nexperimental setting, respectively. The improvement in the offline setting is also significant.\n\n\n7\nTable 3: Compatison of DeepTravel with existing RL alogrithms on Qwen3-8B.\n\n|Col1|Offline|Col3|Col4|Human|\n|---|---|---|---|---|\n|**Model**|**Without constraint**<br>**With constraint**<br>**Easy**<br>**Medium**<br>**Hard**<br>**Easy**<br>**Medium**<br>**Hard**|**Without constraint**<br>**With constraint**<br>**Easy**<br>**Medium**<br>**Hard**<br>**Easy**<br>**Medium**<br>**Hard**|**Online**|**Human**<br>**Evaluation**|\n|Base (Qwen3-8B)<br>with PPO<br>with GRPO<br>with DAPO<br>with DeepTravel|10.95<br>9.47<br>4.76<br>48.26<br>33.25<br>14.62<br>52.36<br>34.06<br>13.52<br>52.06<br>35.52<br>15.04<br>**54.25**<br>**36.84**<br>**20.24**|28.57<br>26.67<br>5.35<br>60.05<br>34.86<br>15.04<br>61.78<br>36.65<br>15.82<br>62.24<br>40.02<br>16.54<br>**64.86**<br>**41.89**<br>**21.40**|26.72<br>45.63<br>47.78<br>46.07<br>**49.75**|26.00<br>62.00<br>64.00<br>64.00<br>**70.00**|\n\n\n\nTable 4: Ablation study of cold-start and RL on Qwen3-8B.\n\n|Model variants|Offline Without constraint With constraint|Col3|Online|Human|\n|---|---|---|---|---|\n|**Model variants**|**Without constraint**<br>**With constraint**<br>**Easy**<br>**Medium**<br>**Hard**<br>**Easy**<br>**Medium**<br>**Hard**|**Without constraint**<br>**With constraint**<br>**Easy**<br>**Medium**<br>**Hard**<br>**Easy**<br>**Medium**<br>**Hard**|**Online**|**Evaluation**|\n|DeepTravel-8B w/o ER<br>DeepTravel-8B w/o CS<br>DeepTravel-8B w/o Traj<br>DeepTravel-8B w/o Turn|51.01<br>32.21<br>8.81<br>45.99<br>25.26<br>16.79<br>50.26<br>35.47<br>18.24<br>52.05<br>28.04<br>5.25|60.86<br>35.00<br>8.75<br>53.57<br>35.56<br>22.18<br>61.06<br>33.25<br>20.75<br>59.04<br>14.24<br>10.76|40.00<br>32.45<br>26.52<br>32.45|66.00<br>48.00<br>66.00<br>58.00|\n|DeepTravel-8B|54.01<br>36.84<br>20.24|64.86<br>41.89<br>21.40|49.75|70.00|\n\n\n\n**Comparison with Existing RL Alogrithms.** We also compare DeepTrave with three recent public\nRL methods on DeepTravel-8B-Cold-Start backbone. For each RL methods, we run 100 training\nsteps with the same training sample. Table 3 reports the results. We highlight the following observations: while all online RL methods improve the base model, our proposed DeepTravel significantly\noutperforms existing RL methods on hard problems, likely due to its reply mechanism.\n\n\n4.3 ABLATION STUDY\nTo validate the effectiveness of each module in DeepTravel, we conduct an ablation study on the\nQwen3-8B dataset. Specifically, we compare the following variants. (1) DeepTravel-8B w/o ER\nremoves the **E** xperience **R** eplay module in RL training process. (2) DeepTravel-8B w/o CS removes\nthe SFT-based **C** old **S** tart stage before conduct reinforcement learning. (3) DeepTravel-8B w/o Traj\nremoves the **Traj** ectory-Level verifier in reinforcement learning training process. (4) DeepTravel8B w/o Turn removes the **Turn** -Level verifier in reinforcement learning training process.\n\n\nAs shown in Table 4, we obtain the following observations. First, the experience replay strategy\nis important for the training. Removing it will decrease model performance. Second, the coldstart stage seems to be critical for RL training as we obtain significant performance decrease after\nremoving it. The potential reason lies on that the cold-start will help LLMs learn basic tool usage,\ninstruction following capacity. Finally, we observe that the turn-level verifier contributes more to\nthe model performance. When removing it, agent’s performance decreases and it performs poorly\non hard problem. The reason may lie that more complex problem requires verification turn-by-turn.\nHowever, the trajectory-level verifier also proves important, as its removal causes a performance\ndecline as well. In addition to its contribution to accuracy, the trajectory-level verifier enhances\ntraining efficiency by removing the need of fine-grained turn-level verification.\n\n\n4.4 IN-DEPTH ANALYSIS\n**Agentic RL Training Analysis.** We present the validation reward, model entropy, average interaction turns, and tool-call accuracy throughout the RL training process in Figure 4. To evaluate the\neffectiveness of DeepTravel, we highlight the following findings: **(i) Impact of the cold-start and**\n**experience replay strategies.** The cold-start strategy helps initialize a reasonably effective policy,\nparticularly in terms of producing a more appropriate number of tool-interaction turns (whereas the\nbase model tends to overuse tool calls). The experience replay strategy contributes little in the very\nearly training stages. However, by progressively replaying previously failed samples, it steadily\nenhances the model’s capacity and eventually leads to substantial improvements over the no-replay\nbaseline in later training steps. **(ii) Impact of the sandbox.** We compare agentic RL training with\n8\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-8-0.png)\n\nFigure 4: Validation reward (final pass rate), model entropy, average interaction turn and tool call\naccuracy (success rate) throughout agentic RL training process.\n\n\nFigure 5: Capacity comparison of the autonomous TP agent across 7 human-annotated dimensions,\nevaluated on 50 randomly sampled real-world online user case.\n\n\nreal-world APIs and with the constructed sandbox. Tool-call accuracy with real APIs is unstable\nand consistently lower than that achieved in the sandbox. Under these circumstances, the TP agent\nshows no clear reward improvement, highlighting the importance of a stable sandbox environment\nfor agentic Rl training. **(iii)** **Non-decreasing** **entropy** **in** **agentic** **RL.** We further observe a nondecreasing entropy phenomenon during agentic RL training, which is also posed by several very\nrecent studies Dong et al. (2025). We think the behind reason lies on that the TP agent need to\ncontinually adapt its policy to the dynamically changing responses of external tools.\n\n\n**Real-World User Study.** We conducted a real-world user study based on evaluation dimensions defined by DiDi’s ES product manager and annotation team, with the results summarized in Figure 5.\nOverall, both the cold-start and RL approaches improved user satisfaction across the seven evaluation dimensions. In particular, the cold-start method substantially enhanced the model’s fundamental\ncapabilities, especially in understanding user intentions and in improving the completeness, feasibility, and clarity of the generated travel itineraries. However, for more advanced capabilities—such\nas capturing and satisfying personalized preferences—the cold-start approach alone proved insufficient, suggesting that these aspects may require large-scale exploration during the RL stage. Finally,\nwe observed that both the base model and the cold-start model suffered from severe hallucination\nissues, with factual error rates reaching up to 50%. RL training is able to effectively address this\nproblem, reducing hallucinations to below 20%. More annotation insights is in Appendix A.5\n5 RELATED WORK\n5.1 LLMS AS TRAVEL PLANNING AGENT\nLLMs have reshaped travel planning (TP) by enabling LLM-powered TP agent to interact with\nexternal tools for itinerary generation that aligns with user preferences. In the literature, two major\nparadigms have emerged to construct TP agent: (i) hand-craft prompt tuning, and (ii) fixed agent\nworkflow design. Hand-craft prompt tuning approaches Shao et al. (2025)—including TravelPlanner\nXie et al. (2024), TripTailor Wang et al. (2025), and ChinaTravel Shao et al. (2024a)—decompose\nthe end-to-end task into multiple sub-steps Ni et al. (2025) and introduce tailored evaluation metrics\nfor each stage. While effective, their practical value is limited by weak grounding to dynamic realworld environment (e.g., tool availability). To this end, recent work integrates prompt strategies\ninto well-structured agent pipelines. Representative studies include TravelAgent Chen et al. (2024),\n9\n\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-8-1.png)\nPTS Shao et al. (2025) and RETAIL Deng et al. (2025) design fixed workflows to enhance intention\nunderstanding, orchestrate external tools, and ensure end-to-end itinerary generation. However,\nthey are still labor-intensive to build and maintain, and they generalize poorly to new user queries or\nchanging tools and resources, limiting the flexibility and autonomy of TP agents.\n\n\n5.2 AGENTIC REINFORCEMENT LEARNING FOR LLMS\nAgentic reinforcement learning (RL) has recently been widely applied across domains to build autonomous AI agents Zhang et al. (2025a), wherein the agent interacts with tools in a dedicated\nsandbox environment and iteratively improves its policy based on received reward feedback Shang\net al. (2025). For instance, ReTool Feng et al. (2025), Kimi-Researcher MoonshotAI (2025) and\nWebSailor Li et al. (2025) are constructed reasoning agent in math, deep research and web domain.\nIn addition, many recent work like rStar2-Agent Shang et al. (2025) and AgentGym-RL Xi et al.\n(2025) make attempts to propose a unified agentic RL training framework across diverse domains,\nfacilitating the construction of foundation agent. Nevertheless, the application of agentic RL in\ntravel planning domain remains unexplored.\n\n\n6 CONCLUSION, LIMITATION AND FUTURE WORK\nIn this work, we propose DeepTravel, the first end-to-end agentic RL training framework to build\nautonomous travel planning (TP) agent, offering new paradigm for current TP studies. We first\nconstruct a robust sandbox, where the TP agent could be trained without limitation of real-world\nAPIs issues (e.g., QPS limits and inconsistency outputs). Then, we propose a hierarchical reward\nmodeling system, which first devise a coarse-grained trajectory verifier for high-level spatiotemporal\nrequirement verification, and then use a turn-level verifier to verify agent’s answer step-by-step.\nFinally, we propose a replay-augmented reinforcement learning alogrithm, which allow travel agent\nto periodically replay previous failed case, improve its out-of-domain generalization capacity. We\ndeploy the resulted autonomous TP agent in DiDi ES App, demonstrating the deployment value\nof DeepTravel. Extensive experiments on online production environment and offline synthetic data\nshow that DeepTravel enable small-size LLMs (e.g., Qwen3-8B/32B) to significantly outperform\nfrontiner reasoning LLMs, such as DeepSeek-R1 and OpenAI-o1/o3. However, DeepTravel relies\non a carefully designed reward system, which limits its extensibility. In the future, we aim to develop\na more flexible reward model and extend this framework to other domains.\n\n\nETHICS AND REPRODUCIBILITY STATEMENT\n**Ethics statement.** This work leverages real-world user data from the DiDi Enterprise Solution App\nfor model training and evaluation. However, we do not store any personal information or release real\nuser queries, so there are no additional privacy or fairness concerns. For synthesized user query, we\nprovide query case example in Appendix A.6 to ease reader understanding. For human annotations\nused in model evaluation, we also provide the full set of evaluation dimensions in Appendix A.5 to\nfacilitate replication of our annotation process.\n\n\n**Reproducibility** **statement.** To ensure reproducibility, we provide a detailed description of the\ntraining prompts in Appendix A.2 and outline the data curation pipeline in Appendix A.3. We believe\nthis information will help the research community reproduce our results. Because the DeepTravel\nincludes proprietary interfaces to DiDi Enterprise Solution, we cannot release a unified sandbox and\ntraining implementation. Nevertheless, we provide comprehensive training details in Appendix A.4,\nincluding compute resources and monitoring metrics to aid reproductionn and understanding.\n\n\nREFERENCES\nSoumyabrata Chaudhuri, Pranav Purkar, Ritwik Raghav, Shubhojit Mallick, Manish Gupta, Abhik\nJana, and Shreya Ghosh. Tripcraft: A benchmark for spatio-temporally fine grained travel planning. _arXiv preprint arXiv:2502.20508_, 2025.\n\n\nAili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, and Jiangjie Chen. Travelagent: An ai assistant\nfor personalized travel planning. _arXiv preprint arXiv:2409.08069_, 2024.\n\n\n10\nBin Deng, Yizhe Feng, Zeming Liu, Qing Wei, Xiangrong Zhu, Shuai Chen, Yuanfang Guo, and\nYunhong Wang. Retail: Towards real-world travel planning for large language models. _arXiv_\n_preprint arXiv:2508.15335_, 2025.\n\n\nGuanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia\nChen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization.\n_arXiv preprint arXiv:2507.19849_, 2025.\n\n\nBowen Fang, Zixiao Yang, Shukai Wang, and Xuan Di. Travellm: Could you plan my new public\ntransit route in face of a network disruption? _arXiv preprint arXiv:2407.14926_, 2024.\n\n\nJiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang,\nJinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms.\n_arXiv preprint arXiv:2504.11536_, 2025.\n\n\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and\nWeizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving. _arXiv_\n_preprint arXiv:2309.17452_, 2023.\n\n\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. _arXiv preprint arXiv:2501.12948_, 2025.\n\n\nZenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi\nHu, Zhanming Shen, Xiaomeng Hu, et al. Reinforcement learning with rubric anchors. _arXiv_\n_preprint arXiv:2508.12790_, 2025.\n\n\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. _arXiv_\n_preprint arXiv:2412.16720_, 2024.\n\n\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and\nJiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement\nlearning. _arXiv preprint arXiv:2503.09516_, 2025.\n\n\nKuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web\nagent. _arXiv preprint arXiv:2507.02592_, 2025.\n\n\nJiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. Agentsims: An\nopen-source sandbox for large language model evaluation. _arXiv_ _preprint_ _arXiv:2308.04026_,\n2023.\n\n\nMoonshotAI. Kimi-researcher end-to-end rl training for emerging agentic capabilities.\n_https://moonshotai.github.io/Kimi-Researcher/_, 2025.\n\n\nPhuong Minh Binh Nguyen, Xuan Lan Pham, and Giang Nu To Truong. The influence of source\ncredibility and inspiration on tourists’ travel planning through travel vlogs. _Journal_ _of_ _Travel_\n_Research_, 64:222 – 237, 2023.\n\n\nHang Ni, Fan Liu, Xinyu Ma, Lixin Su, Shuaiqiang Wang, Dawei Yin, Hui Xiong, and Hao Liu. Tprag: Benchmarking retrieval-augmented large language model agents for spatiotemporal-aware\ntravel planning. _arXiv preprint arXiv:2504.08694_, 2025.\n\n\nOpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025.\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.\n\n\nNing Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng\nDong, Xudong Zhou, Bowen Zhang, et al. rstar2-agent: Agentic reasoning technical report. _arXiv_\n_preprint arXiv:2508.20722_, 2025.\n\n\n11\nJie-Jing Shao, Bo-Wen Zhang, Xiao-Wen Yang, Baizhi Chen, Si-Yu Han, Wen-Da Wei, Guohao\nCai, Zhenhua Dong, Lan-Zhe Guo, and Yu-feng Li. Chinatravel: An open-ended benchmark for\nlanguage agents in chinese travel planning. _arXiv preprint arXiv:2412.13682_, 2024a.\n\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. _arXiv preprint arXiv:2402.03300_, 2024b.\n\n\nZijian Shao, Jiancan Wu, Weijian Chen, and Xiang Wang. Personal travel solver: A preferencedriven llm-solver system for travel planning. In _Proceedings of the 63rd Annual Meeting of the_\n_Association for Computational Linguistics (Volume 1:_ _Long Papers)_, pp. 27622–27642, 2025.\n\n\nJoykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool\nintegration for llms via reinforcement learning. _arXiv preprint arXiv:2505.01441_, 2025.\n\n\nYihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Zhaofeng Wu, Dingyi Zhuang, Jushi Kai, Kebing Hou, Xiaotong Guo, Han Zheng, et al. Itinera: Integrating spatial optimization with large\nlanguage models for open-domain urban itinerary planning. _arXiv_ _preprint_ _arXiv:2402.07204_,\n2024.\n\n\nByteDance Seed Team. Seed-oss open-source models, 2025.\n\n\nKimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen,\nYanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. _arXiv_\n_preprint arXiv:2507.20534_, 2025.\n\n\nKaimin Wang, Yuanzhe Shen, Changze Lv, Xiaoqing Zheng, and Xuan-Jing Huang. Triptailor:\nA real-world benchmark for personalized travel planning. In _Findings_ _of_ _the_ _Association_ _for_\n_Computational Linguistics:_ _ACL 2025_, pp. 9705–9723, 2025.\n\n\nZhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, et al. Agentgym-rl: Training llm agents for long-horizon\ndecision making through multi-turn reinforcement learning. _arXiv_ _preprint_ _arXiv:2509.08755_,\n2025.\n\n\nJian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and\nYu Su. Travelplanner: A benchmark for real-world planning with language agents. _arXiv preprint_\n_arXiv:2402.01622_, 2024.\n\n\nTian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu,\nZhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement\nlearning. _arXiv preprint arXiv:2502.14768_, 2025.\n\n\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. _arXiv_ _preprint_\n_arXiv:2505.09388_, 2025a.\n\n\nDongjie Yang, Chengqiang Lu, Qimeng Wang, Xinbei Ma, Yan Gao, Yao Hu, and Hai Zhao. Plan\nyour travel and travel with your plan: Wide-horizon planning and evaluation via llm. _arXiv_\n_preprint arXiv:2506.12421_, 2025b.\n\n\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong\nLiu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at\nscale. _arXiv preprint arXiv:2503.14476_, 2025.\n\n\nSiliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, and Mingyi Hong. Reinforcing multi-turn reasoning in llm agents via turn-level credit assignment. _arXiv_ _preprint_\n_arXiv:2505.11821_, 2025.\n\n\nGuibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou,\nZhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning\nfor llms: A survey. _arXiv preprint arXiv:2509.02547_, 2025a.\n\n\nHongzhi Zhang, Jia Fu, Jingyuan Zhang, Kai Fu, Qi Wang, Fuzheng Zhang, and Guorui Zhou.\nRlep: Reinforcement learning with experience replay for llm reasoning. _arXiv_ _preprint_\n_arXiv:2507.07451_, 2025b.\n\n\n12\n\n\n### **DeepTravel: An End-to-End Agentic Reinforcement Learning** **Framework for Autonomous Travel Planning Agents** _Supplementary Material_\n\nCONTENTS\n**1** **Introduction** **2**\n**2** **Preliminary** **3**\n**3** **DeepTravel** **3**\n3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.2 Robust SandBox Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.2.1 Toolkit Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.2.2 Mock Data Collection and Update Mechanism . . . . . . . . . . . . . . . 4\n3.3 Hierarchical Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3.4 Replay-Augmented Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 5\n3.4.1 Reasoning Format Cold Start with Supervised Fine-Tuning . . . . . . . . . 5\n3.4.2 Reinforcement Learning with Experience Replay . . . . . . . . . . . . . . 6\n**4** **Experiments** **6**\n4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n4.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n4.3 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n4.4 In-Depth Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n**5** **Related Work** **9**\n5.1 LLMs as Travel Planning Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n5.2 Agentic Reinforcement Learning for LLMs . . . . . . . . . . . . . . . . . . . . . 10\n**6** **Conclusion, Limitation and Future Work** **10**\n**A** **Appendix** **14**\nA.1 Usage of Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nA.2 Prompt Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nA.3 Data Curation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nA.3.1 Data Synthesization Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . 16\nA.3.2 Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nA.4 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nA.5 Human Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA.6 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n13\nA APPENDIX\nA.1 USAGE OF LARGE LANGUAGE MODELS\nIn this paper, we primarily use large language models (LLMs) to generate figure plots. We also\nemploy LLMs to identify and correct clear grammatical errors in the authors’ drafted paragraphs.\n\n\nA.2 PROMPT TEMPLATE\nIn this section, we provide prompt used in this work, including system prompt of DeepTravel and\nthe reward model prompt template of the trajectory-level and turn-level verifier.\n\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-13-0.png)\n\n\n\n\n\n\n\n**System** **Prompt** **of** **DeepTrabel.** We provide the system prompt and reward model prompt of our\nconstructed travel planning agent, enable it to autonomously planning, executing tools and reflecting\non tool responses in multi-step reasoning process for travel itinerary generation. As shown in Tabel\nA.2.1, we provide essential reasoning format, toolkit information and answer rule in the system\nprompt, guiding agent’s behavior.\n\n\n**Prompt** **Template** **of** **Trajectory-Level** **Verifier.** In this work, we construct the reward modeling system using hand-crafted evaluation rubrics. For the trajectory-level verifier, we provide the\nuser query and the agent’s final response. The primary objective is to assess whether the response\nadheres to the annotated, coarse-grained spatiotemporal principles—such as answer completeness,\nalignment with user intent, and conformance to the logical structure of travel planning. These foundational principles are operationalized into six evaluation rubrics, shown in Table A.2.2.\n\n\n14\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-14-0.png)\n\n\n\n\n\n15\n**Prompt** **Template** **of** **Turn-Level** **Verifier.** Beyond the trajectory-level verifier, we also employ a\nturn-level verifier that evaluates the agent’s responses turn by turn. To achieve this, we incorporate\nadditional tool-response information into the prompt. At each turn, we instruct the LLM to generate\nboth the reasoning behind its evaluation and a final conclusion. As shown in Table A.2.3, the verifier\nis prompted to provide step-by-step evaluation reasoning along with a final judgment for every turn.\n\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-15-0.png)\n\n\n\n\n\n\n\nA.3 DATA CURATION\nExcept for the prompt design for TP agent and reward model, one important part is the data curation.\nIn this section, we first introduce the pipeline of data synthesis, and then introduce the detailed\nstatistics information used in DeepTravel, including SFT Cold-start stage, RL training, offline testing\nand online testsing.\n\n\nA.3.1 DATA SYNTHESIZATION PIPELINE\nIn this paper, the primary goal of data synthesis is to construct user queries. Each user query is\ndecomposed into a set of atomic intents, such as _origin_, _destination_, _departure_ _time_, _arrival_ _time_,\n_budget_, and so on. In our method, we systematically enumerate combinations of atomic intents and\nthen inversely generate natural user queries that correspond to these intent structures.\n\n\nOnce the initial pool of queries is obtained, we perform repeated sampling with the Qwen-3-32B\nmodel. Each candidate query is then evaluated with our reward modeling system, which assigns\na difficulty score. For SFT Cold-Start, we primarily use relatively simple queries, while retaining\n16\na small proportion of difficult ones to improve model robustness. For RL, on the contrary, we\nemphasize complex queries, but still include a small fraction of simple cases to ensure coverage.\n\n\nBefore integrating queries into RL training, we additionally conduct a manual inspection stage. This\nhuman filtering step is crucial to remove semantically invalid or ill-posed queries. We found that\nthe quality of queries significantly impacts RL training outcomes, and ensuring that queries are both\nchallenging and learnable is essential for stable optimization.\n\n\nFor test data, we construct evaluation sets by distinguishing queries based on whether they involve\nexplicit user personalization constraints (e.g., budget restrictions or individual travel preferences).\nAccordingly, we build two categories: _offline_ _with_ _constraint_ and _offline_ _without_ _constraint_ . We\ncarefully select 500 queries for each category, resulting in balanced test datasets that cover both\npersonalized and non-personalized travel scenarios.\n\n\nA.3.2 DATASET STATISTICS\nIn this section, we present the dataset statistics for SFT Cold-Start, RL Training, and testing.\n\n\n**SFT** **Cold-Start.** For each model iteration and update, we employ the data synthesis pipeline to\ngenerate user queries. We then conduct repeated sampling using DeepSeek-R1. After filtering\nthe samples with the constructed reward model, we obtain approximately 1K trajectories for the\nbackbone cold-start.\n\n\n**RL** **Training.** The training dataset consists of about 500 high-quality samples, all of which have\nbeen double-checked by human annotators. In the RL training process, we set the validation size to\n50 and the training size to 450.\n\n\n**Online** **and** **Offline** **Testing.** We use both of online and offline testing strategy to validate the\nmodel improvement of DeepTravel framework. For offline testing, We use 500 test samples with\nconstraints (156 easy, 45 medium, and 299 hard samples) and 500 test samples without constraints\n(222 easy, 78 medium, and 200 hard samples). For online testing, we use 6,224 queries collected\nfrom the online production environment of the DiDi ES App.\n\n\nA.4 TRAINING DETAILS\nFor SFT-based cold-start, we use Megatron-LM for fine-tuning, and build our RL training framework\non top of verl. In the RL training process, we select DeepSeek-R1 as the backbone of our verifier.\nFor an 8B-parameter LLM, all training can be completed on a single node with 8 H800 GPUs. For\na 32B-parameter LLM, training requires 32 H800 GPUs across 4 nodes. SFT takes about 2–3 hours\nfor 1,000 training samples. RL takes about 30 hours per 100 steps for 500 samples.\n\n\nBelow we detail the supervision signals and diagnostics we track to ensure stable and effective\ntraining of DeepTravel. For the TP agent, we continuously monitor entropy, gradient norm, average\nresponse length, reward, and average turn count. We also track metrics tied to the broader RL\nloop—tool-call accuracy and external verifier success rate—to capture the influence of the sandbox\nenvironment, the reward modeling system, and the RL algorithm itself. For the RL method, we\nadditionally log the sample keep rate and the loss-mask ratio throughout training. These metrics\njointly inform training stability and failure modes: low entropy suggests poor exploration, while\nexcessively high gradient norms indicate instability; unusually short responses and few turns often\nsignal reward hacking; low tool-call accuracy and verifier success point to systematic execution or\nevaluation errors; and a very low sample keep rate typically means the data regime is misaligned\n(too easy or too hard), reducing the need—or opportunity—for exploration.\n\n\nWe show a training metric monitoring example in Figure 6. As can be seen, for agentic RL, the\nexternal environment is not always table (e.g., sometimes the verifier also will failed due to largescale reward services, and the tool call accuracy is also not stable even we derive a offline sandbox).\nIn addition, different traditional RL training, we didn’t observe the monotonically decreasing entropy. This is because in agenti RL process, agent should deal with continually change tool response\nobtained from the environment. In addition, the interaction turn is also change over the environment.\n\n\n17\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-17-0.png)\n\nFigure 6: Metrics monitoring in RL training process. During agentic RL training, we periodically\nmonitor a set of key indicators: policy entropy, training gradient norm, sample keep rate, loss-mask\nratio, average response length, training reward, verifier success rate, tool-call accuracy, and average\nnumber of turns. Anomalies or regressions in any of these metrics can precipitate training failure,\nwhich also indicates the challenges of agentic RL.\n\n\n18\nA.5 HUMAN ANNOTATION\nAs shown in Table 5, the human annotation process consists of seven dimensions designed to comprehensively assess the quality of AI-powered travel itinerary:\n\n\n- **User Intention Understanding** evaluates the system’s ability to correctly parse and interpret user\ninputs, ensuring all critical travel parameters are accurately captured. This dimension is fundamental\nas misunderstanding user requirements leads to irrelevant recommendations.\n\n\n- **Itinerary** **Completeness** assesses whether the recommendation covers all essential travel components (flights, accommodation, local transportation) and maintains temporal coherence. A complete\nitinerary should provide seamless transitions between different travel segments.\n\n\n- **Itinerary** **Feasibility** examines the practical executability of the proposed itinerary. This includes\nverifying that the schedule is not overly ambitious, transportation connections are realistic, and the\noverall route forms a logical closed loop.\n\n\n- **Itinerary** **Affordability** focuses on the economic and practical aspects of the recommendation,\nensuring resources are actually bookable, prices are accurate, and recommendations comply with\norganizational travel policies while maintaining cost-effectiveness.\n\n\n- **Itinerary Clarity** measures both the efficiency of the reasoning process and the transparency of the\nreasoning provided. Quick responses with clear justifications to convince user.\n\n\n- **Personalized Requirement** evaluates the system’s capability to incorporate individual user preferences and historical patterns, ensuring recommendations align with user habits and preferences.\n\n\n- **Without Factual Hallucination** serves as a critical safety check, identifying instances where the AI\nsystem generates non-existent services or logically inconsistent outputs that could mislead users.\n\n\nTable 5: Evaluation dimensions for TP agent on human annotation.\n\n\n**Evaluation Dimension** **Explanation** **Score** **Scoring Criteria**\nUser Intention Whether accurately identifies user\nUnderstanding requirements: departure/destination, time,\nbudget, travel scenario\nItinerary Completeness Whether recommendation covers all key\ncomponents: flights + hotels + transportation,\nwith coherent timing\nTRUE/FALSE **TRUE** : All elements correctly identified\n**FALSE** : Missing or incorrect identification\nof key elements\nTRUE/FALSE **TRUE** : Complete coverage, coherent\n**FALSE** : Location/time deviations, incomplete components\nItinerary Feasibility Whether the itinerary is practically executable TRUE/FALSE **TRUE** : Closed-loop, reasonable arrangewith reasonable conditions ment\n**FALSE** : Overly tight schedule, unreasonable combinations, non-closed loop\nItinerary Affordability Whether recommended resources are\nbookable, accurate pricing/inventory,\ncompliant with company policy, cost-effective\nItinerary Clarity Whether recommendation is fast (within\n200s) with clear justification (e.g.,\n”cost-effective”, ”close to meeting venue”)\nPersonalized Requirement Whether recommendation considers user\nhistory/preferences (airline preference, time\npreference, geographic preference,\naccommodation type, etc.)\nWithout Factual Whether recommendation contains\nHallucination AI-fabricated content, such as non-existent\nflights/hotels, logical errors, etc.\n\n\n19\nTRUE/FALSE **TRUE** : Real resources, reasonable pricing,\npolicy compliant\n**FALSE** : Slight exceedance (¿15%), unbookable options\nTRUE/FALSE **TRUE** : Fast response with clear reasoning\n**FALSE** : Slow response or vague/unexplainable justifications\nTRUE/FALSE **TRUE** : Accurately matches user preferences\n**FALSE** : Misses personalized tags or violates user habits\nTRUE/FALSE **TRUE** : Factually correct recommendations\n**FALSE** : Contains hallucinations or logical\nerrors\nA.6 CASE STUDY\nDue to ethic requirement, we cannot release the data used for training. To ease reader understanding,\nwe provide a real-world user query, and corresponding agent’s response here.\n\n\nSpecifically, we select several real-world user case from the deployed travel planning service on\nDiDi Enterprise Solution App. The illustrative case hopes to facilitate the reader’s understanding of\nhow DeepTravel enable TP agent autonomously plan, execute tool, and reflect on tool response for\nexploring and refine its intermediate steps throughout multi-turn reasoning process.\n\n\nAs illustrated in Table A.6, the TP agent—powered by DeepTravel-32B—follows a multi-turn toolintegrated reasoning. Concretely: (1) Intent understanding and constraints grounding. (2) Tool\nplanning: Before calling tools, the agent drafts a tool plan with time buffers and fallback branches.\n(3) Tool execution with schema-aligned calls: All tool calls are structured with explicit parameters\nand expected fields, ensuring reproducibility and traceability. (4) Tool-response interpretation and\nevidence binding: The agent parses returned fields and binds them to the generated itinerary via\ncard-style evidence, avoiding hallucinations. (5) Reflection: If constraints are unmet (e.g., arrival\nrisk due to transfer), the agent revises either the departure time window or the station choice, and recalls tools with updated parameters. (6) Finalize the travel Itinerary: The agent assembles a coherent\nplan with justification, alternative options, and risk notes, each item linked to tool evidence.\n\n\n20\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents.pdf-20-0.png)\n\n\n\n\n\n21\n\n\n",
    "kor_md": "## DEEPTRAVEL: 자율 여행 계획 에이전트를 위한 종단간 에이전트 기 반 강화 학습 프레임워크\n\n**Yansong Ning** **[1]** [∗] **, Rui Liu** **[2]** **, Jun Wang** **[2]** **, Kai Chen** **[2]** **, Wei Li** **[2]** **, Jun Fang** **[2]** **Kan Zheng** **[2]** **,**\n**Naiqiang Tan** **[2]** **, Hao Liu** **[1]** [†]\n1 홍콩과학기술대학교(광저우)\n2 디디추싱 유한공사\n`yning092connect.hkust-gz.edu.cn`, `[liuh@ust.hk](mailto:liuh@ust.hk)`\n_{_ `invincibleliu,tannaiqiang` _}_ `@didiglobal.com`\n초록\n여행 계획(TP) 에이전트는 최근 여행 일정 생성을 위해 외부 도구/리소스와 상호작용하는 핵심 구성\n요소로 부상하며 즐거운 사용자 경험을 보장해 왔다. 기존 연구는 수작업 프롬프트와 고정된 에이전\n트 워크플로우에 의존하여 보다 유연하고 자율적인 TP 에이전트 개발을 저해해 왔습니다. 본 논문은\n다단계 추론 과정에서 중간 행동을 탐색·검증·정제하기 위해 자율적으로 계획 수립, 도구 실행, 도구\n반응에 대한 반성을 수행할 수 있는 자율 여행 계획 에이전트 구축을 위한 종단간 에이전트 강화 학습\n프레임워크인 **DeepTravel을** 제안합니다. 이를 위해 먼저 교통, 숙박 및 POI 데이터를 캐싱하여 견\n고한 샌드박스 환경을 구축함으로써, 실제 세계 API의 한계(예: 일관성 없는 출력)에 제약받지 않고\nTP 에이전트 훈련을 용이하게 합니다. 또한 계층적 보상 모델링 시스템을 개발하여, 경로 수준 검증\n기가 먼저 시공간적 타당성을 확인하고 불만족스러운 여행 일정을 필터링한 후, 턴 수준 검증기가 도\n구 응답과의 일정 세부 사항 일관성을 추가로 검증함으로써 효율적이고 정밀한 보상 서비스를 가능\n하게 합니다. 마지막으로, TP 에이전트가 실패 경험 버퍼에서 주기적으로 재현할 수 있도록 하는 응\n답 증강 강화 학습 방법을 제안하여 주목할 만한 에이전트 능력을 구현합니다. 훈련된 TP 에이전트를\nDiDi Enterprise Solutions 앱에 배포하고 포괄적인 온라인 및 오프라인 평가를 수행한 결과,\nDeepTravel이 소규모 LLM(예: Qwen3-32B)이 OpenAI-o1/o3 및 DeepSeek-R1과 같은 기존 최첨단\nLLM을 여행 계획 작업에서 크게 능가할 수 있음을 입증했습니다.\n\n\n그림 1: 합성 오프라인 벤치마크 및 온라인 사용자 데이터에서 DeepTravel의 성능.\n\n\n∗ 디디추싱(Didichuxing) 주식회사 인턴십 기간 중 수행된 연구.\n\n   - 교신저자.\n\n\n1\n\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-0-0.png)\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-1-0.png)\n\n그림 2: 기존 연구와 본 연구의 자율 여행 계획 에이전트 패러다임 비교.\n\n\n1 서론\n여행 계획(TP)은 숙박, 교통, 관심지점(POI) 등 다양한 자원을 통합하여 사용자 선호도에 부합하는 실행 가능한 여정\n(Nguyen et al., 2023)을 생성하는 것을 목표로 한다. 최근 자연어 처리 기술의 발전으로 대규모 언어 모델(LLM)이 여행 계\n획 에이전트 구축에 널리 활용되고 있다 Chen et al. (2024). 이 에이전트는 외부 도구/자원 Gou et al. (2023)을 호출하여 여\n행 일정을 생성함으로써 인간의 이동성에 원활한 경험을 제공한다 Tang et al. (2024). 여행 계획 에이전트는 현대 시민들에\n게 점차 대중적인 도구가 되어가고 있다.\n\n\n최근 문헌에서는 TP 에이전트 구축을 위한 많은 노력이 기울여졌다. 대부분의 기존 접근법은 그림 2(a)에서 보여지듯 주로\n신중하게 설계된 프롬프트에 의존한다. 예를 들어, TravelPlanner(Xie et al., 2024)와 TripTailor(Wang et al., 2025)는 도구\n호출 및 일정 생성을 위해 LLM을 안내하기 위해 작업 특화 프롬프트를 사용한다. 최근 들어 연구자들은 이러한 프롬프트\n엔지니어링 전략을 고정된 에이전트 워크플로에 통합하기 시작했다. 예를 들어, PTS Shao 등(2025)과 RETAIL Deng 등\n(2025)은 사용자 의도 이해를 향상시키고 효과적인 도구 상호작용을 가능하게 하며 정확한 여행 일정 생성을 지원하는 잘\n구조화된 에이전트 파이프라인을 제안합니다. 그러나 이러한 방법은 _노동집약적이며새로운사용자질의에적응하거나도_\n_구호출실패로부터복구하는_ 데 어려움을 겪어, 보다 유연하고 자율적인 TP 에이전트의 구현을 제한합니다.\n\n\n에이전트 기반 강화 학습(RL)은 최근 등장하여 전용 환경에서 도구를 활용하고 피드백을 기반으로 추론을 개선함으로써\n자율적 AI 에이전트 구축 가능성을 제시하였다(Singh et al., 2025). 예를 들어 ReTool(Feng et al., 2025), Kimi-Researcher\nMoonshotAI(2025), WebSailor(Li et al., 2025)는 엔드투엔드 에이전트 기반 RL 훈련을 활용하여 자율 에이전트를 구축하\n였다. (2025), Kimi-Researcher MoonshotAI (2025), WebSailor Li et al. (2025)은 각각 수학, 심층 연구, 웹 도메인을 위한\n자율 에이전트 구축에 엔드투엔드 에이전트 기반 RL 훈련을 활용합니다. 이러한 연구들은 기존 여행 계획 연구의 한계를\n해결하기 위해 자율 TP 에이전트 구축을 위한 맞춤형 에이전트 기반 RL 프레임워크를 제안하도록 우리에게 동기를 부여합\n니다.\n\n\n그러나 그림 2(b)에 표시된 자율적 TP 에이전트를 구축하는 것은 두 가지 핵심 요인으로 인해 쉽지 않습니다. 이 에이전트\n는 다중 회전 추론 내에서 도구 호출과 도구 응답을 자율적으로 교차 실행함으로써 점진적으로 복잡한 TP 작업을 해결할\n수 있습니다. _(1) 동적여행환경._ TP 에이전트는 호텔 예약 가능 여부, 가격, 교통 수단 옵션과 같은 정보가 실시간으로 지속\n적으로 변화하는 매우 동적인 환경에서 작동합니다. 결과적으로 동일한 쿼리도 숙박, 교통, POI 데이터 소스의 업데이트로\n인해 시간에 따라 일관되지 않은 결과를 생성할 수 있습니다. 이러한 지속적으로 진화하는 실제 환경에서 TP 에이전트를\n훈련시키는 것은 여전히 상당한 도전 과제입니다. _(2) 개방형여행과제._ 수학 또는 웹 도메인에 대한 검증된 보상 기반 강화\n학습(RLVR) 패러다임(Guo et al., 2025)과 달리, 여행 계획은 본질적으로 명시적 기준값이 없는 개방형 과제이다. 예를 들\n어 생성된 여행 일정은 개인화된 사용자 선호도와 예산에 따라 달라질 수 있어 결과 검증은 어렵다. 신뢰할 수 있고 확장 가\n능한 보상 신호를 구축하는 방법은 도전적이다.\n\n\n상기 과제를 해결하기 위해, 자율 여행 계획 에이전트 구축을 위한 종단간 에이전트 기반 강화학습 훈련 프레임워크인\n**DeepTravel** 을 제안한다. 구체적으로, 먼저 다양한 타임스탬프에 걸친 다중 실제 세계 API로부터 교통, 숙박, POI 데이터\n2\n를 캐싱하여 동적 도구 상호작용을 시뮬레이션하는 _로버스트샌드박스를_ 구축한다. 이 샌드박스 내에서 TP 에이전트는\nQPS 문제를 극복하면서 대규모 반복적 시행착오 학습을 수행할 수 있다.\n\n\n3\n한계와 출력 불일치를 해결합니다. 또한 _계층적보상모델링_ 시스템을 제안합니다. 여기서 궤적 수준 검증기는 생성된 여정\n의 시공간적 타당성을 보장하고, 턴 수준 검증기는 도구 응답과의 세밀한 일관성을 강제함으로써 훈련을 위한 보다 효율적\n이고 신뢰할 수 있는 보상 신호를 제공합니다. 마지막으로, 순차적 콜드스타트 및 강화학습 과정을 통해 에이전트적 추론\n능력을 촉진하기 위한 _응답증강강화학습(Reply-Augmented Reinforcement Learning)_ 방법을 제안합니다. 실패 경험 버퍼\n로부터 주기적으로 재현하는 방식을 기반으로, TP 에이전트는 이전 추론 행동을 학습하고 개선하여 점진적으로 에이전트\n적 여행 계획 능력을 발휘할 수 있습니다.\n\n\n구축된 TP 에이전트를 DiDi Enterprise Solutions 앱에 배포하고, 다양한 작업 복잡도에 걸쳐 수집된 온라인 실제 사용자 데\n이터와 오프라인 합성 데이터를 모두 사용하여 평가를 수행합니다. 실험 결과는 DeepTravel이 소규모 LLM 백본(예:\n\nQwen3-32B)이 현재 최첨단 추론 LLM(예: OpenAI-o1/o3 및 DeepSeek-R1)과 기존 RL 알고리즘(예: GRPO 및 DAPO)을\n능가하는 성능을 발휘할 수 있음을 입증합니다. 이러한 결과는 Deep-Travel이 자율적 TP 에이전트 구축을 위한 유망한 프\n레임워크임을 입증합니다.\n\n\n우리의 기여는 다음과 같이 요약됩니다: (1) 기존 TP 연구를 발전시키기 위한 새로운 패러다임을 제시하는 최초의 자율 여\n행 계획 에이전트를 구축 및 배포하였습니다. (2) 여행 분야에 특화된 최초의 종단간 에이전트 기반 강화 학습 프레임워크\n를 제안하였으며, 이는 견고한 샌드박스 환경, 신뢰할 수 있는 보상 서비스 및 주기적 경험 재생 전략 하에서 TP 에이전트 훈\n련을 가능하게 합니다. (3) 온라인 및 오프라인 데이터에 대한 광범위한 실험을 통해 제안된 프레임워크의 효과성을 검증하\n고 다양한 여행 계획 작업에서 탁월한 성능을 입증하였습니다.\n\n\n2 서론\n질의와 여행 일정의 정의를 시작으로, 해결하고자 하는 문제를 정의합니다.\n\n\n**정의 1** _**사용자쿼리.**_ _사용자쿼리 q는자연어로표현되며, 사용자의시공간적여행의도와개인화된선호도를나타낸다. 예_\n_를들어, \"상하이에서베이징까지 3일간의공항여행을도와주세요\"라는쿼리는사용자가항공편으로베이징을여행하고 3_\n_일간체류하기를원함을의미한다._\n\n\n**정의 2** _**여행일정.**_ _여행일정 I은숙박, 교통편및여행활동제안, 관심장소(POI) 탐색전략등을통합한상세일일계획이_\n_포함된구조화된계획으로정의된다._\n\n\n참고: POI는 일반적으로 인기 관광지(예: _국립고궁박물관_, _만리장성_ 등)에 해당한다. 이제 문제를 공식화한다:\n\n\n**문제 1** _**에이전트기반여행계획.**_ _쿼리 q가주어지면, 여행에이전트는다단계추론과정에서탐색, 검증및중간행동개선_\n_을위해자동계획, 실행도구활용, 도구응답반영을통해여행요구사항을충족하는여행일정 I을생성한다:_\n\n\n_{τt, at}_ = _πθ_ ( _q, {τ_ 1 _, a_ 1 _, o_ 1 _, τ_ 2 _, a_ 2 _, o_ 2 _, ..., τt_              - 1 _, at_              - 1 _, ot_              - 1 _}_ ) _,_ (1)\n_여기서 πθ는여행에이전트의정책이며, τt_ - 1 _, at_ - 1 _, ot_ - 1 _은각각 t − 1 턴에서에이전트의사고, 행동, 환경관측을나타낸다._\n_생성된여행일정 I는_\n_이전턴의에이전트행동 at에관여한다._\n\n\n자율적 여행 계획의 예시는 그림 2(b)에 설명되어 있으며, 여기서 자율적 TP 에이전트는 외부 도구(예: <tool call>과 </tool\ncall>로 둘러싸인 동작)를 사용하기 전에 신중하게 생각 (즉, `<think>` 와 `</think>` 로 감싸인 사고)를 수행한 후 외부 도\n구 사용(즉, `<tool call>` 과 `</tool call>` 로 감싸인 동작)을 ~~실~~ 행하고 도구 응답을 반영(즉, `<tool response>`\n와 `</tool response>` 로 감싸인 관찰)하여 여행 일정을 생성하기 위한 다단계 추론 과정의 중간 단계를 탐색, 검증 및\n정제합니다.\n\n\n3 DEEPTRAVEL\n4\n이러한 표기법을 바탕으로, 여행 에이전트 구축을 위한 종단간 에이전트 강화 학습 파이프라인인 DeepTravel을 제안합니다.\n\n\n5\n3.1 개요\n그림 3은 DeepTravel의 전체 파이프라인을 보여줍니다. _(1) 강력한샌드박스구축은_ 툴킷 주석, 모의 데이터 수집 및 업데\n이트 메커니즘을 포함하여 시뮬레이션된 실제 도구 상호 작용을 가능하게 합니다. _(2) 계층적보상모델링은_ 궤적 수준 및\n턴 수준의 검증기를 모두 제안하여 신뢰할 수 있고 효율적인 보상 신호를 공동으로 제공합니다. _(3) 응답강화학습은_ 먼저\n에이전트 기반 여행 계획 형식의 콜드 스타트를 위한 SFT를 수행한 다음, 경험 재생을 통한 RL을 수행하여 LLM의 에이전\n트 능력을 더욱 강화합니다.\n\n\n3.2 강건한 샌드박스 구축\nLin 등(2023)의 샌드박스는 TP 에이전트가 도구와 상호작용할 수 있는 안정적인 환경으로 제공되며, 실제 세계의 상호작용\n을 시뮬레이션하면서 실질적인 출력 불일치와 API 제한을 극복합니다. 먼저 툴킷 주석 작업을 시작하고, 이어서 샌드박스\n데이터 수집 및 업데이트 메커니즘을 소개합니다.\n\n\n3.2.1 툴킷 어노테이션\n여행 계획에 흔히 사용되는 여섯 가지 유형의 도구를 주석 처리합니다. 표 1은 구축된 툴킷의 사양을 요약하며, 매개변수 스\n키마와 예시를 포함합니다.\n\n\n항공 **편** **검색.** 항공편 검색은 여행 계획의 핵심 요소인 항공 운송에 관한 정보를 제공합니다(Shao et al., 2024a). 본 연구에\n서는 이 도구에 DiDi Enterprise Solutions(DiDi ES) 앱의 인터페이스 형식을 채택했습니다. 각 도구 호출에는 출발 도시, 도\n착 도시, 출발 날짜가 필요합니다. 예를 들어, _\"flight search('Beijing', 'Shanghai', '2025-07-02')\"_ 는 2025년 7월 2일 _베이징_\n_에서상하이로의_ 항공편 옵션을 검색합니다.\n\n\n**열차** **검색.** 항공편 검색과 유사하게, 열차 검색은 필수적인 철도 운송 정보를 제공합니다. 우리는 DiDi ES 인터페이스 형식\n을 따라 이 도구를 구축합니다. 표 1에 표시된 바와 같이, 각 쿼리에는 출발 도시, 도착 도시 및 출발 날짜가 포함됩니다.\n\n\n**경로** **계획.** 경로 계획은 이동 시간과 비용을 최적화하는 데 매우 중요합니다(Fang et al., 2024). 우리는 DiDi Map이 제공하\n는 경로 계획 서비스를 활용합니다. 각 도구 호출에는 출발지 이름, 목적지 이름, 도시 이름이 필요합니다. 예를 들어, _\"route_\n_planning('국립고궁박물관', '만리장성', '베이징')\"_ 은 베이징의 _국립고궁박물관에서만리장성까지의_ 경로를 계획하고 거리/\n시간 세부 정보를 계산합니다.\n\n\n**호텔** **검색.** 호텔 검색은 TP 에이전트가 사용자 선호도에 기반해 적합한 숙소를 찾을 수 있게 합니다 Yang et al. (2025b). 이\n도구 역시 DiDi ES 인터페이스 형식을 따릅니다. 각 도구 호출에는 도시명, 호텔명, 체크인 날짜, 체크아웃 날짜가 포함됩\n니다. 예를 들어, _\"hotel search('Beijing', 'Atour', '2025-07-02', '2025-07-05')\"는_ 2025년 7월 2일부터 7월 5일까지 베이징\nAtour 호텔의 예약 가능한 객실을 검색합니다.\n\n\n**POI 검색.** POI 검색은 도시 맥락적 의미론을 제공하며, 이는 여행 계획에 널리 채택되어 왔습니다(Xie et al., 2024). 경로\n계획과 유사하게, 우리는 DiDi Map에서 제공하는 POI 검색 서비스를 직접 활용합니다. 각 도구 호출에는 쿼리와 도시명이\n포함됩니다. 예를 들어, _\"poi search('The Great Wall', 'Beijing')\"_ 은 _만리장성의_ 지리적 주소를 얻는 데 도움이 됩니다.\n\n\n**웹** **검색.** 웹에는 풍부한 여행 계획이 존재하며, 이는 여정 생성 시 귀중한 자원으로 활용됩니다(Ni et al., 2025). 우리는\nBocha AI가 제공하는 웹 검색 서비스를 구독합니다. 도구 호출 매개변수는 여행 계획과 관련된 모든 쿼리입니다(예: _\"web_\n_search('Introduction to Beijing')\"_ ).\n\n\n3.2.2 모의 데이터 수집 및 업데이트 메커니즘\n실제 세계에서 호텔, 기차, 항공편의 가격과 가용성이 시간에 따라 변동하는 도구 상호작용의 동적 특성을 시뮬레이션하기\n위해 데이터 캐싱 메커니즘을 제안한다. 이 메커니즘은 실제 세계 API의 QPS 제한과 각 검색 시마다 발생하는 정보 검색\n불일치 문제를 해결하며, 이는 에이전트가 이전 실패 사례로부터 학습하는 능력을 저해할 수 있다.\n\n\n6\n구체적으로, 우리는 항공편, 기차, 호텔 검색 데이터를 위한 주문형 캐싱 전략을 통합한 매일 갱신되는 데이터베이스를 유\n지합니다. 새로운 쿼리가 수신되면 시스템은 자동으로 최신 레코드를 데이터베이스에 저장합니다. 이 메커니즘을 통해 TP\n에이전트는 이전 데이터를 재접근할 수 있습니다.\n\n\n7\n표 1: 샌드박스 내 툴킷 사양.\n\n\n유형 도구 이름 도구 호출 형식 도구 응답 설명 항공편 검색\nflight search(출발 도시명, 도착 도시명, 출발 날짜) 가능한 항공편 옵션\n교통 열차 검색 열차 ~~검~~ 색(출발지 도시 ~~명,~~ 도 ~~착지~~ 도시명, 출발 ~~일~~ ) 가능한 열차 옵션 경로 계획\n경로 ~~계획~~ (출발지 이름, 목 ~~적~~ 지 이름, 도시 이름) 경로, 거리 및 시간\n숙박 호텔 검색 호텔 ~~검~~ 색(도시명, 호 ~~텔~~ 명, 체크인 ~~날짜~~, 체크아웃 날짜 ~~)~~ 이용 가능한 호텔 후보\n~~POI검색~~\n관광 명소 POI 검색(쿼리, 도시명) POI의 상세 주소\n웹 검색 ~~웹~~ 검색(쿼리) 쿼리와 관련된 웹 페이지\nRL 훈련 중 도구의 반응으로, 중간 추론 경로를 탐색, 검증 및 개선하여 실패한 사례로부터 반복적으로 학습할 수 있게 함\nShang et al. (2025).\n\n\n3.3 계층적 보상 모델링\n그런 다음, 우리의 보상 모델링 시스템을 소개합니다. 이 시스템은 에이전트 훈련을 위한 효율적이고 신뢰할 수 있는 보상\n신호를 제공하기 위해 설계된 궤적 수준 검증기와 턴 수준 검증기로 구성됩니다.\n\n\n**경로** **수준** **검증기.** 이 검증기는 생성된 여행 일정의 전반적인 시공간적 실현 가능성을 평가한다. 완전한 추론 궤적 _{τ_ 1 _, a_ 1\n_, o_ 1 _, τ_ 2 _, a_ 2 _, o_ 2 _, . . ., τt, at }_ 가 주어지면, 궤적 수준 검증기는 최종 여행 일정 _at_ 가 필수적인 시공간적 제약 조건(Chaudhuri\net al. (2025)에 제시된 필수적 시공간 제약 조건을 준수하는지 검증합니다. 이러한 제약 조건에는 사건의 논리적 순서, 지리\n적 타당성, 사용자 요구사항 충족(예: 주어진 시간 내에 특정 POI 방문) 등이 포함됩니다. 이 거친 수준의 평가는 무효한 일\n정을 효율적으로 걸러내어 잠재적으로 유효한 계획만 다음 검증 단계로 진행되도록 보장합니다.\n\n\n**턴** **수준** **검증기.** 궤적 수준 검증기의 성공적 검증 후, 턴 수준 검증기는 보다 세분화된 평가를 수행한다. 이 검증기는 에이\n전트의 최종 이동 경로 _at_ 와 각 턴에서 외부 도구가 획득한 정보 간의 일관성을 검토한다(Zeng et al., 2025). 구체적으로, 1\n부터 t − 1까지의 각 턴 _i에_ 대해 검증기는 _at가_ 도구 응답 _o(i)를_ 정확히 반영하는지 평가합니다. 각 턴을 체계적으로 검증\n함으로써, LLM이 생성한 여행 일정의 사실적 환각/오류를 식별하는 데 도움을 줍니다.\n\n\n**공동** **보상** **재가중.** 두 검증기를 공동 보상 재가중 전략으로 결합하여 보상 신호를 제공한다. 궤적 수준 검증기가 위반을 감\n지하면 최종 보상 _r은_ 즉시 0으로 설정되어 계산 자원을 절약한다. 궤적이 궤적 수준 검증을 통과하면 회전 수준 검증기가\n각 회전을 평가한다. 최종 보상 _r은_ 모든 회전이 검증을 통과할 경우에만 1로 설정되며, 이는 완전히 일관되고 유효한 여행\n일정을 나타낸다.\n\n\n이러한 계층적 구조는 보상 모델링 시스템의 효율성과 신뢰성을 모두 보장합니다. 실제 구현에서는 궤적 수준 검증기와 턴\n수준 검증기를 위해 각각 Huang et al. (2025)의 여행 중심 평가 기준을 다수 구축합니다. 인간이 생성한 평가 기준을 바탕으\n로 DeepSeek-R1 기반 검증기가 보상 모델링 서비스를 제공하도록 프롬프트합니다. 자세한 내용은 부록 A.2에서 확인할 수\n있습니다.\n\n\n3.4 재생 강화 학습(REPLAY-AUGMENTED REINFORCEMENT LEARNING)\n이 섹션에서는 릴레이 강화 학습(Relay-Augmented Reinforcement Learning)을 상세히 설명합니다. 이는 두 단계로 구성된\n과정입니다. 먼저 SFT(Supervised Fine-Tuning)를 활용하여 TP 에이전트의 추론 형식을 초기화합니다. 그런 다음\nRL(Reinforcement Learning)을 활용하여 에이전트의 추론 능력을 더욱 향상시키고, 이전 실패 경험으로부터 주기적으로\n학습할 수 있도록 합니다.\n\n\n3.4.1 지도 학습을 통한 추론 형식 콜드 스타트 초기화\n콜 **드** **스타트** **데이터** **합성** **및** **필터링.** 샌드박스 환경에서 DeepSeek-R1의 다중 턴 궤적을 추출하여, 사고, 도구 호출, 도\n구 응답, 최종 답변이 교차하는 완전한 추적 기록 _y_ = _{τ_ 1 _, a_ 1 _, o_ 1 _, . . ., τt, at }_ 를 생성합니다. 생각 _τi_ 는 `<think>` . . .\n\n`</think>` 로 감싸고, 동작 _ai_ 는 `<tool call>` . . . `</tool call>` 로 둘러싸인 함수 호출이거나 `<answer` >. . `.`\n\n\n8\n`</answer>` 로 둘러싸인 최종 여정 _답변 at_ 입니다. 관찰 _oi_ 는 `<tool response` >. . `. </tool response` >로 둘러\n싸인 도구 응답입니다. 그런 다음, 구축된 보상 모델링 시스템을 활용하여 잘못된 궤적을 필터링하고, 마지막으로 엄격한 형\n식 검사를 적용하여 특수 태그로 올바르게 분할된 시퀀스만 유지합니다.\n\n\n9\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-9-0.png)\n\n그림 3: DeepTravel 개요.\n\n\n**훈련** **목표.** TP 에이전트는 시스템 프롬프트 _T를_ 따르고 검증된 도구 통합 궤적을 재현하도록 훈련됩니다. 명령 입력은 T와\n사용자 질의 _q를_ 연결하며, 출력은 검증된 _궤적 y입니다_ . 실제 훈련 과정에서 에이전트의 환경 관측 _oi_ 에 해당하는 토큰은\n손실 계산에서 제외됩니다(Jin et al., 2025). 상세한 프롬프트 템플릿은 부록 A.2에서 확인할 수 있습니다.\n\n\n3.4.2 경험 재생 기반 강화 학습\n콜드 스타트 후, 검증된 실패 궤적을 쿼리 버퍼로 저장한 다음 후속 훈련 단계에서 이를 재생하는 2단계 프로세스를 도출합\n니다(Zhang et al., 2025b).\n\n\n**롤아웃** **및** **재현** **전략** . 그룹 상대 정책 최적화(GRPO) Shao et al. (2024b)의 샘플링 절차에 따라, 각 쿼리 q에 대해 궤적 집합\n_{y_ 1 _, y_ 2 _, ..., yn}을샘플링한다_ . 그룹 내 궤적 중 검증된 정답을 산출하지 못한 경우, 해당 질의를 _경험버퍼 B에저장_ 하여 추후\n재생을 수행한다. 이는 후속 RL 훈련 단계 이후 개선된 정책이 이전에 실패했던 어려운 샘플을 처리할 수 있도록 일반화될\n수 있다는 점에 근거한다(Xie et al., 2025).\n\n\n**정책** **최적화.** RL 훈련 데이터셋을 _D_, 경험 버퍼를 _B로_ 설정하며, B는 고정된 훈련 단계 _γ_ 이후 재현됩니다. 최적화 목표를\n다음과 같이 공식화합니다:\n\n\nmax Eq∼{ _D,B_ } _,_ { _yi_ } _[n ]_ _∼π_ ( _y|q_ )\n\"\n_n_\n1 [Σ]\nmin\n_πθ_ ( _yi|q_ )\n_Ai_ _[,]_\n_πθ_ i=1 _n_ i=1 _πref_ ( _yi | q_ ) (2)\n#\n클립 _πθ_ ( _yi | q_ )\n~~_π_~~ _ref_ ~~(~~ ~~_y_~~ _i_ ~~_| q_~~ ~~)~~ _[,]_ [ 1 ] _[− ε,]_ [ 1 + ] _[ε A][i]_\n_−_ βD KL [ _πθ_ πref [] ]\n여기서 _ε_, _β는_ 하이퍼파라미터이며, n은 롤아웃 크기, D KL은 KL-분산을 나타내고, _Ai_ = _ri− avg_ ( _r_ ) _/std_ ( _r_ )은 그룹 보상 _r_\n= _{r_ 1 _, r_ 2 _, ..., rn}_ 을 기반으로 계산된 이점을 나타냅니다. 본 연구에서는 그룹 보상의 표준 편차가 _std_ ( _r_ ) _≤ η_ ( _η_ = 0.1)\n을 만족할 때 샘플을 필터링하는 방안을 제안한다. 이 전략은 에이전트가 대규모 롤아웃에서도 유사한 보상을 받는 지\n나치게 단순하거나 어려운 샘플을 배제함으로써 현재 정책의 효과적인 탐색을 촉진하는 것을 목표로 한다. 또한 도구 응\n답 토큰( `<tool response>` 로 감싸진)에 대해 손실 마스킹 연산을 활용한다.\n\n`</tool` ~~`re`~~ `sponse>` ) 에이전트가 생성한 토큰에 대해서만 정책 기울기가 계산되도록 보장합니 ~~다~~ .\n\n\n4 실험\n4.1 실험 설정\n**데이터** **큐레이션.** 본 방법은 DiDi ES의 실제 온라인 사용자 데이터와 합성 오프라인 데이터를 조합하여 구축한 네 가지 벤\n치마크에서 평가되었습니다. (1) **온라인** **데이터** : 6월 1일부터 8월 31일까지 3개월 동안 DiDi ES 앱에서 실제 사용자 쿼리\n총 6,224건을 수집했습니다. (2) **오프라인** **데이터** : 두 가지 서로 다른 설정에서 각각 500개의 사용자 쿼리를 합성했습니다:\n\na) 여행 예산이나 개인화된 선호도와 같은 제약 조건이 **없는** 쿼리; b) **제약** **조건이** **있는** 쿼리. 두 설정 모두에서 쿼리 복잡도\n를 주석 처리하여 각 쿼리를 **쉬운(easy), 중간(medium), 복잡한(complex)으로** 분류했습니다. 추가 데이터 큐레이션 세부\n사항과 데이터셋 통계 정보는 부록 A.3에 제공됩니다.\n\n\n10\n표 2: 합성된 오프라인 여행 계획 벤치마크와 DiDi ES 애플리케이션의 실제 온라인 사용자 데이터에 대한 최종 통과율(%)\n결과. 최상의 결과는 **굵게** 표시되었으며, 각 설정에서 최상의 기준선 결과는 밑줄 처리되었습니다.\n\n\n\n\n\n\n\n\n\n\n\n|모델|오프라인 제약 없이 제약 조건 적용 시|Col3|온라인|인간|\n|---|---|---|---|---|\n|**모델**|**제약 없이**<br>**제약 조건 적용 시**<br>**쉬움**<br>**중간**<br>**어려움**<br>**쉬움**<br>**중간**<br>**어려움**|**제약 없이**<br>**제약 조건 적용 시**<br>**쉬움**<br>**중간**<br>**어려움**<br>**쉬움**<br>**중간**<br>**어려움**|**온라인**|**평가**|\n|DeepSeek-R1<br>OpenAI-o1<br>OpenAI-o3<br>K2<br>Qwen3-235B<br>gpt-oss-120B<br>Seed-OSS-36B<br>Qwen3-32B<br>Qwen3-8B|45.55<br>34.74<br>26.00<br>36.57<br>33.16<br>20.60<br>37.30<br>20.11<br>21.19<br>54.01<br>48.42<br>25.52<br>38.69<br>36.84<br>20.24<br>40.15<br>27.37<br>20.83<br>23.65<br>13.16<br>11.19<br>29.85<br>27.89<br>23.21<br>10.95<br>9.47<br>4.76|65.36<br>43.33<br>27.09<br>30.36<br>24.44<br>17.69<br>37.50<br>26.67<br>15.69<br>57.14<br>53.33<br>21.40<br>44.64<br>26.67<br>10.37<br>64.29<br>42.22<br>16.39<br>25.00<br>13.33<br>12.34<br>53.57<br>25.00<br>9.03<br>28.57<br>26.67<br>5.35|52.89<br>30.88<br>29.17<br>53.56<br>42.70<br>37.11<br>19.36<br>32.94<br>26.72|72.00<br>54.00<br>52.00<br>64.00<br>52.00<br>48.00<br>20.00<br>38.00<br>26.00|\n|DeepTravel-8B-콜드 스타트<br>DeepTravel-8B-RL<br>DeepTravel-32B-콜드 스타트<br>DeepTravel-32B-RL|41.09<br>31.58<br>12.64<br>54.25<br>36.84<br>20.24<br>56.42<br>32.95<br>25.60<br>**69.34**<br>**54.74**<br>**29.17**|56.07<br>28.89<br>12.37<br>64.86<br>41.89<br>21.40<br>61.07<br>40.44<br>17.52<br>**73.21**<br>**62.22**<br>**35.75**|40.00<br>49.75<br>50.03<br>**62.77**|58.00<br>70.00<br>66.00<br>**82.00**|\n\n\n**기준선.** 우리는 동일한 프레임워크 하에서 TP 에이전트로 파생된 9개의 추론 LLM과 우리의 방법을 비교합니다. 이러한 기\n준선에는 DeepSeek-R1 Guo et al. (2025), OpenAI-o1 Jaech et al. (2024), OpenAI-o3, K2 Team et al. (2025), Qwen3\n235B, gpt-oss-120B OpenAI (2025), Seed-OSS-36B Team (2025), Qwen3-32B, Qwen3-8B Yang et al. (2025a) 등이 포함\n됩니다. 또한, 제안된 DeepTravel 프레임워크를 세 가지 대표적인 RL 훈련 방법, 즉 PPO Schulman et al. (2017), GRPO\nShao et al. (2024b) 및 DAPO Yu et al. (2025)과 비교합니다.\n\n\n**평가** **지표** **및** **프로토콜.** 평가 지표로는 TravelPlanner Xie et al. (2024)의 최종 통과율(Final Pass Rate)을 사용합니다. 평가\n프로토콜에서는 구축한 보상 모델링 시스템을 적용하여 최종 통과율을 추정합니다. 또한 온라인 및 오프라인 결과에서 각\n각 50건의 사례를 무작위 추출하여 인간 주석자가 생성된 여행 일정이 통과하는지 평가하도록 합니다.\n\n\n**훈련** **세부사항.** Qwen3-8B와 Qwen3-32B를 활용하여 SFT와 RL을 수행하여 DeepTravel-8B와 DeepTravel-32B를 생성합\n니다. SFT 콜드 스타트에는 1K 훈련 샘플을 사용하며, 배치 크기 32, 학습률 5e-6으로 2에포크 동안 훈련합니다. RL 훈련\n에는 500개의 고품질 샘플을 선별하여 제안된 리플레이 강화 RL 알고리즘을 적용했습니다. RL 과정에서 롤아웃 크기는 8\n로 설정하고 학습률은 5e-7을 사용했습니다. 추가로 에이전트의 최대 응답 길이는 32K 토큰으로 설정하고, 최대 상호작용\n턴은 8회로 제한합니다. Qwen3-8B와 Qwen3-32B 훈련에는 각각 8개와 32개의 H800 NVIDIA GPU가 필요하며, 훈련 과\n정은 수백 단계당 약 30시간이 소요됩니다. 자세한 내용은 부록 A.4를 참조하십시오.\n\n\n4.2 주요 결과\n**기존** **추론** **에이전트와의** **비교.** 우리는 SFT 콜드스타트 단계와 RL 훈련 과정 전반에 걸쳐 DeepTravel을 비교합니다. 표 2\n에 보고된 바와 같이, 전반적으로 DeepTravel은 온라인 및 오프라인 평가 설정 모두에서 최첨단 추론 에이전트 대비 상당한\n개선을 달성합니다. 또한 두 가지 핵심 관찰 사항을 강조합니다: **(i) DeepTravel은** **소규모 LLM의** **성능을** **크게** **향상시킵니**\n**다** . 예를 들어 Qwen3-8/32B는 최첨단 수준으로 향상되어 훨씬 더 크고 강력한 프론티어 LLM과 동등하거나 심지어 이를\n능가합니다. 제약 없는 오프라인 설정에서 DeepTravel-8B와 DeepTravel-32B는 최종 통과율 54.25%와 69.34%를 달성하\n여 K2보다 각각 0.1%와 28.9% 우수한 성능을 보입니다. 다른 설정에서 DeepTravel-8B는 비슷한 성능을 보였고\nDeepTravel-32B는 DeepSeek-R1, OpenAI-o1, OpenAI-o3와 같은 프론티어 추론 LLM을 지속적으로 능가했습니다. **(ii) 에**\n**이전트** **기반 RL 훈련은** **도메인별** **추론** **능력을** **지속적으로** **향상시킵니다** . 보고된 바와 같이, 콜드 스타트 단계는 기본 모델\n에 비해 강력한 초기 정책을 수립할 수 있지만, 이후의 에이전트 기반 RL은 놀라운 성능 향상을 가져옵니다. 구체적으로,\n에이전트 기반 RL은 온라인 실험 설정에서 DeepTravel-8B와 DeepTravel-32B의 초기 콜드 스타트 정책을 각각 24%(40.00\n에서 49.75) 및 25.5%(50.03에서 62.77) 향상시켰습니다. 오프라인 설정에서도 상당한 개선이 있었습니다.\n\n\n11\n표 3: Qwen3-8B에서 DeepTravel과 기존 강화 학습 알고리즘 비교.\n\n\n\n\n\n\n\n\n\n|모델|오프라인 제약조건없음|Col3|온라인|인간|\n|---|---|---|---|---|\n|**모델**|**제약 조건 없음**<br>**제약 조건 적용 시**<br>**쉬움**<br>**중간**<br>**어려움**<br>**쉬움**<br>**중간**<br>**어려움**|**제약 조건 없음**<br>**제약 조건 적용 시**<br>**쉬움**<br>**중간**<br>**어려움**<br>**쉬움**<br>**중간**<br>**어려움**|**온라인**|**인간**<br>**평가**|\n|베이스 (Qwen3-8B)<br>|10.95<br>9.47<br>4.76|28.57<br>26.67<br>5.35|26.72|26.00|\n|PPO포함<br>GRPO포함<br>DAPO포함<br>딥트래블과 함께|48.26<br>33.25<br>14.62<br>52.36<br>34.06<br>13.52<br>52.06<br>35.52<br>15.04<br>**54.25**<br>**36.84**<br>**20.24**|60.05<br>34.86<br>15.04<br>61.78<br>36.65<br>15.82<br>62.24<br>40.02<br>16.54<br>**64.86**<br>**41.89**<br>**21.40**|45.63<br>47.78<br>46.07<br>**49.75**|62.00<br>64.00<br>64.00<br>**70.00**|\n\n\n표 4: Qwen3-8B에서 콜드 스타트와 RL에 대한 제거 연구.\n\n\n\n\n\n\n\n**오프라인**\n\n\n\n\n\n|모델 변형|제약 조건 없음 제약 조건 적용 시<br>쉬움 중간 어려움 쉬움 중간 어려움|Col3|온라인|평가|\n|---|---|---|---|---|\n|DeepTravel-8B (ER제외)<br>DeepTravel-8B (CS제외)<br>DeepTravel-8B (트레이닝 제외)<br>DeepTravel-8B (회전 기능 없음)|51.01<br>32.21<br>8.81<br>45.99<br>25.26<br>16.79<br>50.26<br>35.47<br>18.24<br>52.05<br>28.04<br>5.25|60.86<br>35.00<br>8.75<br>53.57<br>35.56<br>22.18<br>61.06<br>33.25<br>20.75<br>59.04<br>14.24<br>10.76|40.00<br>32.45<br>26.52<br>32.45|66.00<br>48.00<br>66.00<br>58.00|\n|딥트래블-8B|54.01<br>36.84<br>20.24|64.86<br>41.89<br>21.40|49.75|70.00|\n\n\n**기존 RL 알고리즘과의** **비교.** 또한 DeepTravel-8B-Cold-Start 백본에서 DeepTravel을 최근 공개된 세 가지 RL 방법과 비교\n합니다. 각 RL 방법에 대해 동일한 훈련 샘플로 100회의 훈련 단계를 실행했습니다. 표 3에 결과를 보고합니다. 다음과 같\n은 관찰 결과를 강조한다: 모든 온라인 RL 방법이 기본 모델을 개선하는 반면, 제안된 DeepTravel은 답장 메커니즘 덕분에\n어려운 문제에서 기존 RL 방법을 현저히 능가한다.\n\n\n4.3 제거 실험\nDeepTravel의 각 모듈 효과를 검증하기 위해 Qwen3-8B 데이터셋에서 제거 실험을 수행합니다. 구체적으로 다음 변형들을\n비교합니다. (1) DeepTravel-8B w/o ER은 강화학습 훈련 과정의 경험 재현(Experience Replay) 모듈을 제거합니다. (2)\nDeepTravel-8B w/o CS는 강화 학습 수행 전 SFT 기반 콜드 스타트 단계를 제거합니다. (3) DeepTravel-8B w/o Traj는 강\n화 학습 훈련 과정의 궤적 수준 검증기를 제거합니다. (4) DeepTravel-8B w/o Turn은 강화 학습 훈련 과정의 **턴** 수준 검증\n기를 제거합니다.\n\n\n표 4에서 볼 수 있듯이, 다음과 같은 관찰 결과를 얻었다. 첫째, 경험 재현 전략은 훈련에 중요하다. 이를 제거하면 모델 성능\n이 저하된다. 둘째, 콜드스타트 단계는 RL 훈련에 중요한 것으로 보인다. 이를 제거한 후 성능이 현저히 떨어졌기 때문이다.\n\n잠재적 원인은 콜드스타트가 LLM이 기본 도구 사용법과 지시사항 수행 능력을 학습하는 데 도움이 되기 때문일 수 있다.\n\n마지막으로, 턴 수준 검증기가 모델 성능에 더 크게 기여한다는 점을 관찰했다. 이를 제거하면 에이전트 성능이 저하되고\n어려운 문제에서 부진한 모습을 보인다. 더 복잡한 문제는 턴별 검증이 필요하기 때문일 수 있다. 그러나 궤적 수준 검증기\n도 제거 시 성능 저하를 유발하므로 중요함이 입증된다. 정확도 기여 외에도 궤적 수준 검증기는 세밀한 턴별 검증의 필요\n성을 제거함으로써 훈련 효율성을 향상시킨다.\n\n\n4.4 심층 분석\n**에이전트** **기반** **강화학습** **훈련** **분석.** 그림 4에서는 강화학습 훈련 과정 전반에 걸친 검증 보상, 모델 엔트로피, 평균 상호작\n용 턴 수, 도구 호출 정확도를 제시합니다. DeepTravel의 효과성을 평가하기 위해 다음과 같은 결과를 강조합니다: **(i) 콜드**\n**스타트** **및** **경험** **재생** **전략의** **영향.** 콜드 스타트 전략은 특히 도구 상호작용 턴 수를 더 적절하게 생성하는 측면에서 합리적\n으로 효과적인 정책을 초기화하는 데 도움이 됩니다(기본 모델은 도구 호출을 과도하게 사용하는 경향이 있음). 경험 재현\n전략은 초기 훈련 단계에서는 거의 기여하지 않습니다. 그러나 이전에 실패한 샘플을 점진적으로 재현함으로써 모델의 능\n력을 꾸준히 향상시키고, 결국 후반 훈련 단계에서 재현이 없는 기준선 대비 상당한 개선을 이끌어냅니다. **(ii) 샌드박스의**\n12\n**영향.** 우리는 에이전트 기반 RL 훈련을\n13\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-13-0.png)\n\n그림 4: 에이전트 기반 강화학습 훈련 과정 전반에 걸친 검증 보상(최종 통과율), 모델 엔트로피, 평균 상호작용 턴 및 도구\n호출 정확도(성공률).\n\n\n그림 5: 50개의 무작위 표본 추출된 실제 온라인 사용자 사례를 기반으로 평가된, 7가지 인간 주석 차원에 걸친 자율적 TP\n에이전트의 성능 비교.\n\n\n실제 API와 구축된 샌드박스 환경에서. 실제 API를 사용한 도구 호출 정확도는 불안정하며 샌드박스 환경에서 달성된 정\n확도보다 지속적으로 낮다. 이러한 상황에서 TP 에이전트는 명확한 보상 개선을 보이지 않아, 에이전트 기반 RL 훈련에 안\n정적인 샌드박스 환경이 중요함을 강조한다. **(iii) 에이전트** **기반 RL에서의** **비감소** **엔트로피.** 에이전트 기반 RL 훈련 중 엔\n트로피가 감소하지 않는 현상을 추가로 관찰했으며, 이는 최근 여러 연구(Dong et al., 2025)에서도 제기된 바 있습니다. 그\n배경에는 TP 에이전트가 외부 도구의 동적으로 변화하는 응답에 지속적으로 정책을 적응시켜야 한다는 점이 있다고 생각\n합니다.\n\n\n**실제** **사용자** **연구.** 우리는 DiDi의 ES 제품 매니저와 주석 팀이 정의한 평가 차원을 기반으로 실제 사용자 연구를 수행했으\n며, 그 결과는 그림 5에 요약되어 있습니다. 전반적으로 콜드 스타트(cold-start)와 강화 학습(RL) 접근법 모두 7가지 평가\n차원에서 사용자 만족도를 향상시켰습니다. 특히 콜드스타트 방식은 모델의 기본 역량을 크게 향상시켰으며, 사용자 의도\n이해와 생성된 여행 일정의 완성도, 실현 가능성, 명확성 개선에서 두드러졌습니다. 그러나 개인화된 선호도 파악 및 충족\n과 같은 고급 기능의 경우 콜드스타트 방식만으로는 불충분한 것으로 나타났으며, 이는 RL 단계에서 대규모 탐색이 필요할\n수 있음을 시사합니다. 마지막으로, 기본 모델과 콜드스타트 모델 모두 심각한 환각 현상 문제를 보였으며, 사실 오류율이\n최대 50%에 달했습니다. RL 훈련은 이 문제를 효과적으로 해결하여 환각 현상을 20% 미만으로 감소시켰습니다. 더 많은\n주석 분석 결과는 부록 A.5에 수록되어 있습니다.\n\n\n5 관련 연구\n5.1 여행 계획 에이전트로서의 대규모 언어 모델(LLM)\nLLM은 LLM 기반 여행 계획 에이전트가 사용자의 선호도에 부합하는 여정 생성을 위해 외부 도구와 상호작용할 수 있도\n록 함으로써 여행 계획(TP)을 재구성했습니다. 문헌에서는 여행 계획 에이전트를 구축하기 위한 두 가지 주요 패러다임이\n등장했습니다: (i) 수작업 프롬프트 튜닝, (ii) 고정된 에이전트 워크플로 설계. 수작업 프롬프트 튜닝 접근법(Shao et al.\n\n2025) — TravelPlanner(Xie et al. 2024), TripTailor(Wang et al. 2025), ChinaTravel(Shao et al. 2024a) 포함 — 은 종단간\n작업을 다중 하위 단계로 분해합니다(Ni et al. (2025)에 의해 다단계로 분해하고 각 단계에 맞춤형 평가 지표를 도입합니다.\n\n효과적이긴 하지만, 동적 실제 환경(예: 도구 가용성)에 대한 기반이 취약하여 실용적 가치가 제한됩니다. 이를 위해 최근\n14\n\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-13-1.png)\n연구에서는 프롬프트 전략을 잘 구조화된 에이전트 파이프라인에 통합합니다. 대표적인 연구로는 TravelAgent Chen et al.\n(2024),\n15\nPTS Shao 등(2025)과 RETAIL Deng 등(2025)은 의도 이해를 강화하고 외부 도구를 조정하며 종단 간 여정 생성을 보장하\n기 위해 고정된 워크플로를 설계합니다. 그러나 이러한 워크플로는 구축 및 유지 관리에 여전히 많은 노력이 필요하며, 새\n로운 사용자 질의나 변화하는 도구 및 자원에 대한 일반화 능력이 부족하여 TP 에이전트의 유연성과 자율성을 제한합니다.\n\n\n5.2 LLM을 위한 에이전트 기반 강화 학습\n대리 강화 학습(RL)은 최근 자율적 AI 에이전트 구축을 위해 다양한 분야에서 널리 적용되고 있다(Zhang et al., 2025a). 여\n기서 에이전트는 전용 샌드박스 환경에서 도구와 상호작용하며, 수신된 보상 피드백을 기반으로 정책을 반복적으로 개선한\n다(Shang et al., 2025). 예를 들어, ReTool Feng et al. (2025), Kimi-Researcher MoonshotAI (2025) 및 WebSailor Li et al.\n\n(2025)는 수학, 심층 연구 및 웹 영역에서 추론 에이전트로 구축되었습니다. 또한 rStar2-Agent Shang et al. (2025) 및\nAgentGym-RL Xi et al. (2025)과 같은 최근의 많은 연구들은 다양한 영역에 걸쳐 통합된 에이전트 기반 RL 훈련 프레임워\n크를 제안하여 기초 에이전트 구축을 용이하게 하려는 시도를 하고 있습니다. 그럼에도 불구하고 여행 계획 영역에서 에이\n전트 기반 RL의 적용은 아직 탐구되지 않은 상태입니다.\n\n\n6 결론, 한계 및 향후 연구 방향\n본 연구에서는 자율 여행 계획(TP) 에이전트 구축을 위한 최초의 종단간 에이전트 기반 강화학습(RL) 훈련 프레임워크인\nDeepTravel을 제안하며, 이는 기존 TP 연구에 새로운 패러다임을 제시한다. 먼저 TP 에이전트가 실제 세계 API 문제(예:\n\nQPS 제한 및 불일치 출력)의 제약 없이 훈련될 수 있는 견고한 샌드박스를 구축한다. 이어서 계층적 보상 모델링 시스템을\n제안한다. 먼저 고수준 시공간 요구사항 검증을 위한 거친 트래지토리 검증기를 설계하고, 이후 턴 단위 검증기를 활용해 에\n이전트의 답변을 단계별로 검증한다. 마지막으로 리플레이 강화 학습 알고리즘을 제안하여 여행 에이전트가 주기적으로 이\n전 실패 사례를 재현함으로써 도메인 외 일반화 능력을 향상시킨다. 결과물인 자율 TP 에이전트를 DiDi ES 앱에 배포하여\nDeepTravel의 적용 가치를 입증했습니다. 온라인 생산 환경과 오프라인 합성 데이터에 대한 광범위한 실험 결과,\nDeepTravel은 소규모 LLM(예: Qwen3-8B/32B)이 DeepSeek-R1 및 OpenAI-o1/o3과 같은 최첨단 추론 LLM을 크게 능가\n하도록 합니다. 그러나 DeepTravel은 신중하게 설계된 보상 시스템에 의존하여 확장성이 제한됩니다. 향후 더 유연한 보상\n모델을 개발하고 이 프레임워크를 다른 도메인으로 확장하는 것을 목표로 합니다.\n\n\n윤리 및 재현성 선언문\n**윤리** **선언문.** 본 연구는 모델 훈련 및 평가를 위해 DiDi 엔터프라이즈 솔루션 앱의 실제 사용자 데이터를 활용합니다. 그러\n나 개인 정보를 저장하거나 실제 사용자 쿼리를 공개하지 않으므로 추가적인 개인정보 보호 또는 공정성 문제는 발생하지\n않습니다. 합성된 사용자 질의에 대해서는 독자의 이해를 돕기 위해 부록 A.6에 질의 사례 예시를 제공합니다. 모델 평가에\n사용된 인간 주석의 경우, 주석 과정 재현을 용이하게 하기 위해 부록 A.5에 평가 차원의 전체 세트를 제공합니다.\n\n\n**재현성** **진술.** 재현성을 보장하기 위해 부록 A.2에 훈련 프롬프트에 대한 상세한 설명을 제공하고, 부록 A.3에 데이터 큐레\n이션 파이프라인을 개요로 제시합니다. 이 정보가 연구 커뮤니티가 우리의 결과를 재현하는 데 도움이 될 것이라고 믿습니\n다. DeepTravel에는 DiDi Enterprise Solution에 대한 독점 인터페이스가 포함되어 있으므로 통합된 샌드박스 및 훈련 구현\n을 공개할 수 없습니다. 그럼에도 불구하고 재현 및 이해를 돕기 위해 컴퓨팅 리소스와 모니터링 지표를 포함한 포괄적인\n훈련 세부 사항을 부록 A.4에 제공합니다.\n\n\n참고문헌\nSoumyabrata Chaudhuri, Pranav Purkar, Ritwik Raghav, Shubhojit Mallick, Manish Gupta, Abhik Jana, and Shreya Ghosh.\nTripcraft: A benchmark for spatio-temporally fine grained travel plan-ning. _arXiv preprint arXiv:2502.20508_, 2025.\n\n\nAili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, Jiangjie Chen. Travelagent: 맞춤형 여행 계획을 위한 AI 어시스턴트.\n\n_arXiv 사전인쇄본 arXiv:2409.08069_, 2024.\n\n\n16\nBin Deng, Yizhe Feng, Zeming Liu, Qing Wei, Xiangrong Zhu, Shuai Chen, Yuanfang Guo, Yunhong Wang. Retail: 대규\n모 언어 모델을 위한 현실 세계 여행 계획 수립을 향하여. _arXiv 사전인쇄본 arXiv:2508.15335_, 2025.\n\n\n동관팅, 마항위, 마카이, 바오리청, 천이페이, 왕종위안, 천종샤, 두자젠, 왕후이양, 장푸정 외. 에이전트 기반 강화 정책 최적\n화. _arXiv 사전인쇄본 arXiv:2507.19849_, 2025.\n\n\nBowen Fang, Zixiao Yang, Shukai Wang, and Xuan Di. Travellm: 네트워크 장애 상황에서 새로운 대중교통 경로를 계획해\n줄 수 있나요? _arXiv 사전인쇄본 arXiv:2407.14926_, 2024.\n\n\nJiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and\nWanjun Zhong. Retool: 강화 학습을 통한 AI의 전략적 도구 사용. _arXiv 사전인쇄본 arXiv:2504.11536_, 2025.\n\n\n구지빈, 샤오즈홍, 공예운, 션이룽, 양위주, 황민리, 두안난, 천웨이주. Tora: 수학 문제 해결을 위한 도구 통합 추론 에이전\n트. _arXiv 사전인쇄본 arXiv:2309.17452_, 2023.\n\n\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang,\nXiao Bi, 외. Deepseek-r1: 강화 학습을 통한 대규모 언어 모델의 추론 능력 촉진. _arXiv 사전인쇄본 arXiv:2501.12948_,\n2025.\n\n\n황전안, 장이홍, 루궈산, 진제위, 쉬하오카이, 자오톈위, 펑루, 후자치, 셴잔밍, 후샤오멍 외. 평가 기준을 활용한 강화 학습.\n\n_arXiv 사전인쇄본 arXiv:2508.12790_, 2025.\n\n\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander\nMadry, Alex Beutel, Alex Carney 외. OpenAI O1 시스템 카드. _arXiv 사전인쇄본 arXiv:2412.16720_, 2024.\n\n\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search\nr1: 강화 학습을 통해 추론하고 검색 엔진을 활용하도록 대규모 언어 모델 훈련하기. _arXiv_ _사전_ _인쇄본_\n_arXiv:2503.09516_, 2025.\n\n\n관 리, 종왕 장, 회펑 인, 리원 장, 리투 오, 자이롱 우, 원뱌오 인, 바이쑤안 리, 정웨이 타오, 신위 왕 외. 웹세일러: 웹 에이전\n트를 위한 초인적 추론 탐색. _arXiv 사전인쇄본 arXiv:2507.02592_, 2025.\n\n\n린 자주, 자오 하오란, 장 아오치, 우 이팅, 핑 후치위, 천 친. Agentsims: 대규모 언어 모델 평가를 위한 오픈소스 샌드박스.\n\n_arXiv 사전인쇄본 arXiv:2308.04026_, 2023.\n\n\nMoonshotAI. Kimi-researcher: 새로운 에이전트 기능을 위한 종단간 RL 훈련.\n_[https://moonshotai.github.io/Kimi-Researcher/](https://moonshotai.github.io/Kimi-Researcher/)_, 2025.\n\n\nPhuong Minh Binh Nguyen, Xuan Lan Pham, Giang Nu To Truong. 여행 블로그를 통한 관광객의 여행 계획에 대한 정보\n원의 신뢰도와 영감의 영향. _여행연구저널_, 64:222 – 237, 2023.\n\n\nHang Ni, Fan Liu, Xinyu Ma, Lixin Su, Shuaiqiang Wang, Dawei Yin, Hui Xiong, and Hao Liu. Tp-rag: 시공간 인식 여행\n계획을 위한 검색 강화형 대규모 언어 모델 에이전트 벤치마킹. _arXiv 사전인쇄본 arXiv:2504.08694_, 2025.\n\n\nOpenAI. gpt-oss-120b & gpt-oss-20b 모델 카드, 2025.\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. 근접 정책 최적화 알고리즘. _arXiv 사전인쇄_\n_본 arXiv:1707.06347_, 2017.\n\n\nNing Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou,\nBowen Zhang, 외. rstar2-agent: 에이전트 추론 기술 보고서. _arXiv 사전인쇄본 arXiv:2508.20722_, 2025.\n\n\n17\nJie-Jing Shao, Bo-Wen Zhang, Xiao-Wen Yang, Baizhi Chen, Si-Yu Han, Wen-Da Wei, Guohao Cai, Zhenhua Dong, Lan\nZhe Guo, Yu-feng Li. Chinatravel: 중국어 여행 계획 언어 에이전트를 위한 개방형 벤치마크. _arXiv_ _사전인쇄본_\n_arXiv:2412.13682_, 2024a.\n\n\n샤오즈홍, 왕페이이, 주치하오, 쉬룬신, 송준샤오, 비샤오, 장하오웨이, 장밍촨, 리위켄, 우양 외. Deepseekmath: 오픈 언어\n모델에서 수학적 추론의 한계를 넓히기. _arXiv 사전인쇄본 arXiv:2402.03300_, 2024b.\n\n\n샤오 지젠, 우 지안칸, 천 웨이젠, 왕 샹. 개인 여행 솔버: 여행 계획을 위한 선호도 기반 LLM-솔버 시스템. _제63회전산언어_\n_학회연차대회논문집 (제1권: 장문논문)_, pp. 27622–27642, 2025.\n\n\nJoykirat Singh, Raghav Magazine, Yash Pandya, Akshay Nambi. 강화 학습을 통한 대규모 언어 모델의 행위적 추론 및 도\n구 통합. _arXiv 사전인쇄본 arXiv:2505.01441_, 2025.\n\n\nYihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Zhaofeng Wu, Dingyi Zhuang, Jushi Kai, Ke-bing Hou, Xiaotong Guo,\nHan Zheng 외. Itinera: 대규모 언어 모델과 공간 최적화를 통합한 개방형 도심 여정 계획. _arXiv_ _사전인쇄본_\n_arXiv:2402.07204_, 2024.\n\n\nByteDance Seed Team. Seed-oss 오픈소스 모델, 2025.\n\n\nKimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun\nChen, Yutian Chen, 외. Kimi k2: 개방형 에이전트 지능. _arXiv 사전인쇄본 arXiv:2507.20534_, 2025.\n\n\n왕카이민, 션위안저, 룽창저, 정샤오칭, 황쑤안징. Triptailor: 맞춤형 여행 계획을 위한 실제 세계 벤치마크. _계산언어학회_\n_연구결과: ACL 2025_, pp. 9705–9723, 2025.\n\n\n시즈헝, 황지쉔, 랴오청양, 황바오다이, 궈홍린, 류자치, 정루이, 예준지에, 장자정, 천원샹 외. Agentgym-rl: 다중 턴 강화 학\n습을 통한 장기적 의사 결정을 위한 대규모 언어 모델 에이전트 훈련. _arXiv 사전인쇄본 arXiv:2509.08755_, 2025.\n\n\nJian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su. Travelplanner: 언어\n에이전트를 활용한 실제 세계 계획 수립을 위한 벤치마크. _arXiv 사전인쇄본 arXiv:2402.01622_, 2024.\n\n\nTian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong\nLuo. Logic-rl: 규칙 기반 강화 학습으로 대규모 언어 모델 추론 능력 해방. _arXiv 사전인쇄본 arXiv:2502.14768_, 2025.\n\n\n양안, 리안펑, 양바오송, 장베이천, 후이빈위안, 정보, 위보원, 가오창, 황청겐, 룽천슈 외. Qwen3 기술 보고서. _arXiv 사전_\n_인쇄본 arXiv:2505.09388_, 2025a.\n\n\n양동제, 루청창, 왕치멍, 마신베이, 가오얀, 후야오, 자오하이. 여행 계획을 세우고 계획대로 여행하라: 대규모 언어 모델을\n통한 광역 계획 및 평가. _arXiv 사전인쇄본 arXiv:2506.12421_, 2025b.\n\n\n유치잉, 장정, 주루오페이, 위안위펑, 좌샤오천, 유에위, 판톈톈, 류가오홍, 류링준, 류신 외. Dapo: 대규모 오픈소스 대규모\n언어 모델 강화 학습 시스템. _arXiv 사전인쇄본 arXiv:2503.14476_, 2025.\n\n\n증실량, 위콴, 윌리엄 브라운, 오아나 프룬자, 유리 네브미바카, 홍밍이. 턴 수준 신용 할당을 통한 대규모 언어 모델 에이전\n트의 다중 턴 추론 강화. _arXiv 사전인쇄본 arXiv:2505.11821_, 2025.\n\n\n장귀빈, 겅허자, 유샤오항, 인전페이, 장자이빈, 탄젤린, 저우헝, 리종즈, 쉐샹위안, 리이장 외. 대규모 언어 모델을 위한 에이\n전트 기반 강화 학습의 현황: 서베이. _arXiv 사전인쇄본 arXiv:2509.02547_, 2025a.\n\n\n홍즈 장, 자 푸, 징위안 장, 카이 푸, 치 왕, 푸정 장, 구오루이 저우. RLEP: 대규모 언어 모델 추론을 위한 경험 재생 기반 강\n화 학습. _arXiv 사전인쇄본 arXiv:2507.07451_, 2025b.\n\n\n18\n**DeepTravel: 자율** **여행** **계획** **에이전트를** **위한** **종단간** **에이전트** **기반** **강화** **학습** **프레임워**\n**크.** _보충자료_\n목차\n**1** **소개** **2**\n**3**\n**2  예비**\n**3  DeepTravel** **3**\n4\n3.1 개요  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n4\n3.2 견고한 샌드박스 구축  . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n\n3.2.1 툴킷 주석 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n\n4\n4\n3.2.2 모의 데이터 수집 및 업데이트 메커니즘  . . . . . . . . . . . . . . .\n\n\n5\n3.3 계층적 보상 모델링 . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n\n3.4 리플레이 강화 학습 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n\n5\n5\n3.4.1 지도 학습 미세 조정을 통한 추론 형식 콜드 스타트 . . . . . . . . .\n\n\n\n3.4.2 경험 재생을 통한 강화 학습 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . .\n\n\n\n6\n**6**\n**4  실험**\n6\n4.1 실험 설정 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n7\n4.2 주요 결과  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n8\n4.3 절제 연구  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n4.4 심층 분석 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n**5** **관련** **연구** **9**\n9\n5.1 여행 계획 에이전트로서의 대규모 언어 모델(LLMs)  . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n10\n5.2 LLM을 위한 대리 강화 학습  . . . . . . . . . . . . . . . . . . . . .\n\n\n**6** **10**\n**결론, 한계** **및** **향후** **연구**\n**A** **부록** **14**\n14\nA.1 대규모 언어 모델의 사용 사례(  )  . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n14\nA.2  프롬프트 템플릿  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n16\nA.3  데이터 큐레이션 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n\nA.3.1  데이터 종합 파이프라인 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n\n16\n17\nA.3.2  데이터셋 통계 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n17\nA.4  훈련 세부 사항 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n\n19\n19\nA.5  인간 주석 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nA.6  사례 연구  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n20\nA 부록\nA.1 대규모 언어 모델의 활용\n본 논문에서는 주로 대규모 언어 모델(LLM)을 사용하여 그림 플롯을 생성합니다. 또한 저자들이 작성한 문단에서 명백한\n문법 오류를 식별하고 수정하기 위해 LLM을 활용합니다.\n\n\nA.2 프롬프트 템플릿\n이 섹션에서는 DeepTravel의 시스템 프롬프트와 궤적 수준 및 턴 수준 검증기의 보상 모델 프롬프트 템플릿을 포함하여 본\n연구에서 사용된 프롬프트를 제공합니다.\n\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-20-0.png)\n\n\n\n\n\n\n\n\n\n\n\n. 표 A.2.1에 표시된 바와 같이, 시스템 프롬프트에는 필수 추론 형식, 툴킷 정보 및 답변 규칙이 제공되어 에이전트의 행동\n을 안내합니다.\n\n\n**궤적** **수준** **검증기** **프롬프트** **템플릿.** 본 연구에서는 수작업으로 제작된 평가 기준표를 활용하여 보상 모델링 시스템을 구축\n합니다. 궤적 수준 검증기의 경우, 사용자 질의와 에이전트의 최종 응답을 제공합니다. 주요 목표는 응답이 주석 처리된 거\n시적 시공간 원칙(예: 답변 완전성, 사용자 의도와의 정합성, 여행 계획의 논리적 구조 준수)을 준수하는지 평가하는 것이다\n. 이러한 기초 원칙은 표 A.2.2에 제시된 여섯 가지 평가 기준표로 구체화된다.\n\n\n21\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-21-0.png)\n\n\n\n\n\n22\n**턴** **수준** **검증기** **프롬프트** **템플릿.** 궤적 수준 검증기 외에도, 우리는 에이전트의 응답을 턴별로 평가하는 턴 수준 검증기를\n활용한다. 이를 위해 프롬프트에 추가적인 도구-응답 정보를 통합한다. 각 턴마다 LLM이 평가 근거와 최종 결론을 모두 생\n성하도록 지시합니다. 표 A.2.3에서 볼 수 있듯이, 검증기는 모든 턴에 대해 단계별 평가 근거와 최종 판단을 함께 제공하도\n록 프롬프트됩니다.\n\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-22-0.png)\n\n\n\n\n\n\n\n터 합성 파이프라인을 소개한 후, DeepTravel에서 사용된 상세 통계 정보(SFT 콜드스타트 단계, RL 훈련, 오프라인 테스트\n및 온라인 테스트 포함)를 소개합니다.\n\n\nA.3.1 데이터 합성 파이프라인\n본 논문에서 데이터 합성의 주요 목표는 사용자 쿼리를 구성하는 것입니다. 각 사용자 쿼리는 _출발지_, _목적지_, _출발시간_, _도_\n_착시간_, _예산_ 등과 같은 원자적 의도(atomic intent) 집합으로 분해됩니다. 본 방법론에서는 원자적 의도의 조합을 체계적으\n로 열거한 후, 이러한 의도 구조에 대응하는 자연스러운 사용자 쿼리를 역으로 생성합니다.\n\n\n초기 쿼리 풀을 확보한 후 Qwen-3-32B 모델로 반복 샘플링을 수행합니다. 각 후보 쿼리는 난이도 점수를 부여하는 보상 모\n델링 시스템으로 평가됩니다. SFT 콜드스타트에서는 주로 비교적 단순한 쿼리를 사용하되, 모델 견고성 향상을 위해 소량\n의 난이도 높은 쿼리를 포함시킵니다. 반면 RL에서는 복잡한 쿼리를 중점적으로 활용하되, 커버리지 확보를 위해 소량의 단\n순 사례를 포함시킵니다.\n\n\n23\n모델의 견고성을 높이기 위해 소량의 어려운 쿼리를 포함합니다. 반대로 RL의 경우 복잡한 쿼리를 중점적으로 사용하지만,\n커버리지를 보장하기 위해 간단한 사례도 소량 포함합니다.\n\n\n쿼리를 RL 훈련에 통합하기 전에 추가로 수동 검사 단계를 수행합니다. 이 인간 필터링 단계는 의미적으로 무효하거나 잘\n못 구성된 쿼리를 제거하는 데 중요합니다. 쿼리의 품질이 RL 훈련 결과에 상당한 영향을 미친다는 점을 발견했으며, 쿼리\n가 도전적이면서도 학습 가능하도록 보장하는 것이 안정적인 최적화에 필수적입니다.\n\n\n테스트 데이터를 위해, 명시적인 사용자 개인화 제약 조건(예: 예산 제한 또는 개인 여행 선호도)이 포함되는지 여부에 따라\n쿼리를 구분하여 평가 세트를 구성합니다. 이에 따라 _제약조건이있는_ 오프라인과 _제약조건이없는오프라인_, 두 가지 범\n주를 구축합니다. 각 범주별로 500개의 쿼리를 신중하게 선별하여 개인화 및 비개인화 여행 시나리오를 모두 포괄하는 균\n형 잡힌 테스트 데이터셋을 생성합니다.\n\n\nA.3.2 데이터셋 통계\n본 절에서는 SFT 콜드스타트, RL 훈련 및 테스트에 대한 데이터셋 통계를 제시합니다.\n\n\n**SFT** **콜드스타트.** 각 모델 반복 및 업데이트 시 데이터 합성 파이프라인을 활용해 사용자 쿼리를 생성합니다. 이후\nDeepSeek-R1을 사용해 반복 샘플링을 수행합니다. 구축된 보상 모델로 샘플을 필터링한 후, 백본 콜드스타트를 위한 약\n1,000개의 궤적을 얻습니다.\n\n\n**RL 훈련.** 훈련 데이터셋은 약 500개의 고품질 샘플로 구성되며, 모두 인간 어노테이터에 의해 이중 확인되었습니다. RL 훈\n련 과정에서 검증 크기는 50, 훈련 크기는 450으로 설정했습니다.\n\n\n**온라인** **및** **오프라인** **테스트.** DeepTravel 프레임워크의 모델 개선을 검증하기 위해 온라인 및 오프라인 테스트 전략을 모두\n사용합니다. 오프라인 테스트의 경우, 제약 조건이 있는 500개의 테스트 샘플(쉬움 156개, 중간 45개, 어려움 299개)과 제\n약 조건이 없는 500개의 테스트 샘플(쉬움 222개, 중간 78개, 어려움 200개)을 사용합니다. 온라인 테스트의 경우, DiDi\nES 앱의 온라인 운영 환경에서 수집한 6,224개의 쿼리를 사용합니다.\n\n\nA.4 훈련 세부 사항\nSFT 기반 콜드 스타트(cold-start)를 위해 Megatron-LM을 파인 튜닝(fine-tuning)에 사용하며, RL 훈련 프레임워크는 verl 위\n에 구축했습니다. RL 훈련 과정에서 검증기(verifier)의 백본(backbone)으로 DeepSeek-R1을 선택했습니다. 80억 파라미터\nLLM의 경우, 8개의 H800 GPU를 장착한 단일 노드에서 모든 훈련을 완료할 수 있습니다. 32B 매개변수 LLM의 경우, 훈\n련에는 4개의 노드에 걸쳐 32개의 H800 GPU가 필요합니다. SFT는 1,000개의 훈련 샘플에 대해 약 2~3시간이 소요됩니다\n. RL은 500개의 샘플에 대해 100단계당 약 30시간이 소요됩니다.\n\n\n아래에서는 DeepTravel의 안정적이고 효과적인 훈련을 보장하기 위해 추적하는 감독 신호와 진단 항목을 상세히 설명합니\n다. TP 에이전트의 경우 엔트로피, 기울기 노름, 평균 응답 길이, 보상, 평균 턴 수를 지속적으로 모니터링합니다. 또한 샌드\n박스 환경, 보상 모델링 시스템, RL 알고리즘 자체의 영향을 포착하기 위해 도구 호출 정확도와 외부 검증기 성공률과 같은\n광범위한 RL 루프 관련 지표도 추적합니다. RL 방법의 경우, 훈련 전반에 걸쳐 샘플 유지율과 손실 마스크 비율을 추가로\n기록합니다. 이러한 지표들은 훈련 안정성과 실패 모드를 종합적으로 알려줍니다: 낮은 엔트로피는 탐색 부족을, 지나치게\n높은 기울기 노름은 불안정성을 시사합니다. 비정상적으로 짧은 응답과 적은 턴 수는 종종 보상 해킹을 의미하며, 낮은 도\n구 호출 정확도와 검증기 성공률은 체계적인 실행 또는 평가 오류를 가리킵니다. 매우 낮은 샘플 유지율은 일반적으로 데이\n터 체제가 부적절하게 조정되었음을(너무 쉽거나 너무 어려움) 의미하여 탐색의 필요성 또는 기회를 감소시킵니다.\n\n\n그림 6은 훈련 지표 모니터링 사례를 보여줍니다. 보시다시피, 에이전트 기반 강화학습의 경우 외부 환경이 항상 안정적이\n지 않습니다(예: 대규모 보상 서비스로 인해 검증자도 실패할 수 있으며, 오프라인 샌드박스를 도출하더라도 도구 호출 정\n확도가 안정적이지 않음). 또한 기존 RL 훈련과 달리 엔트로피가 단조 감소하는 현상을 관찰하지 못했습니다. 이는 에이전\n24\n트 기반 RL 과정에서 에이전트가 환경으로부터 지속적으로 변화하는 도구 응답을 처리해야 하기 때문입니다. 더불어 상호\n작용 순서 역시 환경에 따라 변동합니다.\n\n\n25\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-25-0.png)\n\n그림 6: 강화학습 훈련 과정에서의 메트릭 모니터링. 에이전트 기반 강화학습 훈련 중, 우리는 정책 엔트로피, 훈련 기울기\n노름, 샘플 유지율, 손실-마스크 비율, 평균 응답 길이, 훈련 보상, 검증기 성공률, 도구 호출 정확도, 평균 턴 수 등 일련의 핵\n심 지표를 주기적으로 모니터링합니다. 이러한 메트릭 중 어느 하나에서 이상 현상이나 퇴행이 발생하면 훈련 실패를 초래\n할 수 있으며, 이는 에이전트 기반 강화학습의 어려움을 시사합니다.\n\n\n26\nA.5 인간 주석 작업\n표 5에 표시된 바와 같이, 인간 주석 과정은 AI 기반 여행 일정의 품질을 종합적으로 평가하기 위해 설계된 7가지 차원으로\n구성됩니다:\n\n\n- **사용자** **의도** **이해는** 시스템이 사용자 입력을 정확히 분석하고 해석하여 모든 핵심 여행 매개변수를 정확히 포착하는 능력\n을 평가합니다. 사용자 요구사항을 오해하면 관련성 없는 추천으로 이어지므로 이 차원은 근본적입니다.\n\n\n- **이동** **경로** **완결성은** 추천된 경로가 필수 여행 구성 요소(항공편, 숙박, 현지 교통편)를 모두 포함하고 시간적 일관성을 유지\n하는지 평가합니다. 완결된 이동 경로는 서로 다른 여행 구간 간 원활한 전환을 제공해야 합니다.\n\n\n- **이동** 일정 **실행** **가능성은** 제안된 일정의 실질적 실행 가능성을 검토합니다. 여기에는 일정이 지나치게 야심차지 않은지, 교\n통편 연결이 현실적인지, 전체 경로가 논리적인 순환 구조를 이루는지 확인하는 것이 포함됩니다.\n\n\n- **이동** **경로** **경제성은** 추천의 경제적·실용적 측면에 초점을 맞추며, 자원이 실제로 예약 가능하고 가격이 정확한지, 추천이\n조직의 여행 정책을 준수하면서 비용 효율성을 유지하는지 확인합니다.\n\n\n- **이동** **경로** **명확성은** 추론 과정의 효율성과 제공된 추론의 투명성을 모두 측정합니다. 사용자를 설득할 수 있도록 명확한 근\n거와 함께 신속한 응답을 제공합니다.\n\n\n- **개인화** **요구사항은** 개별 사용자 선호도와 과거 패턴을 반영하는 시스템의 능력을 평가하여 추천이 사용자 습관 및 선호도\n와 부합하는지 확인합니다.\n\n\n- **사실적** **환각** **없음은** AI 시스템이 존재하지 않는 서비스나 논리적으로 모순된 결과를 생성하여 사용자를 오도할 수 있는 사\n례를 식별하는 중요한 안전 점검 역할을 합니다.\n\n\n표 5: 인간 주석에 대한 TP 에이전트의 평가 차원.\n\n\n**평가** **차원** **설명** **점수** **점수** **기준**\n사용자 의도 이해 사용자 요구사항(출발지/목적지, 시간, 예산, 여행 시\n나리오)을 정확히 식별하는지 여부\n~~여정~~ ~~완성도~~ ~~추천이~~ ~~모든~~ ~~핵심~~ ~~구성~~ ~~요소(항공편 + 호텔 + 교통편)를~~ ~~포함~~\n하는지 여부\n구성 요소 포함 여부: 항공편 + 호텔 + 교통편, 일관된 시\n간 구성\n여정 실행 가능성 실질적으로 실행 가능한 여부\n합리적인 조건 하에서\n이동 경로 비용 적정성 추천된 자원이\n예약 가능, 정확한 가격/재고, 회사 정책 준수, 비용 효율성\n이행 계획의 명확성 추천이 빠르며(200초 이내) 명확한 근거를 제시하\n는지(예:\n\n\"비용 효율적\", \"회의 장소와 가깝다\")\n개인화된 요구사항 추천이 사용자의\n~~이력/선호도(항공사~~ ~~선호도, 시간~~ ~~선호도, 지역~~ ~~선호도,~~\n숙박 유형 등)를 고려했는지\n사실적 환각 없음 추천에 포함된 내용이\n실제 존재하지 않는 항공편/호텔, 논리적 오류 등 AI가\n생성한 콘텐츠 포함 여부\n27\n참/거짓 **참** : 모든 요소를 정확히 식별함\n**틀** 림 : 핵심 요소 누락 또는 잘못된 식별\n~~참/거짓~~ ~~**참**~~ ~~: 완전한~~ ~~커버리지, 일관성~~ ~~있음~~\n**거짓** : 위치/시간 편차, 불완전한 구성 요소\n참/거짓 **참** : 폐쇄 루프, 합리적인 배열\n~~**FALSE**~~ ~~: 지나치게~~ ~~빡빡한~~ ~~일정, 비합리적인~~ ~~조합, 비~~\n폐쇄 루프\n참/거짓 **참** : 실제 자원, 합리적 가격, 정책 준수\n**틀림** : 소폭 초과(15% 이내), 예약 불가 옵션\n참/거짓 **참** : 명확한 근거와 빠른 응답\n**거짓** : 느린 응답 또는 모호하거나 설명할 수 없는 변명\n참/거짓 **참** : 사용자 선호도를 정확히 반영함\n~~**거짓**~~ : ~~개인화된~~ ~~태그~~ ~~누락~~ ~~또는~~ ~~사용자~~ ~~습관~~ ~~위반~~\n참/거짓 **참** : 사실적으로 정확한 추천\n**틀림** : 환각이나 논리적 오류 포함\nA.6 사례 연구\n윤리적 요구 사항으로 인해 훈련에 사용된 데이터를 공개할 수 없습니다. 독자의 이해를 돕기 위해 실제 사용자 질의와 이\n에 대한 에이전트의 응답을 예시로 제공합니다.\n\n\n구체적으로, DiDi Enterprise Solution App에 배포된 여행 계획 서비스에서 실제 사용자 사례를 몇 가지 선정하였다. 이 설\n명 사례는 DeepTravel이 다중 회전 추론 과정 전반에 걸쳐 중간 단계를 탐색하고 개선하기 위해 TP 에이전트가 자율적으로\n계획을 수립하고 실행하며 도구 반응을 반영하는 방식을 독자가 이해하는 데 도움이 되기를 바란다.\n\n\n표 A.6에 설명된 바와 같이, DeepTravel-32B로 구동되는 TP 에이전트는 다중 회전 도구 통합 추론을 따릅니다. 구체적으로\n: (1) 의도 이해 및 제약 조건 기반화. (2) 도구 계획: 도구를 호출하기 전에 에이전트는 시간 버퍼와 대체 분기를 포함한 도구\n계획을 초안으로 작성합니다.\n\n(3) 스키마 정렬 호출을 통한 도구 실행: 모든 도구 호출은 명시적 매개변수와 예상 필드로 구조화되어 재현성과 추적성을\n보장합니다. (4) 도구 응답 해석 및 증거 결합: 에이전트는 반환된 필드를 파싱하여 카드 스타일 증거를 통해 생성된 여정에\n결합함으로써 환각 현상을 방지합니다. (5) 재고: 제약 조건이 충족되지 않을 경우(예: 환승으로 인한 도착 위험), 에이전트\n는 출발 시간대 또는 역 선택을 수정하고 업데이트된 매개변수로 도구를 재호출합니다. (6) 여행 일정 확정: 에이전트는 도\n구 증거와 연결된 각 항목(근거, 대체 옵션, 위험 사항)을 포함하여 일관된 계획을 구성합니다.\n\n\n28\n\n\n![](assets/md_images/할루시네이션-논문-2_DeepTravel-An-End-to-End-Agentic-Reinforcement-Learning-Framework-for-Autonomous-Travel-Planning-Agents-ko.pdf-28-0.png)\n\n\n\n\n\n\n```\n</answer>\n```\n\n29\n\n\n",
    "eng_pdf": "pdf/eng/할루시네이션 논문 2_DeepTravel An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents.pdf",
    "kor_pdf": "pdf/kor/할루시네이션 논문 2_DeepTravel An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents ko.pdf"
  },
  {
    "id": "paper_9",
    "title": "E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing",
    "eng_md": "### E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing\n\nShuvom Sadhuka [1,2], Drew Prinster [1,3], Clara Fannjiang [1], Gabriele Scalia [1], Aviv Regev [1],\nHanchen Wang [1,4]\n1Genentech 2MIT 3Johns Hopkins 4Stanford\nDecember 4, 2025\n**Abstract**\n_Agentic_ AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to\na user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such\nas LLM judges and process-reward models, to score the quality of each action in an agent’s trajectory.\nAlthough these heuristic scores can be informative, there are no guarantees of correctness when used to\ndecide whether an agent will yield a successful output. Here, we introduce _e-valuator_, a method to convert\nany black-box verifier score into a decision rule with provable control of false alarm rates. We frame the\nproblem of distinguishing successful trajectories—that is, a sequence of actions that will lead to a correct\nresponse to the user’s prompt—and unsuccessful trajectories as a sequential hypothesis testing problem.\n_E-valuator_ builds on tools from e-processes to develop a sequential hypothesis test that remains statistically\nvalid at every step of an agent’s trajectory, enabling online monitoring of agents over arbitrarily long\nsequences of actions. Empirically, we demonstrate that _e-valuator_ provides greater statistical power and\nbetter false alarm rate control than other strategies across six datasets and three agents. We additionally\nshow that _e-valuator_ can be used for to quickly terminate problematic trajectories and save tokens. Together,\n_e-valuator_ provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions\nrules with statistical guarantees, enabling the deployment of more reliable agentic systems.\n\n### **1 Introduction**\n\n\n_Agents_ are black-box systems that autonomously perform tasks by executing a sequence of actions, collectively referred to as a _trajectory_ . These trajectories can include diverse actions, including interactions with an\nexternal environment through tool calls, writing code, or steps of logical reasoning. Although we typically\nrefer to _agents_ in this paper as large language model (LLM)-based agents responding to user requests, the\n_agentic_ paradigm is more widely applicable, including robotic agents that execute physical tasks (e.g., pick\nup a cup) through a sequence of mechanical actions [3], and gaming agents that play a game (e.g., poker\nor chess) through a sequence of game-legal actions [5, 51]. Agents have wide applications, and have delivered promising early results in diverse fields including drug discovery [14, 52], cell biology and genomics\n[61, 19], and hypothesis verification [18].\nNonetheless, agents make mistakes, and it is important to be able to detect these. To this end, _verifier_\nmodels have been developed to numerically score each action in an agent’s trajectory. These scores are\ntypically used as a proxy for the probability that the trajectory will successfully produce a correct final\noutput. Example verifiers include judge LLMs, which provide a score (as text output) after each step [28],\nand process-reward models, which are finetuned to give a probabilistic prediction of whether each step\nin a trajectory is “correct” or “incorrect” [29, 73, 31]. These verifiers’ scores can then be used to identify\nunsuccessful trajectories, as those that will produce an incorrect final output.\nA key limitation of verifiers to date is that when their scores are used to decide whether to flag a trajectory as incorrect, there are no guarantees on the resulting probability of error of this downstream decision.\nRigorous guarantees may become particularly critical when agents are deployed in high-stakes settings\nwith real-world implications, such as self-driving labs [27], gene editing [42], or hospital operations [13]. In\n1\nparticular, we focus on the _false alarm rate_, or the probability of incorrectly flagging a successful trajectory as\nunsuccessful. Even if verifier scores satisfy popular notions of probabilistic “correctness”, such as marginal\ncalibration, such notions do not provide guarantees on the false alarm rate. Furthermore, each action in a\ntrajectory costs time and resources, and trajectories can be long. We therefore want to detect that a trajectory\nwill be unsuccessful early on—ideally, after as few actions as possible—instead of having to incur the costs\nof the full trajectory before making the decision. These desiderata necessitate assessing the verifier score\nafter each action, in which case it is unclear how to get guarantees on the probability of _ever_ giving a false\nalarm, particularly since the length of the complete trajectory is never known in advance.\nFine-tuning or otherwise building better verifiers does not directly address these issues. Furthermore,\nfine-tuning verifiers for the deployment setting involve meaningful engineering efforts, requiring sufficient\ncompute as well as white-box access to the verifier (and possibly the agent) weights. Even with sufficient\ncompute and access to the verifier weights, it may be impractical to obtain sufficient data appropriate for\nfine-tuning verifiers: one needs not just trajectories representative of the deployment setting, but also a\n“correctness” label for every action in every trajectory [31].\nTo tackle these challenges, we introduce _e-valuator_, a lightweight statistical wrapper that converts scores\nfrom any black-box verifier into a decision rule for detecting unsuccessful agent trajectories, with guarantees on the false alarm rate. To do so, we first frame the problem as a hypothesis test. We assume that\nthe sequence of verifier scores is drawn from one distribution, _p_ 1, for successful trajectories (i.e., those that\nwill produce a correct final output), and another distribution, _p_ 0, for unsuccessful trajectories. For a new\ntrajectory, the problem then reduces to deciding, after as few actions as possible, whether the verifier score\nsequence is drawn from _p_ 1 or _p_ 0. If one could evaluate _p_ 1 and _p_ 0, one could apply a sequential likelihood\nratio test [60], a well-studied sequential hypothesis test that tests at each time _t_ if the scores collected thus\nfar are from _p_ 1 or _p_ 0. However, _p_ 1 and _p_ 0 are typically not known.\nThus, _e-valuator_ works in three steps: (1) **collect** a small calibration set of agent trajectories (i.e., action\nsequences and intermediate states), stepwise verifier scores, and outcome labels (i.e., whether the final\noutput was correct or incorrect) from the deployment setting, (2) **learn** a model of the density ratio between\n_p_ 1 and _p_ 0, the distribution of verifier scores for successful trajectories and unsuccessful trajectories, at each\nstep _t_, respectively, and (3) **find** a decision threshold for flagging unsuccessful trajectories, using a heldout split of the calibration set, while controlling for the false alarm rate (type I error) at a user-specified _α_\n(i.e., rate at which successful trajectories are accidentally flagged as unsuccessful). Importantly, _e-valuator_\ncomplements any ongoing and future improvements to verifiers: while it guarantees control of the false\nalarm rate for any combination of agent and verifier, deploying it with better verifiers will tend to yield\nmore powerful decision rules. _e-valuator_ requires minimal compute and can run on a standard laptop, and\nwe release code as a Python package available in PyPi at https://pypi.org/project/e-valuator/.\n_E-valuator_ builds on approaches from e-values and sequential hypothesis testing. E-values are an alternative to _p_ -values that are particularly useful in settings where one wants to run a sequence of hypothesis\ntests (e.g., “is this ongoing trajectory successful?”) but might not know the number of tests beforehand\n(since trajectories are of variable length).\nEmpirically, across six datasets and three agents, _e-valuator_ provided better false alarm rate (type I error)\ncontrol and statistical power than other baselines, such as a raw verifier or re-calibrated verifier model\nalone. We highlight one potential use case of _e-valuator_ with early termination of unsuccessful trajectories,\nwhich enables recovering up to 90% of the model’s original accuracy with just 80% of the tokens. We also\nexperiment with a non-LLM setting, where a chess engine numerically scores the board after each move.\nTo summarize, our contributions are as follows:\n\n\n1. We formalize the problem of verifying agent outputs and posit sequential hypothesis testing as a\nsolution to boost existing verifier models.\n\n\n2. We introduce _e-valuator_, a statistical wrapper that can flag unsuccessful trajectories with controllable\nerror rates and strong statistical power.\n\n\n3. _E-valuator_ can adapt to any black-box agent-verifier combination, including non-LLM agents. Because\n_e-valuator_ assumes black-box access to the verifier, methods to directly improve verifier models (e.g.,\nvia finetuning) complement it. pro\n\n\n4. We empirically show that _e-valuator_ outperforms baselines across several datasets, verifiers, and agents.\n\n\n2\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing.pdf-2-1.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing.pdf-2-0.png)\n\n\n\n\n\nlated value). The actions and observations together form the _trajectory_ at each step _t_, _Ht_ = ( _o_ 0 _, a_ 1 _, . . ., at, ot_ ).\nAfter each step _t_, a black-box verifier model _v_ takes as input the trajectory _Ht_ and provides a score,\n_St_ = _v_ ( _Ht_ ), which serves as a heuristic evaluation of the quality of the trajectory thus far. Typically,\n_St_ _∈_ [0 _,_ 1], although _e-valuator_ supports score values of any type. The verifier scores form a sequence\n**S** = ( _S_ 1 _, . . ., ST_ ).\nA complete trajectory _HT_ is associated with a binary label, _Y_ _∈{_ 0 _,_ 1 _}_, of whether the final output, _oT_, is\ncorrect ( _Y_ = 1) or not ( _Y_ = 0). [1] We call trajectories where _Y_ = 1 _successful_ trajectories and those with _Y_ = 0\n_unsuccessful_ trajectories. We assume access to _calibration data_, _D_ cal = _{_ ( **S** [(] _[i]_ [)] _, Y_ [(] _[i]_ [)] ) _}_ _[n]_ _i_ =1 [, where][ (] **[S]** [(] _[i]_ [)] _[, Y]_ [(] _[i]_ [)][)][ are]\ndrawn i.i.d from _P_, a joint distribution over variable-length score sequences and their labels, which has\ndensity _p_ .\n\n\n**2.2** **Evaluation via hypothesis testing**\nGiven a new score sequence, **S**, our goal is to determine whether the agent’s trajectory will produce a correct\n( _Y_ = 1) or incorrect ( _Y_ = 0) final output. We formalize this goal as a hypothesis test. Let _P_ 1 and _P_ 0 denote\nthe distributions of score sequences conditioned on correct and incorrect final outputs, respectively, with\nrespective densities _p_ 1 and _p_ 0. That is, _p_ 1( **S** ) = _p_ ( **S** _| Y_ = 1) and _p_ 0( **S** ) = _p_ ( **S** _| Y_ = 0). We assume _P_ 1 = _P_ 0,\nand we wish to test between two hypotheses:\n\n\n_HN_ : **S** _∼_ _P_ 1 (the final output is correct)\n_HA_ : **S** _∼_ _P_ 0 (the final output is incorrect)\nNote that _P_ 1 and _P_ 0 generally encode complex dependencies between the scores over time. That is, blackbox verifier scores are generally not independent samples from a fixed distribution at every step, or otherwise amenable to convenient assumptions.\nWe construct a _sequential_ _test_ between _HN_ and _HA_ that can be run at each step _t_, using only **S** [1: _t_ ], the\nscores based only on the agent’s trajectory up to step _t_ . Specifically, we construct a sequence of test statistics,\n1Our framework also accommodates infinite-length trajectories that proceed indefinitely by labeling them _Y_ = 0, as they never\nproduce a final output _oT_ for some finite _T_ . However, for practical relevance, we focus here on the setting of finite-length trajectories\nthat always produce a final output.\n\n\n3\n( _Mt_ ) _[T]_ _t_ =1 [where] _[ M][t]_ [is a real-valued function of] **[ S]** [1: _t_ ] [, and a real-valued decision threshold,] _[ c][α]_ [, given a user-]\nspecified error level, _α_ _∈_ (0 _,_ 1). The sequential test then proceeds as follows (Alg. 1). For _t_ = 1 _,_ 2 _, . . ._, if\n_Mt_ _≥_ _cα_, we reject _HN_ . If we reach _t_ = _T_, the end of the agent’s trajectory, without rejecting _HN_, then we\naccept _HN_ . Our goal is to satisfy the following two criteria:\n\n\n1. **False alarm rate control** :\n\n\nPr _HN_ [ _∃_ _t ∈_ [ _T_ ] : _Mt_ _≥_ _cα_ ] _≤_ _α,_ (1)\nwhere _T_, the length of the complete trajectory, is also random, and [ _T_ ] := _{_ 1 _, . . ., T_ _}_ . That is, the _false_\n_alarm_ _rate_, or the rate at which we ever reject successful trajectories, is guaranteed to be at most _α_ .\nThis rate is also known classically as type I error. Notice that the probability under consideration is\nthat _Mt_ _ever_ surpasses _cα_ —equivalently, that we _ever_ reject the null hypothesis, _HN_ —regardless of _T_,\nthe total number of actions the agent takes. This notion of validity is known as _anytime validity_ .\n\n\n2. **High** **power** : Unsuccessful trajectories are often rejected. That is, we desire high rates of rejecting\nunsuccessful trajectories, Pr _HA_ [ _∃_ _t ∈_ [ _T_ ] : _Mt_ _≥_ _cα_ ].\n\n\nWe develop a method that satisfies a high-probability version of the first criterion, and empirically\nachieves the second. The challenge of the first criterion is to construct a sequence of statistics, ( _Mt_ ) _[T]_ _t_ =1 [,]\nand decision threshold, _cα_, such that the sequential test is valid, even though we do not _a_ _priori_ know _T_,\nthe length of the complete trajectory. P-values, a standard tool in modern statistical hypothesis testing, are\nnot naturally suited for this setting. Intuitively, the definition of a p-value—a random variable _q_ such that\nPr _HN_ ( _q_ _≤_ _α_ ) _≤_ _α_ for all _α_ _∈_ [0 _,_ 1]—does not by itself imply anything about how a p-value constructed at\na given step depends on those from previous steps. Such dependencies arise when the data (here, **S** [1: _t_ ])\nobserved at each step are dependent, but also simply by virtue of deciding whether or not to construct\nthe next p-value based on the values of the previous ones, even if the data at each step are independent.\nWithout additional assumptions and corresponding techniques to leverage them [12, 22, 45, 53], lack of\nknowledge of this dependence structure makes it unclear how to evaluate the probability of any rejection\n(see Appendix for a more detailed discussion). We turn instead to _e-processes_ [44, 46], another object for\nquantifying evidence against a null hypothesis, whose definition explicitly characterizes the relationship\nbetween current and past values of the test statistic in a way that naturally enables probabilistic reasoning\nin these settings. An e-process for _HN_ is a sequence, ( _Et_ ) _t_, such that each _Et_ is an _e-value_ for _HN_ —that is,\nE _HN_ [ _Et_ ] _≤_ 1—and there exists a _test martingale_ for _HN_, ( _Mt_ ) _t_, such that _Et_ _≤_ _Mt_ always. A test martingale\nfor _HN_ is a sequence, ( _Mt_ ) _t_, that satisfies two conditions:\n\n\n1. **Non-negativity and unit mean** : _Mt_ is non-negative for all _t_, and _EHN_ [ _M_ 0] _≤_ 1.\n\n\n2. **Martingale** . ( _Mt_ ) _t_ is a martingale for _HN_ . In our setting, this means _EHN_ [ _Mt|M_ 0 _, M_ 1 _, . . ., Mt−_ 1] = _Mt−_ 1\nfor all _t_ [2] .\n\n\nNote that any test martingale is an e-process, but that e-processes are a much broader class of processes.\nConstructing our sequence of test statistics, ( _Mt_ ) _[T]_ _t_ =0 [,] [to] [be] [an] [e-process] [allows] [us] [to] [exploit] [certain]\nproperties to reason probabilistically at each step _t_ about the entire sequence, regardless of how many total\nsteps the agent ends up taking. In particular, Ville’s inequality states that for any e-process, ( _Mt_ ) _t_, we have\nPr _HN_ [ _∃_ _t_ : _Mt_ _≥_ 1 _/α_ ] _≤_ _α_ for every _α ∈_ [0 _,_ 1]. In words, with probability at least 1 _−α_, the entire sequence—\neven if it proceeds indefinitely—is below 1 _/α_, for every _α_ _∈_ [0 _,_ 1]. This inequality concretely shows why\nan e-process can be interpreted as a quantification of evidence over time against the null hypothesis. If the\nnull is true, then with high probability (1 _−_ _α_ ), this evidence will stay low forever (specifically, under 1 _/α_ ).\nIf the alternative is true, a well-designed e-process grows large, as described momentarily.\n\n\n2More formally, a martingale satisfies _EHN_ [ _Mt|Ft−_ 1] = _Mt−_ 1 for all _t_, where ( _Ft_ ) _t_ is a filtration. For clarity of exposition and\nspace constraints, we avoid a fully rigorous treatment here, but roughly, this means we are conditioning on all the information available in _Ft−_ 1, which may be more or less information than conditioning on the exact values of the past. This is equivalent to conditioning on the exact values of the past when _Ft_ is the so-called natural filtration.\n\n\n4\nTo construct an e-process for our setting, we use the ratio between the null density, _p_ 1, and alternative\ndensity, _p_ 1. Specifically, set _M_ 0 = 1, and, for each step _t ∈_ [ _T_ ],\n_Mt_ = _[p]_ [0][(] **[S]** [[1:] _[t]_ []][)] (2)\n_p_ 1( **S** [1: _t_ ]) _[.]_\nSince the density ratio process is a test martingale and therefore an e-process [44] (see the Appendix for a\nproof), Algorithm 1 using this choice of _Mt_ and _cα_ = 1 _/α_ allows us to achieve anytime-valid control of the\nfalse alarm rate (Prop. 1).\n\n\n**Algorithm 1** Sequential hypothesis testing framework for agent verifcation.\n**Inputs:** user prompt, _o_ 0; functions that take in scores and output test statistic for each step _t_,\n_Mt_ ( _·_ ) _, t_ = 1 _, ..._ ; decision threshold, _cα_ _∈_ R.\n**Output:** decision, reject _HN_ or accept _HN_ .\n\n1: _cα_ _←_ 1 _/α_\n2: _H_ 0 _←_ ( _o_ 0)\n3: **for** _t_ = 1 _, . . ., T_ **do**\n4: Given _Ht−_ 1, agent executes action _at_, resulting in environment state _ot_ .\n5: Update trajectory, _Ht_ _←_ ( _o_ 0 _, a_ 1 _, o_ 1 _, . . ., at, ot_ ).\n6: Get verifier score, _St_ _←_ _v_ ( _Ht_ ).\n7: Evaluate test statistic for this step, _Mt_ ( **S** [1: _t_ ]) _▷_ **S** [1: _t_ ] := ( _S_ 1 _, . . ., St_ )\n8: **if** _Mt_ ( **S** [1: _t_ ]) _≥_ _cα_ **then**\n9: **return** reject _HN_\n10: **end if**\n11: **end for**\n12: **return** accept _HN_\n**Proposition** **1.** _For_ _any_ _fixed_ _α_ _∈_ (0 _,_ 1) _,_ _Algorithm_ _1_ _using_ _the_ _density_ _ratio_ _process,_ _Mt_ = _p_ 0( **S** [1: _t_ ]) _/p_ 1( **S** [1: _t_ ]) _,_\n_and the decision threshold cα_ = 1 _/α achieves anytime-valid control of the false alarm rate (Eq. 1)._\n\n\n_Proof._ (Sketch) The density ratio process, ( _Mt_ ) _[T]_ _t_ =0 [, is an e-process.] [Anytime-valid control of the false alar][m]\nrate follows by Ville’s inequality. See Appendix 8.1 for a full proof.\n\n\nAlthough Proposition 1 establishes anytime-valid control of the false alarm rate, it does not illuminate\nwhy one should choose the density ratio process rather than any other e-process. It turns out that under _HA_,\nthe density ratio process grows the fastest (in expectation) over time among all e-processes, a notion called\nlog-optimality made precise in Proposition 2. Log-optimality is analogous to being the “most powerful”\ntest statistic in non-sequential hypothesis testing: intuitively, _Mt_ will tend to surpass the decision threshold,\nand correspondingly enable detection of unsuccessful trajectories earlier than other e-processes.\n\n\n**Proposition 2.** _The density ratio process,_ _Mt_ = _pp_ 01(( **SS** [1:[1: _tt_ ]])) _[,]_ _[is log-optimal.]_ _[That is,]_ _[for any other e-process]_ [ (] _[M]_ _t_ _[ ′]_ [)] _[T]_ _t_ =0\n_and stopping time τ_ _, EHA_ [log _Mτ_ ] _≥_ _EHA_ [log _Mτ_ _[′]_ []] _[.]_\n_Proof._ (Sketch) This is a sequential analog of the Neyman-Pearson lemma [38]. The proof, also provided in\n[44], follows by first noting that any particular _Mt_ is the log-optimal e-variable among all e-variables. One\ncan then invoke properties of e-processes to complete the proof. See Appendix 8.1 for a full proof.\n\n\n**2.3** **Density ratio estimation**\nIn practice, the forms of _p_ 1 and _p_ 0 are typically not known. Prior to deployment, the method we propose,\n_e-valuator_, therefore has a calibration phase in which it uses the calibration data, _D_ cal, to learn a model of the\n5\ndensity ratio, _M_ [ˆ] _t_ ( **S** [1: _t_ ]) _≈_ _pp_ 10(( **SS** [1:[1: _tt_ ]]))) [, for each step] _[ t]_ [ (Alg. 2).] [We do this using classifier-based density ratio]\nestimation, an approach based on the following equality by Bayes’ rule [4, 16]:\n\n\n\n\n_[p]_ [0][(] **[S]** [[1:] _[t]_ []][)] _[p]_ [(] _[Y]_ [= 0] _[|]_ **[S]** [[1:] _[t]_ []][)]\n_p_ 1( **S** [1: _t_ ]) [=] _p_ ( _Y_ = 1 _|_ **S** [1: _t_ ])\n_Mt_ = _[p]_ [0][(] **[S]** [[1:] _[t]_ []][)]\n_[p]_ [(] _[Y]_ [= 0] _[|]_ **[S]** [[1:] _[t]_ []][)] _p_ ( _Y_ = 1) (3)\n_p_ ( _Y_ = 1 _|_ **S** [1: _t_ ]) _p_ ( _Y_ = 0) _[.]_\nSpecifically, for each time step _t_, we train a classifier, _f_ [ˆ] _t_, which takes **S** [1: _t_ ] as input and provides an estimate\nof _p_ ( _Y_ = 1 _|_ **S** [1: _t_ ]). We also form an estimate, _π_ ˆ1, of the class probability _p_ ( _Y_ = 1). We plug these two\nestimates into Eq. (3) to form the following estimated density ratio at step _t_ :\n\n\n\n_M_ ˆ _t_ = [1] _[ −]_ _f_ ˆ _t_ ( _[f]_ [ˆ] **S** _[t]_ [(] [1: **[S]** _t_ [[1:] ]) _[t]_ []][)]\n_π_ ˆ1\n_._\n1 _−_ _π_ ˆ1\nWe will refer to running Algorithm 1 with these estimated density ratios, _M_ [ˆ] _t_, and the same decision\nthreshold motivated by Ville’s inequality, _cα_ = 1 _/α_, as _e-valuator_ _with_ 1 _/α_ _threshold_ . The guarantees in\nPropositions 1 and 2 apply when we learn the true density ratios, i.e., _M_ [ˆ] _t_ = _pp_ 01(( **SS** [1:[1: _tt_ ]])) [for] [each] [point] **[S]** [[1:] _[t]_ []]\nand each step _t_ .\nIn our experiments, we used simple logistic regression for the classifier at each step, _f_ [ˆ] _t_, and found that\nestimated density ratios learned from a few hundred calibration points empirically achieved both false\nalarm rate control and superior power than alternative methods (see Appendix 8.2.2 for details).\n\n\n**Algorithm 2** Density ratio estimation using calibration data.\n**Inputs:** calibration data, _D_ cal.\n**Output:** functions that estimate the density ratio for every step, _{M_ [ˆ] _t}t∈_ N+.\n\n1: Split _D_ cal randomly into _D_ DRE and _D_ threshold, where _D_ DRE _∪D_ threshold = _D_ cal.\n2: _π_ ˆ1 _←_ _|D_ DRE1 _|_ �( **S** _,Y_ ) _∈D_ DRE _[Y]_ _▷_ Estimate class probabilities as empirical frequencies.\n\n3: _T_ max _←_ max _{T_ : _∃_ both successful and unsuccessful score sequences in _D_ DRE with length _T_ _}_\n4: **for** _t_ = 1 _, . . ., T_ max **do**\n5: _D_ DRE _,t_ _←{_ ( **S** [1: _t_ ] _, Y_ ) : ( **S** _, Y_ ) _∈D_ DRE : _|_ **S** _| ≥_ _t}_\n6: Use _D_ DRE _,t_ to train probabilistic classifier, _f_ [ˆ], that takes **S** [1: _t_ ] as input and predicts _p_ ( _Y_ = 1 _|_ **S** [1: _t_ ]).\n\n7: _M_ ˆ _t_ ( _·_ ) _←_ [1] _[−]_ _f_ ˆ( _f_ [ˆ] _·_ () _·_ ) 1 _−π_ ˆ1 _π_ ˆ1\n8: **end for**\n9: Set _M_ [ˆ] _t_ ( _·_ ) _←_ _M_ [ˆ] _T_ max ( _·_ ) for all _t > T_ max.\n\n\n**2.4** **Increasing power via quantile estimation**\nGiven the true density ratio process, _Mt_, rejecting _HN_ whenever _Mt_ surpasses the decision threshold _cα_ =\n1 _/α_ achieves anytime-valid control of the false alarm rate (Prop. 1). However, because we use estimated\ndensity ratios in practice, the threshold _cα_ needs to account for density ratio estimation error. Furthermore,\nsetting _cα_ to achieve anytime-validity may be overly conservative, as it guarantees an upper bound on false\nalarm rate even if the agent keeps executing actions indefinitely. In contrast, agents in practice will typically\nproduce a final output or be terminated after a finite number of actions.\nWe thus propose an alternative procedure for setting the decision threshold _cα_ to account for both of\nthese factors, while retaining a high-probability version of anytime false alarm control. We first split the\ncalibration set, _D_ cal into two splits, _D_ DRE and _D_ threshold, where _D_ DRE is first used for density ratio estimation\n(Alg. 2), and then _D_ threshold is used to set _cα_ as follows.\nFirst, note that rejecting _HN_ if the process _M_ [ˆ] _t_ ever surpasses _cα_ is equivalent to rejecting _HN_ if max _t_ _M_ [ˆ] _t_ _≥_\n_cα_ . It thus suffices to set _cα_ to the (1 _−_ _α_ ) quantile of the distribution of max _t_ _M_ [ˆ] _t_ under the null.\n\n\n6\nAccordingly, we focus on successful trajectories in _D_ threshold, which correspond to _HN_ . For each of these\ntrajectories _i_ in the calibration set, we record the maximum estimated density ratio over all steps, _M_ [(] _[i]_ [)] =\nmax _{M_ [ˆ] 1 [(] _[i]_ [)] _[, . . .,]_ _M_ [ˆ] _T_ [(] _[i]_ [)] _[}]_ [.] [This] [maximum] [value,] _[M]_ [ (] _[i]_ [)][,] [is] [a] [sample] [from] [the] [null] [distribution.] [Given] [these]\nsamples, we construct _q_ ˆ1 _−α_, a high-probability upper bound on the (1 _−_ _α_ )-quantile of the null distribution\n(Alg. 3). We refer to running Algorithm 1 with the estimated density ratios, _M_ [ˆ] _t_, and the decision threshold\nset to this high-probability upper bound, _cα_ = _q_ ˆ1 _−α_, as _e-valuator_ _with_ _probably-approximately-correct_ _(PAC)_\n_threshold_, due to the following guarantee. In words, with probability at least 1 _−_ _δ_ for any user-specified\n_δ_ (that is, “probably”), the procedure has anytime control of the false alarm rate under a user-specified _α_\n(“approximately correct”).\n\n\n**Proposition 3.** _Let {M_ [ˆ] _t}t∈_ N+ _denote the estimated density ratio functions output by Algorithm 2._ _For fixed error_\n_level δ_ _∈_ (0 _,_ 1) _and quantile level α ∈_ (0 _,_ 1) _, let cα be the output of Algorithm 3._ _Then Algorithm 1 using {M_ [ˆ] _t}t∈_ N+\n_and decision threshold cα, or_ e-valuator with probably-approximately-correct (PAC) threshold _, satisfies_\n_PrDcal_ ( _PrHN_ ( _∃_ _t ∈_ [ _T_ ] : _Mt_ _≥_ _cα_ _| Dcal_ ) _≤_ _α_ ) _≥_ 1 _−_ _δ_ (4)\n_Proof._ (Sketch) Finding _q_ ˆ1 _−α_ such that _q_ ˆ1 _−α_ _≥_ _q_ 1 _−α_ with high probability reduces to finding the index _k_\nsuch that the order statistic _M_ ( _k_ ) among calibration set null samples is above _q_ 1 _−α_ with high probability.\nSee Appendix 8.1 for the full proof.\n\n### **3 Related Work**\n\n\nAs _e-valuator_ is a statistical wrapper for any verifier, our work is relevant for but orthogonal to prior work on\nbuilding better verifiers for agents. These verifiers are often trained as reward models that estimate a reward\n(e.g., correctness or coherence) after each step in an agent’s action sequence. Among these, process-reward\nmodels (PRMs) are finetuned using agent trajectories where each _step_ is labeled as correct or incorrect, as in\na mathematical reasoning trace [55, 63, 26]. Some prior works also work to calibrate existing PRMs [72, 39],\nalthough calibration alone is insufficient to control false alarm rates, as we do in this paper\nTraining PRMs can be expensive, as it requires (a) access to human-annotated process labels [31] and (b)\nfinetuning existing LLMs [63]. Alternative verifiers to PRMs include LLM-as-a-judge (i.e., prompt-based\nverification) [2] and outcome reward models, which only provide a label for the entire trajectory [10]. There\nare several benchmarks to compare verifiers on trajectories [35] and processes [73].\nSome prior works build hypothesis tests atop classifiers to monitor AI deployments, albeit in different\ncontexts. Vovk et al. [59] propose using conformal test martingales (CTMs) for continual monitoring of\nmodel deployments, and Prinster et al. [41] develop weighted CTMs to enable adaptation at test time and\nanalyze the cause of degradation. Podkopaev and Ramdas [40] leverage sequential testing for directly\ntracking the risk of deployed models, which has been extended to monitoring without labels [20], under\ntest-time adaptation [47], and for unknown shifts [54]. In a similar vein, [7] applies safe anytime-valid\ntesting to sequentially test if a classifier is fair. [21] applies classifier two-sample tests [32] to detect whether\nthere is a covariate shift between the training and deployment setting. More broadly, several prior works\nmodel class-conditional densities of classifier scores and the corresponding density ratios for evaluation of\nAI systems, albeit without hypothesis testing [49, 68]. Although these methods typically model the densities\ngeneratively [11], some also model the densities discriminatively, as we do here [23].\nAnother line of work tries to add formal statistical controls to LLMs. Conformal prediction methods [48]\nquantify uncertainty in individual predictions while providing finite-sample and black-box guarantees in\nthose uncertainties. Conformal prediction has been applied to resample LLM generations until a minimum\nquality requirement is satisfied [43]. These ideas have been further applied to control the factuality of LLM\noutputs [6, 36] and to generally benchmark LLMs’ confidence in their outputs [71]. More recently, Wu et al.\n\n[69] applies the learn-then-test framework [1] to calibrate a stopping rule for LLM reasoning traces, using\nwhite-box access to the LLM’s internal logits.\n_E-valuator_ directly builds on prior work in e-values. E-values were originally introduced as an alternative to p-values, with useful properties for sequential hypothesis testing [44, 57, 58]. E-values provide\nanytime validity over a (potentially infinite) sequence of tests [62, 67, 66, 15]. They have found important\n7\napplications in A/B testing [25], changepoint detection [50, 33] and LLM-based hypothesis verification [18].\nOur instantiation of the e-process is related to the sequential probability ratio test [60].\n\n### **4 Experiments**\n\n\nTo empirically demonstrate that _e-valuator_ achieves false alarm rate control, as well as higher power than\nalternative methods, we conducted comprehensive experiments across six datasets and tasks using four\ndifferent agent-verifier combinations. We additionally show that _e-valuator_ can be applied to recover a large\nfraction of the original accuracy within a limited token budget.\n**Agents** **and** **verifiers.** We conduct experiments on two tool-calling agents, Aviary [37] and Octotools\n[34], one step-by-step reasoning model, Claude Sonnet 4, and on online chess games from a public repository. For Aviary and Octotools, we use Claude Haiku 3.5 as the verifier, asking after each tool call for a\ntext-based probability that the trajectory thus far is successful. For the reasoning model, we use a popular\npretrained process-reward model (PRM) [63], which provides a logit-based probability that each step in\na reasoning trace is correct. Finally, for the chess experiments, we use Stockfish as the reward model, an\nopen-sourced verifier that numerically evaluates the strength of White’s position after each step.\n**Datasets.** We experiment on datasets from three different domains: (1) **mathematical reasoning** (GSM8k\n[9] and MATH [17]), (2) **question-answering** (HotpotQA [70], MedQA [24], and MMLU-Pro [64]), and (3)\n**chess games** . For **chess**, we use open-sourced and annotated games from LiChess. We present results from\nall datasets except GSM8k in the main section, and provide the GSM8k results in the Appendix. Results\nfor each dataset are from one corresponding agent-verifier combination. A full description of the dataset,\nagent, and verifier combinations is available in Appendix 8.3.\n**Baselines** We compare against three baselines inspired by sequential hypothesis testing and calibration.\n\n\n1. The **raw** **verifier** uses the scores from the verifier as-is. The verifier provides some prediction of\nPr( _Y_ = 1 _|Ht_ ), the probability that the agent will produce a successful output, given the trajectory _Ht_\nthus far. Given a user-specified false positive rate, _α_, we reject a trajectory if the score _St_ ever drops\nbelow _α_ .\n\n\n2. The **calibrated** **verifier** uses the same verifier but recalibrates the scores _St_ using the calibration set\n_Dcal_ . Specifically, we use isotonic regression to learn a transformation of the score, _S_ _[′]_ = _f_ [ˆ] ( _S_ ), that\nachieves _marginal_ _calibration_ : Pr( _Y_ = 1 _|S_ _[′]_ ) = _S_ _[′]_ . As with the raw verifier, for a user-specified false\npositive rate, _α_, we reject a trajectory if the score _St_ ever drops below _α_ .\n\n\n3. The **Bonferroni test** utilizes the same density ratio test statistic _Mt_ . However, the decision rule differs:\nit uses the Bonferroni correction to reject each individual test at _α/T_, where _α_ is the user-specified false\npositive rate, and _T_ is the maximum trajectory length found in the calibration set.\n\n\nWe compare these baselines to two variations of _e-valuator_ : (1) **e-valuator** **(** 1 _/α_ **threshold)**, in which\nwe choose the threshold according to Proposition 1 with our plug-in estimate for the density ratios, and\n(2) **e-valuator** **(PAC** **threshold)**, in which we choose the threshold using the procedure in Proposition 3.\nIn general, we recommend using the PAC threshold, although in settings where an agent may run an\narbitrarily large number of steps or never terminate, the 1 _/α_ version may be preferred. For the results\npresented in the main section, we use an 80/20 split of test/calibration data: we calibrate our method (and\nall baselines) on 20% of the data and test on the remaining 80%. We compare other splits of the calibration\nset in Appendix 8.2.2 and find that a couple hundred calibration trajectories is sufficient to control the\nfalse alarm rate. We also visualize the _Mt_ sequences for both successful and unsuccessful trajectories in\nAppendix 8.2.\n\n\n**4.1** **E-valuator provides better false alarm rate control than competing methods**\nWe begin by analyzing the empirical false alarm rate achieved by _e-valuator_ and the competing baselines.\nGiven a particular _α_, the false alarm rate, i.e. the rate at which we flag successful (“null”) trajectories as\nunsuccessful (“alternative”), should be no greater than _α_ .\n\n\n8\n0.6\n0.4\n0.2\n0.0\n\n|Raw verifier Calibrated verifier E-valuator (PAC threshold, ours) False alarm rate ><br>Bonferroni test E-valuator (1/ threshold, ours) Exact control<br>HotpotQA MedQA MMLU-Pro<br>0.6 0.6<br>0.4 0.4<br>0.2 0.2|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n\n0.0 0.1 0.2 0.3 0.4 0.5\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 0.1 0.2 0.3 0.4 0.5\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n\n\n(Nominal false alarm rate)\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n\n\n\n0.0\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n\n\n(Nominal false alarm rate)\n0.0 0.1 0.2 0.3 0.4 0.5\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n\n\n(Nominal false alarm rate)\nFigure 2: **E-valuator controls the false alarm rate and maximizes power better than alternative methods** . Violations\nof the false alarm rate control are marked with an X. Both versions of _e-valuator_ empirically control the false alarm rate\n(type I error) for different choices of _α_ across all datasets. As expected, the 1 _/α_ threshold is more conservative than\nthe PAC threshold, although both control the false alarm rate. **E-valuator** also provides better power than competing\nmethods. The calibrated and raw verifiers occasionally provide comparable power, at the cost of inflating the false\nalarm rate. All plots show the 95% CI over 50 random splits of each dataset.\n\n\nBoth the 1 _/α_ threshold (plug-in version of Proposition 1) and PAC threshold (Proposition 3) versions\nof _e-valuator_ empirically control the false alarm rate across all choices of _α_ and all datasets (Figure 2, top).\nThe raw verifier, which thresholds the raw scores _St_ at _α_, sometimes achieves empirical false alarm rates\nless than the desired _α_ (MedQA and MATH; see Appendix 8.2 for the latter) but not always (HotpotQA,\nfor _α >_ 0 _._ 4). The calibrated verifier, which applies isotonic regression to the raw verifier scores _St_ and then\nuses the same threshold _α_ on the recalibrated scores, does not achieve false alarm rates less than _α_ either.\nAlthough calibration procedures such as isotonic regression aim to achieve E( _Y |S_ ) = _S_, this property does\nnot have any direct implications on the false alarm rate. Furthermore, even if this property held at each\ntimestep, it does not allow us to reason about the false alarm rate in the sequential hypothesis setting (see\nAppendix 8.1 for further discussion). In contrast, the Bonferroni test (Figure 2, orange) controls the false\nalarm rate for all _α_, although even more conservatively than _e-valuator_ with a 1 _/α_ threshold.\nWe expect the 1 _/α_ threshold and PAC thresholds to converge as the trajectory length increases, as the\n1 _/α_ threshold is designed to be valid for any density ratio process, whereas the PAC thresholds are calibrated to the observed null distribution. This expectation aligns with what we empirically observe. For\ninstance, the average trajectory length in MedQA, which achieves a false alarm rate of 0.045 at _α_ = 0 _._ 5 with\na threshold of 1 _/α_, is 2.4 steps. In comparison, the average trajectory length in HotpotQA, which achieves\nat false alarm rate of 0.21 at _α_ = 0 _._ 5 with a threshold of 1 _/α_, is 4.7 steps. Similar plots are available in\nAppendix 8.2 for the MATH and GSM8k datasets.\n\n\n**4.2** **E-valuator provides enhanced power over alternate methods**\nNext we analyze the empirical power across the same datasets and tasks (Figure 2, bottom). Power measures the true positive rate at a given _α_ . That is, power is the rate at which unsuccessful trajectories (“alternative”) are indeed flagged as unsuccessful. Across all datasets and all _α_ s, e-valuator with the PAC\nthreshold provides greatest power among methods that achieve empirical false alarm rates less than _α_\n(Figure 2, top, red).\n\n\n9\n55\n50\n45\n40\n35\n30\n25\n20\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\nTotal tokens used (×10 )\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n|||||~~Calibrate~~|~~ verifier~~|~~ verifier~~|\n|||||<br>E-valuato<br>Raw verif<br>|<br>r (empirical, o<br> ier<br>|urs)<br>|\n||||||||\n||||||||\n|||||~~E-valuato~~<br>False alar|~~r (anytime-va~~<br> m rate >|~~ lid, ours)~~|\n\n\nTotal tokens used (×10 )\n60\n55\n50\n45\n40\n35\nFigure 3: **E-valuator** **recovers** **a** **larger** **fraction** **of** **baseline** **accuracy** **with** **fewer** **tokens.** We compare _e-valuator_ to\nthresholding the verifier scores on the MATH and MMLU-Pro dataset. The verifier terminates unsuccessful trajectories\nrather late, leading to greater inefficiencies in recovering accuracy with fewer tokens. _×_ indicates that the empirical\nfalse alarm rate was greater than the desired level\nIn those instances where the calibrated (or raw) verifier provides better power, it is at the cost of inflated\nfalse alarm rate. For instance, in HotpotQA, at _α_ = 0 _._ 5, the calibrated verifier provides a power of 0.84\nbut inflates the false alarm rate to 0.61, well above the desired level, while empirical _e-valuator_ provides a\npower of 0.81 and controls the false alarm rate at 0.48, and _E-valuator_ (1 _/α_ threshold) also provides strong\npower at 0.62 while controlling the false alarm rate at 0.21, well below _α_ = 0 _._ 5.\n\n\n**4.3** **Case study:** **E-valuator recovers larger fraction of original accuracy in limited to-**\n**ken budget**\nWe also apply _e-valuator_ to a case study in which we terminate any trajectory that crosses the threshold\n_cα_ . We then count the number of tokens that are saved (i.e., how many tokens would have otherwise been\ngenerated if the agent weren’t terminated) and compare that to the total accuracy on the dataset. Note that\nusing the full token budget is the “maximal” accuracy achievable, as this would entail never terminating\nany trajectory.\nComparing _e-valuator_ to the verifier in terms tokens saved versus total accuracy (Figure 3) we find shows\nthat _E-valuator_ outperforms both the raw and calibrated verifiers in recovering accuracy with fewer tokens.\nFor instance, on the MATH dataset, _e-valuator_ achieved 50% total accuracy (86% of the original accuracy\nof 58%) using 81% (fewer than 269,755 tokens) of the original 333,283 tokens. By contrast, the raw and\ncalibrated verifiers each require more than 95% of the original token count to recover 86% of the original\naccuracy. Similarly, on the MMLU-Pro dataset, _e-valuator_ achieved 50% total accuracy using just 233,324\ntokens, whereas the raw and calibrated verifiers each require more than 250,000 tokens.\n\n\n**4.4** **Case study:** **E-valuator for chessbots**\nFinally, we show a use case of _e-valuator_ for monitoring non-LLM agents. In this experiment, we analyze\npublicly available chess games on LiChess, using Stockfish as the verifier. Stockfish provides a real-valued\nscore (possibly negative) called _centipawns_, which is positive when White is an advantageous position and\nnegative when Black is an advantageous position. Stockfish additionally publishes a formula to convert\nthese scores into a win probability [30], which we use to construct the verifier’s raw probabilities. We test\nthe null of White winning against the alternative of Black winning or a draw.\nWe compare the raw and calibrated verifier to _e-valuator_ and find that _e-valuator_ is able to better control\nthe false alarm rate than these baselines (Figure 4). Moreover, _e-valuator_ with the 1 _/α_ threshold performs\nsimilarly to the PAC threshold version. This is expected in chess, where games often feature more moves\n(e.g. 50+ moves) than actions in typical agent trajectories. The 1 _/α_ threshold fails to control the false alarm\n10\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|Raw v<br>Bonfe|erifier<br>rroni test<br>Calibrated verifier<br>E-valuator (1/ threshold, our|erifier<br>rroni test<br>Calibrated verifier<br>E-valuator (1/ threshold, our|erifier<br>rroni test<br>Calibrated verifier<br>E-valuator (1/ threshold, our|erifier<br>rroni test<br>Calibrated verifier<br>E-valuator (1/ threshold, our|erifier<br>rroni test<br>Calibrated verifier<br>E-valuator (1/ threshold, our|s)<br>E-valuator (PAC threshold, ours)<br>Exact control<br>False alarm rate >|\n|Raw v<br>Bonfe|||||||\n|Raw v<br>Bonfe|||||||\n|Raw v<br>Bonfe|||||||\n|Raw v<br>Bonfe|||||||\n|Raw v<br>Bonfe|||||||\n|Raw v<br>Bonfe|||||||\n|Raw v<br>Bonfe|||||||\n|Raw v<br>Bonfe|||||||\n\n\n(Nominal false alarm rate)\n\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n\n\n(Nominal false alarm rate)\nFigure 4: **Chess** . E-valuator controls the false alarm rate and increases power for chess verifiers.\n\n\nrate at low _α_, which is attributable to error in density ratio estimation for long games. Nonetheless, the\nverifier models more severely violate the false alarm rate control than _e-valuator_ (1 _/α_ threshold).\n\n### **5 Discussion**\n\n\nIn this paper, we introduced _e-valuator_, a method to improve any step-by-step agent verifier model using\nsequential hypothesis testing. We convert the problem of detecting whether a trajectory is “successful” or\nnot into a hypothesis testing problem, where we distinguish between verifier scores generated from the\n“successful” versus the “unsuccessful” distribution. Although _e-valuator_ requires evaluating density ratios\nat each step in an agent’s trajectory, we empirically find that learned density ratios provide ample power\nand sufficient false alarm controls.\nThere are several promising future directions to this work. First, one can relax certain assumptions to\navoid estimating the _full_ joint density at each time _t_, such as assuming the verifier scores are i.i.d. at each\nstep, or perhaps that the score at any given step depends at most on the scores from the last _k_ steps. If\none assumes the scores are i.i.d., one could use universal inference algorithms [65] to provide exact guarantees even with empirical (and noisy) estimates of the true density ratio. Second, although we explored\na potential test-time scaling application, one could use _e-valuator_ for more nuanced scaling strategies, such\nas resampling or restarting bad trajectories. Some of these strategies may break the _e-valuator_ assumptions,\nand it would be important to ameliorate the method accordingly. Finally, _e-valuator_ can be extended to more\ncomplex agentic systems, such as multi-agent settings.\n\n### **6 Code and Data**\n\n\nCode is released on GitHub at https://github.com/shuvom-s/e-valuator and additionally as a Python\npackage, e-valuator, available in PyPi at https://pypi.org/project/e-valuator/.\n\n### **7 Acknowledgments**\n\n\nWe thank Bonnie Berger, Ian Waudby-Smith, Kexin Huang, Divya Shanmugam, Kyunghyun Cho, Manish\nRaghavan, the Recht Lab, and Chang Ma for helpful feedback and discussions. This work was done while\nS.S. and D.P. were interns at Genentech.\n\n\n11\n\n\n### **References**\n\n\n[1] Anastasios N Angelopoulos, Stephen Bates, Emmanuel J Cand`es, Michael I Jordan, and Lihua Lei.\nLearn then test: Calibrating predictive algorithms to achieve risk control. _The Annals of Applied Statis-_\n_tics_, 19(2):1641–1662, 2025.\n\n\n[2] Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fern´andez, Albert\nGatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. Llms instead of human\njudges? a large scale empirical study across 20 nlp evaluation tasks. _arXiv_ _preprint_ _arXiv:2406.18403_,\n2024.\n\n\n[3] George A Bekey. On autonomous robots. _The Knowledge Engineering Review_, 13(2):143–146, 1998.\n\n\n[4] S Bickel, M Bruckner, and T Scheffer. Discriminative learning under covariate shift. _J._ _Mach._ _Learn._\n_Res._, 10:2137–2155, 2009.\n\n\n[5] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. _Science_, 365(6456):885–\n890, 2019.\n\n\n[6] John Cherian, Isaac Gibbs, and Emmanuel Candes. Large language model validity via enhanced conformal prediction methods. _Advances in Neural Information Processing Systems_, 37:114812–114842, 2024.\n\n\n[7] Ben Chugg, Santiago Cortes-Gomez, Bryan Wilder, and Aaditya Ramdas. Auditing fairness by betting.\n_Advances in Neural Information Processing Systems_, 36:6070–6091, 2023.\n\n\n[8] Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the case\nof the binomial. _Biometrika_, 26(4):404–413, 1934.\n\n\n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. _arXiv preprint arXiv:2110.14168_, 2021.\n\n\n[10] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. _arXiv preprint arXiv:2205.09712_, 2022.\n\n\n[11] Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates\nusing the em algorithm. _Journal of the Royal Statistical Society:_ _Series C (Applied Statistics)_, 28(1):20–28,\n1979.\n\n\n[12] Dean P Foster and Robert A Stine. _α_ -investing: A procedure for sequential control of expected false\ndiscoveries. _J. R. Stat. Soc. Series B Stat. Methodol._, 70(2):429–444, April 2008.\n\n\n[13] Senay A Gebreab, Khaled Salah, Raja Jayaraman, Muhammad Habib ur Rehman, and Samer Ellaham.\nLLM-based framework for administrative task automation in healthcare. In _2024_ _12th_ _International_\n_Symposium on Digital Forensics and Security (ISDFS)_, pages 1–7. IEEE, April 2024.\n\n\n[14] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom\nMyaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. _arXiv_\n_preprint arXiv:2502.18864_, 2025.\n\n\n[15] Peter Gr¨unwald, Rianne de Heide, and Wouter M Koolen. Safe testing. In _2020 Information theory and_\n_applications workshop (ITA)_, pages 1–54. IEEE, 2020.\n\n\n[16] Michael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical\nmodels, with applications to natural image statistics. _J. Mach. Learn. Res._, 13(11):307–361, 2012.\n\n\n[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. _arXiv preprint_\n_arXiv:2103.03874_, 2021.\n\n\n12\n[18] Kexin Huang, Ying Jin, Ryan Li, Michael Y Li, Emmanuel Candes, and Jure Leskovec. Automated\nhypothesis validation with agentic sequential falsifications. _Forty-second_ _International_ _Conference_ _on_\n_Machine Learning_, 2025.\n\n\n[19] Kexin Huang, Serena Zhang, Hanchen Wang, Yuanhao Qu, Yingzhou Lu, Yusuf Roohani, Ryan Li, Lin\nQiu, Gavin Li, Junze Zhang, et al. Biomni: A general-purpose biomedical ai agent. _biorxiv_, 2025.\n\n\n[20] Salim I Amoukou, Tom Bewley, Saumitra Mishra, Freddy Lecue, Daniele Magazzeni, and Manuela\nVeloso. Sequential harmful shift detection without labels. _Advances_ _in_ _Neural_ _Information_ _Processing_\n_Systems_, 37:129279–129302, 2024.\n\n\n[21] Sooyong Jang, Sangdon Park, Insup Lee, and Osbert Bastani. Sequential covariate shift detection using\nclassifier two-sample tests. In _International conference on machine learning_, pages 9845–9880. PMLR, 2022.\n\n\n[22] Adel Javanmard and Andrea Montanari. Online rules for control of false discovery rate and false\ndiscovery exceedance. _Ann. Stat._, 46(2):526–554, 2018.\n\n\n[23] Disi Ji, Padhraic Smyth, and Mark Steyvers. Can i trust my fairness metric? assessing fairness with\nunlabeled data and bayesian inference. _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_, 33:18600–\n18612, 2020.\n\n\n[24] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease\ndoes this patient have? a large-scale open domain question answering dataset from medical exams.\n_Applied Sciences_, 11(14):6421, 2021.\n\n\n[25] Ramesh Johari, Pete Koomen, Leonid Pekelis, and David Walsh. Peeking at a/b tests: Why it matters,\nand what to do about it. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge_\n_Discovery and Data Mining_, pages 1517–1525, 2017.\n\n\n[26] Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae\nLee, Honglak Lee, and Lu Wang. Process reward models that think. _arXiv preprint arXiv:2504.16828_,\n2025.\n\n\n[27] Shi Xuan Leong, Caleb E Griesbach, Rui Zhang, Kourosh Darvish, Yuchi Zhao, Abhijoy Mandal, Yunheng Zou, Han Hao, Varinia Bernales, and Al´an Aspuru-Guzik. Steering towards safe self-driving\nlaboratories. _Nat. Rev. Chem._, 9(10):707–722, October 2025.\n\n\n[28] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita\nBhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. In _Proceedings of the 2025 Conference on Empirical Methods in_\n_Natural Language Processing_, pages 2757–2791, 2025.\n\n\n[29] Wendi Li and Yixuan Li. Process reward model with q-value rankings. _The_ _Thirteenth_ _International_\n_Conference on Learning Representations_, 2025.\n\n\n[30] Lichess.org. Lichess accuracy metric. [https://lichess.org/page/accuracy,](https://lichess.org/page/accuracy) n.d. Accessed:\n2025-11-17.\n\n\n[31] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In _The Twelfth International_\n_Conference on Learning Representations_, 2023.\n\n\n[32] David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. _arXiv_ _preprint_\n_arXiv:1610.06545_, 2016.\n\n\n[33] Gary Lorden. Procedures for reacting to a change in distribution. _The annals of mathematical statistics_,\npages 1897–1908, 1971.\n\n\n[34] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An agentic\nframework with extensible tools for complex reasoning. _arXiv preprint arXiv:2502.11271_, 2025.\n\n\n13\n[35] Xing Han L`u, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Sta´nczak, Peter Shaw, Christopher J Pal, and Siva Reddy. Agentrewardbench: Evaluating automatic evaluations of web agent trajectories. _arXiv preprint arXiv:2504.08942_, 2025.\n\n\n[36] Christopher Mohri and Tatsunori Hashimoto. Language models with conformal factuality guarantees.\n_arXiv preprint arXiv:2402.10978_, 2024.\n\n\n[37] Siddharth Narayanan, James D Braza, Ryan-Rhys Griffiths, Manu Ponnapati, Albert Bou, Jon Laurent,\nOri Kabeli, Geemi Wellawatte, Sam Cox, Samuel G Rodriques, et al. Aviary: training language agents\non challenging scientific tasks. _arXiv preprint arXiv:2412.21154_, 2024.\n\n\n[38] Jerzy Neyman and Egon Sharpe Pearson. Ix. on the problem of the most efficient tests of statistical\nhypotheses. _Philosophical_ _Transactions_ _of_ _the_ _Royal_ _Society_ _of_ _London._ _Series_ _A,_ _Containing_ _Papers_ _of_ _a_\n_Mathematical or Physical Character_, 231(694-706):289–337, 1933.\n\n\n[39] Young-Jin Park, Kristjan Greenewald, Kaveh Alim, Hao Wang, and Navid Azizan. Know what you\ndon’t know: Uncertainty calibration of process reward models. _arXiv preprint arXiv:2506.09338_, 2025.\n\n\n[40] Aleksandr Podkopaev and Aaditya Ramdas. Tracking the risk of a deployed model and detecting\nharmful distribution shifts. _arXiv preprint arXiv:2110.06177_, 2021.\n\n\n[41] Drew Prinster, Xing Han, Anqi Liu, and Suchi Saria. Watch: Adaptive monitoring for ai deployments\nvia weighted-conformal martingales. In _Forty-second International Conference on Machine Learning_, 2025.\n\n\n[42] Yuanhao Qu, Kaixuan Huang, Ming Yin, Kanghong Zhan, Dyllan Liu, Di Yin, Henry C Cousins,\nWilliam A Johnson, Xiaotong Wang, Mihir Shah, Russ B Altman, Denny Zhou, Mengdi Wang, and\nLe Cong. CRISPR-GPT for agentic automation of gene-editing experiments. _Nat. Biomed. Eng._, pages\n1–14, July 2025.\n\n\n[43] Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S Jaakkola, and Regina\nBarzilay. Conformal language modeling. _arXiv preprint arXiv:2306.10193_, 2023.\n\n\n[44] Aaditya Ramdas and Ruodu Wang. Hypothesis testing with e-values. _arXiv preprint arXiv:2410.23614_,\n2024.\n\n\n[45] Aaditya Ramdas, Tijana Zrnic, Martin Wainwright, and Michael Jordan. SAFFRON: an adaptive algorithm for online control of the false discovery rate. In _International Conference on Machine Learning_,\npages 4286–4294. PMLR, July 2018.\n\n\n[46] Aaditya Ramdas, Peter Gr¨unwald, Vladimir Vovk, and Glenn Shafer. Game-theoretic statistics and\nsafe anytime-valid inference. _Statistical Science_, 38(4):576–601, 2023.\n\n\n[47] Mona Schirmer, Metod Jazbec, Christian A Naesseth, and Eric Nalisnick. Monitoring risks in test-time\nadaptation. _arXiv preprint arXiv:2507.08721_, 2025.\n\n\n[48] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. _Journal_ _of_ _Machine_ _Learning_\n_Research_, 9(3), 2008.\n\n\n[49] Divya Shanmugam, Shuvom Sadhuka, Manish Raghavan, John Guttag, Bonnie Berger, and Emma\nPierson. Evaluating multiple models using labeled and unlabeled data. _arXiv preprint arXiv:2501.11866_,\n2025.\n\n\n[50] Jaehyeok Shin, Aaditya Ramdas, and Alessandro Rinaldo. E-detectors: A nonparametric framework\nfor sequential change detection. _The New England Journal of Statistics in Data Science_, 2023.\n\n\n[51] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without\nhuman knowledge. _nature_, 550(7676):354–359, 2017.\n\n\n14\n[52] Kyle Swanson, Wesley Wu, Nash L Bulaong, John E Pak, and James Zou. The virtual lab of ai agents\ndesigns new sars-cov-2 nanobodies. _Nature_, pages 1–3, 2025.\n\n\n[53] Jinjin Tian and Aaditya Ramdas. ADDIS: an adaptive discarding algorithm for online FDR control\nwith conservative nulls. _Advances in Neural Information Processing Systems_, 32, 2019.\n\n\n[54] Alexander Timans, Rajeev Verma, Eric Nalisnick, and Christian A Naesseth. On continuous monitoring of risk violations under unknown shift. _arXiv preprint arXiv:2506.16416_, 2025.\n\n\n[55] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcomebased feedback. _arXiv preprint arXiv:2211.14275_, 2022.\n\n\n[56] Jean Ville. Jean ville, ´etude critique de la notion de collectif. 1939.\n\n\n[57] Vladimir Vovk and Ruodu Wang. E-values: Calibration, combination and applications. _The Annals of_\n_Statistics_, 49(3):1736–1754, 2021.\n\n\n[58] Vladimir Vovk and Ruodu Wang. Confidence and discoveries with e-values. _Statistical Science_, 38(2):\n329–354, 2023.\n\n\n[59] Vladimir Vovk, Ivan Petej, Ilia Nouretdinov, Ernst Ahlberg, Lars Carlsson, and Alex Gammerman.\nRetrain or not retrain: Conformal test martingales for change-point detection. In _Conformal and Proba-_\n_bilistic Prediction and Applications_, pages 191–210. PMLR, 2021.\n\n\n[60] Abraham Wald and Jacob Wolfowitz. Optimum character of the sequential probability ratio test. _The_\n_Annals of Mathematical Statistics_, pages 326–339, 1948.\n\n\n[61] Hanchen Wang, Yichun He, Paula P Coelho, Matthew Bucci, Abbas Nazir, Bob Chen, Linh Trinh,\nSerena Zhang, Kexin Huang, Vineethkrishna Chandrasekar, et al. Spatialagent: An autonomous ai\nagent for spatial biology. _bioRxiv_, pages 2025–04, 2025.\n\n\n[62] Hongjian Wang and Aaditya Ramdas. Anytime-valid t-tests and confidence sequences for gaussian\nmeans with unknown variance. _Sequential Analysis_, 44(1):56–110, 2025.\n\n\n[63] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang\nSui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. _Proceedings_\n_of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:_ _Long Papers)_, pages\n9426–9439, 2024.\n\n\n[64] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multitask language understanding benchmark. _Advances in Neural Information Processing Systems_, 37:95266–\n95290, 2024.\n\n\n[65] Larry Wasserman, Aaditya Ramdas, and Sivaraman Balakrishnan. Universal inference. _Proceedings of_\n_the National Academy of Sciences_, 117(29):16880–16890, 2020.\n\n\n[66] Ian Waudby-Smith, Edward H Kennedy, and Aaditya Ramdas. Distribution-uniform anytime-valid\nsequential inference. _arXiv preprint arXiv:2311.03343_, 2023.\n\n\n[67] Ian Waudby-Smith, Lili Wu, Aaditya Ramdas, Nikos Karampatziakis, and Paul Mineiro. Anytimevalid off-policy inference for contextual bandits. _ACM/IMS Journal of Data Science_, 1(3):1–42, 2024.\n\n\n[68] Peter Welinder, Max Welling, and Pietro Perona. A lazy man’s approach to benchmarking: semisupervised classifier evaluation and recalibration. In _Proceedings_ _of_ _the_ _IEEE_ _Conference_ _on_ _Computer_ _Vision_\n_and Pattern Recognition_, pages 3262–3269, 2013.\n\n\n[69] Menghua Wu, Cai Zhou, Stephen Bates, and Tommi Jaakkola. Thought calibration: Efficient and\nconfident test-time scaling. _arXiv preprint arXiv:2505.18404_, 2025.\n\n\n15\n[70] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering.\n_arXiv preprint arXiv:1809.09600_, 2018.\n\n\n[71] Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek Wong, Emine Yilmaz, Shuming Shi,\nand Zhaopeng Tu. Benchmarking llms via uncertainty quantification. _Advances in Neural Information_\n_Processing Systems_, 37:15356–15385, 2024.\n\n\n[72] Weiqiu You, Anton Xue, Shreya Havaldar, Delip Rao, Helen Jin, Chris Callison-Burch, and Eric Wong.\nProbabilistic soundness guarantees in llm reasoning chains. In _Proceedings_ _of_ _the_ _2025_ _Conference_ _on_\n_Empirical Methods in Natural Language Processing_, pages 7517–7536, 2025.\n\n\n[73] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren\nZhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. _arXiv_\n_preprint arXiv:2412.06559_, 2024.\n\n### **8 Appendix**\n\n\n**8.1** **Theory**\n**8.1.1** **Proof of Proposition 1**\nLet _p_ 0 and _p_ 1 be the densities of the unsuccessful and successful trajectories’ verifier scores, respectively.\nAssume _p_ 0 _≪_ _p_ 1, and let _Mt_ = _pp_ 01(( **SS** [:[: _tt_ ]])) [and] _[M]_ [0] [=] [1][.] [Set] _[c][α]_ [=] _α_ 1 [for] [a] [user-specified] _[α]_ _[∈]_ [(0] _[,]_ [ 1)][.] [Then,]\nPr _HN_ ( _∃t ∈_ N : _Mt_ _≥_ _cα_ ) _≤_ _α_ . In other words, the probability that the density ratio process _Mt_ ever crosses\n_cα_ = _α_ [1] [is at most] _[ α]_ [.]\n_Proof._ Several prior works prove that the density ratio process is a test martingale and therefore an e-process\nfor _HN_ . See Lemma 2.6 of [62] for a complete proof of this fact. We provide an abridged version here, and\nthen apply Ville’s inequality [56] to achieve anytime control of the false alarm rate (Eq. 1).\nLet _Ft_ = _σ_ ( **S** [1: _t_ ]) be the natural filtration. We show that ( _Mt_ ) _t∈_ N satisfies the definition of a test martingale for _HN_ : **S** _∼_ _P_ 1. First, note that density ratios are always non-negative, so _Mt_ is always non-negative.\nAlso, since by construction _M_ 0 = 1, we have E _HN_ [ _M_ 0] = 1. We now show that ( _Mt_ ) _t∈_ N is a martingale\nunder _HN_, that is, when **S** _∼_ _P_ 1. We have\nE _HN_ [ _Mt_ _| Ft−_ 1] = E _HN_\n\n\n\n\n- _p_ 0( **S** [1: _t_ ])\n_p_ 1( **S** [1: _t_ ])\n_Ft−_ 1\n���\n= _[p]_ [0][(] **[S]** [[1:] _[t][−]_ [1]][)]\n_p_ 1( **S** [1: _t−_ 1])\n\n\n\n\n- _p_ 0( _st_ _|_ **S** [1: _t−_ 1])\n_[|]_ **[S]** [[1:] _[t][−]_ [1]][)] _[ ds][t]_\n_p_ 1( _st_ _|_ **S** [1: _t−_ 1]) _[p]_ [1][(] _[s][t]_\n= _Mt−_ 1\n= _Mt−_ 1\n_p_ 0( _st_ _|_ **S** [1: _t−_ 1]) _dst_\nwhere the third equality holds because the integral of a density is 1.\nThus, the process ( _Mt_ ) _t∈_ N is a test martingale for the null hypothesis. Ville’s inequality [56] then states\nthat, for any _α ∈_ (0 _,_ 1):\n\n\n\n\n_≤_ _α_ E _HN_ [ _M_ 0] _._\n\n\n\nPr _HN_\nsup _Mt_ _≥_ [E] _[H][N]_ [ [] _[M]_ [0][]]\n_t∈_ N _α_\nPlugging in E _HN_ [ _M_ 0] = 1, we get Pr _HN_ �sup _t∈_ N _Mt_ _≥_ _α_ [1] - _≤_ _α_ . Therefore, for _cα_ = _α_ 1 [,] [we] [have] [that]\nPr _HN_ [ _∃t ∈_ N : _Mt_ _≥_ _cα_ ] = Pr _HN_ [sup _t∈_ N _Mt_ _≥_ _cα_ ] = _≤_ _α_ .\n\n\n16\n**8.1.2** **Proof of Proposition 2**\nThe density ratio process given by _Mt_ = _pp_ 01(( **SS** [1:[1: _tt_ ]])) [provides log-optimal growth rate.] [That is, for any other]\ne-process ( _Mt_ _[′]_ [)] _[∞]_ _t_ =1 [and stopping time] _[ τ]_ [,] _[ E][H]_ _A_ [[log] _[ M][τ]_ []] _[ ≥]_ _[E][H]_ _A_ [[log] _[ M][ ′]_ _τ_ []][.]\n_Proof._ A full proof of this theorem appears as Theorem 7.11 in [44]. We refer the reader to that textbook for\na rigorous treatment of the proof. Nonetheless, we provide the key intuition here.\nThis result is an extension of the fact that in a non-sequential hypothesis test of _P_ 1 against _P_ 0, where\n_P_ 0 _≪_ _P_ 1, the likelihood ratio, _E_ = _p_ 0 _/p_ 1, is the log-optimal e-variable: E _P_ 0[log _E_ _[′]_ ] _≤_ E _P_ 0[log _E_ ] for any other\ne-variable _E_ _[′]_ for _P_ 1. To see this, first note that it suffices to consider e-variables of the form _E_ _[′]_ = _dQ/dP_ 1\nfor distributions _Q_ such that _Q ≪_ _P_ 0 _≪_ _P_ 1. We have that\n\n\n\n\n- - - _q_ ( _x_ ) _/p_ 1( _x_ )\n= _p_ 0( _x_ ) log\n_p_ 0( _x_ ) _/p_ 1( _x_ )\n\n\n\n\n- - - _p_ 0( _x_ )\n_L_ ( _dx_ ) = _−_ _p_ 0( _x_ ) log\n_q_ ( _x_ )\n_L_ ( _dx_ ) _≤_ 0 _,_ (5)\nE _HA_\nlog _[E]_\n_E_ _[′]_\nwhere _L_ is the reference measure that the densities _p_ 1 _, p_ 0 _, q_ are defined with respect to. That is, E _HA_ [log _E_ _[′]_ ] _≤_\nE _HA_ [log _E_ ].\nFrom here, one can extend this statement into the sequential setting of e-processes, which is done in\nTheorem 7.11 of the reference.\n\n\n**Algorithm 3** Probably-approximately-correct (PAC) threshold for e-valuator.\n**Inputs:** error level, _δ_ _∈_ [0 _,_ 1]; quantile level, _α ∈_ [0 _,_ 1]; calibration data, _D_ threshold; functions that estimate the\ndensity ratio at each step, _z, t_ = 1 _, . . ., T_ max.\n**Output:** decision threshold, _cα_, that yields e-valuator with anytime-validity with high-probability.\n\n1: _i ←_ 0\n2: **for** ( **S** _, Y_ ) _∈D_ threshold : _Y_ = 1 **do**\n3: _i ←_ _i_ + 1\n4: _M_ 1 _, ..., MT_ _←_ _M_ [ˆ] 1( **S** 1) _, ...,_ _M_ [ˆ] _T_ ( **S** [1: _T_ ])\n5: _M_ [(] _[i]_ [)] _←_ max _t Mt_\n6: **end for**\n7: _n ←_ _i_\n8: Sort _M_ [(1)] _, ..., M_ [(] _[n]_ [)] in ascending order, such that _M_ (1) _≤· · · ≤_ _M_ ( _n_ ). Break ties by flipping a fair coin.\n9: _k_ _←_ min _{i ∈_ [ _n_ ] : Pr[Bin( _n,_ 1 _−_ _α_ ) _≥_ _i_ ] _≤_ _δ}_\n10: _cα_ _←_ _M_ ( _k_ )\n**8.1.3** **Proof of Proposition 3**\nLet _cα_ be chosen according to Algorithm 3. Then,\nPr _D_ cal (Pr _HN_ ( _∃t_ : _Mt_ _≥_ _cα|D_ cal) _≤_ _α_ ) _≥_ 1 _−_ _δ._\n\n\n_Proof._ Suppose our calibration set contains _n_ successful trajectories that are i.i.d. according to the null.\nNote that Pr _HN_ ( _∃t_ : _Mt_ _≥_ _cα|D_ cal) = Pr _HN_ (max _t∈_ N _Mt_ _≤_ _cα|D_ cal), since the event that the entire sequence\n( _Mt_ ) _[∞]_ _t_ =0 [is below] _[ c][α]_ [is equivalent to the event that the maximum is below] _[ c][α]_ [.] [Thus, it suffices to consider]\nthe maximum score over all steps.\nCompute the calibration set maxima _M_ [(1)] _, . . ., M_ [(] _[n]_ [)] . Note that these maxima, _M_ [(] _[i]_ [)] _, i_ = 1 _, . . ., n_ are i.i.d.\nfrom some distribution with (unknown) CDF, _F_ . Define the (1 _−_ _α_ )-quantile of this distribution as\n_q_ 1 _−α_ := inf _{x ∈_ R : _F_ ( _x_ ) _≥_ 1 _−_ _α},_\nsuch that _F_ ( _q_ 1 _−α−_ ) _≤_ 1 _−_ _α ≤_ _F_ ( _q_ 1 _−α_ ), where _F_ ( _q−_ ) := lim _x↑q F_ ( _x_ ). Our goal is to use the calibration data\nto construct a (1 _−_ _δ_ )-confidence upper bound on _q_ 1 _−α_ . That is, we will find _cα_ such that\nPr _D_ cal( _cα_ _< q_ 1 _−α_ ) _≤_ _δ._ (6)\n17\nThen, on the event _{cα_ _≥_ _q_ 1 _−α}_, we have that\n_≤_ Pr _HN_\nmax _t∈_ N _[M][t]_ _[≥]_ _[q]_ [1] _[−][α]_ �� _D_ cal\n= 1 _−_ _F_ ( _q_ 1 _−α−_ ) _≤_ 1 _−_ (1 _−_ _α_ ) = _α._ (7)\nPr _HN_\nmax _[≥]_ _[c][α]_ _[| D]_ [cal]\n_t∈_ N _[M][t]_\nSince by construction, the event _{cα_ _≥_ _q_ 1 _−α}_ occurs with probability at least 1 _−_ _δ_, we have\nmax _t∈_ N _[M][t]_ _[≥]_ _[c][α]_ �� _D_ cal\n\n\n\n\n- _≤_ _α_ _≥_ 1 _−_ _δ_ (8)\nPr _D_ cal (Pr _HN_ ( _∃t_ : _Mt_ _≥_ _cα|D_ cal) _≤_ _α_ ) = Pr _D_ cal\nPr _HN_\nas desired.\nWe construct a (1 _−_ _δ_ )-confidence upper bound on _q_ 1 _−α_ using the following argument, which follows\nideas from Clopper and Pearson [8]. Let _M_ (1) _≤_ _M_ (2) _≤· · ·_ _≤_ _M_ ( _n_ ) denote the order statistics of the\ncalibration score maxima, where ties occupy successive ranks. We will set _cα_ to be one of these order\nstatistics, as follows. Denote _K_ ( _x_ ) := # _{i_ : _M_ [(] _[i]_ [)] _<_ _x}_ . For any _x_ _∈_ R, _K_ ( _x_ ) _∼_ Binomial( _n, F_ ( _x−_ )). Then\nfor any _k_ and _x_, the events _{M_ ( _k_ ) _< x}_ and _{K_ ( _x_ ) _≥_ _k}_ are equivalent, so\nPr _D_ cal( _M_ ( _k_ ) _< x_ ) = Pr _D_ cal ( _K_ ( _x_ ) _≥_ _k_ ) = Pr(Binomial( _n, F_ ( _x−_ )) _≥_ _k_ ) _._ (9)\nIn particular, for any _k_, at the quantile, _q_ 1 _−α_ we have\nPr _D_ cal ( _M_ ( _k_ ) _< q_ 1 _−α_ ) = Pr(Binomial( _n, F_ ( _q_ 1 _−α−_ )) _≥_ _k_ ) _≤_ Pr(Binomial( _n,_ 1 _−_ _α_ ) _≥_ _k_ ) _,_ (10)\nwhere the inequality holds because _F_ ( _q_ 1 _−α−_ ) _≤_ 1 _−_ _α_ and Pr(Binomial( _n, q_ ) _≥_ _k_ is nondecreasing in _q_ .\nTherefore, Algorithm 3 chooses _k_ _[⋆]_ = min _{k_ : Pr(Binomial( _n,_ 1 _−_ _α_ ) _≥_ _k_ ) _≤_ _δ}_ and sets _cα_ = _M_ ( _k⋆_ ). This is\nthe smallest order statistic such that Eq. (6) holds. That is, by construction, _cα_ satisfies\nPr _D_ cal( _cα_ _< q_ 1 _−α_ ) = Pr _D_ cal( _M_ ( _k⋆_ ) _< q_ 1 _−α_ ) _≤_ Pr(Binomial( _n,_ 1 _−_ _α_ ) _≥_ _k_ _[⋆]_ ) _≤_ _δ,_ (11)\nas desired.\n\n\n**8.1.4** **Toy example:** **Marginal calibration does not control false alarm rate**\nA verifier score, _S_, satisfies _marginal calibration_ if\n_p_ ( _Y_ = 1 _| S_ ) = _S_ (12)\nalmost surely. Although a commonly used notion of probabilistic “correctness”, marginal calibration does\nnot enable control of the false alarm rate, as it does not account for the base rate of the null and alternative\n(i.e., _p_ ( _Y_ = 1) and _p_ ( _Y_ = 0)). This is the case in the sequential hypothesis setting, where marginal calibration of _St_ at each step _t_ does not enable anytime control of the false alarm rate, but it is also the case in the\nsimple non-sequential setting, as illustrated by the following example.\nSuppose we have a verifier score, _S_ _∈_ [0 _,_ 1], that only takes on two values: _S_ _∈{_ 0 _._ 005 _,_ 0 _._ 5 _}_, with\n_p_ ( _S_ = 0 _._ 005) = 0 _._ 99 and _p_ ( _S_ = 0 _._ 5) = 0 _._ 01. The verifier score is marginally calibrated, so _p_ ( _Y_ = 1 _|_ _s_ =\n0 _._ 005) = 0 _._ 005 and _p_ ( _Y_ = 1 _|_ _s_ = 0 _._ 5) = 0 _._ 5. As a naive attempt to control the false alarm rate, such that\n_p_ (reject _|_ _Y_ = 1) _≤_ _α_ = 0 _._ 01, we decide to reject whenever _S_ _≤_ _α_ = 0 _._ 01. We now calculate the\nresulting false alarm rate.\nFirst, the base rate of the null is\n_p_ ( _Y_ = 1) = _p_ ( _Y_ = 1 _| S_ = 0 _._ 005) _· p_ ( _S_ = 0 _._ 005) + _p_ ( _Y_ = 1 _| S_ = 0 _._ 5) _· p_ ( _S_ = 0 _._ 5)\n= 0 _._ 005 _·_ 0 _._ 99 + 0 _._ 5 _·_ 0 _._ 01\n= 0 _._ 00995 _._\n\n\n18\nThe false alarm rate is _p_ (reject _|_ _Y_ = 1), which is equivalent to _p_ ( _S_ = 0 _._ 005 _|_ _Y_ = 1) since we reject for\n_S_ _≤_ 0 _._ 01. However,\n_p_ (reject _| Y_ = 1) = _p_ ( _S_ = 0 _._ 005 _| Y_ = 1)\n[= 1] _[ |][ S]_ [= 0] _[.]_ [005)] _[ p]_ [(] _[s]_ [ = 0] _[.]_ [005)]\n= _[p]_ [(] _[Y]_\n_p_ ( _Y_ = 1)\n= [0] _[.]_ [005] _[ ·]_ [ 0] _[.]_ [99]\n0 _._ 00995\n_≈_ 0 _._ 50 _≫_ 0 _._ 01 _._\n\n\nThus, even with a marginally calibrated verifier score, _S_, rejecting the null when _p_ ( _Y_ = 1 _| S_ ) _≤_ _α_ does not\ncontrol the false alarm rate.\n\n\n19\nRaw verifier\nBonferroni test\nCalibrated verifier\nE-valuator (1/ threshold, ours)\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\n1.0\n0.8\n0.6\n0.4\n0.2\n\n\n|Col1|Col2|GSM8K|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n\n\n|-valuator (PAC xact control|C threshold, o|MATH ours)|False alarm|m rate >|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n\n\n\n0.0\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n\n\n(Nominal false alarm rate)\n0.0\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n\n\n(Nominal false alarm rate)\nFigure 5: **GSM8k** **and** **MATH** **results** . The false alarm rate is empirically controlled for both variants of _e-valuator_ .\nAdditionally, _e-valuator_ achieves optimal power among methods that are able to control the false alarm rate.\n\n\n**8.2** **Additional results**\n**8.2.1** **MATH and GSM8k results**\nWe provide results from two additional datasets, MATH [17] and GSM8k [9] in Figure 5. _E-valuator_ empirically controls the false alarm rate for all choices of _α_ and achieves optimal power among methods that\nachieve false alarm rate control.\n\n\n20\n0.5\n0.4\n0.3\n0.2\n0.1\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing.pdf-20-0.png)\n\n\n\n\n\n\n\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n\n\n\n\n\n\n|Calibration<br>1% (n=<br>5% (n=|size<br>50)<br>250)|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|~~10% (n~~<br>20% (n<br>40% (n|~~ 500)~~<br> =1000)<br> =2000)|~~ 500)~~<br> =1000)<br> =2000)|||||\n||||||||\n||||||||\n||||||||\n||||||Metho<br>E-valuator (1<br>E-valuator (P|d<br> /  threshold)<br> AC threshold)|\n\n\n\n0.0 0.1 0.2 0.3 0.4 0.5\n(nominal false alarm rate)\n0.0\n0.0 0.1 0.2 0.3 0.4 0.5\n(nominal false alarm rate)\nFigure 6: **Calibration set size** . On the MATH dataset, the false alarm rate is empirically controlled for most calibration\nset sizes. At very small sizes (1% of the data, or 50 labeled trajectories), _e-valuator_ can sometimes fail to control the false\nalarm rate.\n\n\n**8.2.2** **Ablations of calibration set size**\nWe examine the effect of the size of the calibration set on _e-valuator_ . Recall that the calibration set, _D_ cal is\nused to learn the density ratios _M_ [ˆ] _t_ _≈_ _pp_ 01(( **SS** [1:[1: _tt_ ]])) [.] [For] [the] [empirical] [version] [of] _[e-valuator]_ [,] [we] [split] _[D]_ [cal] [into]\n_D_ DRE and _D_ threshold, learning the density ratios on the former split, and estimating the rejection threshold\non the latter. Because our density ratios are learned, we expect these to ratios to be more accurate as the\ncalibration set size increases.\nWe run this ablation on the MATH dataset, which has 5000 total trajectories. As shown in Figure 6,\nthe size of the calibration set has little effect on the empirical false alarm rates and power. However, at\nvery small amounts of calibration data (1%, or 50 labeled trajectories), the density ratios tend to be noisier,\nleading to greater variance in the false alarm rate and power.\nWe observe that the false alarm rates remain roughly constant as the calibration set size increases (and\nall sizes control the false alarm rate).\n\n\n21\n\n\n# MATH trajectories Chess trajectories\n\nFigure 7: **Example sequences** . On the MATH dataset, many of the rejections are generated at _M_ 1, after the first action,\nindicating the first action of the agent is important in deciding success. On the chess dataset, there is less separation\nbetween _HN_ and _HA_ after the first step but gradually the separation increases.\n\n\n**8.2.3** **Example** _Mt_ **sequences**\nWe additionally show some example _Mt_ sequences (plotted as log( _Mt_ ) for visual clarity) in Figure 7 for the\nMATH and chess datasets. We observe that for both datasets, log( _Mt_ ) inflates for unsuccessful trajectories\nor games where White does not win _HA_ . By contrast, few _Mt_ sequences cross the threshold 1 _/α_ among successful trajectories or games where White wins. Nonetheless, there is visual heterogeneity in the sequences.\nIn general, we expect more powerful verifiers/PRMs to provide stronger visual separation of _HN_ and _HA_\ntrajectories.\n\n\n22\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing.pdf-21-0.png)\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing.pdf-21-1.png)\n**8.3** **Details on datasets, agents, and verifiers**\nWe provide experiments from six different datasets. For each dataset, we use a particular agent-verifier\ncombination, which we list in Table 8.3.\nFor **mathematical reasoning**, we use GSM8k [9] for our tool-calling agent experiments and MATH [17]\nfor our reasoning model experiments. For **question-answering**, we use HotpotQA [70] and MedQA [24]\nfor Aviary and OctoTools experiments, respectively, and MMLU-Pro [64] for the reasoning models. For\n**chess**, we use open-sourced and annotated games from LiChess. We present results from all datasets except\nGSM8k in the main section, and provide the GSM8k results in the Appendix.\nFor tool-calling agents, we provide the verifier (a judge LLM) with the original problem text and list of\ntool calls and arguments used. We then prompt the Claude agent with the following system prompt:\n\n\nYou are an expert in analyzing agent trajectories and estimating the probability of success. Your\nfinal answer should be of the form: [PROBABILITY]: [number between 0 and 1]\nThe probability value should be a number between 0 and 1. LIMIT YOUR RESPONSE TO JUST\n[PROBABILITY]: [number between 0 and 1], or else I will switch to OpenAI.\n\n\nThe agent in question is a LLM-based agent that uses tools to solve problems. The agent may\nnot make more than _{_ max ~~t~~ ool ~~c~~ alls _}_ total tool calls. If it does, it will be terminated with an\nerror.\n\n\nThe tools this agent can use are:\n\n\n   - submit ~~a~~ nswer\n\n\n   - search\n\n\n   - lookup\nYou will be given a partial trajectory of the agent’s actions. Your task is to estimate the probability of success of the agent given the partial trajectory.\n\n\nYour probability should incorporate the following:\n\n\n   - The tools that the agent has used\n\n\n   - The arguments that the agent has used, including the syntax of the arguments\n\n\n   - The problem text\n\n\n   - The number of total tool calls allowed\nHere is the final answer format: [PROBABILITY]: [number between 0 and 1]\n_{_ partial ~~t~~ rajectory _}_\nFor the reasoning model, we simply provide the pretrained verifier the reasoning trace and it outputs\na logits-based probability that the trajectory is successful after each step. For our chess verifier, we upload\nthe game transcript to Stockfish and use its centipawn score [30] as input to _e-valuator_ . To convert these\nscores into a win probability for White, we use the following formula, also published in [30]:\n\n\n\n\n_,_ (13)\n_p_ (White wins _|si_ ) = [1]\n2\n\n\n\n\n- 2\n1 + _e_ _[−]_ [0] _[.]_ [00368208] _[ s][i]_\nwhere _si_ is the Stockfish centipawn score after the _i_ th move.\n\n\n**8.3.1** **Additional computational details**\nWe use the default hyperparameter settings in scikit-learn with logistic regression for all experiments\npresented in this paper. Given a set of verifier scores, all experiments in this paper can be completed in\nunder a minute on a standard laptop.\n\n\n23\n**Dataset** **Domain** **Agent** **Verifer** **Agent Description**\nGSM8k [9] Math reasoning Aviary [37] Claude Haiku Tool-calling agent for\n3.5 math QA. Text-based\nverifier model.\nMATH [17] Math reasoning Claude Sonnet 4 Pretrained Multi-step reasoning\nPRM [63] model, with pretrained\nverifier model.\nHotpotQA [70] QA Aviary Claude Haiku Tool-calling agent for\n3.5 general QA. Text-based\nverifier model.\nMedQA [24] QA OctoTools [34] Claude Haiku Tool-calling agent for\n3.5 medical QA. Text-based\nverifier model.\nMMLU-Pro [64] QA Claude Sonnet 4 Pretrained Multi-step reasoning\nPRM [63] agent, with pretrained\nverifier model.\nLiChess games Chess Human players Stockfish Human played online\ngames with Stockfish\ncentipawn scores.\n\n\nTable 1: List of datasets, agents, and verifiers used in our experiments.\n\n\n24\n\n\n",
    "kor_md": "#### E-평가자: 순차적 가설 검정을 통한 신뢰할 수 있는 에이전트 검증기\n\nShuvom Sadhuka [1,2], Drew Prinster [1,3], Clara Fannjiang [1], Gabriele Scalia [1], Aviv Regev [1], Hanchen Wang [1,4]\n1 Genentech 2 MIT 3 존스 홉킨스 4 스탠퍼드\n2025년 12월 4일\n**초록**\n_행위자형_ AI 시스템은 사용자 프롬프트에 반응하여 추론 단계나 도구 호출과 같은 일련의 행동을 실행합니다. 이러한 궤적의\n성공 여부를 평가하기 위해 연구자들은 LLM 판정자나 프로세스-보상 모델과 같은 검증기를 개발하여 에이전트 궤적 내 각 행\n동의 품질을 점수화합니다. 이러한 휴리스틱 점수는 유용한 정보를 제공할 수 있지만, 에이전트가 성공적인 출력을 생성할지\n여부를 결정하는 데 사용될 때 정확성을 보장하지는 않습니다. 여기서 우리는 블랙박스 검증기 점수를 거짓 경보율을 증명 가\n능하게 제어하는 결정 규칙으로 변환하는 방법인 _e-평가기를_ 소개합니다. 우리는 성공적인 궤적(즉, 사용자의 프롬프트에 대\n한 올바른 응답으로 이어질 행동 시퀀스)과 실패한 궤적을 구별하는 문제를 순차적 가설 검정 문제로 정의합니다. _e-평가기는_\ne-프로세스의 도구를 기반으로 에이전트 궤적의 모든 단계에서 통계적 유효성을 유지하는 순차적 가설 검정법을 개발하여, 임\n의로 긴 행동 시퀀스에서 에이전트를 온라인으로 모니터링할 수 있게 합니다. 경험적으로, 우리는 6개 데이터셋과 3개 에이전\n트에 걸쳐 _e-평가기가_ 다른 전략보다 더 큰 통계적 검정력과 더 우수한 오경보율 제어를 제공함을 입증합니다. 또한 _e-평가기를_\n활용하면 문제적 궤적을 신속히 종료하고 토큰을 절약할 수 있음을 추가로 입증합니다. 종합적으로 _e-평가기는_ 검증자 휴리스틱을 통계\n적 보증을 갖춘 결정 규칙으로 변환하는 경량화되고 모델에 무관한 프레임워크를 제공하여, 보다 신뢰할 수 있는 에이전트 시스템\n의 배포를 가능하게 합니다.\n\n### **1 서론**\n\n\n_에이전트는_ 일련의 행동을 실행하여 자율적으로 작업을 수행하는 블랙박스 시스템으로, 이를 통칭하여 _궤적_ ( _trajectory)_ 이라 부릅니다.\n\n이러한 궤적에는 도구 호출을 통한 외부 환경과의 상호작용, 코드 작성, 논리적 추론 단계 등 다양한 행동이 포함될 수 있습니다. 본 논문에\n서는 일반적으로 사용자 요청에 응답하는 대규모 언어 모델(LLM) 기반 에이전트를 지칭하지만, _에이전트_ 패러다임은 더 광범위하\n게 적용 가능합니다. 예를 들어, 일련의 기계적 동작을 통해 물리적 작업(예: 컵 집기)을 수행하는 로봇 에이전트[3], 게임 규칙에 부\n합하는 일련의 동작을 통해 게임(예: 포커 또는 체스)을 플레이하는 게임 에이전트[5, 51] 등이 포함됩니다. 에이전트는 폭넓게 응용\n될 수 있으며, 신약 개발[14, 52], 세포 생물학 및 유전체학[61, 19], 가설 검증[18] 등 다양한 분야에서 유망한 초기 결과를 보여주고\n있습니다.\n\n그럼에도 불구하고 에이전트는 실수를 저지르며, 이를 탐지할 수 있는 능력이 중요하다. 이를 위해 _검증기_ 모델이 개발되어 에이\n전트의 궤적 내 각 행동에 수치 점수를 부여한다. 이러한 점수는 일반적으로 궤적이 올바른 최종 출력을 성공적으로 생성할 확률의\n대리 지표로 사용된다. 검증기 예시로는 각 단계 후 점수(텍스트 출력)를 제공하는 판정형 대규모 언어 모델(LLM) [28]과, 궤적 내\n각 단계가 \"정확한\"지 \"부정확한\"지에 대한 확률적 예측을 제공하도록 미세 조정된 프로세스-보상 모델 [29, 73, 31]이 있습니다. 이\n러한 검증기의 점수는 최종 출력이 부정확할 것으로 예상되는 실패한 궤적을 식별하는 데 활용될 수 있습니다.\n\n현재 검증기의 주요 한계는 궤적의 오류를 표시할지 여부를 결정하는 데 점수를 활용할 때, 이 하류 결정의 오류 확률에 대한 보\n장이 전혀 없다는 점이다. 자율주행 연구실[27], 유전자 편집[42], 병원 운영[13] 등 현실 세계에 영향을 미치는 고위험 환경에 에이\n전트가 배치될 경우, 엄격한 보증은 특히 중요해질 수 있다.\n\n\n1\n특히, 우리는 _오경보율_, 즉 성공적인 궤적을 실패한 것으로 잘못 표시할 확률에 주목한다. 검증자 점수가 한계 보정과 같은 확률적 \"정확성\"\n에 대한 통념을 충족하더라도, 그러한 개념들은 오경보율에 대한 보장을 제공하지 않는다. 더욱이, 궤적 내 각 행동은 시간과 자원을\n소모하며, 궤적은 길어질 수 있다. 따라서 우리는 궤적이 실패할 것임을 가능한 한 적은 행동 후에 조기에 탐지하고자 합니다. 전체\n궤적의 비용을 감수한 후에야 결정을 내리는 대신 말이죠. 이러한 요구사항은 각 행동 후 검증기 점수를 평가할 것을 필요로 합니다.\n\n이 경우, 특히 완전한 궤적의 길이가 사전에 알려지지 않기 때문에, _언제든_ 오경보를 발생시킬 확률에 대한 보증을 얻는 방법이 불분\n명합니다.\n\n정밀 조정이나 더 나은 검증기 구축은 이러한 문제를 직접 해결하지 못합니다. 더욱이 배포 환경을 위한 검증기 정밀 조정은 상\n당한 공학적 노력을 수반하며, 충분한 컴퓨팅 자원과 검증기(그리고 아마도 에이전트) 가중치에 대한 화이트박스 접근이 필요합니다\n. 충분한 컴퓨팅 자원과 검증기 가중치 접근 권한이 있더라도, 검증기 미세 조정에 적합한 충분한 데이터를 확보하는 것은 비현실적\n일 수 있습니다: 배포 환경을 대표하는 궤적뿐만 아니라, 모든 궤적 내 모든 행동에 대한 \"정확성\" 레이블이 필요합니다[31].\n\n이러한 과제를 해결하기 위해, 우리는 가벼운 통계적 래퍼인 _e-평가기를_ 도입합니다. 이 래퍼는 블랙박스 검증기의 점수를 실패한\n에이전트 궤적을 탐지하기 위한 결정 규칙으로 변환하며, 오탐률에 대한 보증을 제공합니다. 이를 위해 먼저 문제를 가설 검정으로\n설정합니다. 검증기 점수 시퀀스가 성공적 궤적(즉, 올바른 최종 출력을 생성할 궤적)에 대해서는 분포 _p_ (1) 에서, 실패한 궤적에 대해서\n는 분포 _p_ 0 에서 추출된다고 가정합니다. 새로운 궤적에 대해, 문제는 가능한 한 적은 수의 동작 후 검증기 점수 시퀀스가 _p_ 1 에서 추출\n되었는지 _p_ (0) 에서 추출되었는지결정하는 것으로 귀결됩니다. _p_ 1 과 _p_ (0) 을 평가할 수 있다면, 각 _시간 t마다_ 지금까지 수집된 점수가 _p_ 1 에서 유\n래했 [는지] _[p]_ (0) 에서 유래했는지 검 [정하는] [잘] [연구된] [순차적] [가설] [검정인] [순차적] [가능도비] [검정][[60]][을] [적용할] [수] [있을] [것이다][. ][그러나] _[p]_ 1 [과] _[p]_ 0 [은]\n일반적으로 알려져 있지 않다.\n\n따라서 _e-valuator는_ 세 단계로 작동합니다: (1) 배포 환경에서 에이전트 궤적(즉, 행동 시퀀스와 중간 상태), 단계별 검증자 점수,\n결과 레이블(즉, 최종 출력이 정답인지 오답인지)로 구성된 소규모 보정 세트를 **수집합니다** . (2) 각 단계 t에서 각각 성공적 궤적과 실\n패적 궤적에 대한 검증자 점수 분포인 _p_ 1 과 _p_ 0 사이의 밀도 비율 모델을 학습합니다. 여기서 p(1) **은** 각 _단계 t에서_ 성공적 궤적에 대한\n검증기 점수의 분포를, **p** (0)은 실패한 궤적에 대한 검증기 점수의 분포를 **나타냅니다** . (3) 보정 세트의 홀드아웃 분할을 사용하여 실\n패한 궤적을 표시하기 위한 결정 임계값을 찾습니다. 이때 사용자가 지정한 _α_ 수준(즉, 성공적 궤적이 실수로 실패한 궤적으로 표시\n되는 비율)에서 오경보율(제1종 오류)을 제어합니다. 중요한 점은 _e-평가기가_ 검증기의 현재 및 향후 개선 사항을 보완한다는 것입\n니다: 모든 에이전트와 검증기 조합에 대해 오경보율 제어를 보장하면서도, 더 우수한 검증기와 함께 배포할 경우 더 강력한 결정 규\n칙을 도출하는 경향이 있습니다. _e-평가기는_ 최소한의 컴퓨팅 자원을 필요로 하며 표준 노트북에서 실행 가능하며, 코드는\nPyPi(https://pypi.org/project/e-valuator/)에서 이용 가능한 Python 패키지로 공개합니다.\n\n_E-valuator는_ e-값과 순차적 가설 검정 접근법을 기반으로 합니다. e-값은 p-값의 대안으로, 일련의 가설 검정(예: \"현재 진행 중\n인 궤적이 성공적인가?\")을 수행하고자 하지만 사전에 검정 횟수를 알 수 없는 상황(궤적 길이가 가변적이기 때문)에서 특히 유용합\n니다.\n\n실증적으로, 6개 데이터셋과 3개 에이전트에 걸쳐 _e-평가기는_ 원시 검증기나 재교정 검증기 모델 단독과 같은 다른 기준 모델보\n다 더 우수한 오경보율(제1종 오류) 제어 및 통계적 검정력을 제공했습니다. _e-평가기의_ 잠재적 활용 사례로 실패한 궤적의 조기 종료\n를 제시합니다. 이를 통해 토큰의 80%만으로 모델의 원래 정확도를 최대 90%까지 회복할 수 있습니다. 또한 비-LLM 환경(체스 엔\n진이 각 수 후 보드 상태를 수치화하여 점수화하는 경우)에서도 실험을 수행했습니다.\n\n요약하면, 우리의 기여는 다음과 같습니다:\n\n\n1. 에이전트 출력 검증 문제를 정식화하고, 기존 검증기 모델을 강화하기 위한 해결책으로 순차적 가설 검정을 제안한다.\n\n\n2. 통제 가능한 오류율과 강력한 통계적 검정력을 통해 실패한 경로를 식별할 수 있는 통계적 래퍼인 _e-평가기를_ 소개합니다.\n\n\n3. _e-평가기는_ 비-LLM 에이전트를 포함한 모든 블랙박스 에이전트-검증기 조합에 적응할 수 있습니다. _e-평가기는_ 검증기에 대한 블랙\n박스 접근을 가정하므로, 검증기 모델을 직접 개선하는 방법(예: 미세 조정)이 이를 보완합니다.\n\n\n4. _e-평가기는_ 여러 데이터 세트, 검증기 및 에이전트에서 기준을 능가하는 성능을 보인다는 것을 경험적으로 입증합니다.\n\n\n2\n**1.** 궤적, 점수 및 결과 **수집**\n**2.** 단계별 밀도 비율 **학습** (eprocess)\n**3.** 오경보율을 제어하기 위한 임계\n값 **찾기**\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-2-2.png)\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-2-1.png)\n\n\n\n_α_\n_Mt_\n_Mt_ =\n_p_ ( **S** [ **1** : **t** ] | _Y_ = 0)\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-2-3.png)\n\n\n\n\n\n성공 (Y=1) 실패 (Y=0)\n그림 1: **e-평가기** **개요** . _e-평가기는_ 세 단계로 작동한다. 첫째, 소규모 교정 세트인 궤적, [검증기] [점수][, ][라벨을] 수집한다 [. ][둘째][, ][각] _[시간][단계][ t][에서]_ [밀도]\n비율 _M ˆ t_ _를_ 학습한다. 셋째, 빌의 부등식 또는 분위수 기반 경험적 접근법을 사용하여 오경보율을 제어하는 결정 임계값을 찾는다. 주어진\n임계값에서, 실패한 궤적(빨간색)은 성공한 궤적(녹색)보다 더 높은 비율로 거부되어야 합니다.\n\n### **2 방법**\n\n\n**2.1** **문제** **설정**\n사용자 프롬프트 _o_ (0) 이주어지면, 에이전트는 _T_ ∈ N+ 의 행동 시퀀스 ( _a_ 1 _, . . ., a_ T )를 실행한다. 여기서 N+ := { 1 _,_ 2 _, ..._ } 이며, _T는_\n프롬프트와 에이전트의 내부 무작위성에 모두 의존하는 확률변수이다. 각 행동 _a_ (t) 에는행동 실행 후 환경 상태를 포착하는 관측값 _o_ t\n가 연관됩니다(예: _a_ t 가 중간 산술 계산을 수행하는 경우, _o_ (t) 에는계산된 값이 포함될 수 있음). 행동과 관측은 함께 각 단계 _t에서궤_\n_적H_ t= ( _o_ 0 _, a_ 1 _, ..., a_ t _, o_ t)를 형성합니다. 각 단계 _t_ 이후, 블랙박스 검증기 모델 _v는_ 궤적 _H_ t를 입력으로 받아 지금까지의 궤적 품질\n에 대한 휴리스틱 평가 역할을 하는 점수 _S_ t= _v_ ( _H_ t)를 제공합니다. 일반적으로 _S_ t ∈ [0 _,_ 1]이지만, _e-valuator는_ 모든 유형의 점\n수 값을 지원합니다. 검증자의 점수는 시퀀스 **S** = ( _S_ 1 _, ..., S_ T)를 형성합니다.\n\n완전한 궤적 _H_ T는 최종 출력 _o_ T가 올바른지( _Y_ = 1) 아닌지( _Y_ = 0)에 대한 이진 레이블 _Y_ ∈ { 0 _,_ 1 } 과 연관된다. [1] 우리는 _Y_ = 1인 궤\n적을 _성공적인_ 궤적이라 부르고, _Y_ = 0인 궤적은\n_실패한_ 궤적. 우리는 _보정데이터_ D cal= { ( **S** [(] [i] [)] _, Y_ [(] [i] ) )} [n] 에 대한 접근이 가능하다고 가정한다. 여기서 ( **S** [(] i=1 [i] [)] _, Y_ [(] [i] ) )는\n변수 길이 점수 시퀀스와 그 레이블에 대한 공동 분포인 _P에서_ 독립 동일 분포(i.i.d)로 추출된 것으로, 밀도 함수 _p를_ 가집니다.\n\n\n**2.2** **가설** **검정을** **통한** **평가**\n새로운 점수 시퀀스 **S가** 주어졌을 때, 우리의 목표는 에이전트의 궤적이 최종적으로 올바른( _Y_ = 1) 출력인지 잘못된( _Y_ = 0) 출력인\n지 판단하는 것이다. 이 목표를 가설 검정으로 형식화한다. _P_ 1 과 _P_ (0) 은각각 올바른 최종 출력과 잘못된 최종 출력에 조건부로 주어\n진 점수 시퀀스의 분포를 나타내며, 각각의 밀도 함수는 _p_ 1 과 _p_ 0 이다. 즉, _p_ 1 ( **S** ) = _p_ ( **S** | _Y_ = 1) 및 _p_ 0 ( **S** ) = _p_ ( **S** | _Y_ = 0)이다. 우리는\n_P_ 1 ≠ _P_ 0 이라고 가정하며, 다음 두 가설 사이에서 검정을 수행하고자 한다:\n\n\nH N : **S** ∼ _P_ 1 (최종 출력은 올바름)\nH A: **S** ∼ _P_ 0 (최종 출력이 틀림)\n_P_ 1 과 _P_ (0) 은일반적으로 시간에 따른 점수 간의 복잡한 의존성을 인코딩합니다. 즉, 블랙박스 검증기 점수는 일반적으로 매 단계마다\n고정된 분포에서 독립적으로 추출된 표본이 아니며, 편리한 가정을 적용하기 어렵습니다.\n\n우리는 각 _단계 t에서_ 실행 가능한 H N과 H A사이의 _순차적검정을_ 구축합니다. 이 _검정은_ 오직 **S** ([1:] t ] 만을 사용하며, 이는 에이전트\n의 _t_ 단계까지의 궤적만을 기반으로 한 점수입니다. 구체적으로, 우리는 일련의 검정 통계량을 구성합니다.\n\n\n1 본 프레임워크는 무한 길이의 궤적도 수용하는데, 이는 유한한 _시간_ _T_ 에 대해 최종 출력 _oT_ 를 생성하지 않으므로 _Y_ = 0 으로 표기한다. 그러나 실용적 관련성\n을 위해, 본 논문에서는 항상 최종 출력을 생성하는 유한 길이의 궤적 설정에만 초점을 맞춘다.\n\n\n3\n( _M_ t ) [T] t=1 여기서 _M_ t는 **S** ([1:] t ] 의 실수 값 함수이며, _c_ α는 사용자가 주어진 실수 값 결정 임계값입니다.\n지정된 오류 수준, _α_ ∈ (0 _,_ 1). 순차 검정은 다음과 같이 진행됩니다(알고리즘 1). _t_ = 1 _,_ 2 _, ...에_ 대해, _M_ t ≥ _c_ (α)이면 H (N)을 기각합니다.\n에이전트의 궤적 끝점인 _t_ = _T에_ 도달할 때까지 H N을 기각하지 않았다면 H (N)을 수용합니다. 우리의 목표는 다음 두 기준을 충족하\n는 것입니다:\n\n\n1. **오경보율** **제어** :\n\n\nPrHN [ ∃ _t_ ∈ [ _T_ ] : _M_ t ≥ _c_ α ] ≤ _α,_ (1)\n여기서 _T_ 는 완전한 궤적의 길이이며, 또한 임의의 값을 가진다. [ _T_ ] := { 1 _, ..., T_ } 이다. 즉, _오경보율_, 즉 성공적인 궤적을 거부하\n는 비율은 최대 _α_ 이하로 보장된다. 이 비율은 고전적으로 제1종 오류로도 알려져 있다. 고려 중인 확률은 다음과 같다.\n\n_M_ t _가c_ α를 초과하는 경우 — 즉, _T_ N _이_ 무엇이든 간에, 에이전트가 취하는 총 행동 횟수 T에 관계없이, 우리가 귀무 가설 H (N)을\n기각하는 경우 — 이 유효성 개념은 _언제든지유효성_ ( _anytime validity_ )으로 알려져 있다.\n\n\n2. **높은** **검정력** : 실패한 궤적은 종종 기각된다. 즉, 실패한 궤적에 대한 높은 기각률 PrHA[ ∃ _t_ ∈ [ _T_ ] : _M_ t ≥ _c_ α]를 원한다.\n\n\n우리는 첫 번째 기준의 고확률 버전을 만족하는 방법을 개발하며, 경험적으로\n두 번째 기준을 달성한다. 첫 번째 기준의 도전 과제는 통계량 시퀀스 ( _M_ t) [T] 를 구성하는 것이다. t=1\n그리고 _사전적으로_ 완전한 궤적의 길이 _T를_ 알지 못하더라도 순차 검정이 유효하도록 하는 결정 임계값 _c_ α가 존재한다. 현대 통계적\n가설 검정에서 표준 도구인 p-값은\n이러한 설정에는 본질적으로 적합하지 않다. 직관적으로, p-값의 정의—모든 _α_ ∈ [0 _,_ 1]에 대해 PrHN ( _q_ ≤ _α_ ) ≤ _α를_ 만족하는 확률변\n수 q—는 그 자체로 특정 단계에서 생성된 p-값이 이전 단계의 p-값에 어떻게 의존하는지에 대해 아무것도 암시하지 않는다. 이러한\n의존성은 데이터(여기서는 **S** ([1:] t ] )가\n각 단계에서 관측된 값들은 상호 의존적일 뿐만 아니라, 각 단계의 데이터가 독립적이라 하더라도 단순히 이전 값들을 기반으로 다\n음 p-값을 생성할지 여부를 결정하는 과정 자체에 의존한다. 이러한 의존 구조에 대한 지식 없이 추가 가정과 이를 활용하는 기술[12,\n22, 45, 53] 없이서는 어떤 기각의 확률을 평가해야 할지 불분명하다(자세한 논의는 부록 참조). 대신 우리는 귀무 가설에 대한 증거\n를 정량화하는 또 다른 대상인 _e-process_ [44, 46]로 눈을 돌린다. 이 정의는 귀무 가설 하에서 검정 통계량의 현재 값과 과거 값 사이\n의 관계를 명시적으로 특성화하며, 이러한 설정에서 확률적 추론을 자연스럽게 가능하게 한다. H(N)에 대한 e-process는 (E(t))(t) 순\n서로, 각 단계에서 관측된 값들이 서로 의존적일 뿐만 아니라, 각 단계의 데이터가 독립적이라\n현재 검정 통계량과 과거 값 사이의 관계를 자연스럽게 확률적 추론을 가능하게 하는 방식으로 명시적으로 특성화합니다. H N에 대\n한 e-과정은 ( _E_ t )t 라는 시퀀스로, 각 _E_ t 는 H N에 대한 _e-값입니다_ . 즉,\nE HN[ _E_ t] ≤ 1—그리고 H N에 대한 _시험마팅게일_ ( _M_ tt)이 존재하여 항상 _E_ t ≤ _M_ t를 만족한다. 시험 마팅게일\nH (N)에 대한 시험 마팅게일은 두 조건을 만족하는 순열 ( _M_ t )(t) 이다:\n\n\n1. **비음성과** **단위** **평균** : 모든 _t에_ 대해 _M_ t 는 비음수이며, _E_ HN [ _M_ 0 ] ≤ 1이다.\n\n2. **마팅게일** . ( _M_ t)t는 H N에 대한 마팅게일이다. 본 연구 설정에서 이는 다음을 의미한다: _E_ HN[ _M_ t _|M_ 0 _, M_ 1 _, ..., M_ t− 1 ] = _M_ (t) − 1\n모든 _t_ [(2)][에] 대해 성립함을 의미한다.\n\n\n모든 검정 마팅게일은 e-과정이지만, e-과정은 훨씬 더 광범위한 과정의 범주임을 유의하십시오.\n\n테스트 통계량 시퀀스 ( _M_ t) [T] 를 e-과정으로 구성함으로써 우리는 t=0 특정 특성을 활용할 수 있다.\n\n각 _단계 t에서_ 전체 시퀀스에 대해 확률적으로 추론할 수 있는 속성들로, 에이전트가 최종적으로 취하는 총 단계 수와 무관합니다. 특히,\n빌의 부등식은 모든 e-프로세스 ( _M_ t )t 에 대해 다음을 나타냅니다.\nPrHN [ ∃ _t_ : _M_ t ≥ _1/α_ ] ≤ _α_ 모든 _α_ ∈ [0 _,_ 1]에 대해 성립합니다. 즉, 최소한 1 _−α_ 의 확률로 전체 시퀀스—\n비록 무한히 진행된다 하더라도—모든 _α_ ∈ [0 _,_ 1]에 대해 _1/α_ 미만이다. 이 부등식은 e-process가 왜 무효 가설에 대한 시간에 따른 증\n거의 정량화로 해석될 수 있는지 구체적으로 보여준다.\n귀무가 참이라면, 높은 확률(1 - _α_ )로 이 증거는 영원히 낮게 유지될 것이다(구체적으로 _1/α_ 미만). 대립가설이 참이라면, 잘 설계된\ne-과정은 잠시 후 설명하듯이 커진다.\n\n\n\n2 보다 형식적으로, 마팅게일은 모든 _t_ _에대해_ _EH_ [ _Mt |Ft−_ 1 ] = _Mt−_ (1) 을 만족시키며, 여기서 ( _Ft_ ) _t_ 는 필터링이다. 설명의 명확성과\nN\n공간 제약으로 인해 여기서는 완전히 엄밀한 처리를 피하지만, 대략적으로 이는 _Ft−_ (1) 에 이용 가능한 모든 정보에 조건을 거친다는 것을 의미하며, 이는 과거의 정\n확한 값에 조건을 거치는 것보다 더 많거나 적은 정보일 수 있다. 이는 _Ft_ _가_ [소위] [자연] [여과][(natural filtration)][일] [때] [과거의] [정확한] [값에] [조건을] [거치는] 것과 동등하다\n.\n\n\n4\n우리 설정에서 e-프로세스를 구성하기 위해, 우리는 무효 밀도 _p_ 1 과 대안 밀도 _p_ 1 사이의 비율을 사용한다. 구체적으로, _M_ 0 = 1로 설\n정하고, 각 단계 _t_ ∈ [ _T_ ]에 대해\np0 [(] **[S]** [1: t ] [)]\n_M_ t= _[ .]_ (2)\n_p_ ( **S** 1 [1: t ] )\n밀도 비율 과정은 시험 마팅게일이며 따라서 e-과정[44]이다(증명 참조). 이 선택된 _M_ t및 _c_ α= _1/α를_ 사용하는 알고리즘 1은 언제든지\n유효한 오경보율 제어(정리 1)를 달성할 수 있게 한다.\n\n\n**알고리즘 1** 에이전트 검증을 위한 순차적 가설 검정 프레임워크.\n\n**입력:** 사용자 프롬프트, _o_ 0 ; 각 단계 _t에_ 대해 점수를 입력으로 받아 검정 통계를 출력하는 함수, _M_ t ( - ) _, t_ = 1 _, ..._ ; 결정 임\n계값, _c_ α ∈ R .\n\n**출력:** 결정, H N기각 또는 H N수용.\n\n1: _c_ α ← _1/α_\n2: _H_ 0 ← ( _o_ 0 )\n3: _t_ = 1 _, . . ., T_ **에** **대해**\n4: _H_ t− 1 인 경우, 에이전트는 행동 _a_ (t) 를 실행하여환경 상태 _o_ (t) 를 생성한다.\n\n\n\n5: 궤적 업데이트, _H_ t ← ( _o_ 0 _, a_ 1 _, o_ 1 _, ..., a_ t _, o_ t).\n\n\n\n6: 검증자 점수 획득: _S_ t ← _v_ ( _H_ t).\n\n\n\n7: 이 단계에 대한 검정통계량 _M_ t ( **S** ([1:] t ] )를 평가한다 _▷_ **S** [1: t ] := ( _S_ 1 _, ..., S_ t)\n8: _M_ t( **S** ([1:] t ] ) ≥ _c_ α **인** **경우**\n9: **반환:** H N기각\n10: **end if**\n11: **for 종료**\n12: H (N)을수락함\n**제안 1.** _임의의고정된α_ ∈ (0 _,_ 1) _에대해, 밀도비율과정M_ t= _p_ 0 ( **S** ([1:] t ] ) _/p_ 1 ( **S** ([1:] t ] ) _, 그리고결정임계값c_ α = _1/α 를사용하는알고리즘 1은_\n_언제나유효한허위경보율제어(식 1)를달성한다._\n\n\n_증명._ (개요) 밀도 비율 과정 ( _M_ t) [T] 는 e-과정이다. 허위경보율에 t=0 대한 언제나 유효한 제어는\n에 대한 언제나 유효한 제어는 빌의 부등식에 의해 따릅니다. 완전한 증명은 부록 8.1을 참조하십시오.\n\n\n제1정리는 언제나 유효한 오경보율 제어를 확립하지만, 왜 다른 e-과정 대신 밀도비 과정을 선택해야 하는지는 설명하지 못한다\n. H (A) 하에서밀도비 과정은 모든 e-과정 중 시간이 지남에 따라 (기대값 기준) 가장 빠르게 성장하는데, 이 개념을\n제2정리에서 정밀하게 정의된 로그최적성(log-optimality)이라고 한다. 로그최적성은 비순차적 가설검정에서 \"가장 강력한\" 검정통\n계량과 유사하다: 직관적으로, _M_ (t)는의사결정 임계값을 초과하는 경향이 있으며, 이에 따라 다른 e-과정보다 더 일찍 실패한 궤적을\n탐지할 수 있게 한다.\n\n\n\n**제2정리.** _밀도비율과정, M_ =\np0 (S [1:t] ) _, 는로그최적이다. 즉, 다른어떤 e-과정_ ( _M_ ′ )T\nt p1 [(][S] [1:t] [)] t t=0\n_및정지시간τ 에대해, E_ HA [log _M_ τ ] ≥ _E_ HA [log _M_ [′] ] _이_ τ _성립한다._\n\n_증명._ (개요) 이는 Neyman-Pearson 정리[38]의 순차적 아날로그이다. [44]에도 제시된 이 증명은, 먼저 특정 _M_ t가 모든 e-변수 중 로\n그 최적 e-변수임을 주목함으로써 시작된다. 이후 e-과정의 성질을 활용하여 증명을 완성할 수 있다. 완전한 증명은 부록 8.1을 참조\n하라.\n\n\n**2.3** **밀도** **비** **추정**\n실제 적용 시 _p_ 1 과 _p_ 0 의 형태는 일반적으로 알려져 있지 않습니다. 따라서 제안된 방법인 _e-평가기는_ 배포 전에 보정 단계(calibration\nphase)를 거치며, 이 단계에서 보정 데이터 D (cal)를활용하여\n5\np0 (S [1:t] ) 의 _모델을_ 학습합니다(알고리즘 2). 이를 위해 분류기 기반 밀도 비율\n밀도 비율, [Mˆ] t ( **S** [1: t ] [)] [ ≈]\np 1  [1: (S t] ))\n1  [1:t]\n추정, 베이즈 법칙에 의한 다음 등식에 기반한 접근법 [4, 16]:\n\n\n\n_M_ t = [p0] [(] **[S]** [[1:] [t] []] [)]\np1 [(] **[S]** [1: t ] [)]\n_p_ ( _Y_ = **0|S** ([1: t ]) ) _p_ ( _Y_ = 1)\n=\n_p_ ( _Y_ = **1|S** [1: t ] ) _p_ ( _Y_ = 0)\n_._ (3)\n구체적으로, 각 시간 단계 _t에대해_, 입력으로 **S** ([1:] t ] 를 받아 _p_ ( _Y_ = **1|S** ([1:] t ] )의 추정값을 제공하는 분류기 [fˆ] t를 훈련합니다. 또한 클래\n스 확률 _p_ ( _Y_ = 1)의 추정값 πˆ 1 을 형성합니다. 이 두 추정값을 식 (3)에 대입하여 단계 _t_ 에서의 추정 밀도 비율을 다음과 같이 구합\n니다:\n\n\n\nMˆ t = [1 ] [− ] [fˆ][t][ (] **[S]** [([1:] [t] [])] [ )]\nfˆt [(] **[S]** [1: t ] [)]\nπ ˆ 1\n_._\n1 - π ˆ 1\n이러한 추정된 밀도 비율 [M ˆ] t 와 빌의 부등식에 기반한 동일한 결정 임계값 _c_ α= _1/α를_ 사용하여 알고리즘 1을 실행하는 것을 _1/α 임_\n_계값을가진 e-평가기라고_ 부를 것이다.\n\n\n\n\n[1: t ]\n진술 1과 진술 2의 보장은 진정한 밀도 비율, 즉 [M ˆ] t\n와 각 단계 _t에_ 대해 학습할 때 적용된다.\n\n\n\np0 (S [1:t] ) 각 점 **S에** 대해\n=\np 1  [1: (S ] )\n1  [1:t]\n실험에서 우리는 각 단계의 분류기 [fˆ] t에 대해 단순 로지스틱 회귀를 사용했으며, 수백 개의 교정점에서 학습된 추정 밀도 비율이 경험\n적으로 오경보율과 오탐지율 모두를 달성하고 다른 방법보다 우수한 성능을 보인다는 것을 발견했습니다(자세한 내용은 부록 8.2.2 참조).\n\n경보율 제어와 우수한 성능을 모두 달성함을 발견했습니다(자세한 내용은 부록 8.2.2 참조).\n\n\n**알고리즘 2** 교정 데이터를 이용한 밀도 비율 추정.\n\n**입력:** 보정 데이터, D cal .\n\n\n\n**출력:** 모든 단계에 대한 밀도 비율을 추정하는 함수, { [M ˆ] t } t ∈ N+ .\n\n\n\n1: D cal을 D DRE와 D threshold로 무작위 분할하며, D DRE ∪ D threshold= D (cal)이 성립한다.\n\n\n\ncal 1 DRE Σ\n2: πˆ 1 ←\n1\n|DDRE | (S,Y ) ∈D DRE _[Y]_ _▷_ 경험적 빈도로 클래스 확률을 추정합니다.\n\n\n\n3: _T_ max ← max { _T_ : D DRE내 길이 _T인_ 성공 및 실패 점수 시퀀스가 모두 존재하는 경우 }\n4: **for** _t_ = 1 _, . . ., T_ max **do**\n5: D DRE,t ← { ( **S** [1: t ] _, Y_ ) : ( **S** _, Y_ ) ∈ D DRE : |S| ≥ _t_ }\n6: D DRE,t 를 사용하여 **S** ([1:] t ] 를 입력으로 받아 _p_ ( _Y_ = 1 | **S** ([1:] t ] )를 예측하는 확률적 분류기 fˆ를 훈련한다.\n\n\n\n7: _M_ [ˆ] ( - ) ~~←~~\nfˆ( - ) 1−πˆ1\n8: **for 종료**\n) ~~←~~ 1−fˆ( - )  πˆ 1\nt\n9: 모든 _t > T_ max에 대해 [Mˆ] t ( - ) ← [M ˆ] T max( - )로 설정한다.\n\n\n**2.4** **분위수** **추정법을** **통한** **전력** **증가**\n진밀도비 과정 _M_ t 가 주어졌을 때, _M_ t 가 결정 임계값 _c_ α = _1/α_ 를 초과할 때마다 H (N) 을기각하면 언제나 유효한 오경보율 제어가 달\n성된다(정리 1). 그러나 실제로는 추정된 밀도비를 사용하므로, 임계값 _c_ α 는 밀도비 추정 오차를 고려해야 한다. 또한, 언제나 유효성\n을 달성하기 위해 _c_ (α)를 설정하는 것은 지나치게 보수적일 수 있다. 이는 에이전트가 무한히 행동을 계속 실행하더라도 허위경보율의\n상한을 보장하기 때문이다. 반면 실제 에이전트는 일반적으로\n유한한 수의 행동 후 최종 출력을 생성하거나 종료됩니다.\n\n따라서 우리는 언제든지 오경보 제어를 높은 확률로 유지하면서 이 두 요소를 모두 고려하기 위한 결정 임계값 _c_ α설정의 대체 절\n차를 제안한다. 먼저 보정 집합 D cal을 D DRE와 D threshold로 분할한다. 여기서 D DRE는 먼저 밀도 비율 추정(Alg. 2)에 사용되며,이후\nD (threshold)를 사용하여c(α)를다음과같이 설정한다.\n(알고리즘 2)에 사용되고, 이후 D threshold는 다음과 같이 _c_ α _를_ 설정하는 데 사용됩니다.\n\n먼저, 과정 [M ˆ] t 가 _c_ (α)를 초과하는 경우 H (N)을 기각하는 것은 maxt [M ˆ] t ≥ _c_ α일 때 H (N)을 기각하는 것과 동등합니다.\n\n_c_ α . 따라서 _c_ α 를 귀무가설 하에서 maxt [ M ˆ] t 분포의 (1 - _α_ ) 분위수로 설정하는 것으로 충분하다.\n\n\n6\n따라서 우리는 D threshold내의 성공적 궤적에 주목하며, 이는 H N에 해당한다. 보정 집합 내 각 궤적 _i에_ 대해, 모든 단계에 걸쳐 추정된\n최대 밀도 비율 _M_ [(] [i] [)] =\nmax { [Mˆ ] [(] [i] [)] _, . . .,_ [Mˆ ] [(] [i] [)] } . 이 최대값 _M_ [(] [i] [)] 는 귀무 분포의 표본이다. 이러한\n1 T\n표본을 바탕으로, 우리는 qˆ 1 −(α) 를구성하는데, 이는 무효 분포의 (1 - _α_ )-분위수에 대한 높은 확률 상한이다.\n(알고리즘 3). 추정된 밀도 비율 [M ˆ] t 와 결정 임계값을 이 고확률 상한 _c_ α= qˆ 1 −α로 설정하여 알고리즘 1을 실행하는 것을, 다음 보증\n에 따라 _'확률적으로근사적으로정확한(PAC)_ ' _임계값을가진 e-평가기라고_ 부른다. 즉, 사용자가 지정한 _δ_ 에 대해 최소 1 - _δ의_ 확률\n로(즉, \"확률적으로\"), 이 절차는 사용자가 지정한 _α_\n(\"대략적으로 정확함\") 조건 하에서 언제나 오경보율을 제어할 수 있음을 의미한다.\n\n\n**제안 3.** _알고리즘 2에의해출력된추정밀도비율함수를_ { [M ˆ] t } t ∈ N+ _로표기한다. 고정된오차수준δ_ ∈ (0 _,_ 1) _및사분위수수준α_ ∈ (0 _,_\n1) _에대해,_ _c_ α _를알고리즘 3의출력으로정의한다. 그러면_ { [M ˆ] t } t ∈ N+ _와결정임계값c_ (α) 를 _사용하는알고리즘 1, 즉_ PAC(probablyapproximately-correct) 임계값을 가진 e-평가기는 _다음을만족한다._\n\n\n_Pr_ Dcal ( _Pr_ HN ( ∃ _t_ ∈ [ _T_ ] : _M_ t ≥ _c_ α | D _cal_ ) ≤ _α_ ) ≥ 1              - _δ_ (4)\n_증명._ (개요) 높은 확률로 qˆ 1 −α ≥ _q_ 1 −α 를 만족하는 qˆ 1 (−)α 를 찾는 것은 교정 집합의 무효 _표본들_ 중에서 순서 통계량 _M_ ( k ) 가 _q_ 1 −α 보\n다 높을확률이 높은 지표 k를 찾는 문제로귀결된다. 완전한 증명은 부록 8.1을 참조하라.\n\n### **3 관련 연구**\n\n\n_e-평가기는_ 모든 검증기를 위한 통계적 래퍼이므로, 본 연구는 에이전트를 위한 더 나은 검증기 구축에 관한 기존 연구와 관련이 있으\n나 독립적입니다. 이러한 검증기는 종종 에이전트의 행동 시퀀스에서 각 단계 이후 보상(예: 정확성 또는 일관성)을 추정하는 보상 모\n델로 훈련됩니다. 이 중 프로세스 보상 모델(PRM)은 수학적 추론 추적[55, 63, 26]과 같이 각 _단계가_ 정답/오답으로 라벨링된 에이전트\n궤적을 활용해 미세 조정됩니다. 기존 PRM을 보정하는 연구[72, 39]도 존재하나, 본 논문에서 수행한 것처럼 오경보율을 제어하기에는\n보정만으로는 불충분합니다.\n\nPRM 훈련은 (a) 인간이 주석 처리한 프로세스 레이블 접근[31]과 (b) 기존 LLM의 미세 조정[63]이 필요하므로 비용이 많이 들\n수 있습니다. PRM의 대안 검증기에는 LLM-as-a-judge(즉, 프롬프트 기반 검증)[2]과 전체 궤적에 대해서만 레이블을 제공하는 결과\n보상 모델[10]이 있습니다. 궤적[35] 및 프로세스[73]에 대한 검증기를 비교하기 위한 여러 벤치마크가 존재합니다.\n\n일부 선행 연구는 분류기 위에 가설 검정을 구축하여 AI 배포를 모니터링하지만, 서로 다른 맥락에서 수행되었다. Vovk 등[59]\n은 모델 배포의 지속적 모니터링을 위해 적합성 테스트 마팅게일(CTM) 사용을 제안했으며, Prinster 등[41]은 테스트 시점에서의 적\n응을 가능하게 하고 성능 저하의 원인을 분석하기 위해 가중 CTM을 개발했다. Podkopaev와 Ramdas [40]는 순차적 검증을 활용하\n여 배포된 모델의 위험을 직접 추적하며, 이는 라벨 없는 모니터링 [20], 테스트 시간 적응 [47], 알려지지 않은 변화 [54]로 확장되었\n다. 비슷한 맥락에서 [7]은 분류기가 공정한지 순차적으로 테스트하기 위해 안전한 언제든지 유효한 테스트를 적용한다. [21]은 훈\n련 환경과 배포 환경 간 공변량 시프트가 있는지 탐지하기 위해 분류기 두 표본 검정[32]을 적용한다. 보다 광범위하게, 여러 선행 연\n구는 가설 검정 없이도 AI 시스템 평가를 위해 분류기 점수의 클래스 조건부 밀도 및 해당 밀도 비율을 모델링한다[49, 68]. 이러한\n방법들은 일반적으로 밀도를 생성적으로 모델링하지만[11], 일부는 본 연구와 같이 분별적으로 모델링하기도 한다[23].\n\n또 다른 연구 분야는 대규모 언어 모델(LLM)에 공식적인 통계적 통제 장치를 추가하려는 시도이다. 적합 예측법[48]은 개별 예\n측의 불확실성을 정량화하면서, 해당 불확실성에 대해 유한 표본 및 블랙박스 보증을 제공한다. 준거 예측은 최소 품질 요구사항이\n충족될 때까지 LLM 생성물을 재표본화하는 데 적용되었습니다[43]. 이러한 아이디어는 LLM 출력의 사실성 제어[6, 36] 및 LLM 출\n력에 대한 신뢰도 일반적 벤치마킹[71]으로 확장 적용되었습니다. 최근 Wu 등은...\n\n[69]은 LLM의 내부 로짓에 대한 화이트박스 접근을 활용하여 LLM 추론 추적에 대한 중지 규칙을 보정하기 위해 학습 후 테스트 프\n레임워크[1]를 적용한다.\n\n_E-평가기는_ 기존 e-값 연구를 직접 발전시킨 것이다. e-값은 원래 p-값의 대안으로 제안되었으며, 순차적 가설 검정[44, 57, 58]\n에 유용한 특성을 지닌다. e-값은 (잠재적으로 무한한) 일련의 검정에서 언제든지 유효성을 제공한다[62, 67, 66, 15]. 이는 A/B 테스\n트[25], 변화점 탐지[50, 33], LLM 기반 가설 검증[18]에서 중요한 응용을 찾았다. 우리의 e-프로세스 구현은 A/B 테스트[25], 변화\n점 탐지[50, 33], LLM 기반 가설 검증[18]에서의 순차적 확률 비율 검정[62, 67, 66, 15]과 관련이 있다.\n\n\n7\nA/B 테스트[25], 변화점 탐지[50, 33], LLM 기반 가설 검증[18] 등에서 중요한 응용 사례를 확보했습니다. 우리의 e-process 구현은 순차적 확\n률비 검정[60]과 관련이 있습니다.\n\n### **4 실험**\n\n\n_e-평가기가_ 오경보율 제어와 대체 방법보다 높은 검정력을 달성함을 경험적으로 입증하기 위해, 우리는 네 가지 서로 다른 에이전트\n검증기 조합을 사용하여 여섯 개의 데이터셋과 작업에 걸쳐 포괄적인 실험을 수행했습니다. 또한 _e-평가기가_ 제한된 토큰 예산 내에\n서 원래 정확도의 상당 부분을 회복하는 데 적용될 수 있음을 추가로 보여줍니다.\n\n**에이전트와** **검증기.** 우리는 두 가지 도구 호출 에이전트인 Aviary [37]와 Octotools [34], 하나의 단계별 추론 모델인 Claude\nSonnet 4, 그리고 공개 저장소의 온라인 체스 게임에 대해 실험을 수행했습니다. Aviary와 Octotools의 경우, 검증자로 Claude\nHaiku 3.5를 사용하며, 각 도구 호출 후 현재까지의 경로가 성공적일 확률을 텍스트 기반 확률로 요청합니다. 추론 모델의 경우, 널리\n사용되는 사전 훈련된 프로세스 보상 모델(PRM) [63]을 사용하며, 이는 추론 추적의 각 단계가 올바른지 여부에 대한 로짓 기반 확\n률을 제공합니다. 마지막으로 체스 실험에서는 오픈소스 검증기인 Stockfish를 보상 모델로 사용하며, 각 단계 후 백의 위치 강도를\n수치적으로 평가합니다.\n\n**데이터셋.** 우리는 세 가지 서로 다른 분야의 데이터셋을 대상으로 실험을 수행합니다: (1) **수학적** **추론** (GSM8k [9] 및 MATH [17]), (2) **질문**\n**응답** (HotpotQA [70], MedQA [24], MMLU-Pro [64]), (3) **체스** **게임** . **체스의** 경우, LiChess에서 공개된 주석이 달린 대국을 사용합니다.\n\n본론에서는 GSM8k를 제외한 모든 데이터셋의 결과를 제시하며, GSM8k 결과는 부록에 제공합니다. 각 데이터셋에 대한 결과는 해\n당 에이전트-검증기 조합 하나에서 나온 것입니다. 데이터셋, 에이전트, 검증기 조합에 대한 전체 설명은 부록 8.3에서 확인할 수 있\n습니다.\n\n**기준** **모델** 순차적 가설 검정 및 보정에서 영감을 받은 세 가지 기준 모델과 비교합니다.\n\n\n1. **원시** **검증기는** 검증기에서 산출된 점수를 그대로 사용합니다. 검증기는 현재까지의 궤적 _H_ (t)를 주어진 조건에서 에이전트\n가 성공적인 출력을 생성할 확률 Pr( _Y_ = _1|H_ t)에 대한 예측을 제공합니다. 사용자가 지정한 오탐률 _α를_ 기준으로, 점수 _S_ t가\n_α_ 미만으로 떨어지는 경우 해당 궤적을 거부합니다.\n\n\n2. **보정된** **검증기는** 동일한 검증기를 사용하지만, 보정 세트 D (cal)를 사용하여 점수 _S_ t를 재보정합니다. 구체적으로, 우리는 등\n척성 회귀를 사용하여 점수 변환 _S_ [′] = [fˆ] ( _S_ )를 학습합니다. 이 변환은 _한계보정_ (Pr( _Y_ = _1|S_ [′] ) = _S_ [′] )을 달성합니다. 원시 검증\n기와 마찬가지로, 사용자가 지정한 오탐률 _α에_ 대해, 점수 _S_ t가 어느 시점에서든 _α_ 아래로 떨어지면 해당 궤적을 거부합\n니다.\n\n\n3. **본페로니** **검정은** 동일한 밀도 비율 검정 통계량 _M_ (t)를사용합니다. 그러나 결정 규칙은 다릅니다: 본페로니 보정을 사용하여 각\n개별 검정을 _α/T_ 수준에서 기각합니다. 여기서 _α는_ 사용자가 지정한 오탐률이며, _T는_ 교정 집합에서 발견된 최대 궤적 길이입\n니다.\n\n\n이러한 기준선들을 _e-평가기의_ 두 가지 변형과 비교합니다: (1) **e-평가기(** _1/α_ **임계값)** 는 밀도 비율에 대한 플러그인 추정값을 사\n용하여 제1정리에 따라 임계값을 선택하는 방식이며,\n(2) **e-평가기(PAC 임계값)** 는 제3의 제안에 따른 절차로 임계값을 선택합니다. 일반적으로 PAC 임계값 사용을 권장하지만, 에이\n전트가 임의로 많은 수의 단계를 실행하거나 종료되지 않을 수 있는 환경에서는 _1/α_ 버전이 선호될 수 있습니다. 본문에 제시\n된 결과에서는 테스트/보정 데이터를 80/20으로 분할합니다: 데이터의 20%로 방법론(및 모든 기준선)을 보정하고, 나머지 80%로\n테스트합니다. 부록 8.2.2에서는 교정 세트의 다른 분할 방식을 비교하여 수백 개의 교정 궤적으로도 오경보율을 제어할 수 있음\n을 확인했습니다. 또한 부록 8.2에서는 성공 및 실패 궤적의 _M_ t시퀀스를 시각화했습니다.\n\n\n**4.1** **E-평가기는** **경쟁** **방법보다** **우수한** **오경보율** **제어** **성능을** **제공합니다**\n먼저 e-평가기와 경쟁 기준선의 경험적 오경보율을 분석합니다. 특정 _α_ 값에서 오경보율, 즉 성공적(\"null\") 궤적을 실패\n(\"alternative\")로 잘못 표시하는 비율은 _α를_ 초과해서는 안 됩니다.\n\n\n8\n원시 검증기\n본페로니 검정\n보정 검증기\nE- 평가기 (1/ 임계값, 당사 개발 )\n\n\n0. 6\n\n\n0. 4\n\n\n0. 2\n오경보율 \nE- 평가기 (PAC 임계값, 당사 )\n정확한 제어\n\n\n0. 6\n\n\n0. 4\n\n\n0. 2\n\n\n\n0. 6\n\n\n0. 4\n\n\n0. 2\n\n\n\n0. 0\n\n\n\n0. 0\n\n\n\n0. 0\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-8-0.png)\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-8-1.png)\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-8-2.png)\n\n\n\n( 명목상 오탐률 ) ( 명목상 오경보율 ) ( 명목상 오경보율 )\n그림 2: **E-평가기는** **다른** **방법들보다** **오경보율을** **제어하고** **검정력을** **극대화합니다** . 오경보율 제어 위반은 X로 표시됩니다. 두 버전의 _E-평가기는_ 모\n든 데이터셋에서 _α의_ 다양한 선택에 대해 경험적으로 오경보율(제1종 오류)을 제어합니다. 예상대로 _1/α_ 임계값은 PAC 임계값보다 보수적\n이지만, 둘 다 오경보율을 제어합니다. **E-평가기는** 경쟁 방법보다 우수한 검정력을 제공한다. 보정된 검증기와 원시 검증기는 가짜 경보율을\n높이는 대가로 때때로 유사한 검정력을 제공한다. 모든 그래프는 각 데이터셋의 50회 무작위 분할에 대한 95% 신뢰구간을 보여준다.\n\n\n_e-평가기의 1/α_ 임계값 버전(제1정리의 플러그인 버전)과 PAC 임계값 버전(제3정리) 모두 모든 _α_ 선택과 모든 데이터셋에서 경\n험적으로 오탐률을 제어한다(그림 2 상단). 원시 검증기(원시 점수 _S_ (t)를 _α로_ 임계값 처리)는 때때로 _원하는 α보다_ 낮은 경험적 오경보\n율을 달성하기도 하지만(MedQA 및 MATH; 후자의 경우 부록 8.2 참조) 항상 그런 것은 아니다(HotpotQA, _α >_ 0 _._ 4). 원시 검증기 점\n수 _S_ t에 등척성 회귀를 적용한 후 재조정된 점수에 동일한 임계값 _α를_ 사용하는 보정 검증기도 _α보다_ 낮은 오경보율을 달성하지 못\n한다. 등척성 회귀와 같은 보정 절차는 E( _Y |S_ ) = _S를_ 달성하는 것을 목표로 하지만, 이 속성은\n허위 경보율에 직접적인 영향을 미치지 않습니다. 더욱이, 이 특성이 각 시간 단계에서 유지된다 하더라도\n시간 단계에서 이 속성이 유지된다 하더라도, 순차적 가설 검정 설정에서 오경보율을 추론할 수 없습니다(자세한 논의는 부록 8.1 참\n조). 반면 본페로니 검정(그림 2, 주황색)은 모든 _α에_ 대해 오경보율을 통제합니다. 다만 _1/α_ 임계값을 사용하는 _e-평가기보다_ 훨씬\n보수적으로 _통제합니다_ .\n\n1/α 임계값과 PAC 임계값은 궤적 길이가 증가함에 따라 수렴할 것으로 예상됩니다. _1/α_ 임계값은 모든 밀도 비율 과정에 대해\n유효하도록 설계된 반면, PAC 임계값은 관측된 무효 분포에 맞춰 보정되었기 때문입니다. 이러한 예상은 우리가 경험적으로 관찰한\n결과와 일치합니다. 예를 들어, MedQA에서 _α_ = 0 _._ 5, _1/α_ 임계값으로 0.045의 오경보율을 달성하는 평균 궤적 길이는 2.4 단계입니\n다. 이에 비해 HotpotQA에서 _α_ = 0 _._ 5, 임계값 _1/α로_ 0.21의 오경보율을 달성하는 평균 궤적 길이는 4.7단계입니다. MATH 및\nGSM8k 데이터셋에 대한 유사한 플롯은 부록 8.2에서 확인할 수 있습니다.\n\n\n**4.2** **E-평가기는** **대체** **방법** **대비** **향상된** **검정력을** **제공합니다**\n다음으로 동일한 데이터셋과 작업에 대한 경험적 검정력을 분석합니다(그림 2 하단). 검정력은 _주어진 α에서의_ 진양성률을 측정합\n니다. 즉, 검정력은 실패한 궤적(\"대안\")이 실제로 실패로 표시되는 비율입니다. 모든 데이터셋과 모든 α 값에서, PAC 임계값을 적\n용한 e-valuator는 경험적 오탐률을 _α_ 미만으로 달성하는 방법들 중 가장 높은 검정력을 제공합니다(그림 2 상단, 빨간색).\n\n\n9\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-8-3.png)\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-8-4.png)\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-8-5.png)\nMATH MMLU-Pro\n50\n55\n45\n50\n40\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-9-1.png)\n\n\n\n25\n20\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n||<br>|<br>|<br>|<br>|<br>|\n\n\n\n사용된 토큰 총량 (×10 )\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-9-0.png)\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n|||||~~보정 검증기~~<br>E-평가기 (경<br>~~기~~|험적, 당사 제품) 원|시 검증|\n||||||||\n||||||||\n||<br>|||<br> <br><br>E-평가기 (항|<br> <br>상 유효, 당사) 오경|<br>보율 >|\n\n\n사용된 토큰 총계 (×10 )\n35\n그림 3: **E-평가기는** **더** **적은** **토큰으로** **기준** **정확도의** **더** **큰** **비율을** **회복합니다.** 우리는 MATH 및 MMLU-Pro 데이터셋에서 검증기 점수에\n임계값을 적용하는 방법과 _E-평가기를_ 비교합니다. 검증기는 실패한 경로를 상당히 늦게 종료하여, 더 적은 토큰으로 정확도를 회복하는 데\n더 큰 비효율성을 초래합니다. _×_ _는_ 경험적 오경보율이 원하는 수준보다 높았음을 나타냅니다.\n\n\n보정된(또는 원시) 검증기가 더 나은 검출력을 제공하는 경우, 이는 허위 경보율의 증가를 대가로 합니다. 예를 들어 HotpotQA\n에서 _α_ = 0 _._ 5일 때, 보정 검증기는 검정력을 0.84로 제공하지만 오경보율을 0.61로 부풀려 원하는 수준을 훨씬 초과하는 반면, 경험적\n_e-평가기는_ 검정력 0.81을 제공하면서 오경보율을 0.48로 통제합니다. _E-평가기_ ( _1/α_ 임계값) 역시 0.62의 강력한 검정력을 제공하면\n서 오탐률을 0.21로 제어하여 _α_ = 0 _._ 5보다 훨씬 낮은 수준을 유지합니다.\n\n\n**4.3** **사례** **연구: 제한된** **토큰** **예산에서 E-평가기가** **원래** **정확도의** **더** **큰** **비율을** **회복함**\n또한 임계값 c(α)를 초과하는 모든 궤적을 종료하는 사례 연구에 _e-평가기를_ 적용합니다. 이후 저장된 토큰 수(즉, 에이전트가 종료되\n지 않았다면 생성되었을 토큰 수)를 계산하고 데이터셋의 총 정확도와 비교합니다. 전체 토큰 예산을 사용하는 것이 어떤 궤적도 종\n료하지 않음을 의미하므로 달성 가능한 \"최대\" 정확도임을 유의하십시오.\n\n토큰 절감 대비 총 정확도 측면에서 _e-평가기를_ 검증기와 비교한 결과(그림 3), _e-평가기가_ 원시 검증기와 보정 검증기 모두보다 적은\n토큰으로 정확도를 회복하는 데 더 우수한 성능을 보인다는 것을 알 수 있습니다. 예를 들어, MATH 데이터셋에서 _e-평가기는_ 원래\n333,283개의 토큰 중 81%(269,755개 미만)를 사용하여 50%의 총 정확도(원본 정확도 58%의 86%)를 달성했습니다. 반면, 원본 정\n확도의 86%를 회복하기 위해 원본 토큰 수의 95% 이상이 각각 필요했습니다. 마찬가지로 MMLU-Pro 데이터셋에서 _e-평가기는_ 단\n233,324개의 토큰으로 50%의 총 정확도를 달성한 반면, 원본 및 보정 검증기는 각각 250,000개 이상의 토큰이 필요했습니다.\n\n\n**4.4** **사례** **연구: 체스봇용 E-평가기**\n마지막으로, 비-LLM 에이전트 모니터링을 위한 _e-평가기의_ 사용 사례를 제시합니다. 본 실험에서는 Stockfish를 검증기로 활용하여\nLiChess에서 공개된 체스 대국을 분석합니다. Stockfish는 백이 유리한 위치일 때 양수, 흑이 유리한 위치일 때 음수가 되는 '센티폰\n( _centipawns_ )'이라는 실수 점수를 제공합니다. Stockfish는 이 점수를 승률로 변환하는 공식[30]도 공개하며, 이를 검증기의 원시 확\n률 값을 구성하는 데 활용합니다. 우리는 백의 승리라는 귀무 가설을 흑의 승리 또는 무승부라는 대립 가설과 대조하여 검증합니다.\n\n우리는 원시 검증기와 보정된 검증기를 _e-평가기와_ 비교하여 _e-평가기가_ 이러한 기준선보다 오경보율을 더 잘 제어할 수 있음을\n확인했습니다(그림 4). 또한 _1/α_ 임계값을 적용한 _e-평가기는_ PAC 임계값 버전과 유사한 성능을 보입니다. 이는 체스에서 게임이 일\n반적인 에이전트 궤적의 동작보다 더 많은 수(예: 50수 이상)를 포함하는 경우가 많기 때문에 예상되는 결과입니다. _1/α_ 임계값은 오\n경보를 제어하지 못함\n10\n원시 검증기\n본페로니 검정\n\n\n0. 7\n0.6\n\n\n0. 5\n\n\n0. 4\n\n\n0. 3\n보정 검증기\nE- 평가기 (1/ α 임계값, 본 연구 )\n오경보율 \n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-10-1.png)\n\n0. 2\n\n\n0. 1\n\n\n0. 0\nE- 평가기 (PAC 임계값, 당사 )\n정확한 제어\n\n\n1. 0\n\n\n0. 8\n\n\n0. 6\n\n\n0. 4\n\n\n0. 2\n\n\n0. 0\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-10-0.png)\n\n\n\n그림 4: **체스** . E-평가기는 오경보율을 제어하고 체스 검증기의 검정력을 증가시킨다.\n\n\n낮은 _α에서_ 발생하는 오류는 장시간 게임의 밀도 비율 추정 오차에 기인한다. 그럼에도 검증기 모델은 _e-평가기_ ( _1/α_ 임계값)보다 오\n경보율 제어를 훨씬 더 심각하게 위반한다.\n\n### **5 논의**\n\n\n본 논문에서는 순차적 가설 검정을 활용하여 단계별 에이전트 검증기 모델을 개선하는 방법인 _e-평가기를_ 소개한다. 우리는 궤적이\n\"성공적\"인지 여부를 감지하는 문제를 가설 검정 문제로 전환하여, \"성공적\" 분포와 \"비성공적\" 분포에서 생성된 검증자 점수를 구\n분합니다. _e-평가기는_ 에이전트 궤적의 각 단계에서 밀도 비율을 평가해야 하지만, 학습된 밀도 비율이 충분한 검정력과 충분한 오경\n보 제어를 제공한다는 것을 경험적으로 확인했습니다.\n\n이 연구에는 몇 가지 유망한 향후 연구 방향이 존재한다. 첫째, 검증자의 점수가 각 단계에서 독립동일분포(i.i.d.)라고 가정하거\n나, 특정 단계의 점수가 _지난_ k단계의 점수들에만 의존한다고 가정하는 등 특정 가정을 완화하여 각 _시간 t마다전체_ 공동밀도함수를\n추정하지 않도록 할 수 있다. 점수가 독립동일분포(i.i.d.)라고 가정하면, 진정한 밀도 비율에 대한 경험적(그리고 노이즈가 있는) 추\n정값을 사용하더라도 보편적 추론 알고리즘[65]을 활용하여 정확한 보증을 제공할 수 있습니다. 둘째, 우리는 잠재적인 테스트 시간\n스케일링 적용을 탐구했지만, _e-평가기를_ 재표본화나 불량 궤적 재시작과 같은 더 미묘한 스케일링 전략에 사용할 수 있습니다. 이러\n한 전략 중 일부는 _e-평가기의_ 가정을 위반할 수 있으므로, 방법론을 적절히 개선하는 것이 중요할 것이다. 마지막으로, _e-평가기는_ 다\n중 에이전트 환경과 같은 더 복잡한 에이전트 시스템으로 확장될 수 있다.\n\n### **6 코드 및 데이터**\n\n\n[코드는 GitHub(https://github.com/shuvom-s/e-valuator)에](https://github.com/shuvom-s/e-valuator) 공개되었으며, 추가로 Python 패키지인 `e-valuator` 로\n[PyPi(https://pypi.org/project/e-valuator/)에서](https://pypi.org/project/e-valuator/) 이용 가능합니다.\n\n### **7 감사의 말씀**\n\n\n유용한 피드백과 토론을 제공해 주신 Bonnie Berger, Ian Waudby-Smith, Kexin Huang, Divya Shanmugam, Kyunghyun Cho,\nManish Raghavan, Recht Lab, Chang Ma께 감사드립니다. 본 연구는\nS.S.와 D.P.가 Genentech에서 인턴으로 근무하던 중 수행되었습니다.\n\n\n11\n**참고문헌**\n[1] Anastasios N Angelopoulos, Stephen Bates, Emmanuel J Cande`s, Michael I Jordan, Lihua Lei. 학습 후 테스트: 위험 관리를 위한\n예측 알고리즘 보정. _응용통계학연보_, 19(2):1641–1662, 2025.\n\n\n[2] 안나 바바레스코, 라파엘라 베르나르디, 레오나르도 베르톨라치, 데스몬드 엘리엇, 라켈 페르난데스, 앨버트 개트, 에삼 갈레브,\n마리오 줄리아넬리, 마이클 한나, 알렉산더 콜러 외. 인간 심사관 대신 대규모 언어 모델? 20개 자연어 처리 평가 과제에 걸친\n대규모 실증 연구. _arXiv 사전인쇄본 arXiv:2406.18403_, 2024.\n\n\n[3] 조지 A. 베키. 자율 로봇에 관하여. _지식공학리뷰_, 13(2):143–146, 1998.\n\n\n[4] S Bickel, M Bruckner, T Scheffer. 공변량 이동 하에서의 판별적 학습. _J. Mach. Learn. Res._, 10:2137–2155, 2009.\n\n\n[5] 노암 브라운, 투오마스 샌드홀름. 멀티플레이어 포커를 위한 초인적 인공지능. _Science_, 365(6456):885–890, 2019.\n\n\n[6] 존 체리안, 아이작 깁스, 엠마누엘 칸데스. 강화된 준정형 예측법을 통한 대규모 언어 모델 유효성 검증. _신경정보처리시스템발전_,\n37:114812–114842, 2024.\n\n\n[7] 벤 처그, 산티아고 코르테스-고메즈, 브라이언 와일더, 아디티야 람다스. 베팅을 통한 공정성 검증.\n\n_신경정보처리시스템의발전_, 36:6070–6091, 2023.\n\n\n[8] Charles J Clopper and Egon S Pearson. 이항 분포 사례를 통한 신뢰 구간 또는 기준 구간의 활용. _Biometrika_, 26(4):404–413, 1934.\n\n\n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob\nHilton, Reiichiro Nakano 외. 수학 단어 문제 해결을 위한 검증자 훈련. _arXiv 사전인쇄본 arXiv:2110.14168_, 2021.\n\n\n[10] 안토니아 크레스웰, 머레이 샤나한, 이리나 히긴스. 선택 추론: 해석 가능한 논리적 추론을 위한 대규모 언어 모델 활용. _arXiv_\n_사전인쇄본 arXiv:2205.09712_, 2022.\n\n\n[11] 알렉산더 필립 다위드와 앨런 M 스킨. EM 알고리즘을 이용한 관측자 오류율의 최대우도추정. _왕립통계학회지: 시리즈 C (응용통계_\n_학)_, 28(1):20–28, 1979.\n\n\n[12] Dean P Foster and Robert A Stine. α-투자: 예상 허위 발견률의 순차적 통제를 위한 절차. _J. R. Stat. Soc. Series B Stat. Methodol._,\n70(2):429–444, 2008년 4월.\n\n\n[13] Senay A Gebreab, Khaled Salah, Raja Jayaraman, Muhammad Habib ur Rehman, Samer Ellaham. 의료 행정 업무 자동화를 위\n한 LLM 기반 프레임워크. _2024 제12회디지털포렌식및보안국제심포지엄(ISDFS)_, 1–7쪽. IEEE, 2024년 4월.\n\n\n[14] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix\nWeissenberger, Keran Rong, Ryutaro Tanno 외. 인공지능 공동 과학자 구축을 향하여. _arXiv 사전인쇄본 arXiv:2502.18864_, 2025.\n\n\n[15] Peter Grünwald, Rianne de Heide, Wouter M Koolen. 안전한 테스트. _2020 정보이론및응용워크숍(ITA)_, 1–54쪽. IEEE, 2020.\n\n\n[16] Michael U Gutmann and Aapo Hyväriinen. 노이즈 대비 추정법을 이용한 비정규화 통계 모델 추정 및 자연 이미지 통계 적용. _J._\n\n_Mach. Learn. Res._, 13(11):307–361, 2012.\n\n\n[17] 댄 헨드릭스, 콜린 번스, 사우라브 카다바스, 아쿨 아로라, 스티븐 바사트, 에릭 탕, 던 송, 제이콥 스타인하르트. 수학 데이터셋\n을 활용한 수학적 문제 해결 측정. _arXiv 사전인쇄본 arXiv:2103.03874_, 2021.\n\n\n12\n[18] 황커신, 진잉, 리라이언, 리마이클, 칸데스엠마누엘, 레스코베크주레. 에이전트 기반 순차적 반증에 의한 자동화된 가설 검증. _제42회_\n_국제기계학습컨퍼런스_, 2025.\n\n\n[19] 황커신, 장세레나, 왕한천, 추위안하오, 루잉저우, 유수프 루하니, 리라이언, 추린, 리개빈, 장준제 외. Biomni: 범용 생의학 AI\n에이전트. _biorxiv_, 2025.\n\n\n[20] 살림 이 아무쿠, 톰 뷰리, 사우미트라 미슈라, 프레디 르큐, 다니엘레 마가제니, 마누엘라 벨로소. 라벨 없이 순차적 유해 변동 탐지.\n\n_신경정보처리시스템발전_, 37:129279–129302, 2024.\n\n\n[21] 장수용, 박상돈, 이인섭, 오스버트 바스타니. 분류기 두 표본 검정을 이용한 순차적 공변량 이동 탐지. _국제기계학습학회_, 9845–9880\n쪽. PMLR, 2022.\n\n\n[22] Adel Javanmard, Andrea Montanari. 거짓 발견률 및 거짓 발견 초과율 통제를 위한 온라인 규칙. _Ann. Stat._, 46(2):526–554, 2018.\n\n\n[23] 디시 지, 패드레이크 스미스, 마크 스테이버스. 공정성 지표를 신뢰할 수 있을까? 라벨링되지 않은 데이터와 베이지안 추론을 통한\n공정성 평가. _신경정보처리시스템발전_, 33:18600–18612, 2020.\n\n\n[24] 진디, 이린판, 나심우파톨레, 웡웨이훙, 팡한이, 피터졸로비츠. 이 환자는 어떤 질병을 앓고 있을까? 의학 시험에서 추출한 대\n규모 오픈 도메인 질문응답 데이터셋. _응용과학_, 11(14):6421, 2021.\n\n\n[25] 라메시 조하리, 피트 쿠멘, 레오니드 페켈리스, 데이비드 월시. A/B 테스트의 미리 보기: 그 중요성과 대처 방안. _제23회 ACM_\n_SIGKDD 국제지식발견및데이터마이닝컨퍼런스논문집_, 1517–1525쪽, 2017.\n\n\n[26] 무함마드 칼리파, 리샤브 아가르왈, 라자누겐 로게스와란, 김재겸, 펑하오, 이문태, 이홍락, 왕루. 사고하는 프로세스 보상 모델\n. _arXiv 사전인쇄본 arXiv:2504.16828_, 2025.\n\n\n[27] Shi Xuan Leong, Caleb E Griesbach, Rui Zhang, Kourosh Darvish, Yuchi Zhao, Abhijoy Mandal, Yun-heng Zou, Han Hao,\nVarinia Bernales, Ala´n Aspuru-Guzik. 안전한 자율 주행 실험실을 향한 조향. _Nat. Rev. Chem._, 9(10):707–722, 2025년 10월.\n\n\n[28] 리 다웨이, 장 보한, 황 량지에, 알리모하마드 베이기, 자오 청슈아이, 탄 젠, 암리타 바타차르지, 장 위쑤안, 천위 첸, 우 톈하오 외. 생\n성에서 판단으로: 판사 역할을 하는 대규모 언어 모델의 기회와 도전. _2025년자연어처리경험적방법론컨퍼런스논문집_, 2757–\n2791쪽, 2025.\n\n\n[29] Wendi Li 및 Yixuan Li. Q-값 순위를 이용한 프로세스 보상 모델. _제13회학습표현국제컨퍼런스_, 2025.\n\n\n[30] Lichess.org. Lichess 정확도 측정 기준. `[https://lichess.org/page/accuracy](https://lichess.org/page/accuracy)`, 발행일 미상. 접속일: 2025-11-17.\n\n\n[31] 헌터 라이트먼, 비닛 코사라주, 유리 부르다, 해리슨 에드워즈, 보웬 베이커, 테디 리, 얀 라이케, 존 슐만, 일리아 수츠케버, 칼 코베.\n\n단계별 검증 방법. _제12회국제학습표현컨퍼런스_, 2023.\n\n\n[32] David Lopez-Paz and Maxime Oquab. 분류기 두 표본 검정 재검토. _arXiv 사전인쇄본 arXiv:1610.06545_, 2016.\n\n\n[33] Gary Lorden. 분포 변화에 대응하기 위한 절차. _수학통계연보_, 1897–1908쪽, 1971.\n\n\n[34] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, James Zou. Octotools: 복잡한 추론을 위한 확장 가능한 도구를 갖춘\n에이전트 프레임워크. _arXiv 사전인쇄본 arXiv:2502.11271_, 2025.\n\n\n13\n[35] 싱 한 루, 아미르호세인 카제므네자드, 니콜라스 미드, 아킬 파텔, 신동찬, 알레한드라 잠브라노, 카롤리나 스탄차크, 피터 쇼, 크리스\n토퍼 J. 팔, 시바 레디. Agentrewardbench: 웹 에이전트 궤적의 자동 평가 평가. _arXiv 사전인쇄본 arXiv:2504.08942_, 2025.\n\n\n[36] Christopher Mohri and Tatsunori Hashimoto. 언어 모델의 사실성 보장을 위한 등각 변환 적용.\n\n_arXiv 프리프린트 arXiv:2402.10978_, 2024.\n\n\n[37] Siddharth Narayanan, James D Braza, Ryan-Rhys Griffiths, Manu Ponnapati, Albert Bou, Jon Laurent, Ori Kabeli, Geemi\nWellawatte, Sam Cox, Samuel G Rodriques 외. Aviary: 훈련 언어 에이전트에 대한 도전적인 과학적 과제. _arXiv 사전인쇄본_\n_arXiv:2412.21154_, 2024.\n\n\n[38] 예르지 네이만과 에곤 샤프 피어슨. 통계적 가설 검정의 가장 효율적인 방법에 관한 문제. _런던왕립학회철학논문집. 수학또는물_\n_리학관련논문시리즈 A_, 231(694-706):289–337, 1933.\n\n\n[39] Young-Jin Park, Kristjan Greenewald, Kaveh Alim, Hao Wang, and Navid Azizan. 모르는 것을 알라: 프로세스 보상 모델의 불확\n실성 보정. _arXiv 사전인쇄본 arXiv:2506.09338_, 2025.\n\n\n[40] 알렉산드르 포드코파예프와 아디티야 람다스. 배포된 모델의 위험 추적 및 유해한 분포 변화 탐지. _arXiv_ _사전인쇄본_\n_arXiv:2110.06177_, 2021.\n\n\n[41] 드류 프린스터, 싱 한, 안치 류, 수치 사리아. Watch: 가중-준등방성 마팅게일을 통한 AI 배포를 위한 적응형 모니터링. _제42회국제기_\n_계학습컨퍼런스_, 2025.\n\n\n[42] Yuanhao Qu, Kaixuan Huang, Ming Yin, Kanghong Zhan, Dyllan Liu, Di Yin, Henry C Cousins, William A Johnson,\nXiaotong Wang, Mihir Shah, Russ B Altman, Denny Zhou, Mengdi Wang, Le Cong. 유전자 편집 실험의 에이전트 자동화를\n위한 CRISPR-GPT. _Nat. Biomed. Eng._, pp. 1–14, 2025년 7월.\n\n\n[43] Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S Jaakkola, Regina Barzilay. 등각 언어 모델링. _arXiv_\n_사전인쇄본 arXiv:2306.10193_, 2023.\n\n\n[44] 아디티야 람다스와 루오두 왕. e-값을 이용한 가설 검정. _arXiv 사전인쇄본 arXiv:2410.23614_, 2024.\n\n\n[45] 아디티야 람다스, 티야나 즈르니치, 마틴 웨인라이트, 마이클 조던. SAFFRON: 거짓 발견률의 온라인 제어를 위한 적응형 알고리즘\n. _국제기계학습컨퍼런스_, 4286–4294쪽. PMLR, 2018년 7월.\n\n\n[46] 아디티야 람다스, 피터 그룬발트, 블라디미르 보브크, 글렌 셰이퍼. 게임 이론적 통계학과 안전한 언제든지 유효한 추론. _통계과학_\n, 38(4):576–601, 2023.\n\n\n[47] 모나 쉬르머, 메토드 야즈벡, 크리스티안 A. 네세스, 에릭 날리스닉. 테스트 시간 적응에서의 위험 모니터링. _arXiv 사전인쇄본_\n_arXiv:2507.08721_, 2025.\n\n\n[48] Glenn Shafer, Vladimir Vovk. \"준형 예측에 관한 튜토리얼.\" _Journal of Machine Learning Research_, 9(3), 2008.\n\n\n[49] 디비야 샨무감, 슈봄 사두카, 마니쉬 라가반, 존 거타그, 보니 버거, 엠마 피어슨. 라벨링된 데이터와 라벨링되지 않은 데이터를 활용\n한 다중 모델 평가. _arXiv 사전인쇄본 arXiv:2501.11866_, 2025.\n\n\n[50] 신재혁, 아디티야 람다스, 알레산드로 리날도. E-detectors: 순차적 변화 감지를 위한 비모수적 프레임워크. _뉴잉글랜드데이터과학_\n_통계저널_, 2023.\n\n\n[51] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas\nBaker, Matthew Lai, Adrian Bolton 외. 인간 지식 없이 바둑 게임 마스터하기. _nature_, 550(7676):354–359, 2017.\n\n\n14\n[52] 카일 스완슨, 웨슬리 우, 내시 L 불라옹, 존 E 박, 제임스 저우. AI 에이전트의 가상 실험실이 새로운 SARS-CoV-2 나노바디를\n설계하다. _네이처_, 1–3쪽, 2025.\n\n\n[53] 진진 티안, 아디티야 람다스. ADDIS: 보수적 무효 가설을 위한 온라인 FDR 제어를 위한 적응형 폐기 알고리즘. _신경정보처리시스_\n_템발전_, 32, 2019.\n\n\n[54] 알렉산더 티만스, 라지브 베르마, 에릭 날리스닉, 크리스티안 A. 네세스. 알려지지 않은 시프트 하에서의 위험 위반 지속적 모\n니터링에 관하여. _arXiv 사전인쇄본 arXiv:2506.16416_, 2025.\n\n\n[55] 조너선 우에사토, 네이트 쿠시먼, 라마나 쿠마르, 프랜시스 송, 노아 시겔, 리사 왕, 안토니아 크레스웰, 제프리 어빙, 이리나 히\n긴스. 과정 및 결과 기반 피드백을 통한 수학 단어 문제 해결. _arXiv 사전인쇄본 arXiv:2211.14275_, 2022.\n\n\n[56] 장 빌. 집단 개념에 대한 비판적 연구. 1939.\n\n\n[57] 블라디미르 보브크와 루오두 왕. E-값: 보정, 결합 및 응용. _통계연보_, 49(3):1736–1754, 2021.\n\n\n[58] 블라디미르 보브크와 루오두 왕. e-값을 통한 신뢰도와 발견. _통계과학_, 38(2): 329–354, 2023.\n\n\n[59] 블라디미르 보브크, 이반 페테이, 일리아 누레트디노프, 에른스트 알베르그, 라스 칼슨, 알렉스 가머만. 재훈련할 것인가, 말 것인가:\n\n변화점 탐지를 위한 등각 검정 마팅게일. 《 _등각및확률적예측과응용_ 》, 191–210쪽. PMLR, 2021.\n\n\n[60] 아브라함 월드와 제이콥 울포위츠. 순차적 확률비 검정의 최적 특성. _수리통계학연보_, 326–339쪽, 1948.\n\n\n[61] 한천 왕, 이춘 허, 폴라 P. 코엘류, 매튜 부치, 압바스 나지르, 밥 첸, 린 트린, 세레나 장, 케신 황, 비니트크리슈나 찬드라세카르\n외. Spatialagent: 공간 생물학을 위한 자율 AI 에이전트. _bioRxiv_, 2025–04쪽, 2025.\n\n\n[62] 홍젠 왕(Hongjian Wang)과 아디티야 람다스(Aaditya Ramdas). 분산이 알려지지 않은 가우시안 평균에 대한 언제든지 유효한\nt-검정 및 신뢰도 시퀀스. _시퀀셜애널리시스(Sequential Analysis)_, 44(1):56–110, 2025.\n\n\n[63] 왕페이이, 리레이, 샤오즈홍, 쉬런신, 다이다마이, 리이페이, 천더리, 우위, 수이즈팡. Math-shepherd: 인간 주석 없이 단계별로 대규\n모 언어 모델 검증 및 강화. _컴퓨터언어학협회제62차연례회의논문집(제1권: 장문논문)_, 9426–9439쪽, 2024.\n\n\n[64] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan\nHe, Ziyan Jiang 외. Mmlu-pro: 보다 견고하고 도전적인 다중 작업 언어 이해 벤치마크. _신경정보처리시스템의발전_, 37:95266–\n95290, 2024.\n\n\n[65] Larry Wasserman, Aaditya Ramdas, Sivaraman Balakrishnan. 범용 추론. _Proceedings of_ _the National Academy of Sciences_,\n117(29):16880–16890, 2020.\n\n\n[66] Ian Waudby-Smith, Edward H Kennedy, Aaditya Ramdas. 분포 균일한 언제든지 유효한 순차적 추론. _arXiv_ _사전인쇄본_\n_arXiv:2311.03343_, 2023.\n\n\n[67] Ian Waudby-Smith, Lili Wu, Aaditya Ramdas, Nikos Karampatziakis, Paul Mineiro. 상황 기반 밴디트에 대한 언제든지 유효한 오프\n-폴리시 추론. _ACM/IMS 데이터과학저널_, 1(3):1–42, 2024.\n\n\n[68] Peter Welinder, Max Welling, Pietro Perona. 벤치마킹을 위한 게으른 사람의 접근법: 반감독 학습 분류기 평가 및 재조정. _IEEE 컴_\n_퓨터비전및패턴인식학회논문집_, 3262–3269쪽, 2013.\n\n\n[69] Menghua Wu, Cai Zhou, Stephen Bates, Tommi Jaakkola. 사고 보정: 효율적이고 확신 있는 테스트 시간 스케일링. _arXiv 사전인쇄_\n_본 arXiv:2505.18404_, 2025.\n\n\n15\n[70] 양지린, 치펑, 장사이정, 요슈아 벤지오, 윌리엄 W. 코헨, 루슬란 살라흐트디노프, 크리스토퍼 D. 매닝. HotpotQA: 다양하고\n설명 가능한 다단계 질문응답을 위한 데이터셋. _arXiv 사전인쇄본 arXiv:1809.09600_, 2018.\n\n\n[71] 예팡화, 양밍밍, 팡젠후이, 왕롱위, 데릭 웡, 에미네 일마즈, 시슈밍, 투자오펑. 불확실성 정량화를 통한 대규모 언어 모델 벤치마킹.\n\n_신경정보처리시스템발전_, 37:15356–15385, 2024.\n\n\n[72] _위_ 취 유, 안톤 쉬에, 슈레야 하발다르, 델립 라오, 헬렌 진, 크리스 캘리슨-버치, 에릭 웡. 대규모 언어 모델 추론 체인에서의 확률적 타\n당성 보증. _2025년자연어처리경험적방법론컨퍼런스논문집_, 7517–7536쪽, 2025.\n\n\n[73] 정추제, 장전루, 장베이천, 린룬지, 루커밍, 유보원, 류다이헝, 저우징런, 린준양. Processbench: 수학적 추론에서의 과정 오류 식별.\n\n_arXiv 사전인쇄본 arXiv:2412.06559_, 2024.\n\n### **8 부록**\n\n\n**8.1** **이론**\n**8.1.1** **제1정리** **증명**\n_p_ 0 및 _p_ 1 을 각각 실패한 궤적과 성공한 궤적의 검증자 점수 밀도로 정의한다.\n_p_ ≪ _p_ 라고 가정하고, M = p0 ((S [:t] ~~)~~ ) 및 _M_ = 1이라 정의한다. _c_\n_p_ ≪ _p_ 라고 가정하고, M = p0 ((S [:t] ~~)~~ ) 및 _M_ = 1이라 정의한다. _c_ = [1] 사용자 지정 _α_ ∈ (0 _,_ 1)에 대해. 그러면,\n0 1 t p1 (S [:t] ) 0 α α\nPrHN ( _∃t_ ∈ N : _M_ t ≥ _c_ α ) ≤ _α_ . 즉, 밀도 비율 과정 _M_ t 가 어느 시점에서든\n_c_ α = [1] α은 최대 _α이다_ .\n\n\n\nα\n_증명._ 여러 선행 연구에서 밀도 비율 과정이 시험 마팅게일이며 따라서 H(N)에 대한 e-과정임을 증명하였다. 이 사실에 대한 완전한 증명은\n[62]의 Lemma 2.6을 참조하라. 여기서는 요약된 버전을 제시한 후, Ville의 부등식 [56]을 적용하여 오경보율(식 (1))에 대한 언제든\n지 제어를 달성한다.\n\n자연 여과 F t= _σ_ ( **S** ([1:], t, ]) )이라 하자. 우리는 ( _M_ t)t∈N이 H N: **S** ∼ _P_ 1 에대한테스트 마틴게일의 정의를 만족함을 보여준다. 먼저, 밀도\n비율은 항상 비음수이므로 _M_ t는 항상 비음수임을 주목한다. 또한 정의상 _M_ 0 = 1이므로, E HN[ _M_ 0 ] = 1이 성립한다. 이제 ( _M_ t)t∈N이\nH N아래에서 마팅게일, 즉 **S** ∼ _P_ (1) 일 때 성립함을 증명한다. 우리는\nE [ _M_ | F ] = E\n_p_ 0 ( **S** [1: t ] )\n~~F~~\nt−1\nHN t t−1 HN _p_ 1 ( **S** [1: t ] )\np0 [(] **[S]** [1:t−1] [) ] [∫]\n1 t\n~~=~~\n[(] **[S]** [1:t−1] [) ] p0 [(] st [| ] **[S]** [1:t−1] [) ]\n_p_ 1 ( **S** [1:t−1] [)] _p_ 1 ( _s_ t | **S** [1:t−1] ~~_p_~~ ( _s_ | **S** [)] 1 t [1:t−1] ) _ds_ t\n∫\n= Mt−1 p0 [(] st [| ] **[S]** [1:t−1] [) ] dst\n= Mt−1\n여기서 세 번째 등식은 밀도 함수의 적분이 1이므로 성립한다.\n\n따라서 과정 ( _M_ t )t∈N 은 귀무 가설에 대한 시험 마팅게일이다. 빌의 부등식[56]은 모든 _α_ ∈ (0 _,_ 1)에 대해 다음과 같이 명시한다:\n\n\n\nPrHN sup _M_ t\n~~≥~~ [ E] ~~[H]~~ [N][ [] _α_ _[M]_ [0] [ ]] ≤ _α_ E HN [ _M_ 0 ] _._\n\n\n\nt∈N\nE H[ _M_ 0 ] = 1을 대입하면 PrH\nE H[ _M_ 0 ] = 1을 대입하면N PrH supt∈N _M_ t ≥ [1] ≤ _α_ N.  따라서 _c_ α= [1] ~~일~~ 때α, 우리는\nPrHN [ _∃t_ ∈ N : _M_ t ≥ _c_ α ] = PrHN [supt∈N _M_ t ≥ _c_ α ] =≤ _α_ .\n\n\n\nsupt∈N _M_ t ≥ [1]\nN N α α\n16\n**8.1.2** **제2정리** **증명**\np0 (S [1:t] ()) 는 ~~로~~ 그 최적 성장률을 제공한다. 즉, 다른 어떤\n=\n_M_ t에 의해 주어지는 밀도 비율 과정\np 1  [1: (S t] )\nt 1  [1: ( t] )\ne-process ( _M_ [′] ) [∞] 및 정지 시간 _τ_, _E_ H [log _M_ τ ] ≥ _E_ H [log _M_ [′] ].\n\n\n\nt t=1 A A τ\nA A\n_증명._ 이 정리의 완전한 증명은 [44]의 정리 7.11에 실려 있다. 엄밀한 증명에 대해서는 해당 교재를 참조하기 바란다. 그럼에도 불구\n하고, 여기서 핵심적인 직관을 제시한다.\n\n\n\n이 결과는 _P_ 1 대 _P_ (0) 에대한 비순차적 가설 검정에서 다음과 같은 사실의 확장이다:\n_P_ 0 ≪ _P_ 1 일 때, 가능도 비율 _E_ = _p_ 0 _/p_ 1 이 로그-최적 e-변수임을 확장한 것이다: E P[log _E_ [′] ] ≤ E P[log _E_ ] (다른 모든 e-변수에 대해 성립).\n\n\n\n0 0\ne-변수 _E_ [′] 는 _P_ 1 에 대해 정의된다. 이를 보이기 위해, 먼저 Q의 분포 Q에 대해 Q ≪ P [(] 0) ≪ P(1) 을 만족하는 Q에 대한 _E_ [(′] ) = _dQ/dP_ 1 형태의 e-변수\n를 고려하는 것으로 충분함을 주목한다.\n_Q_ ≪ _P_ 0 ≪ _P_ 1 을 만족하는 분포 _Q에_ 대해 E (′)  = dQ/dP(1) 형태의e-변수를 고려하는 것으로 충분합니다. 우리는\n_L_ ( _dx_ ) = \n∫\n=\n∫\n_p_ ( _x_ ) log _[p]_ [0] [(] _[x]_ [)]\nE\nlo ~~g~~ = _p_ ( _x_ ) log _L_ ( _dx_ ) =   - _p_ ( _x_ ) log _L_ ( _dx_ ) ≤ 0 _,_ (5)\nHA _E_ [′] 0 _p_ 0 ( _x_ ) _/p_ 1 ( _x_ ) 0 _q_ ( _x_ )\nlo ~~g~~ _[E]_\n_p_ ( _x_ ) log _[ q]_ [(] _[x]_ [)] _[/p]_ [1] [(] _[(x)]_ [)]\n여기서 _L은_ 밀도 함수 _p_ 1 _, p_ 0 _, q_ 가 정의되는 기준 측도이다. 즉, E HA[log _E_ [′] ] ≤\nE HA[log _E_ ].\n\n여기서 이 진술을 e-process의 순차적 설정으로 확장할 수 있으며, 이는 참고문헌의 정리 7.11에서 수행된다.\n\n\n**알고리즘 3** e-평가자에 대한 대략적으로 올바른(PAC) 임계값.\n\n**입력:** 오차 수준, _δ_ ∈ [0 _,_ 1]; 분위수 수준, _α_ ∈ [0 _,_ 1]; 교정 데이터, D threshold ; 각 단계 _z, t_ = 1 _, . . ., T_ max에서 밀도 비율을 추정하는 함수\n들.\n\n**출력:** 높은 확률로 언제든지 유효한 e-평가기를 산출하는 결정 임계값, _c_ α\n1: _i_ ← 0\n2: **for** ( **S** _, Y_ ) ∈ D threshold : _Y_ = 1 **do**\n3: _i_ ← _i_ + 1\n4: _M_ 1 _, ..., M_ T ← [Mˆ] 1 ( **S** 1 ) _, ...,_ [M ˆ] T  ( **S** [1: T ] )\n5: _M_ [(] [i] [)] ← maxt _M_ t\n6: **for 종료**\n7: _n_ ← _i_\n8: _M_ [(1)] _, ..., M_ [(] [n] [)] 를 오름차순으로 정렬하여 _M_ (1) ≤ · · · ≤ _M_ ( n ) 가 되도록 한다. 동점 발생 시 공정한 동전 던지기로 결정한다.\n\n9: _k_ ← min { _i_ ∈ [ _n_ ] : Pr[Bin( _n,_ 1  - _α_ ) ≥ _i_ ] ≤ _δ_ }\n10: _c_ α ← _M_ ( k )\n**8.1.3** **제3정리** **증명**\n알고리즘 3에 따라 _c_ α를 선택하자. 그러면,\nPrDcal (PrHN ( _∃t_ : _M_ t ≥ _c_ α |D cal ) ≤ _α_ ) ≥ 1                       - _δ._\n\n\n\n_증명._ 우리의 교정 집합이 귀무 가설에 따라 독립 동일 분포(i.i.d.)인 _n개의_ 성공적 궤적을 포함한다고 가정하자. PrHN ( _∄t_ : _M_ t\n≥ _c_ α |D cal ) = PrHN (maxt∈N _M_ t ≤ _c_ α |D cal )임을 유의하라. 이는 전체 시퀀스\n( _M_ t) [∞] t=0 가 _c_ α미만이라는 사건은 최대값이 _c_ α미만이라는 사건과 동등하다. 따라서 다음을 고려하면 충분하다.\n모든 단계에 걸친 최대 점수를 고려하는 것으로 충분하다.\n\n\n\nt=0\n교정 세트 최대값 _M_ [(1)] _, . . ., M_ [(] [n] [()) ] [를] 계산하라. 이 최대값들, _M_ [(] [i] [)] _, i_ = 1 _, . . ., n_ 은 (알려지지 않은) 누적분포함수 _F_ 를 갖는\n어떤 분포로부터 독립동일분포(i.i.d.)임을 유의하십시오. 이 분포의 (1 - _α_ )-분위수를 다음과 같이 정의합니다.\n\n\n_q_ 1 −α := inf { _x_ ∈ R : _F_ ( _x_ ) ≥ 1                 - _α_ } _,_\n_F_ ( _q_ 1 −α - ) ≤ 1 - _α_ ≤ _F_ ( _q_ 1 −α )를 만족하도록, 여기서 _F_ ( q− ) := limx↑q _F_ ( _x_ ). 우리의 목표는 교정 데이터를 사용하여 _q_ 1 −(α) 에 대한(1\n\n- _δ_ )-신뢰 상한을 구성하는 것이다. 즉, 다음과 같은 _c_ α 를 찾게 된다.\n\nPrDcal ( _c_ α _< q_ 1 −α ) ≤ _δ._ (6)\n17\n그러면, 사건 { _c_ α ≥ _q_ 1 −α } 에 대해, 우리는 다음을 얻는다.\n\n\n\n= 1 - _F_ ( _q_ 1 −α - ) ≤ 1 - (1 - _α_ ) = _α._ (7)\n≤ PrHN\nmax _M_ t ≥ _q_ 1 −α D cal\nPrHN\nmax _M_ t ≥ _c_ α | D cal\nt∈N t∈N\n구성상 사건 { _c_ α ≥ _q_ 1 −α } 는 최소한 1 - _δ의_ 확률로 발생하므로, 다음이 성립한다.\n\n\n\nPrDcal(PrHN( _∃t_ : _M_ t ≥ _c_ α |D cal) ≤ _α_ ) = PrDcal\nPrHN\nmax _M_ t ≥ _c_ α D cal\nt∈N\n≤ _α_\n≥ 1 - _δ_ (8)\n원하는 대로.\n\n우리는 Clopper와 Pearson[8]의 아이디어를 따르는 다음 논증을 사용하여 _q_ 1 −α에 대한 (1  - _δ_ )-신뢰 상한을 구성한다. _M_ (1)≤\n_M_ (2)≤ · · · ≤ _M_ ( (n )) 을 교정 점수 최대값의 순서 통계량으로 정의하며, 동점자는 연속된 순위를 차지한다. _c_ α를 다음과 같이 이러\n한 순서 통계량 중 하나로 설정한다. _K_ ( _x_ ) := # { _i_ : _M_ [(] [i] [)] _< x_ } 로 표기한다. 모든 _x_ ∈ R 에 대해, _K_ ( _x_ ) ∼ Binomial( _n, F_ ( x− )) 이다.\n그러면 모든 _k_ 와 _x_ 에 대해, 사건 { _M_ ( k ) _< x_ } 와 { _K_ ( _x_ ) ≥ _k_ } 는 동등하므로,\nPrDcal( _M_ ( k ) _< x_ ) = PrDcal( _K_ ( _x_ ) ≥ _k_ ) = Pr(이항분포( _n, F_ ( x− )) ≥ _k_ ) _._ (9)\n특히, 모든 _k_ 에 대해, 분위수 _q_ 1 −α 에서 우리는\nPrDcal ( _M_ ( k ) _< q_ 1 −α ) = Pr(Binomial( _n, F_ ( _q_ 1 −α         - )) ≥ _k_ ) ≤ Pr(Binomial( _n,_ 1         - _α_ ) ≥ _k_ ) _,_ (10)\n부등식이 성립하는 이유는 _F_ ( _q_ 1 −α - ) ≤ 1 - α이며, Pr(이항분포( _n, q_ ) ≥ _k_ ) _가 q에대해_ 비감소함수이기 때문이다. 따라서 알고리\n즘 3은 _k_ [⋆] = min { _k_ : Pr(이항분포( _n,_ 1 - _α_ ) ≥ _k_ ) ≤ _δ_ } 를 선택하고 _c_ α = _M_ ( k ⋆ ) 로 설정한다. 이는 식 (6)이 성립하는 최소 순서통계\n량이다. 즉, 정의에 따라 _c_ (α) 는다음을 만족한다.\n\nPrD (cal _c_ α _< q_ 1 −α ) = PrD ( _M_ ( calk ⋆ ) _< q_ 1 −α ) ≤ Pr(이항분포( _n,_ 1               - _α_ ) ≥ _k_ [⋆] ) ≤ _δ,_ (11)\n원하는 대로.\n\n\n**8.1.4** **간단한** **예시: 한계** **보정은** **오경보율을** **제어하지** **못함**\n검증자 점수 _S가다음조건을_ 만족하면 _부분교정을_ 충족한다.\n\n\n_p_ ( _Y_ = 1 | _S_ ) = _S_ (12)\n거의 확실하게. 흔히 사용되는 확률적 \"정확성\" 개념인 한계 보정은 무효 가설과 대립 가설의 기본 확률(즉, _p_ ( _Y_ = 1)과 _p_ ( _Y_ = 0))을\n고려하지 않기 때문에 오경보율을 제어할 수 없다. 이는 순차적 가설 검정 설정에서 각 _단계 t에서 S_ t의 한계 _보정이_ 언제든지 오경보\n율을 제어할 수 없게 하는 경우와 동일하며, 다음 예시가 보여주는 것처럼 단순한 비순차적 설정에서도 마찬가지이다.\n\n검증자 점수 _S_ ∈ [0 _,_ 1]이 두 가지 값만 취한다고 가정하자: _S_ ∈ { 0 _._ 005 _,_ 0 _._ 5 }, 여기서 _p_ ( _S_ = 0 _._ 005) = 0 _._ 99이고 _p_ ( _S_ = 0 _._ 5) =\n0 _._ 01이다. 검증자 점수는 한계 교정되어 있으므로, _p_ ( _Y_ = 1 | _s_ = 0 _._ 005) = 0 _._ 005이고 _p_ ( _Y_ = 1 | _s_ = 0 _._ 5) = 0 _._ 5이다. 오탐률을 제어\n하기 위한 단순한 시도로서, _p_ (거절 | _Y_ = 1) ≤ _α_ = 0 _._ 01을 충족하도록 _S_ ≤ _α_ = 0 _._ 01일 때마다 거절하기로 결정합니다. 이제 결\n과적인 오탐률을 계산합니다.\n\n먼저, 귀무 가설의 기본 확률은\n_p_ ( _Y_ = 1) = _p_ ( _Y_ = 1 | _S_ = 0 _._ 005)       - _p_ ( _S_ = 0 _._ 005) + _p_ ( _Y_ = 1 | _S_ = 0 _._ 5)       - _p_ ( _S_ = 0 _._ 5)\n= 0 _._ 005               - 0 _._ 99 + 0 _._ 5               - 0 _._ 01\n= 0 _._ 00995 _._\n\n\n18\n거짓 경보율은 _p_ (거절 | _Y_ = 1)이며, 이는 _p_ ( _S_ = 0 _._ 005 | _Y_ = 1)과 동등하다. 왜냐하면 우리는\n_S_ ≤ 0 _._ 005에서 기각하기 때문이다. 그러나\n_p_ (거절 | _Y_ = 1) = _p_ ( _S_ = 0 _._ 005 | _Y_ = 1)\n_p_ ( _Y_ = 1 | _S_ = 0 _._ 005) _p_ ( _S_ = 0 _._ 005)\n=\n_p_ ( _Y_ = 1)\n0 _._ 005                          - 0 _._ 99\n=\n0 _._ 00995\n≈ 0 _._ 50 ≫ 0 _._ 01 _._\n\n\n따라서, 검증자 점수 _S가_ 약간만 보정된 경우에도, _p_ ( _Y_ = 1 | _S_ ) ≤ _α일_ 때 귀무가설을 기각하는 것은 오탐률을 통제하지 못한다.\n\n\n19\n원시 검증기\n본페로니 검정\n보정 검증기\nE- 평가자 (1/ 임계값, 당사 )\nE- 평가기 (PAC 임계값, 본 연구 )\n정확한 제어\n\n\n0. 7\n오경보율 \n0.7\n\n\n\n\n\n0. 0\n\n\n1. 0\n\n\n\n0. 0\n\n\n1. 0\n\n\n\n\n\n0. 0\n\n|Col1|Col2|GSM8K|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n|0<br>0.|1<br>0.|2<br>0.|3<br>0.|4<br>0.|\n||||||\n||||||\n||||||\n||||||\n||||||\n|~~ 0~~<br>~~0. ~~|~~ 1~~<br>~~0. ~~|~~ 2~~<br>~~0. ~~|~~ 3~~<br>~~0. ~~|~~ 4~~<br>~~0. ~~|\n\n\n\n( 명목상 오경보율 )\n\n\n\n0. 0\n\n|Col1|Col2|MATH|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n|0<br>0.|1<br>0.|2<br>0.|3<br>0.|4<br>0.|\n||||||\n||||||\n||||||\n||||||\n||||||\n|~~ 0~~<br>~~0. ~~|~~ 1~~<br>~~0. ~~|~~ 2~~<br>~~0. ~~|~~ 3~~<br>~~0. ~~|~~ 4~~<br>~~0. ~~|\n\n\n\n( 명목상 오경보율 )\n그림 5: **GSM8k 및 MATH 결과** . 두 가지 _e-평가기_ 변형 모두에서 오경보율은 경험적으로 제어됩니다. 또한 _e-평가기는_ 오경보율을 제어할\n수 있는 방법들 중에서 최적의 검정력을 달성합니다.\n\n\n**8.2** **추가** **결과**\n**8.2.1** **MATH 및 GSM8k 결과**\n그림 5에는 두 개의 추가 데이터셋인 MATH [17]과 GSM8k [9]의 결과를 제시합니다. _E-평가기는_ 모든 _α_ 선택에 대해 경험적으로\n오경보율을 제어하며, 오경보율 제어를 달성하는 방법들 중에서 최적의 검정력을 달성합니다.\n\n\n20\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-20-0.png)\n\n\n\n\n\n\n\n\n\n|보정 크기<br>1% (n=50)<br>5% (n=250<br>10% (n=50<br>20% (n=10|)<br>0)<br>00)|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|<br>40% (n=20|<br> 00)|<br> 00)|||||\n||||||||\n||||||||\n||||||방법<br>E-평가기 (1/임계값<br>계값)|) E-평가기 (PAC임|\n\n\n( 명목상 오경보율 )\n\n\n\n0. 0\n\n\n\n\n\n0. 0\n그림 6: **교정** **세트** **크기** . MATH 데이터셋에서 대부분의 교정 세트 크기에서 오경보율은 경험적으로 제어됩니다. 매우 작은 크기(데이터의 1% 또는\n50개의 라벨링된 궤적)에서는 _e-평가기가_ 오경보율을 제어하지 못할 수 있습니다.\n\n\n**8.2.2** **교정** **세트** **크기** **제거** **실험**\n교정 세트의 크기가 _e-평가기에_ 미치는 영향을 검토합니다. 교정 세트 D cal는\np0 (S [1:t] ) 의 밀도 비율을 학습하는 데 사용됩니다. 경험적 버전의 _e-평가자를_\n밀도 비율 [M ˆ] t ≈\ncal [D(DRE)]\n위해, 우리는 D\np 1  [1: (S t] )\n1  [1:t]\nD DRE 및 D threshold 로분할하여, 전자의 분할에서 밀도 비율을 학습하고, 거부 임계값을 추정합니다.\n후자에 대해. 우리의 밀도 비율이 학습된 것이기 때문에,\n교정 세트 크기가 증가함에 따라 더 정확해질 것으로 예상합니다.\n\n총 5000개의 궤적을 포함하는 MATH 데이터셋에서 이 제거 실험을 수행했습니다. 그림 6에서 볼 수 있듯이, 교정 세트의 크기\n는 경험적 오경보율과 검정력에 거의 영향을 미치지 않습니다. 그러나 교정 데이터가 매우 적은 양(1%, 즉 50개의 라벨링된 궤적)일\n경우, 밀도 비율이 더 노이즈가 많아지는 경향이 있어 오경보율과 검정력의 분산이 커집니다.\n\n교정 세트 크기가 증가함에 따라 오경보율이 대략 일정하게 유지되는 것을 관찰했습니다(모든 크기가 오경보율을 제어함).\n\n\n21\n\n\n# MATH 궤적 체스 궤적\n\n그림 7: **예시** **시퀀스** . MATH 데이터셋에서 많은 거절 사례가 첫 번째 행동 이후인 M1에서 발생하는데, 이는 에이전트의 첫 번째 행동이 성공\n여부를 결정하는 데 중요함을 시사한다. 체스 데이터셋에서는 [첫] [번째] [단계] [이후][ HN][과] _HA_ [간의] 차이가 [적지만] [점차] [그] [차이가] [커진다][.]\n**8.2.3** **예시** _M_ t **시퀀스**\n또한 MATH 및 체스 데이터셋에 대한 _M_ t시퀀스 예시(시각적 명확성을 위해 log( _M_ t)로 플롯)를 그림 7에 제시합니다. 두 데이터셋\n모두에서 백이 H (A)를 승리하지 못하는 실패한 궤적 또는 게임에서 log( _M_ t)가 증가하는 것을 관찰합니다. 반면, 백이 승리하는 성공적\n인 궤적이나 게임에서는 _M_ t시퀀스가 임계값 _1/α를_ 넘는 경우가 거의 없습니다. 그럼에도 시퀀스 간 시각적 이질성이 존재합니다. 일\n반적으로 더 강력한 검증기/PRM일수록 H N과 H A 궤적을 시각적으로 더 명확하게 구분할 것으로 예상됩니다.\n\n\n22\n\n\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-21-0.png)\n\n![](assets/md_images/할루시네이션-논문-3_E-valuator-Reliable-Agent-Verifiers-with-Sequential-Hypothesis-Testing-ko.pdf-21-1.png)\n**8.3** **데이터셋, 에이전트** **및** **검증자에** **대한** **세부** **사항**\n우리는 여섯 가지 서로 다른 데이터셋을 활용한 실험을 제공합니다. 각 데이터셋마다 특정 에이전트-검증기 조합을 사용하며, 이를\n표 8.3에 나열합니다.\n\n**수학적** **추론의** 경우, 도구 호출 에이전트 실험에는 GSM8k [9]를, 추론 모델 실험에는 MATH [17]를 사용합니다. **질문응답의** 경우,\nAviary 및 OctoTools 실험에는 각각 HotpotQA [70]와 MedQA [24]를, 추론 모델에는 MMLU-Pro [64]를 사용합니다. **체스** 실험에\n는 LiChess의 오픈소스 주석 처리된 대국 자료를 사용했습니다. 본론에서는 GSM8k를 제외한 모든 데이터셋의 결과를 제시하며,\nGSM8k 결과는 부록에 수록합니다.\n\n도구 호출 에이전트의 경우 검증자(판정 LLM)에게 원본 문제 텍스트와 사용된 도구 호출 및 인자 목록을 제공합니다. 이후\nClaude 에이전트에게 다음과 같은 시스템 프롬프트를 제시합니다:\n\n\n당신은 에이전트 궤적 분석 및 성공 확률 추정에 전문성을 지닌 전문가입니다. 최종 답변은 다음 형식으로 작성하십시오:\n\n`[` 확률 `]: [0` 과 `1` 사이의 숫자 `]`\n확률 값은 0과 1 사이의 숫자여야 합니다. 답변은 반드시\n`[` 확률 `]: [0` 과 `1` 사이의 숫자 `]` 로만 제한하십시오. 그렇지 않으면 OpenAI로 전환하겠습니다.\n\n\n해당 에이전트는 LLM 기반 에이전트로, 문제 해결을 위해 도구를 사용합니다. 에이전트는 총 { `max tool calls` }\n회 이상의 도구 호출을 할 수 없 ~~습~~ 니다. ~~이~~ 를 초과할 경우 오류와 함께 종료됩니다.\n\n\n이 에이전트가 사용할 수 있는 도구는 다음과 같습니다:\n\n#### • 답변 제출 • 검색 • 조회\n\n\n에이전트의 부분적인 행동 경로가 제공됩니다. 귀하의 임무는 이 부분 경로를 바탕으로 에이전트의 성공 확률을 추정\n하는 것입니다.\n\n\n확률 계산 시 다음 사항을 반영해야 합니다:\n\n#### • 에이전트가 사용한 도구들 • 에이전트가 사용한 인자, 인자의 구문 포함 • 문제 텍스트 • 허용된 총 도구 호출 횟수\n\n\n최종 답변 형식은 다음과 같습니다: `[` 확률 `]: [0` 과 `1` 사이의 숫자 `]`\n{ 부분 궤적 }\n추론 모델의 경우, 사전 훈련된 검증기에 추론 경로를 제공하면 각 단계 이후 궤적이 성공할 확률을 로짓 기반 확률로 출력합니\n다. 체스 검증기의 경우, 게임 기록을 Stockfish에 업로드하고 그 센티폰 점수[30]를 _e-valuator의_ 입력값으로 사용합니다. 이 점수를\n백의 승리 확률로 변환하기 위해 [30]에 발표된 다음 공식을 사용합니다:\n\n\n\n_p_ (백이 _이긴다|s_ ) ~~=~~ [ 1]\n2\ni 2 1+ _e_ [ −0] [.] [00368208 ] [si]\n_,_ (13)\n여기서 _s_ i는 i번째 수 이후의 Stockfish 센티폰 점수입니다.\n\n\n**8.3.1** **추가** **계산** **세부** **사항**\n23\n본 논문에서 제시된 모든 실험에는 `scikit-learn` 의 기본 하이퍼파라미터 설정과 로지스틱 회귀 분석을 사용했습니다. 검증자 점\n수 집합이 주어지면, 본 논문의 모든 실험은 표준 노트북에서 1분 이내에 완료될 수 있습니다.\n\n\n24\n**데이터** **세트** **도메인** **에이전트** **검증기** **에이전트** **설명**\nGSM8k [9] Aviary [37]\n수학적 추론 클로드 하이쿠 3.5 수학 QA를 위한 도구 호출 에\n이전트. 텍스트 기반 검증기\n모델.\n\n\n\nMATH [17]\n수학 추론 클로드 소네트 4 사전 훈련된 PRM\n[63]\n사전 훈련된 검증기 모델을 갖\n춘 다단계 추론 모델.\n\n\n\nHotpotQA [70] QA Claude Haiku 3.5\n새장 일반 QA를 위한 도구 호출 에\n이전트. 텍스트 기반 검증기\n모델.\n\nMedQA [24] QA OctoTools [34] Claude Haiku 3.5\n의료 QA용 도구 호출 에이전\n트. 텍스트 기반 검증기 모델.\n\n\n\nMMLU-Pro [64] QA Claude Sonnet 4\n사전 훈련된 PRM\n[63]\n사전 훈련된 검증기 모델을 갖\n춘 다단계 추론 에이전트.\n\n\n\nStockfish\nLiChess 게임 체스 인간 플레이어 Stockfish centipawn 점수가\n포함된 인간 플레이어의 온라\n인 게임.\n\n\n표 1: 실험에 사용된 데이터 세트, 에이전트 및 검증자 목록.\n\n\n25\n\n\n",
    "eng_pdf": "pdf/eng/할루시네이션 논문 3_E-valuator Reliable Agent Verifiers with Sequential Hypothesis Testing.pdf",
    "kor_pdf": "pdf/kor/할루시네이션 논문 3_E-valuator Reliable Agent Verifiers with Sequential Hypothesis Testing ko.pdf"
  }
];
